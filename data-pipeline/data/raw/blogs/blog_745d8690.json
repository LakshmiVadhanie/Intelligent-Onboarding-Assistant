{
  "content": "Software projects have different evolving needs and requirements. Some have\nsaid that\nsoftware is never finished, merely abandoned\n. Some software\nprojects are small and others are large with complex integrations. Some have\ndependencies on external projects, while others are self-contained.\nRegardless of the size and complexity, the need to validate and ensure\nfunctionality remains paramount.\nCI/CD pipelines can help with the challenge of building and validating software projects consistently, but, much like the software itself, these pipelines can become complex with many dependencies. This is where ideas like\nparent-child pipelines\nand data exchange in CI/CD setups become incredibly important.\nIn this article, we will cover common CI/CD data exchange challenges users may encounter with parent-child pipelines in GitLab â and how to solve them. You'll learn how to turn complex CI/CD processes into more manageable setups.\nUsing parent-child pipelines\nThe pipeline setup in the image below illustrates a scenario where a project could require a large, complex pipeline. The whole project resides in one repository and contains different modules. Each module requires its own set of build and test automation steps.\nOne approach to address the CI/CD configuration in a scenario like this is to break down the larger pipeline into smaller ones (i.e., child pipelines) and keep a common CI/CD process that is shared across all modules in charge of the whole orchestration (i.e., parent pipeline).\nThe parent-child pipeline pattern allows a single pipeline to orchestrate one or many downstream pipelines. Similar to how a single pipeline coordinates the execution of multiple\njobs\n, the parent pipeline coordinates the running of full pipelines with one or more jobs.\nThis pattern has been shown to be helpful in a variety of use cases:\nBreaking down large, complex pipelines into smaller, manageable pieces\nConditionally executing certain pipelines as part of a larger CI/CD process\nExecuting pipelines in parallel\nHelping manage user permissions to access and run certain pipelines\nGitLabâs current CI/CD structure supports this pattern and makes it simple to implement parent-child pipelines. While there are many benefits when using the parent-child pipeline pattern with GitLab, one question we often get is how to share data between the parent and child pipelines. In the next sections, weâll go over how to make use of GitLab variables and artifacts to address this concern.\nSharing variables\nThere are cases where it is necessary to pass the output from a parent pipeline job to a child pipeline. These outputs can be shared as variables,\nartifacts\n, and\ninputs\n.\nConsider a case where we create a custom variable\nvar_1\nduring the runtime of a job:\nstages:\n- build\n- triggers\n# This job only creates a variable\ncreate_var_job:\nstage: build\nscript:\n- var_1=\"Hi, I'm a Parent pipeline variable\"\n- echo \"var_1=$var_1\" >> var.env\nartifacts:\nreports:\ndotenv: var.env\nNotice that the variable is created as part of the script steps in the job (during runtime). In this example, we are using a simple string\n\"Hi, I'm a Parent pipeline variable\"\nto illustrate the main syntax required to later share this variable with a child pipeline. Let's break down the\ncreate_var_job\nand analyze the main steps from this GitLab job\nFirst, we need to save\nvar_1\nas\ndotenv\n:\nscript:\n- var_1=\"Hi, I'm a pipeline variable\"\n- echo \"var_1=$var_1\" >> var.env\nAfter saving\nvar_1\nas\nvar.env\n, the next important step is to make this variable available as an artifact produced by the\ncreate_var_job\n. To do that, we use the following syntax:\nartifacts:\nreports:\ndotenv: var.env\nUp to this point, we have created a variable during runtime and saved it as a\ndotenv\nreport. Now let's add the job that should trigger the child pipeline:\ntelco_service_a:\nstage: triggers\ntrigger:\ninclude: service_a/.gitlab-ci.yml\nrules:\n- changes:\n- service_a/*\nThe goal of\ntelco_service_a\njob is to find the\n.gitlab-ci.yml\nconfiguration of the child pipeline,  which is defined in this case as\nservice_a,\nand trigger its execution. Let's examine this job:\ntelco_service_a:\nstage: triggers\ntrigger:\ninclude: service_a/.gitlab-ci.yml\nWe see it belongs to another\nstage\nof the pipeline named\ntriggers.\nThis job will run only after\ncreate_var_job\nfrom the first stage successfully finishes and where the variable\nvar_1\nwe want to pass is created.\nAfter defining the stage, we use the reserved words\ntrigger\nand\ninclude\nto tell GitLab where to search for the child pipeline configuration, as illustrated in the YAML below:\ntrigger:\ninclude: service_a/.gitlab-ci.yml\nOur child-pipeline YAML configuration is under\nservice_a/.gitlab-ci.yml\nfolder in the GitLab repository, for this example.\nChild pipelines folders with configurations\nTake into consideration that the repository structure depicted above can vary. What matters is properly pointing the\ntriggers: include\nproperties at the location of your child-pipeline configuration in your repository.\nFinally, we use\nrules: changes\nto indicate to GitLab that this child pipeline should be triggered only if there is any change in any file in the\nservice_a/.gitlab-ci.yml\ndirectory, as illustrated in the following code snippet:\nrules:\n- changes:\n- service_a/*\nUsing this rule helps to optimize cost by triggering the child pipeline job only when necessary. This approach is particularly valuable in a monorepo architecture where specific modules contain numerous components, allowing us to avoid running their dedicated pipelines when no changes have been made to their respective codebases.\nConfiguring the parent pipeline\nUp to this point, we have put together our parent pipeline. Here's the full code snippet for this segment:\n# Parent Pipeline Configuration\n# This pipeline creates a custom variable and triggers a child pipeline\nstages:\n- build\n- trigger\ncreate_var_job:\nstage: build\nscript:\n- var_1=\"Hi, I'm a Parent pipeline variable\"\n- echo \"var_1=$var_1\" >> var.env\nartifacts:\nreports:\ndotenv: var.env\ntelco_service_a:\nstage: triggers\ntrigger:\ninclude: service_a/.gitlab-ci.yml\nrules:\n- changes:\n- service_a/*\nWhen GitLab executes the YAML configuration in the GitLab UI, the parent pipeline gets rendered as follows:\nNotice the label \"trigger job,\" which indicates this job will start the execution of another pipeline configuration.\nConfiguring the child pipeline\nMoving forward, let's now focus on the child pipeline configuration, where we expect to inherit and print the value of the\nvar_1\ncreated in the parent pipeline.\nThe pipeline configuration in\nservice_a/.gitlab_ci.yml\nhas the following definition:\nstages:\n- build\nbuild_a:\nstage: build\nscript:\n- echo \"this job inherits the variable from the Parent pipeline:\"\n- echo $var_1\nneeds:\n- project: gitlab-da/use-cases/7-4-parent-child-pipeline\njob: create_var_job\nref: main\nartifacts: true\nLike before, let's break down this pipeline and highlight the main parts to achieve our goal. This pipeline only contains one stage (i.e.,\nbuild)\nand one job (i.e.,\nbuild_a)\n. The script in the job contains two steps:\nbuild_a:\nstage: build\nscript:\n- echo \"this job inherits the variable from the Parent pipeline:\"\n- echo $var_1\nThese two steps print output during the execution. The most interesting one is the second step,\necho $var_1\n, where we expect to print the variable value inherited from the parent pipeline. Remember, this was a simple string with value:\n\"Hi, I'm a Parent pipeline variable.\"\nInheriting variables using needs\nTo set and link this job to inherit variables from the parent pipeline, we use the reserved GitLab CI properties\nneeds\nas depicted in the following snippet:\nneeds:\n- project: gitlab-da/use-cases/7-4-parent-child-pipeline\njob: create_var_job\nref: main\nartifacts: true\nUsing the \"needs\" keyword, we define dependencies that must be completed before running this job. In this case, we pass four different values. Let's walk through each one  of them:\nProject:\nThe complete namespace of the project where the main\ngitlab-ci.yml\ncontaining the parent pipeline YAML is located. Make sure to include the absolute path.\nJob:\nThe specific job name in the parent pipeline from where we want to inherit the variable.\nRef:\nThe name of the branch where the main\ngitlab-ci.yml\ncontaining the parent pipeline YAML is located.\nArtifacts:\nWhere we set a boolean value, indicating that artifacts from the parent pipeline job should be downloaded and made available to this child pipeline job.\nNote:\nThis specific approach using the needs property is only available to GitLab Premium and Ultimate users. We will cover another example for GitLab community users later on.\nPutting it all together\nNow let's assume we make a change to any of the files under\nservice_a\nfolder and commit the changes to the repository. When GitLab detects the change, the rule we set up will trigger the child job pipeline execution. This gets displayed in the GitLab UI as follows:\nClicking on the\ntelco_service_a\nwill take us to the jobs in the child pipeline:\nWe can see the parent-child relationship, and finally, by clicking on the\nbuild_a job\n, we can visually verify the variable inheritance in the job execution log:\nThis output confirms the behavior we expected. The custom runtime variable\nvar_1\ncreated in the parent job is inherited in the child job, unpacked from the\ndotenv\nreport, and its value accessible as can be confirmed in Line 26 above.\nThis use case illustrates how to share custom variables that can contain any value between pipelines. This example is intentionally simple and can be extrapolated to more realistic scenarios. Take, for instance, the following CI/CD configuration, where the custom variable we need to share is the tag of a Docker image:\n# Pipeline\nbuild-prod-image:\ntags: [ saas-linux-large-amd64 ]\nimage: docker:20.10.16\nstage: build\nservices:\n- docker:20.10.16-dind\nscript:\n- docker login -u $CI_REGISTRY_USER -p $CI_REGISTRY_PASSWORD $CI_REGISTRY\n- docker build -t $PRODUCTION_IMAGE .\n- docker push $PRODUCTION_IMAGE\n- echo \"UPSTREAM_CONTAINER_IMAGE=$PRODUCTION_IMAGE\" >> prodimage.env\nartifacts:\nreports:\ndotenv: prodimage.env\nrules:\n- if: '$CI_COMMIT_BRANCH == \"main\"'\nwhen: always\n- when: never\nAnd use the variable with the Docker image tag, in another job that updates a Helm manifest file:\nupdate-helm-values:\nstage: update-manifests\nimage:\nname: alpine:3.16\nentrypoint: [\"\"]\nbefore_script:\n- apk add --no-cache git curl bash yq\n- git remote set-url origin https://${CI_USERNAME}:${GITOPS_USER}@${SERVER_PATH}/${PROJECT_PATH}\n- git config --global user.email \"\n[email protected]\n\"\n- git config --global user.name \"GitLab GitOps\"\n- git pull origin main\nscript:\n- cd src\n- echo $UPSTREAM_CONTAINER_IMAGE\n- yq eval -i \".spec.template.spec.containers[0].image |= \\\"$UPSTREAM_CONTAINER_IMAGE\\\"\" store-deployment.yaml\n- cat store-deployment.yaml\n- git pull origin main\n- git checkout -B main\n- git commit -am '[skip ci] prod image update'\n- git push origin main\nneeds:\n- project: gitlab-da/use-cases/devsecops-platform/simply-find/simply-find-front-end\njob: build-prod-image\nref: main\nartifacts: true\nMastering how to share variables between pipelines while maintaining the relationship between them enables us to create more sophisticated workflow orchestration that can meet our software building needs.\nUsing GitLab Package Registry to share artifacts\nWhile the needs feature mentioned above works great for Premium and Ultimate users, GitLab also has features to help achieve similar results for Community Edition users. One suggested approach is to store artifacts in the\nGitLab Package Registry\n.\nUsing a combination of the variables provided in GitLab CI/CD jobs and the GitLab API, you can upload artifacts to the GitLab Package Registry from a parent pipeline. In the child pipeline, you can then access the uploaded artifact from the package registry using the same variables and API to access the artifact. Letâs take a look at the example pipeline and some supplementary scripts that illustrate this:\ngitlab-ci.yml (parent pipeline)\n# Parent Pipeline Configuration\n# This pipeline creates an artifact, uploads it to Package Registry, and triggers a child pipeline\nstages:\n- create-upload\n- trigger\nvariables:\nPACKAGE_NAME: \"pipeline-artifacts\"\nPACKAGE_VERSION: \"$CI_PIPELINE_ID\"\nARTIFACT_FILE: \"artifact.txt\"\n# Job 1: Create and upload artifact to Package Registry\ncreate-and-upload-artifact:\nstage: create-upload\nimage: alpine:latest\nbefore_script:\n- apk add --no-cache curl bash\nscript:\n- bash scripts/create-artifact.sh\n- bash scripts/upload-to-registry.sh\nrules:\n- if: $CI_PIPELINE_SOURCE == \"push\"\n# Job 2: Trigger child pipeline\ntrigger-child:\nstage: trigger\ntrigger:\ninclude: child-pipeline.yml\nstrategy: depend\nvariables:\nPARENT_PIPELINE_ID: $CI_PIPELINE_ID\nPACKAGE_NAME: $PACKAGE_NAME\nPACKAGE_VERSION: $PACKAGE_VERSION\nARTIFACT_FILE: $ARTIFACT_FILE\nrules:\n- if: $CI_PIPELINE_SOURCE == \"push\"\nchild-pipeline.yml\n# Child Pipeline Configuration\n# This pipeline downloads the artifact from Package Registry and processes it\nstages:\n- download-process\nvariables:\n# These variables are passed from the parent pipeline\nPACKAGE_NAME: \"pipeline-artifacts\"\nPACKAGE_VERSION: \"$PARENT_PIPELINE_ID\"\nARTIFACT_FILE: \"artifact.txt\"\n# Job 1: Download and process artifact from Package Registry\ndownload-and-process-artifact:\nstage: download-process\nimage: alpine:latest\nbefore_script:\n- apk add --no-cache curl bash\nscript:\n- bash scripts/download-from-registry.sh\n- echo \"Processing downloaded artifact...\"\n- cat $ARTIFACT_FILE\n- echo \"Artifact processed successfully!\"\nupload-to-registry.sh\n#!/bin/bash\nset -e\n# Configuration\nPACKAGE_NAME=\"${PACKAGE_NAME:-pipeline-artifacts}\"\nPACKAGE_VERSION=\"${PACKAGE_VERSION:-$CI_PIPELINE_ID}\"\nARTIFACT_FILE=\"${ARTIFACT_FILE:-artifact.txt}\"\n# Validate required variables\nif [ -z \"$CI_PROJECT_ID\" ]; then\necho \"Error: CI_PROJECT_ID is not set\"\nexit 1\nfi\nif [ -z \"$CI_JOB_TOKEN\" ]; then\necho \"Error: CI_JOB_TOKEN is not set\"\nexit 1\nfi\nif [ -z \"$CI_API_V4_URL\" ]; then\necho \"Error: CI_API_V4_URL is not set\"\nexit 1\nfi\nif [ ! -f \"$ARTIFACT_FILE\" ]; then\necho \"Error: Artifact file '$ARTIFACT_FILE' not found\"\nexit 1\nfi\n# Construct the upload URL\nUPLOAD_URL=\"${CI_API_V4_URL}/projects/${CI_PROJECT_ID}/packages/generic/${PACKAGE_NAME}/${PACKAGE_VERSION}/${ARTIFACT_FILE}\"\n# Upload the file using curl\nresponse=$(curl -w \"%{http_code}\" -o /tmp/upload_response.json \\\n--header \"JOB-TOKEN: $CI_JOB_TOKEN\" \\\n--upload-file \"$ARTIFACT_FILE\" \\\n\"$UPLOAD_URL\")\nif [ \"$response\" -eq 201 ]; then\necho \"Upload successful!\"\nelse\necho \"Upload failed with HTTP code: $response\"\nexit 1\nfi\ndownload-from-regsitry.sh\n#!/bin/bash\nset -e\n# Configuration\nPACKAGE_NAME=\"${PACKAGE_NAME:-pipeline-artifacts}\"\nPACKAGE_VERSION=\"${PACKAGE_VERSION:-$PARENT_PIPELINE_ID}\"\nARTIFACT_FILE=\"${ARTIFACT_FILE:-artifact.txt}\"\n# Validate required variables\nif [ -z \"$CI_PROJECT_ID\" ]; then\necho \"Error: CI_PROJECT_ID is not set\"\nexit 1\nfi\nif [ -z \"$CI_JOB_TOKEN\" ]; then\necho \"Error: CI_JOB_TOKEN is not set\"\nexit 1\nfi\nif [ -z \"$CI_API_V4_URL\" ]; then\necho \"Error: CI_API_V4_URL is not set\"\nexit 1\nfi\nif [ -z \"$PACKAGE_VERSION\" ]; then\necho \"Error: PACKAGE_VERSION is not set\"\nexit 1\nfi\n# Construct the download URL\nDOWNLOAD_URL=\"${CI_API_V4_URL}/projects/${CI_PROJECT_ID}/packages/generic/${PACKAGE_NAME}/${PACKAGE_VERSION}/${ARTIFACT_FILE}\"\n# Download the file using curl\nresponse=$(curl -w \"%{http_code}\" -o \"$ARTIFACT_FILE\" \\\n--header \"JOB-TOKEN: $CI_JOB_TOKEN\" \\\n--fail-with-body \\\n\"$DOWNLOAD_URL\")\nif [ \"$response\" -eq 200 ]; then\necho \"Download successful!\"\nelse\necho \"Download failed with HTTP code: $response\"\nexit 1\nfi\nIn this example, the parent pipeline uploads a file to the GitLab Package Registry by calling a script named\nupload-to-registry.sh\n. The script gives the artifact a name and version and constructs the API call to upload the file to the package registry. The parent pipeline is able to authenticate using a\n$CI_JOB_TOKEN\nto push the artifact.txt file to the registry.\nThe child pipeline operates the same as the parent pipeline by using a script to construct the API call to download the artifact.txt file from the package registry. It also is able to authenticate to the registry using the\n$CI_JOB_TOKEN\n.\nSince the GitLab Package Registry is available to all GitLab users, it helps to serve as a central location for storing and versioning artifacts. It is a great option for users working with many kinds of artifacts and needing to version artifacts for workflows even beyond CI/CD.\nUsing inputs to pass variables to a child pipeline\nIf you made it this far in this tutorial, and you have plans to start creating new pipeline configurations, you might want to start by evaluating if your use case can benefit from using\ninputs\nto pass variables to other pipelines.\nUsing inputs is a recommended way to pass variables when you need to define specific values in a CI/CD job and have those values remain fixed during the pipeline run. Inputs might offer certain advantages over the method we implemented before. For example, with inputs, you can include data validation through options (i.e., values must be one of these: [âstaging', âprodâ]), variable descriptions, type checking, and assign default values before the pipeline run.\nConfiguring CI/CD inputs\nConsider the following parent pipeline configuration:\n# .gitlab-ci.yml (main file)\nstages:\n- trigger\ntrigger-staging:\nstage: trigger\ntrigger:\ninclude:\n- local: service_a/.gitlab-ci.yml\ninputs:\nenvironment: staging\nversion: \"1.0.0\"\nLet's zoom in at the main difference between the code snippet above and the previous parent pipeline examples in this tutorial:\ntrigger:\ninclude:\n- local: service_a/.gitlab-ci.yml\ninputs:\nenvironment: staging\nversion: \"1.0.0\"\nThe main difference is using the reserved word \"inputs\". This part of the YAML configuration can be read in natural language as: âtrigger the child pipeline defined in\nservice_a.gitlab-ci.yml\nand make sure to pass âenvironment: stagingâ and âversion:1.0.0â as input variables that the child pipeline will know how to use.\nReading CI/CD inputs in child pipelines\nMoving to the child pipeline, it must contain in its declaration a spec that defines the inputs it can take. For each input, it is possible to add a little description, a set of predefined options the input value can take, and the type of value it will take. This is illustrated as follows:\n# target pipeline or child-pipeline in this case\nspec:\ninputs:\nenvironment:\ndescription: \"Deployment environment\"\noptions: [staging, production]\nversion:\ntype: string\ndescription: \"Application version\"\n---\nstages:\n- deploy\n# Jobs that will use the inputs\ndeploy:\nstage: deploy\nscript:\n-  echo \"Deploying version $[[ inputs.version ]] to $[[ inputs.environment ]]\"\nNotice from the code snippet that after defining the spec, there is a YAML document separator \"---\"  followed by the actual child pipeline definition where we access the variables\n$[[ inputs.version ]]\nand\n$[[ inputs.environment ]]\"\nfrom the defined inputs using input interpolation.\nGet hands-on with parent-child pipelines, artifacts, and more\nWe hope this article has helped with navigating the challenge of sharing variables and artifacts in parent-child pipeline setups.\nTo try these examples for yourself, feel free to view or fork the\nPremium/Ultimate\nand the\nGitLab Package Registry\nexamples of sharing artifacts.\nYou can also sign up for a\n30-day free trial of GitLab Ultimate\nto experience all the features GitLab has to offer. Thanks for reading!",
  "metadata": {
    "title": "Variable and artifact sharing in GitLab parent-child pipelines",
    "url": "https://about.gitlab.com/blog/variable-and-artifact-sharing-in-gitlab-parent-child-pipelines/",
    "published": "2025-10-16T00:00:00.000Z",
    "updated": "2025-10-16T00:00:00.000Z",
    "author": "Daniel Helfand",
    "categories": [],
    "summary": "<p>Software projects have different evolving needs and requirements. Some have\nsaid that <em>software is never finished, merely abandoned</em>. Some software\nprojects are small and others are large with complex integrations. Some have\ndependencies on external projects, while others are self-contained.\nRegardless of the size and complexity, the need to validate and ensure\nfunctionality remains paramount.</p>\n<p>CI/CD pipelines can help with the challenge of building and validating software projec",
    "fetched_at": "2025-10-24T18:27:54.511581",
    "source": "gitlab_blog"
  },
  "processing": {
    "content_length": 19555,
    "word_count": 2850,
    "is_relevant": true
  }
}