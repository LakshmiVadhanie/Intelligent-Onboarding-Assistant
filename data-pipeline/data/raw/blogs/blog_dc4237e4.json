{
  "content": "Creating dbt models manually is a tedious process that can consume hours of a data engineer's time. Especially when no (big) business transformations are made, it is not the most attractive part of an engineer's work with data.\nBut what if you could automate this entire process? In this walkthrough, I'll show you exactly how\nGitLab Duo Agent Platform\ncan generate comprehensive dbt models in just minutes, complete with proper structure, tests, and documentation.\nWhat we're building\nOur marketing team wants to effectively manage and optimize advertising investments. One of the advertising platforms is Reddit, so, therefore, we are extracting data from the Reddit Ads API to our enterprise\nData Platform\nSnowflake. At GitLab, we have three layers of storage:\nraw\nlayer - first landing point for unprocessed data from external sources; not ready for business use\nprep\nlayer - first transformation layer with source models; still not ready for general business use\nprod\nlayer - final transformed data ready for business use and Tableau reporting\nFor this walkthrough, data has already landed in the raw layer by our extraction solution Fivetran, and we'll generate dbt models that handle the data through the\nprep\nlayer to the\nprod\nlayer.\nWithout having to write a single line of dbt code ourselves, by the end of the walkthrough we will have:\nSource models\nin the prep layer\nWorkspace models\nin the prod layer\nComplete dbt configurations\nfor all 13 tables (which includes 112 columns) in the Reddit Ads dataset\nTest queries\nto validate the outcomes\nThe entire process will take less than 10 minutes, compared to the hours it would typically require manually. Here are the steps to follow:\n1. Prepare the data structure\nBefore GitLab Duo can generate our models, it needs to understand the complete table structure. The key is running a query against Snowflake's information schema, because we are currently investigating how to connect GitLab Duo via Model Context Protocol (\nMCP\n) to our Snowflake instance:\nSELECT\ntable_name,\ncolumn_name,\ndata_type,\nis_nullable,\nCASE\nWHEN is_nullable = 'NO' THEN 'PRIMARY_KEY'\nELSE NULL\nEND as key_type\nFROM raw.information_schema.columns\nWHERE table_schema = 'REDDIT_ADS'\nORDER BY table_name, ordinal_position;\nThis query captures:\nAll table and column names\nData types for proper model structure\nNullable constraints\nPrimary key identification (non-nullable columns in this dataset)\nPro tip:\nIn the Reddit Ads dataset, all non-nullable columns serve as primary keys â a pattern. I validated by checking tables like\nad_group\n, which has two non-nullable columns (\naccount_id\nand\nid\n) that are both marked as primary keys. Running this query returned 112 rows of metadata that I exported as a CSV file for model generation. While this manual step works well today, we're investigating a direct GitLab Duo integration with our Data Platform via MCP to automate this process entirely.\n2. Set up GitLab Duo\nThere are two ways to interact with\nGitLab Duo\n:\nWeb UI chat function\nVisual Studio Code plugin\nI chose the VS Code plugin because I can run the dbt models locally to test them.\n3. Enter the 'magic' prompt\nHere's the exact prompt I used to generate all the dbt code:\nCreate dbt models for all the tables in the file structure.csv.\nI want to have the source models created, with a filter that dedupes the data based on the primary key. Create these in a new folder reddit_ads.\nI want to have workspace models created and store these in the workspace_marketing schema.\nTake this MR as example: [I've referenced to previous source implementation]. Here is the same done for Source A, but now it needs to be done for Reddit Ads.\nPlease check the dbt style guide when creating the code: https://handbook.gitlab.com/handbook/enterprise-data/platform/dbt-guide/\nKey elements that made this prompt effective:\nClear specifications\nfor both source and workspace models.\nReference example\nfrom a previous similar merge request.\nStyle guide reference\nto ensure code quality and consistency.\nSpecific schema targeting\nfor proper organization.\n4. GitLab Duo's process\nAfter submitting the prompt, GitLab Duo got to work. The entire generation process took a few minutes, during which GitLab Duo:\nRead and analyzed\nthe CSV input file.\nExamined table structures\nfrom the metadata.\nReferenced our dbt style guide\nfor coding standards.\nTook similar merge request into account\nto properly structure.\nGenerated source models\nfor all 13 tables.\nCreated workspace models\nfor all 13 tables.\nGenerated supporting dbt files\n:\nsources.yml\nconfiguration.\nschema.yml\nfiles with tests and documentation.\nUpdated\ndbt_project.yml\nwith schema references.\nThe results\nThe output was remarkable:\n1 modified file:\ndbt_project.yml (added reddit_ads schema configuration)\n29 new files:\n26 dbt models\n(13 source + 13 workspace)\n3 YAML files\nNearly 900 lines of code\ngenerated automatically\nBuilt-in data tests,\nincluding unique constraints on primary key columns\nGeneric descriptions\nfor all models and columns\nProper deduplication logic\nin source models\nClean, consistent code structure\nfollowing the GitLab dbt style guide\ntransform/snowflake-dbt/\nâââ dbt_project.yml                                                    [MODIFIED]\nâââ models/\nâââ sources/\nâ   âââ reddit_ads/\nâ       âââ reddit_ads_ad_group_source.sql                        [NEW]\nâ       âââ reddit_ads_ad_source.sql                              [NEW]\nâ       âââ reddit_ads_business_account_source.sql                [NEW]\nâ       âââ reddit_ads_campaign_source.sql                        [NEW]\nâ       âââ reddit_ads_custom_audience_history_source.sql         [NEW]\nâ       âââ reddit_ads_geolocation_source.sql                     [NEW]\nâ       âââ reddit_ads_interest_source.sql                        [NEW]\nâ       âââ reddit_ads_targeting_community_source.sql             [NEW]\nâ       âââ reddit_ads_targeting_custom_audience_source.sql       [NEW]\nâ       âââ reddit_ads_targeting_device_source.sql                [NEW]\nâ       âââ reddit_ads_targeting_geolocation_source.sql           [NEW]\nâ       âââ reddit_ads_targeting_interest_source.sql              [NEW]\nâ       âââ reddit_ads_time_zone_source.sql                       [NEW]\nâ       âââ schema.yml                                            [NEW]\nâ       âââ sources.yml                                           [NEW]\nâââ workspaces/\nâââ workspace_marketing/\nâââ reddit_ads/\nâââ schema.yml                                        [NEW]\nâââ wk_reddit_ads_ad.sql                              [NEW]\nâââ wk_reddit_ads_ad_group.sql                        [NEW]\nâââ wk_reddit_ads_business_account.sql                [NEW]\nâââ wk_reddit_ads_campaign.sql                        [NEW]\nâââ wk_reddit_ads_custom_audience_history.sql         [NEW]\nâââ wk_reddit_ads_geolocation.sql                     [NEW]\nâââ wk_reddit_ads_interest.sql                        [NEW]\nâââ wk_reddit_ads_targeting_community.sql             [NEW]\nâââ wk_reddit_ads_targeting_custom_audience.sql       [NEW]\nâââ wk_reddit_ads_targeting_device.sql                [NEW]\nâââ wk_reddit_ads_targeting_geolocation.sql           [NEW]\nâââ wk_reddit_ads_targeting_interest.sql              [NEW]\nâââ wk_reddit_ads_time_zone.sql                       [NEW]\nSample generated code\nHere's an example of the generated code quality. For the\ntime_zone\ntable, GitLab Duo created:\nPrep Layer Source Model\nWITH source AS (\nSELECT *\nFROM {{ source('reddit_ads','time_zone') }}\nQUALIFY ROW_NUMBER() OVER (PARTITION BY id ORDER BY _fivetran_synced DESC) = 1\n),\nrenamed AS (\nSELECT\nid::VARCHAR                               AS time_zone_id,\ncode::VARCHAR                             AS time_zone_code,\ndst_offset::NUMBER                        AS time_zone_dst_offset,\nis_dst_active::BOOLEAN                    AS is_time_zone_dst_active,\nname::VARCHAR                             AS time_zone_name,\noffset::NUMBER                            AS time_zone_offset,\n_fivetran_synced::TIMESTAMP               AS fivetran_synced_at\nFROM source\n)\nSELECT * FROM renamed\nSchema.yml\nmodels:\n- name: reddit_ads_time_zone_source\ndescription: Time zone data from Reddit Ads system\ncolumns:\n- name: time_zone_id\ndescription: Unique identifier for time zone records\ndata_tests:\n- unique\n- not_null\n- name: time_zone_code\ndescription: Code for the time zone\n- name: time_zone_dst_offset\ndescription: Daylight saving time offset for the time zone\n- name: is_time_zone_dst_active\ndescription: Flag indicating if daylight saving time is active\n- name: time_zone_name\ndescription: Name of the time zone\n- name: time_zone_offset\ndescription: Offset for the time zone\n- name: fivetran_synced_at\ndescription: Timestamp when the record was last synced by Fivetran\nSource.yml\nsources:\n- name: reddit_ads\ndatabase: RAW\nschema: reddit_ads\nloaded_at_field: _fivetran_synced\nloader: fivetran\ndescription: Reddit Ads data\nquoting:\ndatabase: true\nschema: false\nidentifier: false\ntables:\n- name: time_zone\nWorkspace Model\nWITH source AS (\nSELECT *\nFROM {{ ref('reddit_ads_time_zone_source') }}\n)\nSELECT * FROM source\n5. Quality validation\nNow that the code looks good, I pushed it to the MR and executed\nCI test pipeline\nto test the code and validate the outcome. I asked GitLab Duo to create a validation query:\nCreate a test query to test the row counts between the raw layer and the workspace layer. Keep in mind that we do deduplication, so we can compare both using distinct on the primary keys.\nThe AI generated a comprehensive validation query that:\nCompared row counts between raw and workspace layers.\nAccounted for deduplication logic.\nTested all 13 tables.\nCalculated data retention percentages.\nGenerated SQL Test query\n-- Reddit Ads Row Count Validation Test\n-- Compares distinct counts between RAW layer and WORKSPACE_MARKETING layer\n-- Accounts for deduplication logic in source models\nWITH raw_counts AS (\n-- Single primary key tables\nSELECT 'ad' AS table_name, COUNT(DISTINCT id) AS raw_count\nFROM RAW.REDDIT_ADS.AD\nUNION ALL\nSELECT 'business_account' AS table_name, COUNT(DISTINCT id) AS raw_count\nFROM RAW.REDDIT_ADS.BUSINESS_ACCOUNT\nUNION ALL\nSELECT 'campaign' AS table_name, COUNT(DISTINCT id) AS raw_count\nFROM RAW.REDDIT_ADS.CAMPAIGN\nUNION ALL\nSELECT 'custom_audience_history' AS table_name, COUNT(DISTINCT id) AS raw_count\nFROM RAW.REDDIT_ADS.CUSTOM_AUDIENCE_HISTORY\nUNION ALL\nSELECT 'geolocation' AS table_name, COUNT(DISTINCT id) AS raw_count\nFROM RAW.REDDIT_ADS.GEOLOCATION\nUNION ALL\nSELECT 'interest' AS table_name, COUNT(DISTINCT id) AS raw_count\nFROM RAW.REDDIT_ADS.INTEREST\nUNION ALL\nSELECT 'time_zone' AS table_name, COUNT(DISTINCT id) AS raw_count\nFROM RAW.REDDIT_ADS.TIME_ZONE\n-- Composite primary key tables\nUNION ALL\nSELECT 'ad_group' AS table_name, COUNT(DISTINCT CONCAT(account_id, '|', id)) AS raw_count\nFROM RAW.REDDIT_ADS.AD_GROUP\nUNION ALL\nSELECT 'targeting_community' AS table_name, COUNT(DISTINCT CONCAT(ad_group_id, '|', community_id)) AS raw_count\nFROM RAW.REDDIT_ADS.TARGETING_COMMUNITY\nUNION ALL\nSELECT 'targeting_custom_audience' AS table_name, COUNT(DISTINCT CONCAT(ad_group_id, '|', custom_audience_id)) AS raw_count\nFROM RAW.REDDIT_ADS.TARGETING_CUSTOM_AUDIENCE\nUNION ALL\nSELECT 'targeting_device' AS table_name, COUNT(DISTINCT _fivetran_id) AS raw_count\nFROM RAW.REDDIT_ADS.TARGETING_DEVICE\nUNION ALL\nSELECT 'targeting_geolocation' AS table_name, COUNT(DISTINCT CONCAT(ad_group_id, '|', geolocation_id)) AS raw_count\nFROM RAW.REDDIT_ADS.TARGETING_GEOLOCATION\nUNION ALL\nSELECT 'targeting_interest' AS table_name, COUNT(DISTINCT CONCAT(ad_group_id, '|', interest_id)) AS raw_count\nFROM RAW.REDDIT_ADS.TARGETING_INTEREST\n),\nworkspace_counts AS (\n-- Workspace layer counts using primary keys from schema.yml\nSELECT 'ad' AS table_name, COUNT(DISTINCT ad_id) AS workspace_count\nFROM REDDIT_DBT_MODEL_GENERATION_PROD.WORKSPACE_MARKETING.WK_REDDIT_ADS_AD\nUNION ALL\nSELECT 'business_account' AS table_name, COUNT(DISTINCT business_account_id) AS workspace_count\nFROM REDDIT_DBT_MODEL_GENERATION_PROD.WORKSPACE_MARKETING.WK_REDDIT_ADS_BUSINESS_ACCOUNT\nUNION ALL\nSELECT 'campaign' AS table_name, COUNT(DISTINCT campaign_id) AS workspace_count\nFROM REDDIT_DBT_MODEL_GENERATION_PROD.WORKSPACE_MARKETING.WK_REDDIT_ADS_CAMPAIGN\nUNION ALL\nSELECT 'custom_audience_history' AS table_name, COUNT(DISTINCT custom_audience_id) AS workspace_count\nFROM REDDIT_DBT_MODEL_GENERATION_PROD.WORKSPACE_MARKETING.WK_REDDIT_ADS_CUSTOM_AUDIENCE_HISTORY\nUNION ALL\nSELECT 'geolocation' AS table_name, COUNT(DISTINCT geolocation_id) AS workspace_count\nFROM REDDIT_DBT_MODEL_GENERATION_PROD.WORKSPACE_MARKETING.WK_REDDIT_ADS_GEOLOCATION\nUNION ALL\nSELECT 'interest' AS table_name, COUNT(DISTINCT interest_id) AS workspace_count\nFROM REDDIT_DBT_MODEL_GENERATION_PROD.WORKSPACE_MARKETING.WK_REDDIT_ADS_INTEREST\nUNION ALL\nSELECT 'time_zone' AS table_name, COUNT(DISTINCT time_zone_id) AS workspace_count\nFROM REDDIT_DBT_MODEL_GENERATION_PROD.WORKSPACE_MARKETING.WK_REDDIT_ADS_TIME_ZONE\n-- Composite primary key tables\nUNION ALL\nSELECT 'ad_group' AS table_name, COUNT(DISTINCT CONCAT(ad_group_account_id, '|', ad_group_id)) AS workspace_count\nFROM REDDIT_DBT_MODEL_GENERATION_PROD.WORKSPACE_MARKETING.WK_REDDIT_ADS_AD_GROUP\nUNION ALL\nSELECT 'targeting_community' AS table_name, COUNT(DISTINCT CONCAT(targeting_community_ad_group_id, '|', targeting_community_id)) AS workspace_count\nFROM REDDIT_DBT_MODEL_GENERATION_PROD.WORKSPACE_MARKETING.WK_REDDIT_ADS_TARGETING_COMMUNITY\nUNION ALL\nSELECT 'targeting_custom_audience' AS table_name, COUNT(DISTINCT CONCAT(targeting_custom_audience_ad_group_id, '|', targeting_custom_audience_id)) AS workspace_count\nFROM REDDIT_DBT_MODEL_GENERATION_PROD.WORKSPACE_MARKETING.WK_REDDIT_ADS_TARGETING_CUSTOM_AUDIENCE\nUNION ALL\nSELECT 'targeting_device' AS table_name, COUNT(DISTINCT targeting_device_fivetran_id) AS workspace_count\nFROM REDDIT_DBT_MODEL_GENERATION_PROD.WORKSPACE_MARKETING.WK_REDDIT_ADS_TARGETING_DEVICE\nUNION ALL\nSELECT 'targeting_geolocation' AS table_name, COUNT(DISTINCT CONCAT(targeting_geolocation_ad_group_id, '|', targeting_geolocation_id)) AS workspace_count\nFROM REDDIT_DBT_MODEL_GENERATION_PROD.WORKSPACE_MARKETING.WK_REDDIT_ADS_TARGETING_GEOLOCATION\nUNION ALL\nSELECT 'targeting_interest' AS table_name, COUNT(DISTINCT CONCAT(targeting_interest_ad_group_id, '|', targeting_interest_id)) AS workspace_count\nFROM REDDIT_DBT_MODEL_GENERATION_PROD.WORKSPACE_MARKETING.WK_REDDIT_ADS_TARGETING_INTEREST\n)\n-- Final comparison with validation results\nSELECT\nr.table_name,\nr.raw_count,\nw.workspace_count,\nr.raw_count - w.workspace_count AS count_difference,\nCASE\nWHEN r.raw_count = w.workspace_count THEN 'â PASS'\nWHEN r.raw_count > w.workspace_count THEN 'â ï¸ RAW > WORKSPACE (Expected due to deduplication)'\nELSE 'â FAIL - WORKSPACE > RAW (Unexpected)'\nEND AS validation_status,\nROUND((w.workspace_count::FLOAT / r.raw_count::FLOAT) * 100, 2) AS data_retention_percentage\nFROM raw_counts r\nJOIN workspace_counts w ON r.table_name = w.table_name\nORDER BY r.table_name;\nRunning this query showed:\nZero differences\nin row counts after deduplication\n100% data retention\nacross all tables\nAll tests passed\nsuccessfully\nThe bottom line: Massive time savings\nTraditional approach:\n6-8 hours of manual coding, testing, and debugging\nGitLab Duo approach:\n6-8 minutes of generation + review time\nThis represents a 60x improvement in developer efficiency (from 6-8 hours to 6-8 minutes), while maintaining high code quality.\nBest practices for success\nBased on this experience, here are key recommendations:\nPrepare your metadata\nExtract complete table structures including data types and constraints.\nIdentify primary keys and relationships upfront.\nExport clean, well-formatted CSV input files.\nNote:\nBy connecting GitLab Duo via MCP to your (meta)data, you could exclude this manual step.\nProvide clear context\nReference existing example MRs when possible.\nSpecify your coding standards and style guides.\nBe explicit about folder structure and naming conventions.\nValidate thoroughly\nAlways create validation queries for data integrity.\nTest locally before merging.\nRun your CI/CD pipeline to catch any issues.\nLeverage AI for follow-up tasks\nGenerate test queries automatically.\nCreate documentation templates.\nBuild validation scripts.\nWhat's next\nThis demonstration shows how AI-powered development tools like GitLab Duo are also transforming data engineering workflows. The ability to generate hundreds of lines of production-ready code in minutes â  complete with tests, documentation, and proper structure â represents a fundamental shift in how we approach repetitive development tasks.\nBy leveraging AI to handle the repetitive aspects of dbt model creation, data engineers can focus on higher-value activities like data modeling strategy, performance optimization, and business logic implementation.\nReady to try this yourself?\nStart with a small dataset, prepare your metadata carefully, and watch as GitLab Duo transforms hours of work into minutes of automated generation.\nTrial GitLab Duo Agent Platform today.\nRead more\nGitLab 18.3: Expanding AI orchestration in software engineering\nGitLab Duo Agent Platform Public Beta: Next-gen AI orchestration and more",
  "metadata": {
    "title": "How GitLab Duo Agent Platform transforms DataOps",
    "url": "https://about.gitlab.com/blog/how-gitlab-duo-agent-platform-transforms-dataops/",
    "published": "2025-09-16T00:00:00.000Z",
    "updated": "2025-09-16T00:00:00.000Z",
    "author": "Dennis van Rooijen",
    "categories": [],
    "summary": "<p>Creating dbt models manually is a tedious process that can consume hours of a data engineer's time. Especially when no (big) business transformations are made, it is not the most attractive part of an engineer's work with data.</p>\n<p>But what if you could automate this entire process? In this walkthrough, I'll show you exactly how <a href=\"https://about.gitlab.com/gitlab-duo/agent-platform/\">GitLab Duo Agent Platform</a> can generate comprehensive dbt models in just minutes, complete with pr",
    "fetched_at": "2025-10-24T18:28:06.100771",
    "source": "gitlab_blog"
  },
  "processing": {
    "content_length": 17447,
    "word_count": 2011,
    "is_relevant": true
  }
}