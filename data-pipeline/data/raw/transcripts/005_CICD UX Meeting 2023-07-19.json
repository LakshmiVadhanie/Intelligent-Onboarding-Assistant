{
  "title": "CI/CD UX Meeting 2023-07-19",
  "video_id": "WYYRkq0J93A",
  "url": "https://www.youtube.com/watch?v=WYYRkq0J93A",
  "transcript": " All right, today is the 19th July and we are meeting for CI, CD, UX discussions. There are some standing topics. There's just one impact, which is by me. So in bi-planes security, we are kind of experiencing something like we are getting community contributions, which are not directly related to any issues. And those contributions, they are not very small in nature. So what ends up happening is it kind of like ends up being in conflict with something that we already have plans for. And we try to make changes so that we can accommodate it like we can take some parts from the contribution and get it implemented. But the discussion has been like going really long on those. And so Jocelyn and I, we thought like maybe we should bring it up with the contributors suggesting. And that's the discussion that's going on there. So probably they might bring in a couple more automations to the process or some additional label that would require contributors to ensure that they at least look up for issues which are relating to the change that they are proposing. And that would also help us make sure that we are not discouraging anyone from making contributions. Because of course, if we just continue the discussions for a month or more than that and just keep on changing, making changes to the decisions, it's not, I would say for someone who's taking out time from their personal time to do this work, it's not a great experience for them. So yeah, I'll keep everyone posted on what we decide or what changes the contributors access team decides on rather. But this is the slack thread where it's all happening. So if you want to just follow up. Yeah, Gina, to you. Yeah, because this happened to me as well. And it gets a little frustrating especially for designers because we tend to have to go back and like retroactively design based on the contribution. And for runner, we saw it was the same contributor every time who was doing it. So we ended up talking with the contributors success team and then met with them plus the contributor and kind of like talked about our roadmap so that we talked about our roadmap and also what they were thinking on their roadmap so we can come together on those things. And what I suggested, I mean, obviously we have to like be open to them contributing. So I suggested that if they have like a new feature that they want to create, they create an issue first and tag me and that puts a lot of responsibility on us. But I also think it gets us involved earlier. So that's what they've been doing lately and it actually has been helping a lot in regards to like making sure it fits in with features that already exist or that we even like see coming up in the roadmap. Yeah, that's a very relevant example. So like this is case by case like for your team writing, but to make sure that you come up with something that's generally applicable to everyone, it's I think good for the contributors that are just suggesting to come up with some new rules probably. Yeah. Okay, if there are no more comments, let's move to product design. So for the first section which is which board, Helena has some updates that she's working on the backfill requirement for Alice position. And if there's anything that any of the team members have been working on with Ali, they can just let her know so she can work on finding alternatives. Any others, I think NFI for Emily that the pairs would be rooted by the end of July. And for the rest of us as well of course. Yeah, next is Gina. Yeah, I'm going to drop a little bit early from this meeting. But also the things that I've been working on are mental model research for runners, which I completed and thank you to Eric is not on the call, but if she washes the recording, she helps me a time with this project. So yeah, thank you Eric. And she gave me a skeleton of a report to be able to fill out with all the analysis. It's still working progress, but you're welcome to check it out. I also added a summary to the issue though. So what came out of it was. Defined groups of the runner concepts that users think about when they're using runners. And then we also had them like come up with their own terms to describe certain definitions, which almost all were not aligned with what we use at GitLab. But conceptually, you saw some connections there. They just weren't the exact same terms. And then the final exercise was to have them match the terms to definitions, like the terms that we use at GitLab to the definitions. And most of those they did, they were not able to match accurately as we defined them at GitLab. So there was a lot of learnings. But what I'm seeing now is the next thing I want to do with that air can suggested is work with the team to come up with names for each group of concepts. And then we're going to send out another survey to like 100 participants if we can get that many and have them select which name like they feel best describes the concepts in that group and then ask them where they're using that and their workflow and what tasks they're using it for. So then we can figure out how we want to make changes from there. Like if I wanted to make the design changes for that configuration example that I shared last week, that would be like a way to help us validate that that is needed. But what I'm seeing now is that the team is very, it's difficult for the team to like understand that these concepts were grouped together because it doesn't align with how runner actually works. So yeah, it's, it's, people are having a hard time looking past like the technical way that it works and then how people are grouping them. And I was wondering if anyone has run into that problem and kind of how you're approaching it with your team. Will they want to ask your questions? Well, I did want to give you space to see if anybody has like thoughts on the question that you just raised because my question's a little bit unrelated. Okay. I think what Emily did with environments was something similar because they challenged the existing ideas of concepts that were like that were there since forever in GitLab. So I don't know, maybe Emily, any insights like how did the team take take it when they learnt. But this was rather something that the team was involved in. So probably that's how it was different because it was not coming from outside but from discussions within the team. The buyout would have been on the roadside floor. We also had a lot of research to back that our current feature wasn't working. So I think that made it easy to convince the team because through like the group level environments research, additional research before we were aware like this feature wasn't really hitting the mark. So collectively, I think it was much easier to move forward versus yeah, it coming from someone else. So yeah, which is totally different in our use case because people are very happy with runners. It almost feels like we internally because we're touching it all the time. We have a much deeper understanding of it than our users, the majority of our users I would say, but also the admins or platform engineers who are managing their own runners have a very similar understanding as we do. So it's hard to get it's hard for me to be like, they're not understanding it when we know that they are because people love the feature. The one thing that helped us like I know we are at a very, I would say, nascent stage in the entire native secrets manager concept, but one thing that helped us was the competitor evaluation because like the entire team got to like take a peek into how the rest of the industry is referring to different concepts. And that also helped them relate with the research data that Erica had like created reports on. So everything together, they were able to relate one thing to another and that helped in like getting that conviction. I could definitely start with that. I have like a Google Doc with competitor terminology in it, but I didn't really share that widely with our group so that can be a good place to start also. Thank you. Thank you. Yeah, just just certainly back to my question. How did you decide to get survey data or you know target 100 people? Yeah, so Erica suggested that we run the survey for I think a month and a half I think it was or we or when we get to 100 people like whichever one comes first and I she was saying 100 because like that's the average number that they've been seeing trends lately from surveys. But if you have other suggestions, I am very open to it. Yeah, I've used like survey sample size calculators, which I can certainly link to here. So if you know like how many people are using a particular feature because I know that we have like a lot of internal metrics on like who's a user of this stage group or that stage group. You could use that as a little bit of a guide until like okay if our population is X, then we should survey this many people to get somewhat of a representative sample size. But yeah, I'll include one below. Okay. I think another thing was that I was kind of saying that we need to move fast with this. So she probably changed her answer based on that. Okay. Yeah, I mean that that certainly was a factor too. Yeah, yeah. But I still would like to look at that survey sample size and see what we do. Yeah. So I added a link there. If you click into it, you'll notice that it has like three different sections. You don't really have to mess with confidence interval or margin of error. You just have to enter the population size. So that's the number that you would have to either guesstimate or you know get some analytics on to populate that field. Okay, thank you. This is nice. Great. I think the sharing will. Okay, right. Yeah. So is everyone that I noticed they still typing going on. Good. So just a small update with the outcome of the design sprint. We were trying to figure out the best way to go forward with the research here and we've started with a just internal customer interviews with the SRA and delivery team around this service concept getting a better idea of what they consider services. How we would populate that list and after we're done those internal interviews, which I think we have like six. Happening this week. Big thank you to Victor for scheduling them. We'll be able to move forward with like external user testing. Just an update on that front. The other one is I'll be working on these design issue around designing for external jobs. And I believe this touches kind of like the job details page, the job list page and the pipeline details page. I'm not sure if any of those fell into the verified teams that have designers, but if they do just let me know so I can involve you and kind of feedback for these pages. I can help because I like I took part in a lot of free searches for those pages so I can help with any feedback, but otherwise there's no designer for a pipeline execution. I can also help because runner has that very small portion in this, but there is at least a link to the runner that runs the job and then there's a lot of output from runner in the job log that might be helpful. Sounds good. Yeah, this is mostly just designing it around this new like external job thing and what visuals will show, but I think it'll touch like all of these pages so. Figuring that out as I go forward. And also please leave me in because pipeline authoring it's not just owning, but we are also touching some pipeline details page and then job log as well, so I'm happy to help. Thanks. I'm like, can I ask a question just on like what external job means. Yeah, so this is one that I'm just starting into, so I have to kind of look for it, but this is as far as I know a say I job that's being executed outside of Git lab. Like on actions, so for example on actions. Okay. Like it's. No worries. It's so what this is is I'm reading this that is like it's adding a new job status, which will represent CI jobs that are currently being executed by a service external to Git lab. So adding in like appending kind of state as that is being run. And then yeah, so I'll actually link the engineering issue here that explains it a bit more than the design issue. That is helpful. I'm also reading the epic and it says external jobs are not picked up by a runner, so it might not even. And actually with me and if so, you don't have to work me in. I heard about this use case and some research is an else in the conference lately, but I don't know I still cannot drop animals if I need to read through this. Yeah, this is new for 16 three I just started looking into it. So as I learn more. I'll reach out, but it's a fairly new thing we're looking into. So I don't have all the details right now, but I'm sure over the next week I'll have more. So I'm mostly focusing on just one thing besides a couple of community contributions this week, and that's the like finishing the PC prototype. So I created the user map and flow on like jam, it looks so good. And based on this particular story mapping, I would be like designing the different pages or the different actions for this flow and that's for this. Another thing that I wanted to share was I'm starting to add like I've added a slot to my calendar from starting next week for every Thursday morning. And for time that suits all of us, like all the members in the CICD team at least. And this is for like to have a sync time with me to discuss just anything that relates to public presence. We can just like use this sync time to discuss a blog proposal to edit a blog that's already there to like write abstracts, select conferences, designing content representations. And just anything that comes to your mind if we can use this time for that and to be like how you can add yourself, you can just like go to the link, add it to your calendar. And for a virtual week, you've booked it others would not book it for that particular week. Yeah, and that is it. And just do I like, I noticed it goes to a calendar block. Do you get notified or do we have to like book it with you to let you know, um, let's try. I don't know if that happens. Because when I clicked on it, it just goes to a calendar event, but no one's invited to it. So. Yeah, okay, I'm also not a part of it. That is amazing. Okay, now I am. I'll work further on this. Doesn't look very foolproof. Thanks for putting it together. We can just like sync. Like catch up over Slack and just books lots manually. This doesn't work. I'm not so good with automating things. And that's it. Yeah, so you can take it out. I just wanted to say plus one, but yeah, it's my turn. Okay, so I'll type it later. So thanks, Biti. I have two items that I like to share that. Dove and Mark and myself, we're planning the Q3 that what could be the CICD catalog beta and then what could be included technically and also from the user standpoint. It's almost like defined and then we also announce it to the team and then now team is on board. So that's exciting. That means. So just to tell you a little bit of more details, we'll gather more and made a data from now on so that we just just don't need to hold like this depend on the repeat.mv that user right. This is your component a and b and see it like for now, like we have to just let them write everything and then just all the depend on this instruction, but we like to kind of provide a better assistance and suggestion system. So this is exciting for me because it'll open the doors for the further UX opportunities. I like to share this. And the other point is probably you might see this in the MR pipeline tap. There is a now failed job number is surfacing in those tap. So that was the idea for this packathon. And then luckily we get a lot of positive feedback at the same time. So we started to get a lot of bug reports, for example, there's four failed job, but the width is as is zero failed job and with even with the total emoji so like, oh, okay, we should fix it now. We're also working on this. It just popped up, but it's exciting that people kind of coming in and as they and then if you also have any other feedback, please, please let me know I'll also link the app for issues so that you could leave some feedback around UX and also the UI tax as well. This was a really good improvement like a tiny one, but so impactful. Yeah, I was excited and then yeah, we will have more issues. Thanks Eric is on leave, so move it over to bill. Thanks, engine. So I just wanted to quickly touch on a judicial police about the survey calculator that I spoke about a couple of minutes ago added some information earlier on, but I would just recommend, you know, working with your researcher, if you decide to use one of those types of tools to figure out. What's the appropriate like margin of error and confidence interval because if you leave it as is that's those things are more like academic standard. So it'll tell you that you need probably more than you really do. So if you have like tight deadlines or, you know, whatever it might be, you might have to adjust that a little bit to get a better sense of what's a more reasonable number of people to survey. So that's I'm going to be on leave starting next Monday. I'll be out for four weeks. So I'll get back. Second half of August. I've created my Q3 prioritization issue and link to that here. So I'm going to be out of the stuff that's represented there's just carrying over from this quarter that's still active. And then I'm going to be picking up some projects that are paused. And then since Ali has departed, get lab I'm coordinating with the PM Lauren and then hi, to determine next steps for the solution validation project that he was leading. So we're meeting on Thursday to discuss this more sink call, but we're also having a sink discussions about it. Like kind of if high on is going to be able to drive it while I'm out if we're going to just kind of keep it pause for the next couple of weeks until I'm back those sort of things. So that's what's going on with me right now. Enjoy your family time. Yeah. Yeah, I need it. Yeah, my wife goes back to work next week for the first time in like 12 weeks. So I'm going to be on full dead duty 24, 7. So give it her a little bit of a break. Okay, so with that we have at the end of the agenda anything that anyone wants to bring up. If not, then let's play more time back. Yeah, let's meet later. Fine.",
  "segments": [
    {
      "id": 0,
      "seek": 0,
      "start": 0.0,
      "end": 10.64,
      "text": " All right, today is the 19th July and we are meeting for CI, CD, UX discussions.",
      "tokens": [
        50364,
        1057,
        558,
        11,
        965,
        307,
        264,
        1294,
        392,
        7370,
        293,
        321,
        366,
        3440,
        337,
        37777,
        11,
        6743,
        11,
        40176,
        11088,
        13,
        50896
      ],
      "temperature": 0.0,
      "avg_logprob": -0.391005444882521,
      "compression_ratio": 1.3798882681564246,
      "no_speech_prob": 0.07211033254861832
    },
    {
      "id": 1,
      "seek": 0,
      "start": 10.64,
      "end": 14.4,
      "text": " There are some standing topics.",
      "tokens": [
        50896,
        821,
        366,
        512,
        4877,
        8378,
        13,
        51084
      ],
      "temperature": 0.0,
      "avg_logprob": -0.391005444882521,
      "compression_ratio": 1.3798882681564246,
      "no_speech_prob": 0.07211033254861832
    },
    {
      "id": 2,
      "seek": 0,
      "start": 14.4,
      "end": 18.32,
      "text": " There's just one impact, which is by me.",
      "tokens": [
        51084,
        821,
        311,
        445,
        472,
        2712,
        11,
        597,
        307,
        538,
        385,
        13,
        51280
      ],
      "temperature": 0.0,
      "avg_logprob": -0.391005444882521,
      "compression_ratio": 1.3798882681564246,
      "no_speech_prob": 0.07211033254861832
    },
    {
      "id": 3,
      "seek": 0,
      "start": 18.32,
      "end": 25.28,
      "text": " So in bi-planes security, we are kind of experiencing something like we are getting community",
      "tokens": [
        51280,
        407,
        294,
        3228,
        12,
        564,
        12779,
        3825,
        11,
        321,
        366,
        733,
        295,
        11139,
        746,
        411,
        321,
        366,
        1242,
        1768,
        51628
      ],
      "temperature": 0.0,
      "avg_logprob": -0.391005444882521,
      "compression_ratio": 1.3798882681564246,
      "no_speech_prob": 0.07211033254861832
    },
    {
      "id": 4,
      "seek": 2528,
      "start": 25.28,
      "end": 30.240000000000002,
      "text": " contributions, which are not directly related to any issues.",
      "tokens": [
        50364,
        15725,
        11,
        597,
        366,
        406,
        3838,
        4077,
        281,
        604,
        2663,
        13,
        50612
      ],
      "temperature": 0.0,
      "avg_logprob": -0.20914801065023844,
      "compression_ratio": 1.6313131313131313,
      "no_speech_prob": 0.055190589278936386
    },
    {
      "id": 5,
      "seek": 2528,
      "start": 30.240000000000002,
      "end": 36.24,
      "text": " And those contributions, they are not very small in nature.",
      "tokens": [
        50612,
        400,
        729,
        15725,
        11,
        436,
        366,
        406,
        588,
        1359,
        294,
        3687,
        13,
        50912
      ],
      "temperature": 0.0,
      "avg_logprob": -0.20914801065023844,
      "compression_ratio": 1.6313131313131313,
      "no_speech_prob": 0.055190589278936386
    },
    {
      "id": 6,
      "seek": 2528,
      "start": 36.24,
      "end": 42.8,
      "text": " So what ends up happening is it kind of like ends up being in conflict with something",
      "tokens": [
        50912,
        407,
        437,
        5314,
        493,
        2737,
        307,
        309,
        733,
        295,
        411,
        5314,
        493,
        885,
        294,
        6596,
        365,
        746,
        51240
      ],
      "temperature": 0.0,
      "avg_logprob": -0.20914801065023844,
      "compression_ratio": 1.6313131313131313,
      "no_speech_prob": 0.055190589278936386
    },
    {
      "id": 7,
      "seek": 2528,
      "start": 42.8,
      "end": 44.92,
      "text": " that we already have plans for.",
      "tokens": [
        51240,
        300,
        321,
        1217,
        362,
        5482,
        337,
        13,
        51346
      ],
      "temperature": 0.0,
      "avg_logprob": -0.20914801065023844,
      "compression_ratio": 1.6313131313131313,
      "no_speech_prob": 0.055190589278936386
    },
    {
      "id": 8,
      "seek": 2528,
      "start": 44.92,
      "end": 51.92,
      "text": " And we try to make changes so that we can accommodate it like we can take some parts",
      "tokens": [
        51346,
        400,
        321,
        853,
        281,
        652,
        2962,
        370,
        300,
        321,
        393,
        21410,
        309,
        411,
        321,
        393,
        747,
        512,
        3166,
        51696
      ],
      "temperature": 0.0,
      "avg_logprob": -0.20914801065023844,
      "compression_ratio": 1.6313131313131313,
      "no_speech_prob": 0.055190589278936386
    },
    {
      "id": 9,
      "seek": 5192,
      "start": 51.92,
      "end": 54.72,
      "text": " from the contribution and get it implemented.",
      "tokens": [
        50364,
        490,
        264,
        13150,
        293,
        483,
        309,
        12270,
        13,
        50504
      ],
      "temperature": 0.0,
      "avg_logprob": -0.20667555780694036,
      "compression_ratio": 1.7459677419354838,
      "no_speech_prob": 0.021485252305865288
    },
    {
      "id": 10,
      "seek": 5192,
      "start": 54.72,
      "end": 57.92,
      "text": " But the discussion has been like going really long on those.",
      "tokens": [
        50504,
        583,
        264,
        5017,
        575,
        668,
        411,
        516,
        534,
        938,
        322,
        729,
        13,
        50664
      ],
      "temperature": 0.0,
      "avg_logprob": -0.20667555780694036,
      "compression_ratio": 1.7459677419354838,
      "no_speech_prob": 0.021485252305865288
    },
    {
      "id": 11,
      "seek": 5192,
      "start": 57.92,
      "end": 62.08,
      "text": " And so Jocelyn and I, we thought like maybe we should bring it up with the contributors",
      "tokens": [
        50664,
        400,
        370,
        508,
        905,
        49449,
        293,
        286,
        11,
        321,
        1194,
        411,
        1310,
        321,
        820,
        1565,
        309,
        493,
        365,
        264,
        45627,
        50872
      ],
      "temperature": 0.0,
      "avg_logprob": -0.20667555780694036,
      "compression_ratio": 1.7459677419354838,
      "no_speech_prob": 0.021485252305865288
    },
    {
      "id": 12,
      "seek": 5192,
      "start": 62.08,
      "end": 63.08,
      "text": " suggesting.",
      "tokens": [
        50872,
        18094,
        13,
        50922
      ],
      "temperature": 0.0,
      "avg_logprob": -0.20667555780694036,
      "compression_ratio": 1.7459677419354838,
      "no_speech_prob": 0.021485252305865288
    },
    {
      "id": 13,
      "seek": 5192,
      "start": 63.08,
      "end": 65.36,
      "text": " And that's the discussion that's going on there.",
      "tokens": [
        50922,
        400,
        300,
        311,
        264,
        5017,
        300,
        311,
        516,
        322,
        456,
        13,
        51036
      ],
      "temperature": 0.0,
      "avg_logprob": -0.20667555780694036,
      "compression_ratio": 1.7459677419354838,
      "no_speech_prob": 0.021485252305865288
    },
    {
      "id": 14,
      "seek": 5192,
      "start": 65.36,
      "end": 70.64,
      "text": " So probably they might bring in a couple more automations to the process or some additional",
      "tokens": [
        51036,
        407,
        1391,
        436,
        1062,
        1565,
        294,
        257,
        1916,
        544,
        3553,
        763,
        281,
        264,
        1399,
        420,
        512,
        4497,
        51300
      ],
      "temperature": 0.0,
      "avg_logprob": -0.20667555780694036,
      "compression_ratio": 1.7459677419354838,
      "no_speech_prob": 0.021485252305865288
    },
    {
      "id": 15,
      "seek": 5192,
      "start": 70.64,
      "end": 77.8,
      "text": " label that would require contributors to ensure that they at least look up for issues",
      "tokens": [
        51300,
        7645,
        300,
        576,
        3651,
        45627,
        281,
        5586,
        300,
        436,
        412,
        1935,
        574,
        493,
        337,
        2663,
        51658
      ],
      "temperature": 0.0,
      "avg_logprob": -0.20667555780694036,
      "compression_ratio": 1.7459677419354838,
      "no_speech_prob": 0.021485252305865288
    },
    {
      "id": 16,
      "seek": 7780,
      "start": 77.8,
      "end": 81.92,
      "text": " which are relating to the change that they are proposing.",
      "tokens": [
        50364,
        597,
        366,
        23968,
        281,
        264,
        1319,
        300,
        436,
        366,
        29939,
        13,
        50570
      ],
      "temperature": 0.0,
      "avg_logprob": -0.1542422749171747,
      "compression_ratio": 1.777327935222672,
      "no_speech_prob": 0.03916838392615318
    },
    {
      "id": 17,
      "seek": 7780,
      "start": 81.92,
      "end": 86.24,
      "text": " And that would also help us make sure that we are not discouraging anyone from making",
      "tokens": [
        50570,
        400,
        300,
        576,
        611,
        854,
        505,
        652,
        988,
        300,
        321,
        366,
        406,
        21497,
        3568,
        2878,
        490,
        1455,
        50786
      ],
      "temperature": 0.0,
      "avg_logprob": -0.1542422749171747,
      "compression_ratio": 1.777327935222672,
      "no_speech_prob": 0.03916838392615318
    },
    {
      "id": 18,
      "seek": 7780,
      "start": 86.24,
      "end": 87.24,
      "text": " contributions.",
      "tokens": [
        50786,
        15725,
        13,
        50836
      ],
      "temperature": 0.0,
      "avg_logprob": -0.1542422749171747,
      "compression_ratio": 1.777327935222672,
      "no_speech_prob": 0.03916838392615318
    },
    {
      "id": 19,
      "seek": 7780,
      "start": 87.24,
      "end": 92.64,
      "text": " Because of course, if we just continue the discussions for a month or more than that",
      "tokens": [
        50836,
        1436,
        295,
        1164,
        11,
        498,
        321,
        445,
        2354,
        264,
        11088,
        337,
        257,
        1618,
        420,
        544,
        813,
        300,
        51106
      ],
      "temperature": 0.0,
      "avg_logprob": -0.1542422749171747,
      "compression_ratio": 1.777327935222672,
      "no_speech_prob": 0.03916838392615318
    },
    {
      "id": 20,
      "seek": 7780,
      "start": 92.64,
      "end": 99.0,
      "text": " and just keep on changing, making changes to the decisions, it's not, I would say for",
      "tokens": [
        51106,
        293,
        445,
        1066,
        322,
        4473,
        11,
        1455,
        2962,
        281,
        264,
        5327,
        11,
        309,
        311,
        406,
        11,
        286,
        576,
        584,
        337,
        51424
      ],
      "temperature": 0.0,
      "avg_logprob": -0.1542422749171747,
      "compression_ratio": 1.777327935222672,
      "no_speech_prob": 0.03916838392615318
    },
    {
      "id": 21,
      "seek": 7780,
      "start": 99.0,
      "end": 104.64,
      "text": " someone who's taking out time from their personal time to do this work, it's not a great experience",
      "tokens": [
        51424,
        1580,
        567,
        311,
        1940,
        484,
        565,
        490,
        641,
        2973,
        565,
        281,
        360,
        341,
        589,
        11,
        309,
        311,
        406,
        257,
        869,
        1752,
        51706
      ],
      "temperature": 0.0,
      "avg_logprob": -0.1542422749171747,
      "compression_ratio": 1.777327935222672,
      "no_speech_prob": 0.03916838392615318
    },
    {
      "id": 22,
      "seek": 7780,
      "start": 104.64,
      "end": 105.64,
      "text": " for them.",
      "tokens": [
        51706,
        337,
        552,
        13,
        51756
      ],
      "temperature": 0.0,
      "avg_logprob": -0.1542422749171747,
      "compression_ratio": 1.777327935222672,
      "no_speech_prob": 0.03916838392615318
    },
    {
      "id": 23,
      "seek": 10564,
      "start": 105.64,
      "end": 111.68,
      "text": " So yeah, I'll keep everyone posted on what we decide or what changes the contributors",
      "tokens": [
        50364,
        407,
        1338,
        11,
        286,
        603,
        1066,
        1518,
        9437,
        322,
        437,
        321,
        4536,
        420,
        437,
        2962,
        264,
        45627,
        50666
      ],
      "temperature": 0.0,
      "avg_logprob": -0.3259114091114331,
      "compression_ratio": 1.554585152838428,
      "no_speech_prob": 0.004130060318857431
    },
    {
      "id": 24,
      "seek": 10564,
      "start": 111.68,
      "end": 113.72,
      "text": " access team decides on rather.",
      "tokens": [
        50666,
        2105,
        1469,
        14898,
        322,
        2831,
        13,
        50768
      ],
      "temperature": 0.0,
      "avg_logprob": -0.3259114091114331,
      "compression_ratio": 1.554585152838428,
      "no_speech_prob": 0.004130060318857431
    },
    {
      "id": 25,
      "seek": 10564,
      "start": 113.72,
      "end": 116.12,
      "text": " But this is the slack thread where it's all happening.",
      "tokens": [
        50768,
        583,
        341,
        307,
        264,
        29767,
        7207,
        689,
        309,
        311,
        439,
        2737,
        13,
        50888
      ],
      "temperature": 0.0,
      "avg_logprob": -0.3259114091114331,
      "compression_ratio": 1.554585152838428,
      "no_speech_prob": 0.004130060318857431
    },
    {
      "id": 26,
      "seek": 10564,
      "start": 116.12,
      "end": 118.92,
      "text": " So if you want to just follow up.",
      "tokens": [
        50888,
        407,
        498,
        291,
        528,
        281,
        445,
        1524,
        493,
        13,
        51028
      ],
      "temperature": 0.0,
      "avg_logprob": -0.3259114091114331,
      "compression_ratio": 1.554585152838428,
      "no_speech_prob": 0.004130060318857431
    },
    {
      "id": 27,
      "seek": 10564,
      "start": 118.92,
      "end": 121.44,
      "text": " Yeah, Gina, to you.",
      "tokens": [
        51028,
        865,
        11,
        34711,
        11,
        281,
        291,
        13,
        51154
      ],
      "temperature": 0.0,
      "avg_logprob": -0.3259114091114331,
      "compression_ratio": 1.554585152838428,
      "no_speech_prob": 0.004130060318857431
    },
    {
      "id": 28,
      "seek": 10564,
      "start": 121.44,
      "end": 125.08,
      "text": " Yeah, because this happened to me as well.",
      "tokens": [
        51154,
        865,
        11,
        570,
        341,
        2011,
        281,
        385,
        382,
        731,
        13,
        51336
      ],
      "temperature": 0.0,
      "avg_logprob": -0.3259114091114331,
      "compression_ratio": 1.554585152838428,
      "no_speech_prob": 0.004130060318857431
    },
    {
      "id": 29,
      "seek": 10564,
      "start": 125.08,
      "end": 129.92000000000002,
      "text": " And it gets a little frustrating especially for designers because we tend to have to go",
      "tokens": [
        51336,
        400,
        309,
        2170,
        257,
        707,
        16522,
        2318,
        337,
        16196,
        570,
        321,
        3928,
        281,
        362,
        281,
        352,
        51578
      ],
      "temperature": 0.0,
      "avg_logprob": -0.3259114091114331,
      "compression_ratio": 1.554585152838428,
      "no_speech_prob": 0.004130060318857431
    },
    {
      "id": 30,
      "seek": 12992,
      "start": 129.92,
      "end": 135.6,
      "text": " back and like retroactively design based on the contribution.",
      "tokens": [
        50364,
        646,
        293,
        411,
        18820,
        45679,
        1715,
        2361,
        322,
        264,
        13150,
        13,
        50648
      ],
      "temperature": 0.0,
      "avg_logprob": -0.20278491436595647,
      "compression_ratio": 1.7197802197802199,
      "no_speech_prob": 0.01168893650174141
    },
    {
      "id": 31,
      "seek": 12992,
      "start": 135.6,
      "end": 140.51999999999998,
      "text": " And for runner, we saw it was the same contributor every time who was doing it.",
      "tokens": [
        50648,
        400,
        337,
        24376,
        11,
        321,
        1866,
        309,
        390,
        264,
        912,
        42859,
        633,
        565,
        567,
        390,
        884,
        309,
        13,
        50894
      ],
      "temperature": 0.0,
      "avg_logprob": -0.20278491436595647,
      "compression_ratio": 1.7197802197802199,
      "no_speech_prob": 0.01168893650174141
    },
    {
      "id": 32,
      "seek": 12992,
      "start": 140.51999999999998,
      "end": 146.92,
      "text": " So we ended up talking with the contributors success team and then met with them plus",
      "tokens": [
        50894,
        407,
        321,
        4590,
        493,
        1417,
        365,
        264,
        45627,
        2245,
        1469,
        293,
        550,
        1131,
        365,
        552,
        1804,
        51214
      ],
      "temperature": 0.0,
      "avg_logprob": -0.20278491436595647,
      "compression_ratio": 1.7197802197802199,
      "no_speech_prob": 0.01168893650174141
    },
    {
      "id": 33,
      "seek": 12992,
      "start": 146.92,
      "end": 153.48,
      "text": " the contributor and kind of like talked about our roadmap so that we talked about our",
      "tokens": [
        51214,
        264,
        42859,
        293,
        733,
        295,
        411,
        2825,
        466,
        527,
        35738,
        370,
        300,
        321,
        2825,
        466,
        527,
        51542
      ],
      "temperature": 0.0,
      "avg_logprob": -0.20278491436595647,
      "compression_ratio": 1.7197802197802199,
      "no_speech_prob": 0.01168893650174141
    },
    {
      "id": 34,
      "seek": 15348,
      "start": 153.48,
      "end": 160.28,
      "text": " roadmap and also what they were thinking on their roadmap so we can come together on those",
      "tokens": [
        50364,
        35738,
        293,
        611,
        437,
        436,
        645,
        1953,
        322,
        641,
        35738,
        370,
        321,
        393,
        808,
        1214,
        322,
        729,
        50704
      ],
      "temperature": 0.0,
      "avg_logprob": -0.17721842736313023,
      "compression_ratio": 1.6926406926406927,
      "no_speech_prob": 0.0023794977460056543
    },
    {
      "id": 35,
      "seek": 15348,
      "start": 160.28,
      "end": 162.12,
      "text": " things.",
      "tokens": [
        50704,
        721,
        13,
        50796
      ],
      "temperature": 0.0,
      "avg_logprob": -0.17721842736313023,
      "compression_ratio": 1.6926406926406927,
      "no_speech_prob": 0.0023794977460056543
    },
    {
      "id": 36,
      "seek": 15348,
      "start": 162.12,
      "end": 169.67999999999998,
      "text": " And what I suggested, I mean, obviously we have to like be open to them contributing.",
      "tokens": [
        50796,
        400,
        437,
        286,
        10945,
        11,
        286,
        914,
        11,
        2745,
        321,
        362,
        281,
        411,
        312,
        1269,
        281,
        552,
        19270,
        13,
        51174
      ],
      "temperature": 0.0,
      "avg_logprob": -0.17721842736313023,
      "compression_ratio": 1.6926406926406927,
      "no_speech_prob": 0.0023794977460056543
    },
    {
      "id": 37,
      "seek": 15348,
      "start": 169.67999999999998,
      "end": 174.83999999999997,
      "text": " So I suggested that if they have like a new feature that they want to create, they create",
      "tokens": [
        51174,
        407,
        286,
        10945,
        300,
        498,
        436,
        362,
        411,
        257,
        777,
        4111,
        300,
        436,
        528,
        281,
        1884,
        11,
        436,
        1884,
        51432
      ],
      "temperature": 0.0,
      "avg_logprob": -0.17721842736313023,
      "compression_ratio": 1.6926406926406927,
      "no_speech_prob": 0.0023794977460056543
    },
    {
      "id": 38,
      "seek": 15348,
      "start": 174.83999999999997,
      "end": 179.23999999999998,
      "text": " an issue first and tag me and that puts a lot of responsibility on us.",
      "tokens": [
        51432,
        364,
        2734,
        700,
        293,
        6162,
        385,
        293,
        300,
        8137,
        257,
        688,
        295,
        6357,
        322,
        505,
        13,
        51652
      ],
      "temperature": 0.0,
      "avg_logprob": -0.17721842736313023,
      "compression_ratio": 1.6926406926406927,
      "no_speech_prob": 0.0023794977460056543
    },
    {
      "id": 39,
      "seek": 15348,
      "start": 179.23999999999998,
      "end": 182.95999999999998,
      "text": " But I also think it gets us involved earlier.",
      "tokens": [
        51652,
        583,
        286,
        611,
        519,
        309,
        2170,
        505,
        3288,
        3071,
        13,
        51838
      ],
      "temperature": 0.0,
      "avg_logprob": -0.17721842736313023,
      "compression_ratio": 1.6926406926406927,
      "no_speech_prob": 0.0023794977460056543
    },
    {
      "id": 40,
      "seek": 18296,
      "start": 182.96,
      "end": 187.92000000000002,
      "text": " So that's what they've been doing lately and it actually has been helping a lot in",
      "tokens": [
        50364,
        407,
        300,
        311,
        437,
        436,
        600,
        668,
        884,
        12881,
        293,
        309,
        767,
        575,
        668,
        4315,
        257,
        688,
        294,
        50612
      ],
      "temperature": 0.0,
      "avg_logprob": -0.22202176314133865,
      "compression_ratio": 1.6692607003891051,
      "no_speech_prob": 0.0039063734002411366
    },
    {
      "id": 41,
      "seek": 18296,
      "start": 187.92000000000002,
      "end": 192.72,
      "text": " regards to like making sure it fits in with features that already exist or that we even",
      "tokens": [
        50612,
        14258,
        281,
        411,
        1455,
        988,
        309,
        9001,
        294,
        365,
        4122,
        300,
        1217,
        2514,
        420,
        300,
        321,
        754,
        50852
      ],
      "temperature": 0.0,
      "avg_logprob": -0.22202176314133865,
      "compression_ratio": 1.6692607003891051,
      "no_speech_prob": 0.0039063734002411366
    },
    {
      "id": 42,
      "seek": 18296,
      "start": 192.72,
      "end": 195.16,
      "text": " like see coming up in the roadmap.",
      "tokens": [
        50852,
        411,
        536,
        1348,
        493,
        294,
        264,
        35738,
        13,
        50974
      ],
      "temperature": 0.0,
      "avg_logprob": -0.22202176314133865,
      "compression_ratio": 1.6692607003891051,
      "no_speech_prob": 0.0039063734002411366
    },
    {
      "id": 43,
      "seek": 18296,
      "start": 195.16,
      "end": 198.52,
      "text": " Yeah, that's a very relevant example.",
      "tokens": [
        50974,
        865,
        11,
        300,
        311,
        257,
        588,
        7340,
        1365,
        13,
        51142
      ],
      "temperature": 0.0,
      "avg_logprob": -0.22202176314133865,
      "compression_ratio": 1.6692607003891051,
      "no_speech_prob": 0.0039063734002411366
    },
    {
      "id": 44,
      "seek": 18296,
      "start": 198.52,
      "end": 203.4,
      "text": " So like this is case by case like for your team writing, but to make sure that you come",
      "tokens": [
        51142,
        407,
        411,
        341,
        307,
        1389,
        538,
        1389,
        411,
        337,
        428,
        1469,
        3579,
        11,
        457,
        281,
        652,
        988,
        300,
        291,
        808,
        51386
      ],
      "temperature": 0.0,
      "avg_logprob": -0.22202176314133865,
      "compression_ratio": 1.6692607003891051,
      "no_speech_prob": 0.0039063734002411366
    },
    {
      "id": 45,
      "seek": 18296,
      "start": 203.4,
      "end": 210.08,
      "text": " up with something that's generally applicable to everyone, it's I think good for the contributors",
      "tokens": [
        51386,
        493,
        365,
        746,
        300,
        311,
        5101,
        21142,
        281,
        1518,
        11,
        309,
        311,
        286,
        519,
        665,
        337,
        264,
        45627,
        51720
      ],
      "temperature": 0.0,
      "avg_logprob": -0.22202176314133865,
      "compression_ratio": 1.6692607003891051,
      "no_speech_prob": 0.0039063734002411366
    },
    {
      "id": 46,
      "seek": 21008,
      "start": 210.08,
      "end": 217.08,
      "text": " that are just suggesting to come up with some new rules probably.",
      "tokens": [
        50364,
        300,
        366,
        445,
        18094,
        281,
        808,
        493,
        365,
        512,
        777,
        4474,
        1391,
        13,
        50714
      ],
      "temperature": 0.0,
      "avg_logprob": -0.37055418831961495,
      "compression_ratio": 1.4450261780104712,
      "no_speech_prob": 0.041799455881118774
    },
    {
      "id": 47,
      "seek": 21008,
      "start": 217.08,
      "end": 220.08,
      "text": " Yeah.",
      "tokens": [
        50714,
        865,
        13,
        50864
      ],
      "temperature": 0.0,
      "avg_logprob": -0.37055418831961495,
      "compression_ratio": 1.4450261780104712,
      "no_speech_prob": 0.041799455881118774
    },
    {
      "id": 48,
      "seek": 21008,
      "start": 220.08,
      "end": 227.20000000000002,
      "text": " Okay, if there are no more comments, let's move to product design.",
      "tokens": [
        50864,
        1033,
        11,
        498,
        456,
        366,
        572,
        544,
        3053,
        11,
        718,
        311,
        1286,
        281,
        1674,
        1715,
        13,
        51220
      ],
      "temperature": 0.0,
      "avg_logprob": -0.37055418831961495,
      "compression_ratio": 1.4450261780104712,
      "no_speech_prob": 0.041799455881118774
    },
    {
      "id": 49,
      "seek": 21008,
      "start": 227.20000000000002,
      "end": 232.08,
      "text": " So for the first section which is which board, Helena has some updates that she's working",
      "tokens": [
        51220,
        407,
        337,
        264,
        700,
        3541,
        597,
        307,
        597,
        3150,
        11,
        49294,
        575,
        512,
        9205,
        300,
        750,
        311,
        1364,
        51464
      ],
      "temperature": 0.0,
      "avg_logprob": -0.37055418831961495,
      "compression_ratio": 1.4450261780104712,
      "no_speech_prob": 0.041799455881118774
    },
    {
      "id": 50,
      "seek": 21008,
      "start": 232.08,
      "end": 236.24,
      "text": " on the backfill requirement for Alice position.",
      "tokens": [
        51464,
        322,
        264,
        646,
        31072,
        11695,
        337,
        16004,
        2535,
        13,
        51672
      ],
      "temperature": 0.0,
      "avg_logprob": -0.37055418831961495,
      "compression_ratio": 1.4450261780104712,
      "no_speech_prob": 0.041799455881118774
    },
    {
      "id": 51,
      "seek": 23624,
      "start": 236.24,
      "end": 240.36,
      "text": " And if there's anything that any of the team members have been working on with Ali, they",
      "tokens": [
        50364,
        400,
        498,
        456,
        311,
        1340,
        300,
        604,
        295,
        264,
        1469,
        2679,
        362,
        668,
        1364,
        322,
        365,
        12020,
        11,
        436,
        50570
      ],
      "temperature": 0.0,
      "avg_logprob": -0.2561873233679569,
      "compression_ratio": 1.5450643776824033,
      "no_speech_prob": 0.03785811364650726
    },
    {
      "id": 52,
      "seek": 23624,
      "start": 240.36,
      "end": 245.48000000000002,
      "text": " can just let her know so she can work on finding alternatives.",
      "tokens": [
        50570,
        393,
        445,
        718,
        720,
        458,
        370,
        750,
        393,
        589,
        322,
        5006,
        20478,
        13,
        50826
      ],
      "temperature": 0.0,
      "avg_logprob": -0.2561873233679569,
      "compression_ratio": 1.5450643776824033,
      "no_speech_prob": 0.03785811364650726
    },
    {
      "id": 53,
      "seek": 23624,
      "start": 245.48000000000002,
      "end": 253.92000000000002,
      "text": " Any others, I think NFI for Emily that the pairs would be rooted by the end of July.",
      "tokens": [
        50826,
        2639,
        2357,
        11,
        286,
        519,
        13576,
        40,
        337,
        15034,
        300,
        264,
        15494,
        576,
        312,
        25277,
        538,
        264,
        917,
        295,
        7370,
        13,
        51248
      ],
      "temperature": 0.0,
      "avg_logprob": -0.2561873233679569,
      "compression_ratio": 1.5450643776824033,
      "no_speech_prob": 0.03785811364650726
    },
    {
      "id": 54,
      "seek": 23624,
      "start": 253.92000000000002,
      "end": 257.56,
      "text": " And for the rest of us as well of course.",
      "tokens": [
        51248,
        400,
        337,
        264,
        1472,
        295,
        505,
        382,
        731,
        295,
        1164,
        13,
        51430
      ],
      "temperature": 0.0,
      "avg_logprob": -0.2561873233679569,
      "compression_ratio": 1.5450643776824033,
      "no_speech_prob": 0.03785811364650726
    },
    {
      "id": 55,
      "seek": 23624,
      "start": 257.56,
      "end": 260.24,
      "text": " Yeah, next is Gina.",
      "tokens": [
        51430,
        865,
        11,
        958,
        307,
        34711,
        13,
        51564
      ],
      "temperature": 0.0,
      "avg_logprob": -0.2561873233679569,
      "compression_ratio": 1.5450643776824033,
      "no_speech_prob": 0.03785811364650726
    },
    {
      "id": 56,
      "seek": 23624,
      "start": 260.24,
      "end": 266.16,
      "text": " Yeah, I'm going to drop a little bit early from this meeting.",
      "tokens": [
        51564,
        865,
        11,
        286,
        478,
        516,
        281,
        3270,
        257,
        707,
        857,
        2440,
        490,
        341,
        3440,
        13,
        51860
      ],
      "temperature": 0.0,
      "avg_logprob": -0.2561873233679569,
      "compression_ratio": 1.5450643776824033,
      "no_speech_prob": 0.03785811364650726
    },
    {
      "id": 57,
      "seek": 26616,
      "start": 266.16,
      "end": 271.44,
      "text": " But also the things that I've been working on are mental model research for runners, which",
      "tokens": [
        50364,
        583,
        611,
        264,
        721,
        300,
        286,
        600,
        668,
        1364,
        322,
        366,
        4973,
        2316,
        2132,
        337,
        33892,
        11,
        597,
        50628
      ],
      "temperature": 0.0,
      "avg_logprob": -0.2162885914678159,
      "compression_ratio": 1.6111111111111112,
      "no_speech_prob": 0.00023308437084779143
    },
    {
      "id": 58,
      "seek": 26616,
      "start": 271.44,
      "end": 276.8,
      "text": " I completed and thank you to Eric is not on the call, but if she washes the recording,",
      "tokens": [
        50628,
        286,
        7365,
        293,
        1309,
        291,
        281,
        9336,
        307,
        406,
        322,
        264,
        818,
        11,
        457,
        498,
        750,
        48616,
        264,
        6613,
        11,
        50896
      ],
      "temperature": 0.0,
      "avg_logprob": -0.2162885914678159,
      "compression_ratio": 1.6111111111111112,
      "no_speech_prob": 0.00023308437084779143
    },
    {
      "id": 59,
      "seek": 26616,
      "start": 276.8,
      "end": 279.16,
      "text": " she helps me a time with this project.",
      "tokens": [
        50896,
        750,
        3665,
        385,
        257,
        565,
        365,
        341,
        1716,
        13,
        51014
      ],
      "temperature": 0.0,
      "avg_logprob": -0.2162885914678159,
      "compression_ratio": 1.6111111111111112,
      "no_speech_prob": 0.00023308437084779143
    },
    {
      "id": 60,
      "seek": 26616,
      "start": 279.16,
      "end": 282.40000000000003,
      "text": " So yeah, thank you Eric.",
      "tokens": [
        51014,
        407,
        1338,
        11,
        1309,
        291,
        9336,
        13,
        51176
      ],
      "temperature": 0.0,
      "avg_logprob": -0.2162885914678159,
      "compression_ratio": 1.6111111111111112,
      "no_speech_prob": 0.00023308437084779143
    },
    {
      "id": 61,
      "seek": 26616,
      "start": 282.40000000000003,
      "end": 287.96000000000004,
      "text": " And she gave me a skeleton of a report to be able to fill out with all the analysis.",
      "tokens": [
        51176,
        400,
        750,
        2729,
        385,
        257,
        25204,
        295,
        257,
        2275,
        281,
        312,
        1075,
        281,
        2836,
        484,
        365,
        439,
        264,
        5215,
        13,
        51454
      ],
      "temperature": 0.0,
      "avg_logprob": -0.2162885914678159,
      "compression_ratio": 1.6111111111111112,
      "no_speech_prob": 0.00023308437084779143
    },
    {
      "id": 62,
      "seek": 26616,
      "start": 287.96000000000004,
      "end": 290.84000000000003,
      "text": " It's still working progress, but you're welcome to check it out.",
      "tokens": [
        51454,
        467,
        311,
        920,
        1364,
        4205,
        11,
        457,
        291,
        434,
        2928,
        281,
        1520,
        309,
        484,
        13,
        51598
      ],
      "temperature": 0.0,
      "avg_logprob": -0.2162885914678159,
      "compression_ratio": 1.6111111111111112,
      "no_speech_prob": 0.00023308437084779143
    },
    {
      "id": 63,
      "seek": 26616,
      "start": 290.84000000000003,
      "end": 294.72,
      "text": " I also added a summary to the issue though.",
      "tokens": [
        51598,
        286,
        611,
        3869,
        257,
        12691,
        281,
        264,
        2734,
        1673,
        13,
        51792
      ],
      "temperature": 0.0,
      "avg_logprob": -0.2162885914678159,
      "compression_ratio": 1.6111111111111112,
      "no_speech_prob": 0.00023308437084779143
    },
    {
      "id": 64,
      "seek": 29472,
      "start": 294.72,
      "end": 299.04,
      "text": " So what came out of it was.",
      "tokens": [
        50364,
        407,
        437,
        1361,
        484,
        295,
        309,
        390,
        13,
        50580
      ],
      "temperature": 0.0,
      "avg_logprob": -0.18546295166015625,
      "compression_ratio": 1.4648648648648648,
      "no_speech_prob": 8.091462950687855e-05
    },
    {
      "id": 65,
      "seek": 29472,
      "start": 299.04,
      "end": 306.04,
      "text": " Defined groups of the runner concepts that users think about when they're using runners.",
      "tokens": [
        50580,
        9548,
        2001,
        3935,
        295,
        264,
        24376,
        10392,
        300,
        5022,
        519,
        466,
        562,
        436,
        434,
        1228,
        33892,
        13,
        50930
      ],
      "temperature": 0.0,
      "avg_logprob": -0.18546295166015625,
      "compression_ratio": 1.4648648648648648,
      "no_speech_prob": 8.091462950687855e-05
    },
    {
      "id": 66,
      "seek": 29472,
      "start": 306.04,
      "end": 314.88000000000005,
      "text": " And then we also had them like come up with their own terms to describe certain definitions,",
      "tokens": [
        50930,
        400,
        550,
        321,
        611,
        632,
        552,
        411,
        808,
        493,
        365,
        641,
        1065,
        2115,
        281,
        6786,
        1629,
        21988,
        11,
        51372
      ],
      "temperature": 0.0,
      "avg_logprob": -0.18546295166015625,
      "compression_ratio": 1.4648648648648648,
      "no_speech_prob": 8.091462950687855e-05
    },
    {
      "id": 67,
      "seek": 29472,
      "start": 314.88000000000005,
      "end": 320.8,
      "text": " which almost all were not aligned with what we use at GitLab.",
      "tokens": [
        51372,
        597,
        1920,
        439,
        645,
        406,
        17962,
        365,
        437,
        321,
        764,
        412,
        16939,
        37880,
        13,
        51668
      ],
      "temperature": 0.0,
      "avg_logprob": -0.18546295166015625,
      "compression_ratio": 1.4648648648648648,
      "no_speech_prob": 8.091462950687855e-05
    },
    {
      "id": 68,
      "seek": 32080,
      "start": 320.8,
      "end": 324.32,
      "text": " But conceptually, you saw some connections there.",
      "tokens": [
        50364,
        583,
        3410,
        671,
        11,
        291,
        1866,
        512,
        9271,
        456,
        13,
        50540
      ],
      "temperature": 0.0,
      "avg_logprob": -0.18993233619852268,
      "compression_ratio": 1.680952380952381,
      "no_speech_prob": 0.024358849972486496
    },
    {
      "id": 69,
      "seek": 32080,
      "start": 324.32,
      "end": 326.84000000000003,
      "text": " They just weren't the exact same terms.",
      "tokens": [
        50540,
        814,
        445,
        4999,
        380,
        264,
        1900,
        912,
        2115,
        13,
        50666
      ],
      "temperature": 0.0,
      "avg_logprob": -0.18993233619852268,
      "compression_ratio": 1.680952380952381,
      "no_speech_prob": 0.024358849972486496
    },
    {
      "id": 70,
      "seek": 32080,
      "start": 326.84000000000003,
      "end": 331.24,
      "text": " And then the final exercise was to have them match the terms to definitions, like the",
      "tokens": [
        50666,
        400,
        550,
        264,
        2572,
        5380,
        390,
        281,
        362,
        552,
        2995,
        264,
        2115,
        281,
        21988,
        11,
        411,
        264,
        50886
      ],
      "temperature": 0.0,
      "avg_logprob": -0.18993233619852268,
      "compression_ratio": 1.680952380952381,
      "no_speech_prob": 0.024358849972486496
    },
    {
      "id": 71,
      "seek": 32080,
      "start": 331.24,
      "end": 334.84000000000003,
      "text": " terms that we use at GitLab to the definitions.",
      "tokens": [
        50886,
        2115,
        300,
        321,
        764,
        412,
        16939,
        37880,
        281,
        264,
        21988,
        13,
        51066
      ],
      "temperature": 0.0,
      "avg_logprob": -0.18993233619852268,
      "compression_ratio": 1.680952380952381,
      "no_speech_prob": 0.024358849972486496
    },
    {
      "id": 72,
      "seek": 32080,
      "start": 334.84000000000003,
      "end": 344.28000000000003,
      "text": " And most of those they did, they were not able to match accurately as we defined them at",
      "tokens": [
        51066,
        400,
        881,
        295,
        729,
        436,
        630,
        11,
        436,
        645,
        406,
        1075,
        281,
        2995,
        20095,
        382,
        321,
        7642,
        552,
        412,
        51538
      ],
      "temperature": 0.0,
      "avg_logprob": -0.18993233619852268,
      "compression_ratio": 1.680952380952381,
      "no_speech_prob": 0.024358849972486496
    },
    {
      "id": 73,
      "seek": 32080,
      "start": 344.28000000000003,
      "end": 345.28000000000003,
      "text": " GitLab.",
      "tokens": [
        51538,
        16939,
        37880,
        13,
        51588
      ],
      "temperature": 0.0,
      "avg_logprob": -0.18993233619852268,
      "compression_ratio": 1.680952380952381,
      "no_speech_prob": 0.024358849972486496
    },
    {
      "id": 74,
      "seek": 32080,
      "start": 345.28000000000003,
      "end": 348.04,
      "text": " So there was a lot of learnings.",
      "tokens": [
        51588,
        407,
        456,
        390,
        257,
        688,
        295,
        2539,
        82,
        13,
        51726
      ],
      "temperature": 0.0,
      "avg_logprob": -0.18993233619852268,
      "compression_ratio": 1.680952380952381,
      "no_speech_prob": 0.024358849972486496
    },
    {
      "id": 75,
      "seek": 34804,
      "start": 348.04,
      "end": 354.0,
      "text": " But what I'm seeing now is the next thing I want to do with that air can suggested is",
      "tokens": [
        50364,
        583,
        437,
        286,
        478,
        2577,
        586,
        307,
        264,
        958,
        551,
        286,
        528,
        281,
        360,
        365,
        300,
        1988,
        393,
        10945,
        307,
        50662
      ],
      "temperature": 0.0,
      "avg_logprob": -0.15711070006748415,
      "compression_ratio": 1.749003984063745,
      "no_speech_prob": 0.0024543856270611286
    },
    {
      "id": 76,
      "seek": 34804,
      "start": 354.0,
      "end": 359.56,
      "text": " work with the team to come up with names for each group of concepts.",
      "tokens": [
        50662,
        589,
        365,
        264,
        1469,
        281,
        808,
        493,
        365,
        5288,
        337,
        1184,
        1594,
        295,
        10392,
        13,
        50940
      ],
      "temperature": 0.0,
      "avg_logprob": -0.15711070006748415,
      "compression_ratio": 1.749003984063745,
      "no_speech_prob": 0.0024543856270611286
    },
    {
      "id": 77,
      "seek": 34804,
      "start": 359.56,
      "end": 364.72,
      "text": " And then we're going to send out another survey to like 100 participants if we can get",
      "tokens": [
        50940,
        400,
        550,
        321,
        434,
        516,
        281,
        2845,
        484,
        1071,
        8984,
        281,
        411,
        2319,
        10503,
        498,
        321,
        393,
        483,
        51198
      ],
      "temperature": 0.0,
      "avg_logprob": -0.15711070006748415,
      "compression_ratio": 1.749003984063745,
      "no_speech_prob": 0.0024543856270611286
    },
    {
      "id": 78,
      "seek": 34804,
      "start": 364.72,
      "end": 370.28000000000003,
      "text": " that many and have them select which name like they feel best describes the concepts",
      "tokens": [
        51198,
        300,
        867,
        293,
        362,
        552,
        3048,
        597,
        1315,
        411,
        436,
        841,
        1151,
        15626,
        264,
        10392,
        51476
      ],
      "temperature": 0.0,
      "avg_logprob": -0.15711070006748415,
      "compression_ratio": 1.749003984063745,
      "no_speech_prob": 0.0024543856270611286
    },
    {
      "id": 79,
      "seek": 34804,
      "start": 370.28000000000003,
      "end": 376.0,
      "text": " in that group and then ask them where they're using that and their workflow and what tasks",
      "tokens": [
        51476,
        294,
        300,
        1594,
        293,
        550,
        1029,
        552,
        689,
        436,
        434,
        1228,
        300,
        293,
        641,
        20993,
        293,
        437,
        9608,
        51762
      ],
      "temperature": 0.0,
      "avg_logprob": -0.15711070006748415,
      "compression_ratio": 1.749003984063745,
      "no_speech_prob": 0.0024543856270611286
    },
    {
      "id": 80,
      "seek": 34804,
      "start": 376.0,
      "end": 377.72,
      "text": " they're using it for.",
      "tokens": [
        51762,
        436,
        434,
        1228,
        309,
        337,
        13,
        51848
      ],
      "temperature": 0.0,
      "avg_logprob": -0.15711070006748415,
      "compression_ratio": 1.749003984063745,
      "no_speech_prob": 0.0024543856270611286
    },
    {
      "id": 81,
      "seek": 37772,
      "start": 377.72,
      "end": 383.0,
      "text": " So then we can figure out how we want to make changes from there.",
      "tokens": [
        50364,
        407,
        550,
        321,
        393,
        2573,
        484,
        577,
        321,
        528,
        281,
        652,
        2962,
        490,
        456,
        13,
        50628
      ],
      "temperature": 0.0,
      "avg_logprob": -0.14333587763260822,
      "compression_ratio": 1.68,
      "no_speech_prob": 0.00022534283925779164
    },
    {
      "id": 82,
      "seek": 37772,
      "start": 383.0,
      "end": 387.44000000000005,
      "text": " Like if I wanted to make the design changes for that configuration example that I shared",
      "tokens": [
        50628,
        1743,
        498,
        286,
        1415,
        281,
        652,
        264,
        1715,
        2962,
        337,
        300,
        11694,
        1365,
        300,
        286,
        5507,
        50850
      ],
      "temperature": 0.0,
      "avg_logprob": -0.14333587763260822,
      "compression_ratio": 1.68,
      "no_speech_prob": 0.00022534283925779164
    },
    {
      "id": 83,
      "seek": 37772,
      "start": 387.44000000000005,
      "end": 394.56,
      "text": " last week, that would be like a way to help us validate that that is needed.",
      "tokens": [
        50850,
        1036,
        1243,
        11,
        300,
        576,
        312,
        411,
        257,
        636,
        281,
        854,
        505,
        29562,
        300,
        300,
        307,
        2978,
        13,
        51206
      ],
      "temperature": 0.0,
      "avg_logprob": -0.14333587763260822,
      "compression_ratio": 1.68,
      "no_speech_prob": 0.00022534283925779164
    },
    {
      "id": 84,
      "seek": 37772,
      "start": 394.56,
      "end": 402.36,
      "text": " But what I'm seeing now is that the team is very, it's difficult for the team to like understand",
      "tokens": [
        51206,
        583,
        437,
        286,
        478,
        2577,
        586,
        307,
        300,
        264,
        1469,
        307,
        588,
        11,
        309,
        311,
        2252,
        337,
        264,
        1469,
        281,
        411,
        1223,
        51596
      ],
      "temperature": 0.0,
      "avg_logprob": -0.14333587763260822,
      "compression_ratio": 1.68,
      "no_speech_prob": 0.00022534283925779164
    },
    {
      "id": 85,
      "seek": 37772,
      "start": 402.36,
      "end": 406.52000000000004,
      "text": " that these concepts were grouped together because it doesn't align with how runner actually",
      "tokens": [
        51596,
        300,
        613,
        10392,
        645,
        41877,
        1214,
        570,
        309,
        1177,
        380,
        7975,
        365,
        577,
        24376,
        767,
        51804
      ],
      "temperature": 0.0,
      "avg_logprob": -0.14333587763260822,
      "compression_ratio": 1.68,
      "no_speech_prob": 0.00022534283925779164
    },
    {
      "id": 86,
      "seek": 40652,
      "start": 406.52,
      "end": 408.68,
      "text": " works.",
      "tokens": [
        50364,
        1985,
        13,
        50472
      ],
      "temperature": 0.0,
      "avg_logprob": -0.2594280242919922,
      "compression_ratio": 1.5154639175257731,
      "no_speech_prob": 0.013258134946227074
    },
    {
      "id": 87,
      "seek": 40652,
      "start": 408.68,
      "end": 414.56,
      "text": " So yeah, it's, it's, people are having a hard time looking past like the technical way",
      "tokens": [
        50472,
        407,
        1338,
        11,
        309,
        311,
        11,
        309,
        311,
        11,
        561,
        366,
        1419,
        257,
        1152,
        565,
        1237,
        1791,
        411,
        264,
        6191,
        636,
        50766
      ],
      "temperature": 0.0,
      "avg_logprob": -0.2594280242919922,
      "compression_ratio": 1.5154639175257731,
      "no_speech_prob": 0.013258134946227074
    },
    {
      "id": 88,
      "seek": 40652,
      "start": 414.56,
      "end": 417.35999999999996,
      "text": " that it works and then how people are grouping them.",
      "tokens": [
        50766,
        300,
        309,
        1985,
        293,
        550,
        577,
        561,
        366,
        40149,
        552,
        13,
        50906
      ],
      "temperature": 0.0,
      "avg_logprob": -0.2594280242919922,
      "compression_ratio": 1.5154639175257731,
      "no_speech_prob": 0.013258134946227074
    },
    {
      "id": 89,
      "seek": 40652,
      "start": 417.35999999999996,
      "end": 423.2,
      "text": " And I was wondering if anyone has run into that problem and kind of how you're approaching",
      "tokens": [
        50906,
        400,
        286,
        390,
        6359,
        498,
        2878,
        575,
        1190,
        666,
        300,
        1154,
        293,
        733,
        295,
        577,
        291,
        434,
        14908,
        51198
      ],
      "temperature": 0.0,
      "avg_logprob": -0.2594280242919922,
      "compression_ratio": 1.5154639175257731,
      "no_speech_prob": 0.013258134946227074
    },
    {
      "id": 90,
      "seek": 40652,
      "start": 423.2,
      "end": 426.15999999999997,
      "text": " it with your team.",
      "tokens": [
        51198,
        309,
        365,
        428,
        1469,
        13,
        51346
      ],
      "temperature": 0.0,
      "avg_logprob": -0.2594280242919922,
      "compression_ratio": 1.5154639175257731,
      "no_speech_prob": 0.013258134946227074
    },
    {
      "id": 91,
      "seek": 40652,
      "start": 426.15999999999997,
      "end": 431.0,
      "text": " Will they want to ask your questions?",
      "tokens": [
        51346,
        3099,
        436,
        528,
        281,
        1029,
        428,
        1651,
        30,
        51588
      ],
      "temperature": 0.0,
      "avg_logprob": -0.2594280242919922,
      "compression_ratio": 1.5154639175257731,
      "no_speech_prob": 0.013258134946227074
    },
    {
      "id": 92,
      "seek": 43100,
      "start": 432.0,
      "end": 440.32,
      "text": " Well, I did want to give you space to see if anybody has like thoughts on the question",
      "tokens": [
        50414,
        1042,
        11,
        286,
        630,
        528,
        281,
        976,
        291,
        1901,
        281,
        536,
        498,
        4472,
        575,
        411,
        4598,
        322,
        264,
        1168,
        50830
      ],
      "temperature": 0.0,
      "avg_logprob": -0.3170830241420813,
      "compression_ratio": 1.4385964912280702,
      "no_speech_prob": 0.03007572330534458
    },
    {
      "id": 93,
      "seek": 43100,
      "start": 440.32,
      "end": 445.56,
      "text": " that you just raised because my question's a little bit unrelated.",
      "tokens": [
        50830,
        300,
        291,
        445,
        6005,
        570,
        452,
        1168,
        311,
        257,
        707,
        857,
        38967,
        13,
        51092
      ],
      "temperature": 0.0,
      "avg_logprob": -0.3170830241420813,
      "compression_ratio": 1.4385964912280702,
      "no_speech_prob": 0.03007572330534458
    },
    {
      "id": 94,
      "seek": 43100,
      "start": 445.56,
      "end": 448.08,
      "text": " Okay.",
      "tokens": [
        51092,
        1033,
        13,
        51218
      ],
      "temperature": 0.0,
      "avg_logprob": -0.3170830241420813,
      "compression_ratio": 1.4385964912280702,
      "no_speech_prob": 0.03007572330534458
    },
    {
      "id": 95,
      "seek": 43100,
      "start": 448.08,
      "end": 454.76,
      "text": " I think what Emily did with environments was something similar because they challenged",
      "tokens": [
        51218,
        286,
        519,
        437,
        15034,
        630,
        365,
        12388,
        390,
        746,
        2531,
        570,
        436,
        17737,
        51552
      ],
      "temperature": 0.0,
      "avg_logprob": -0.3170830241420813,
      "compression_ratio": 1.4385964912280702,
      "no_speech_prob": 0.03007572330534458
    },
    {
      "id": 96,
      "seek": 45476,
      "start": 454.76,
      "end": 465.84,
      "text": " the existing ideas of concepts that were like that were there since forever in GitLab.",
      "tokens": [
        50364,
        264,
        6741,
        3487,
        295,
        10392,
        300,
        645,
        411,
        300,
        645,
        456,
        1670,
        5680,
        294,
        16939,
        37880,
        13,
        50918
      ],
      "temperature": 0.0,
      "avg_logprob": -0.240002610466697,
      "compression_ratio": 1.5859030837004404,
      "no_speech_prob": 0.09426508098840714
    },
    {
      "id": 97,
      "seek": 45476,
      "start": 465.84,
      "end": 474.15999999999997,
      "text": " So I don't know, maybe Emily, any insights like how did the team take take it when they",
      "tokens": [
        50918,
        407,
        286,
        500,
        380,
        458,
        11,
        1310,
        15034,
        11,
        604,
        14310,
        411,
        577,
        630,
        264,
        1469,
        747,
        747,
        309,
        562,
        436,
        51334
      ],
      "temperature": 0.0,
      "avg_logprob": -0.240002610466697,
      "compression_ratio": 1.5859030837004404,
      "no_speech_prob": 0.09426508098840714
    },
    {
      "id": 98,
      "seek": 45476,
      "start": 474.15999999999997,
      "end": 476.15999999999997,
      "text": " learnt.",
      "tokens": [
        51334,
        18991,
        13,
        51434
      ],
      "temperature": 0.0,
      "avg_logprob": -0.240002610466697,
      "compression_ratio": 1.5859030837004404,
      "no_speech_prob": 0.09426508098840714
    },
    {
      "id": 99,
      "seek": 45476,
      "start": 476.15999999999997,
      "end": 478.32,
      "text": " But this was rather something that the team was involved in.",
      "tokens": [
        51434,
        583,
        341,
        390,
        2831,
        746,
        300,
        264,
        1469,
        390,
        3288,
        294,
        13,
        51542
      ],
      "temperature": 0.0,
      "avg_logprob": -0.240002610466697,
      "compression_ratio": 1.5859030837004404,
      "no_speech_prob": 0.09426508098840714
    },
    {
      "id": 100,
      "seek": 45476,
      "start": 478.32,
      "end": 483.03999999999996,
      "text": " So probably that's how it was different because it was not coming from outside but from discussions",
      "tokens": [
        51542,
        407,
        1391,
        300,
        311,
        577,
        309,
        390,
        819,
        570,
        309,
        390,
        406,
        1348,
        490,
        2380,
        457,
        490,
        11088,
        51778
      ],
      "temperature": 0.0,
      "avg_logprob": -0.240002610466697,
      "compression_ratio": 1.5859030837004404,
      "no_speech_prob": 0.09426508098840714
    },
    {
      "id": 101,
      "seek": 45476,
      "start": 483.03999999999996,
      "end": 484.03999999999996,
      "text": " within the team.",
      "tokens": [
        51778,
        1951,
        264,
        1469,
        13,
        51828
      ],
      "temperature": 0.0,
      "avg_logprob": -0.240002610466697,
      "compression_ratio": 1.5859030837004404,
      "no_speech_prob": 0.09426508098840714
    },
    {
      "id": 102,
      "seek": 48404,
      "start": 484.36,
      "end": 487.84000000000003,
      "text": " The buyout would have been on the roadside floor.",
      "tokens": [
        50380,
        440,
        2256,
        346,
        576,
        362,
        668,
        322,
        264,
        3060,
        1812,
        4123,
        13,
        50554
      ],
      "temperature": 0.0,
      "avg_logprob": -0.2571898784834085,
      "compression_ratio": 1.6613545816733069,
      "no_speech_prob": 0.005036527756601572
    },
    {
      "id": 103,
      "seek": 48404,
      "start": 487.84000000000003,
      "end": 494.28000000000003,
      "text": " We also had a lot of research to back that our current feature wasn't working.",
      "tokens": [
        50554,
        492,
        611,
        632,
        257,
        688,
        295,
        2132,
        281,
        646,
        300,
        527,
        2190,
        4111,
        2067,
        380,
        1364,
        13,
        50876
      ],
      "temperature": 0.0,
      "avg_logprob": -0.2571898784834085,
      "compression_ratio": 1.6613545816733069,
      "no_speech_prob": 0.005036527756601572
    },
    {
      "id": 104,
      "seek": 48404,
      "start": 494.28000000000003,
      "end": 499.20000000000005,
      "text": " So I think that made it easy to convince the team because through like the group level",
      "tokens": [
        50876,
        407,
        286,
        519,
        300,
        1027,
        309,
        1858,
        281,
        13447,
        264,
        1469,
        570,
        807,
        411,
        264,
        1594,
        1496,
        51122
      ],
      "temperature": 0.0,
      "avg_logprob": -0.2571898784834085,
      "compression_ratio": 1.6613545816733069,
      "no_speech_prob": 0.005036527756601572
    },
    {
      "id": 105,
      "seek": 48404,
      "start": 499.20000000000005,
      "end": 504.72,
      "text": " environments research, additional research before we were aware like this feature wasn't",
      "tokens": [
        51122,
        12388,
        2132,
        11,
        4497,
        2132,
        949,
        321,
        645,
        3650,
        411,
        341,
        4111,
        2067,
        380,
        51398
      ],
      "temperature": 0.0,
      "avg_logprob": -0.2571898784834085,
      "compression_ratio": 1.6613545816733069,
      "no_speech_prob": 0.005036527756601572
    },
    {
      "id": 106,
      "seek": 48404,
      "start": 504.72,
      "end": 506.96000000000004,
      "text": " really hitting the mark.",
      "tokens": [
        51398,
        534,
        8850,
        264,
        1491,
        13,
        51510
      ],
      "temperature": 0.0,
      "avg_logprob": -0.2571898784834085,
      "compression_ratio": 1.6613545816733069,
      "no_speech_prob": 0.005036527756601572
    },
    {
      "id": 107,
      "seek": 48404,
      "start": 506.96000000000004,
      "end": 512.9200000000001,
      "text": " So collectively, I think it was much easier to move forward versus yeah, it coming from",
      "tokens": [
        51510,
        407,
        24341,
        11,
        286,
        519,
        309,
        390,
        709,
        3571,
        281,
        1286,
        2128,
        5717,
        1338,
        11,
        309,
        1348,
        490,
        51808
      ],
      "temperature": 0.0,
      "avg_logprob": -0.2571898784834085,
      "compression_ratio": 1.6613545816733069,
      "no_speech_prob": 0.005036527756601572
    },
    {
      "id": 108,
      "seek": 51292,
      "start": 512.92,
      "end": 513.92,
      "text": " someone else.",
      "tokens": [
        50364,
        1580,
        1646,
        13,
        50414
      ],
      "temperature": 0.0,
      "avg_logprob": -0.23424186426050522,
      "compression_ratio": 1.5081967213114753,
      "no_speech_prob": 0.0004898280603811145
    },
    {
      "id": 109,
      "seek": 51292,
      "start": 513.92,
      "end": 520.88,
      "text": " So yeah, which is totally different in our use case because people are very happy with",
      "tokens": [
        50414,
        407,
        1338,
        11,
        597,
        307,
        3879,
        819,
        294,
        527,
        764,
        1389,
        570,
        561,
        366,
        588,
        2055,
        365,
        50762
      ],
      "temperature": 0.0,
      "avg_logprob": -0.23424186426050522,
      "compression_ratio": 1.5081967213114753,
      "no_speech_prob": 0.0004898280603811145
    },
    {
      "id": 110,
      "seek": 51292,
      "start": 520.88,
      "end": 524.4799999999999,
      "text": " runners.",
      "tokens": [
        50762,
        33892,
        13,
        50942
      ],
      "temperature": 0.0,
      "avg_logprob": -0.23424186426050522,
      "compression_ratio": 1.5081967213114753,
      "no_speech_prob": 0.0004898280603811145
    },
    {
      "id": 111,
      "seek": 51292,
      "start": 524.4799999999999,
      "end": 530.36,
      "text": " It almost feels like we internally because we're touching it all the time.",
      "tokens": [
        50942,
        467,
        1920,
        3417,
        411,
        321,
        19501,
        570,
        321,
        434,
        11175,
        309,
        439,
        264,
        565,
        13,
        51236
      ],
      "temperature": 0.0,
      "avg_logprob": -0.23424186426050522,
      "compression_ratio": 1.5081967213114753,
      "no_speech_prob": 0.0004898280603811145
    },
    {
      "id": 112,
      "seek": 51292,
      "start": 530.36,
      "end": 536.68,
      "text": " We have a much deeper understanding of it than our users, the majority of our users I would",
      "tokens": [
        51236,
        492,
        362,
        257,
        709,
        7731,
        3701,
        295,
        309,
        813,
        527,
        5022,
        11,
        264,
        6286,
        295,
        527,
        5022,
        286,
        576,
        51552
      ],
      "temperature": 0.0,
      "avg_logprob": -0.23424186426050522,
      "compression_ratio": 1.5081967213114753,
      "no_speech_prob": 0.0004898280603811145
    },
    {
      "id": 113,
      "seek": 53668,
      "start": 536.68,
      "end": 544.64,
      "text": " say, but also the admins or platform engineers who are managing their own runners have a very",
      "tokens": [
        50364,
        584,
        11,
        457,
        611,
        264,
        5910,
        1292,
        420,
        3663,
        11955,
        567,
        366,
        11642,
        641,
        1065,
        33892,
        362,
        257,
        588,
        50762
      ],
      "temperature": 0.0,
      "avg_logprob": -0.22657794383034777,
      "compression_ratio": 1.5654761904761905,
      "no_speech_prob": 0.03205691650509834
    },
    {
      "id": 114,
      "seek": 53668,
      "start": 544.64,
      "end": 548.92,
      "text": " similar understanding as we do.",
      "tokens": [
        50762,
        2531,
        3701,
        382,
        321,
        360,
        13,
        50976
      ],
      "temperature": 0.0,
      "avg_logprob": -0.22657794383034777,
      "compression_ratio": 1.5654761904761905,
      "no_speech_prob": 0.03205691650509834
    },
    {
      "id": 115,
      "seek": 53668,
      "start": 548.92,
      "end": 554.2399999999999,
      "text": " So it's hard to get it's hard for me to be like, they're not understanding it when we know",
      "tokens": [
        50976,
        407,
        309,
        311,
        1152,
        281,
        483,
        309,
        311,
        1152,
        337,
        385,
        281,
        312,
        411,
        11,
        436,
        434,
        406,
        3701,
        309,
        562,
        321,
        458,
        51242
      ],
      "temperature": 0.0,
      "avg_logprob": -0.22657794383034777,
      "compression_ratio": 1.5654761904761905,
      "no_speech_prob": 0.03205691650509834
    },
    {
      "id": 116,
      "seek": 53668,
      "start": 554.2399999999999,
      "end": 557.7199999999999,
      "text": " that they are because people love the feature.",
      "tokens": [
        51242,
        300,
        436,
        366,
        570,
        561,
        959,
        264,
        4111,
        13,
        51416
      ],
      "temperature": 0.0,
      "avg_logprob": -0.22657794383034777,
      "compression_ratio": 1.5654761904761905,
      "no_speech_prob": 0.03205691650509834
    },
    {
      "id": 117,
      "seek": 55772,
      "start": 558.72,
      "end": 567.96,
      "text": " The one thing that helped us like I know we are at a very, I would say, nascent stage in",
      "tokens": [
        50414,
        440,
        472,
        551,
        300,
        4254,
        505,
        411,
        286,
        458,
        321,
        366,
        412,
        257,
        588,
        11,
        286,
        576,
        584,
        11,
        5382,
        2207,
        3233,
        294,
        50876
      ],
      "temperature": 0.0,
      "avg_logprob": -0.24555319708746834,
      "compression_ratio": 1.656084656084656,
      "no_speech_prob": 0.052426956593990326
    },
    {
      "id": 118,
      "seek": 55772,
      "start": 567.96,
      "end": 572.72,
      "text": " the entire native secrets manager concept, but one thing that helped us was the competitor",
      "tokens": [
        50876,
        264,
        2302,
        8470,
        14093,
        6598,
        3410,
        11,
        457,
        472,
        551,
        300,
        4254,
        505,
        390,
        264,
        27266,
        51114
      ],
      "temperature": 0.0,
      "avg_logprob": -0.24555319708746834,
      "compression_ratio": 1.656084656084656,
      "no_speech_prob": 0.052426956593990326
    },
    {
      "id": 119,
      "seek": 55772,
      "start": 572.72,
      "end": 579.2,
      "text": " evaluation because like the entire team got to like take a peek into how the rest of the",
      "tokens": [
        51114,
        13344,
        570,
        411,
        264,
        2302,
        1469,
        658,
        281,
        411,
        747,
        257,
        19604,
        666,
        577,
        264,
        1472,
        295,
        264,
        51438
      ],
      "temperature": 0.0,
      "avg_logprob": -0.24555319708746834,
      "compression_ratio": 1.656084656084656,
      "no_speech_prob": 0.052426956593990326
    },
    {
      "id": 120,
      "seek": 55772,
      "start": 579.2,
      "end": 582.84,
      "text": " industry is referring to different concepts.",
      "tokens": [
        51438,
        3518,
        307,
        13761,
        281,
        819,
        10392,
        13,
        51620
      ],
      "temperature": 0.0,
      "avg_logprob": -0.24555319708746834,
      "compression_ratio": 1.656084656084656,
      "no_speech_prob": 0.052426956593990326
    },
    {
      "id": 121,
      "seek": 58284,
      "start": 582.84,
      "end": 589.9200000000001,
      "text": " And that also helped them relate with the research data that Erica had like created reports",
      "tokens": [
        50364,
        400,
        300,
        611,
        4254,
        552,
        10961,
        365,
        264,
        2132,
        1412,
        300,
        37429,
        632,
        411,
        2942,
        7122,
        50718
      ],
      "temperature": 0.0,
      "avg_logprob": -0.2796426057815552,
      "compression_ratio": 1.6238095238095238,
      "no_speech_prob": 0.006471116095781326
    },
    {
      "id": 122,
      "seek": 58284,
      "start": 589.9200000000001,
      "end": 591.2,
      "text": " on.",
      "tokens": [
        50718,
        322,
        13,
        50782
      ],
      "temperature": 0.0,
      "avg_logprob": -0.2796426057815552,
      "compression_ratio": 1.6238095238095238,
      "no_speech_prob": 0.006471116095781326
    },
    {
      "id": 123,
      "seek": 58284,
      "start": 591.2,
      "end": 596.6,
      "text": " So everything together, they were able to relate one thing to another and that helped in",
      "tokens": [
        50782,
        407,
        1203,
        1214,
        11,
        436,
        645,
        1075,
        281,
        10961,
        472,
        551,
        281,
        1071,
        293,
        300,
        4254,
        294,
        51052
      ],
      "temperature": 0.0,
      "avg_logprob": -0.2796426057815552,
      "compression_ratio": 1.6238095238095238,
      "no_speech_prob": 0.006471116095781326
    },
    {
      "id": 124,
      "seek": 58284,
      "start": 596.6,
      "end": 599.6,
      "text": " like getting that conviction.",
      "tokens": [
        51052,
        411,
        1242,
        300,
        24837,
        13,
        51202
      ],
      "temperature": 0.0,
      "avg_logprob": -0.2796426057815552,
      "compression_ratio": 1.6238095238095238,
      "no_speech_prob": 0.006471116095781326
    },
    {
      "id": 125,
      "seek": 58284,
      "start": 599.6,
      "end": 605.08,
      "text": " I could definitely start with that.",
      "tokens": [
        51202,
        286,
        727,
        2138,
        722,
        365,
        300,
        13,
        51476
      ],
      "temperature": 0.0,
      "avg_logprob": -0.2796426057815552,
      "compression_ratio": 1.6238095238095238,
      "no_speech_prob": 0.006471116095781326
    },
    {
      "id": 126,
      "seek": 58284,
      "start": 605.08,
      "end": 611.88,
      "text": " I have like a Google Doc with competitor terminology in it, but I didn't really share that",
      "tokens": [
        51476,
        286,
        362,
        411,
        257,
        3329,
        16024,
        365,
        27266,
        27575,
        294,
        309,
        11,
        457,
        286,
        994,
        380,
        534,
        2073,
        300,
        51816
      ],
      "temperature": 0.0,
      "avg_logprob": -0.2796426057815552,
      "compression_ratio": 1.6238095238095238,
      "no_speech_prob": 0.006471116095781326
    },
    {
      "id": 127,
      "seek": 61188,
      "start": 611.88,
      "end": 618.88,
      "text": " widely with our group so that can be a good place to start also.",
      "tokens": [
        50364,
        13371,
        365,
        527,
        1594,
        370,
        300,
        393,
        312,
        257,
        665,
        1081,
        281,
        722,
        611,
        13,
        50714
      ],
      "temperature": 0.0,
      "avg_logprob": -0.4981669393079034,
      "compression_ratio": 1.3557046979865772,
      "no_speech_prob": 0.0011481434339657426
    },
    {
      "id": 128,
      "seek": 61188,
      "start": 618.88,
      "end": 621.88,
      "text": " Thank you.",
      "tokens": [
        50714,
        1044,
        291,
        13,
        50864
      ],
      "temperature": 0.0,
      "avg_logprob": -0.4981669393079034,
      "compression_ratio": 1.3557046979865772,
      "no_speech_prob": 0.0011481434339657426
    },
    {
      "id": 129,
      "seek": 61188,
      "start": 621.88,
      "end": 623.88,
      "text": " Thank you.",
      "tokens": [
        50864,
        1044,
        291,
        13,
        50964
      ],
      "temperature": 0.0,
      "avg_logprob": -0.4981669393079034,
      "compression_ratio": 1.3557046979865772,
      "no_speech_prob": 0.0011481434339657426
    },
    {
      "id": 130,
      "seek": 61188,
      "start": 623.88,
      "end": 627.48,
      "text": " Yeah, just just certainly back to my question.",
      "tokens": [
        50964,
        865,
        11,
        445,
        445,
        3297,
        646,
        281,
        452,
        1168,
        13,
        51144
      ],
      "temperature": 0.0,
      "avg_logprob": -0.4981669393079034,
      "compression_ratio": 1.3557046979865772,
      "no_speech_prob": 0.0011481434339657426
    },
    {
      "id": 131,
      "seek": 61188,
      "start": 627.48,
      "end": 633.72,
      "text": " How did you decide to get survey data or you know target 100 people?",
      "tokens": [
        51144,
        1012,
        630,
        291,
        4536,
        281,
        483,
        8984,
        1412,
        420,
        291,
        458,
        3779,
        2319,
        561,
        30,
        51456
      ],
      "temperature": 0.0,
      "avg_logprob": -0.4981669393079034,
      "compression_ratio": 1.3557046979865772,
      "no_speech_prob": 0.0011481434339657426
    },
    {
      "id": 132,
      "seek": 63372,
      "start": 633.72,
      "end": 642.5600000000001,
      "text": " Yeah, so Erica suggested that we run the survey for I think a month and a half I think",
      "tokens": [
        50364,
        865,
        11,
        370,
        37429,
        10945,
        300,
        321,
        1190,
        264,
        8984,
        337,
        286,
        519,
        257,
        1618,
        293,
        257,
        1922,
        286,
        519,
        50806
      ],
      "temperature": 0.0,
      "avg_logprob": -0.19295769929885864,
      "compression_ratio": 1.4802259887005649,
      "no_speech_prob": 0.005404940340667963
    },
    {
      "id": 133,
      "seek": 63372,
      "start": 642.5600000000001,
      "end": 651.9200000000001,
      "text": " it was or we or when we get to 100 people like whichever one comes first and I she was",
      "tokens": [
        50806,
        309,
        390,
        420,
        321,
        420,
        562,
        321,
        483,
        281,
        2319,
        561,
        411,
        24123,
        472,
        1487,
        700,
        293,
        286,
        750,
        390,
        51274
      ],
      "temperature": 0.0,
      "avg_logprob": -0.19295769929885864,
      "compression_ratio": 1.4802259887005649,
      "no_speech_prob": 0.005404940340667963
    },
    {
      "id": 134,
      "seek": 63372,
      "start": 651.9200000000001,
      "end": 660.2,
      "text": " saying 100 because like that's the average number that they've been seeing trends lately",
      "tokens": [
        51274,
        1566,
        2319,
        570,
        411,
        300,
        311,
        264,
        4274,
        1230,
        300,
        436,
        600,
        668,
        2577,
        13892,
        12881,
        51688
      ],
      "temperature": 0.0,
      "avg_logprob": -0.19295769929885864,
      "compression_ratio": 1.4802259887005649,
      "no_speech_prob": 0.005404940340667963
    },
    {
      "id": 135,
      "seek": 66020,
      "start": 660.2,
      "end": 661.2,
      "text": " from surveys.",
      "tokens": [
        50364,
        490,
        22711,
        13,
        50414
      ],
      "temperature": 0.0,
      "avg_logprob": -0.1867239087126976,
      "compression_ratio": 1.5534883720930233,
      "no_speech_prob": 0.01945028454065323
    },
    {
      "id": 136,
      "seek": 66020,
      "start": 661.2,
      "end": 666.2,
      "text": " But if you have other suggestions, I am very open to it.",
      "tokens": [
        50414,
        583,
        498,
        291,
        362,
        661,
        13396,
        11,
        286,
        669,
        588,
        1269,
        281,
        309,
        13,
        50664
      ],
      "temperature": 0.0,
      "avg_logprob": -0.1867239087126976,
      "compression_ratio": 1.5534883720930233,
      "no_speech_prob": 0.01945028454065323
    },
    {
      "id": 137,
      "seek": 66020,
      "start": 666.2,
      "end": 677.6400000000001,
      "text": " Yeah, I've used like survey sample size calculators, which I can certainly link to here.",
      "tokens": [
        50664,
        865,
        11,
        286,
        600,
        1143,
        411,
        8984,
        6889,
        2744,
        4322,
        3391,
        11,
        597,
        286,
        393,
        3297,
        2113,
        281,
        510,
        13,
        51236
      ],
      "temperature": 0.0,
      "avg_logprob": -0.1867239087126976,
      "compression_ratio": 1.5534883720930233,
      "no_speech_prob": 0.01945028454065323
    },
    {
      "id": 138,
      "seek": 66020,
      "start": 677.6400000000001,
      "end": 682.5600000000001,
      "text": " So if you know like how many people are using a particular feature because I know that",
      "tokens": [
        51236,
        407,
        498,
        291,
        458,
        411,
        577,
        867,
        561,
        366,
        1228,
        257,
        1729,
        4111,
        570,
        286,
        458,
        300,
        51482
      ],
      "temperature": 0.0,
      "avg_logprob": -0.1867239087126976,
      "compression_ratio": 1.5534883720930233,
      "no_speech_prob": 0.01945028454065323
    },
    {
      "id": 139,
      "seek": 66020,
      "start": 682.5600000000001,
      "end": 688.72,
      "text": " we have like a lot of internal metrics on like who's a user of this stage group or that",
      "tokens": [
        51482,
        321,
        362,
        411,
        257,
        688,
        295,
        6920,
        16367,
        322,
        411,
        567,
        311,
        257,
        4195,
        295,
        341,
        3233,
        1594,
        420,
        300,
        51790
      ],
      "temperature": 0.0,
      "avg_logprob": -0.1867239087126976,
      "compression_ratio": 1.5534883720930233,
      "no_speech_prob": 0.01945028454065323
    },
    {
      "id": 140,
      "seek": 68872,
      "start": 688.72,
      "end": 691.08,
      "text": " stage group.",
      "tokens": [
        50364,
        3233,
        1594,
        13,
        50482
      ],
      "temperature": 0.0,
      "avg_logprob": -0.2987326345136089,
      "compression_ratio": 1.3696969696969696,
      "no_speech_prob": 0.00330843566916883
    },
    {
      "id": 141,
      "seek": 68872,
      "start": 691.08,
      "end": 700.44,
      "text": " You could use that as a little bit of a guide until like okay if our population is X, then",
      "tokens": [
        50482,
        509,
        727,
        764,
        300,
        382,
        257,
        707,
        857,
        295,
        257,
        5934,
        1826,
        411,
        1392,
        498,
        527,
        4415,
        307,
        1783,
        11,
        550,
        50950
      ],
      "temperature": 0.0,
      "avg_logprob": -0.2987326345136089,
      "compression_ratio": 1.3696969696969696,
      "no_speech_prob": 0.00330843566916883
    },
    {
      "id": 142,
      "seek": 68872,
      "start": 700.44,
      "end": 707.6,
      "text": " we should survey this many people to get somewhat of a representative sample size.",
      "tokens": [
        50950,
        321,
        820,
        8984,
        341,
        867,
        561,
        281,
        483,
        8344,
        295,
        257,
        12424,
        6889,
        2744,
        13,
        51308
      ],
      "temperature": 0.0,
      "avg_logprob": -0.2987326345136089,
      "compression_ratio": 1.3696969696969696,
      "no_speech_prob": 0.00330843566916883
    },
    {
      "id": 143,
      "seek": 68872,
      "start": 707.6,
      "end": 711.72,
      "text": " But yeah, I'll include one below.",
      "tokens": [
        51308,
        583,
        1338,
        11,
        286,
        603,
        4090,
        472,
        2507,
        13,
        51514
      ],
      "temperature": 0.0,
      "avg_logprob": -0.2987326345136089,
      "compression_ratio": 1.3696969696969696,
      "no_speech_prob": 0.00330843566916883
    },
    {
      "id": 144,
      "seek": 68872,
      "start": 711.72,
      "end": 713.48,
      "text": " Okay.",
      "tokens": [
        51514,
        1033,
        13,
        51602
      ],
      "temperature": 0.0,
      "avg_logprob": -0.2987326345136089,
      "compression_ratio": 1.3696969696969696,
      "no_speech_prob": 0.00330843566916883
    },
    {
      "id": 145,
      "seek": 71348,
      "start": 713.48,
      "end": 718.08,
      "text": " I think another thing was that I was kind of saying that we need to move fast with this.",
      "tokens": [
        50364,
        286,
        519,
        1071,
        551,
        390,
        300,
        286,
        390,
        733,
        295,
        1566,
        300,
        321,
        643,
        281,
        1286,
        2370,
        365,
        341,
        13,
        50594
      ],
      "temperature": 0.0,
      "avg_logprob": -0.34084320068359375,
      "compression_ratio": 1.5561497326203209,
      "no_speech_prob": 0.0005638896836899221
    },
    {
      "id": 146,
      "seek": 71348,
      "start": 718.08,
      "end": 723.48,
      "text": " So she probably changed her answer based on that.",
      "tokens": [
        50594,
        407,
        750,
        1391,
        3105,
        720,
        1867,
        2361,
        322,
        300,
        13,
        50864
      ],
      "temperature": 0.0,
      "avg_logprob": -0.34084320068359375,
      "compression_ratio": 1.5561497326203209,
      "no_speech_prob": 0.0005638896836899221
    },
    {
      "id": 147,
      "seek": 71348,
      "start": 723.48,
      "end": 724.48,
      "text": " Okay.",
      "tokens": [
        50864,
        1033,
        13,
        50914
      ],
      "temperature": 0.0,
      "avg_logprob": -0.34084320068359375,
      "compression_ratio": 1.5561497326203209,
      "no_speech_prob": 0.0005638896836899221
    },
    {
      "id": 148,
      "seek": 71348,
      "start": 724.48,
      "end": 729.08,
      "text": " Yeah, I mean that that certainly was a factor too.",
      "tokens": [
        50914,
        865,
        11,
        286,
        914,
        300,
        300,
        3297,
        390,
        257,
        5952,
        886,
        13,
        51144
      ],
      "temperature": 0.0,
      "avg_logprob": -0.34084320068359375,
      "compression_ratio": 1.5561497326203209,
      "no_speech_prob": 0.0005638896836899221
    },
    {
      "id": 149,
      "seek": 71348,
      "start": 729.08,
      "end": 730.6800000000001,
      "text": " Yeah, yeah.",
      "tokens": [
        51144,
        865,
        11,
        1338,
        13,
        51224
      ],
      "temperature": 0.0,
      "avg_logprob": -0.34084320068359375,
      "compression_ratio": 1.5561497326203209,
      "no_speech_prob": 0.0005638896836899221
    },
    {
      "id": 150,
      "seek": 71348,
      "start": 730.6800000000001,
      "end": 737.48,
      "text": " But I still would like to look at that survey sample size and see what we do.",
      "tokens": [
        51224,
        583,
        286,
        920,
        576,
        411,
        281,
        574,
        412,
        300,
        8984,
        6889,
        2744,
        293,
        536,
        437,
        321,
        360,
        13,
        51564
      ],
      "temperature": 0.0,
      "avg_logprob": -0.34084320068359375,
      "compression_ratio": 1.5561497326203209,
      "no_speech_prob": 0.0005638896836899221
    },
    {
      "id": 151,
      "seek": 71348,
      "start": 737.48,
      "end": 738.48,
      "text": " Yeah.",
      "tokens": [
        51564,
        865,
        13,
        51614
      ],
      "temperature": 0.0,
      "avg_logprob": -0.34084320068359375,
      "compression_ratio": 1.5561497326203209,
      "no_speech_prob": 0.0005638896836899221
    },
    {
      "id": 152,
      "seek": 73848,
      "start": 738.48,
      "end": 741.28,
      "text": " So I added a link there.",
      "tokens": [
        50364,
        407,
        286,
        3869,
        257,
        2113,
        456,
        13,
        50504
      ],
      "temperature": 0.0,
      "avg_logprob": -0.20114298926459417,
      "compression_ratio": 1.5806451612903225,
      "no_speech_prob": 0.018096573650836945
    },
    {
      "id": 153,
      "seek": 73848,
      "start": 741.28,
      "end": 745.84,
      "text": " If you click into it, you'll notice that it has like three different sections.",
      "tokens": [
        50504,
        759,
        291,
        2052,
        666,
        309,
        11,
        291,
        603,
        3449,
        300,
        309,
        575,
        411,
        1045,
        819,
        10863,
        13,
        50732
      ],
      "temperature": 0.0,
      "avg_logprob": -0.20114298926459417,
      "compression_ratio": 1.5806451612903225,
      "no_speech_prob": 0.018096573650836945
    },
    {
      "id": 154,
      "seek": 73848,
      "start": 745.84,
      "end": 750.6800000000001,
      "text": " You don't really have to mess with confidence interval or margin of error.",
      "tokens": [
        50732,
        509,
        500,
        380,
        534,
        362,
        281,
        2082,
        365,
        6687,
        15035,
        420,
        10270,
        295,
        6713,
        13,
        50974
      ],
      "temperature": 0.0,
      "avg_logprob": -0.20114298926459417,
      "compression_ratio": 1.5806451612903225,
      "no_speech_prob": 0.018096573650836945
    },
    {
      "id": 155,
      "seek": 73848,
      "start": 750.6800000000001,
      "end": 755.6,
      "text": " You just have to enter the population size.",
      "tokens": [
        50974,
        509,
        445,
        362,
        281,
        3242,
        264,
        4415,
        2744,
        13,
        51220
      ],
      "temperature": 0.0,
      "avg_logprob": -0.20114298926459417,
      "compression_ratio": 1.5806451612903225,
      "no_speech_prob": 0.018096573650836945
    },
    {
      "id": 156,
      "seek": 73848,
      "start": 755.6,
      "end": 762.32,
      "text": " So that's the number that you would have to either guesstimate or you know get some analytics",
      "tokens": [
        51220,
        407,
        300,
        311,
        264,
        1230,
        300,
        291,
        576,
        362,
        281,
        2139,
        695,
        279,
        372,
        2905,
        420,
        291,
        458,
        483,
        512,
        15370,
        51556
      ],
      "temperature": 0.0,
      "avg_logprob": -0.20114298926459417,
      "compression_ratio": 1.5806451612903225,
      "no_speech_prob": 0.018096573650836945
    },
    {
      "id": 157,
      "seek": 73848,
      "start": 762.32,
      "end": 766.48,
      "text": " on to populate that field.",
      "tokens": [
        51556,
        322,
        281,
        1665,
        5256,
        300,
        2519,
        13,
        51764
      ],
      "temperature": 0.0,
      "avg_logprob": -0.20114298926459417,
      "compression_ratio": 1.5806451612903225,
      "no_speech_prob": 0.018096573650836945
    },
    {
      "id": 158,
      "seek": 76648,
      "start": 767.48,
      "end": 770.48,
      "text": " Okay, thank you.",
      "tokens": [
        50414,
        1033,
        11,
        1309,
        291,
        13,
        50564
      ],
      "temperature": 0.0,
      "avg_logprob": -0.33838094983782085,
      "compression_ratio": 1.368421052631579,
      "no_speech_prob": 0.00403293501585722
    },
    {
      "id": 159,
      "seek": 76648,
      "start": 770.48,
      "end": 772.48,
      "text": " This is nice.",
      "tokens": [
        50564,
        639,
        307,
        1481,
        13,
        50664
      ],
      "temperature": 0.0,
      "avg_logprob": -0.33838094983782085,
      "compression_ratio": 1.368421052631579,
      "no_speech_prob": 0.00403293501585722
    },
    {
      "id": 160,
      "seek": 76648,
      "start": 772.48,
      "end": 774.48,
      "text": " Great.",
      "tokens": [
        50664,
        3769,
        13,
        50764
      ],
      "temperature": 0.0,
      "avg_logprob": -0.33838094983782085,
      "compression_ratio": 1.368421052631579,
      "no_speech_prob": 0.00403293501585722
    },
    {
      "id": 161,
      "seek": 76648,
      "start": 774.48,
      "end": 777.48,
      "text": " I think the sharing will.",
      "tokens": [
        50764,
        286,
        519,
        264,
        5414,
        486,
        13,
        50914
      ],
      "temperature": 0.0,
      "avg_logprob": -0.33838094983782085,
      "compression_ratio": 1.368421052631579,
      "no_speech_prob": 0.00403293501585722
    },
    {
      "id": 162,
      "seek": 76648,
      "start": 777.48,
      "end": 779.48,
      "text": " Okay, right.",
      "tokens": [
        50914,
        1033,
        11,
        558,
        13,
        51014
      ],
      "temperature": 0.0,
      "avg_logprob": -0.33838094983782085,
      "compression_ratio": 1.368421052631579,
      "no_speech_prob": 0.00403293501585722
    },
    {
      "id": 163,
      "seek": 76648,
      "start": 779.48,
      "end": 782.48,
      "text": " Yeah.",
      "tokens": [
        51014,
        865,
        13,
        51164
      ],
      "temperature": 0.0,
      "avg_logprob": -0.33838094983782085,
      "compression_ratio": 1.368421052631579,
      "no_speech_prob": 0.00403293501585722
    },
    {
      "id": 164,
      "seek": 76648,
      "start": 782.48,
      "end": 789.48,
      "text": " So is everyone that I noticed they still typing going on.",
      "tokens": [
        51164,
        407,
        307,
        1518,
        300,
        286,
        5694,
        436,
        920,
        18444,
        516,
        322,
        13,
        51514
      ],
      "temperature": 0.0,
      "avg_logprob": -0.33838094983782085,
      "compression_ratio": 1.368421052631579,
      "no_speech_prob": 0.00403293501585722
    },
    {
      "id": 165,
      "seek": 76648,
      "start": 789.48,
      "end": 790.48,
      "text": " Good.",
      "tokens": [
        51514,
        2205,
        13,
        51564
      ],
      "temperature": 0.0,
      "avg_logprob": -0.33838094983782085,
      "compression_ratio": 1.368421052631579,
      "no_speech_prob": 0.00403293501585722
    },
    {
      "id": 166,
      "seek": 76648,
      "start": 790.48,
      "end": 795.48,
      "text": " So just a small update with the outcome of the design sprint.",
      "tokens": [
        51564,
        407,
        445,
        257,
        1359,
        5623,
        365,
        264,
        9700,
        295,
        264,
        1715,
        25075,
        13,
        51814
      ],
      "temperature": 0.0,
      "avg_logprob": -0.33838094983782085,
      "compression_ratio": 1.368421052631579,
      "no_speech_prob": 0.00403293501585722
    },
    {
      "id": 167,
      "seek": 79548,
      "start": 795.48,
      "end": 812.48,
      "text": " We were trying to figure out the best way to go forward with the research here and we've started with a just internal customer interviews with the SRA and delivery team around this service concept getting a better idea of what they consider services.",
      "tokens": [
        50364,
        492,
        645,
        1382,
        281,
        2573,
        484,
        264,
        1151,
        636,
        281,
        352,
        2128,
        365,
        264,
        2132,
        510,
        293,
        321,
        600,
        1409,
        365,
        257,
        445,
        6920,
        5474,
        12318,
        365,
        264,
        318,
        3750,
        293,
        8982,
        1469,
        926,
        341,
        2643,
        3410,
        1242,
        257,
        1101,
        1558,
        295,
        437,
        436,
        1949,
        3328,
        13,
        51214
      ],
      "temperature": 0.0,
      "avg_logprob": -0.13951130656452923,
      "compression_ratio": 1.6306306306306306,
      "no_speech_prob": 0.0028591447044163942
    },
    {
      "id": 168,
      "seek": 79548,
      "start": 812.48,
      "end": 818.48,
      "text": " How we would populate that list and after we're done those internal interviews, which I think we have like six.",
      "tokens": [
        51214,
        1012,
        321,
        576,
        1665,
        5256,
        300,
        1329,
        293,
        934,
        321,
        434,
        1096,
        729,
        6920,
        12318,
        11,
        597,
        286,
        519,
        321,
        362,
        411,
        2309,
        13,
        51514
      ],
      "temperature": 0.0,
      "avg_logprob": -0.13951130656452923,
      "compression_ratio": 1.6306306306306306,
      "no_speech_prob": 0.0028591447044163942
    },
    {
      "id": 169,
      "seek": 81848,
      "start": 818.48,
      "end": 820.48,
      "text": " Happening this week.",
      "tokens": [
        50364,
        7412,
        4559,
        341,
        1243,
        13,
        50464
      ],
      "temperature": 0.0,
      "avg_logprob": -0.12980555416492934,
      "compression_ratio": 1.6071428571428572,
      "no_speech_prob": 0.018653610721230507
    },
    {
      "id": 170,
      "seek": 81848,
      "start": 820.48,
      "end": 823.48,
      "text": " Big thank you to Victor for scheduling them.",
      "tokens": [
        50464,
        5429,
        1309,
        291,
        281,
        15777,
        337,
        29055,
        552,
        13,
        50614
      ],
      "temperature": 0.0,
      "avg_logprob": -0.12980555416492934,
      "compression_ratio": 1.6071428571428572,
      "no_speech_prob": 0.018653610721230507
    },
    {
      "id": 171,
      "seek": 81848,
      "start": 823.48,
      "end": 827.48,
      "text": " We'll be able to move forward with like external user testing.",
      "tokens": [
        50614,
        492,
        603,
        312,
        1075,
        281,
        1286,
        2128,
        365,
        411,
        8320,
        4195,
        4997,
        13,
        50814
      ],
      "temperature": 0.0,
      "avg_logprob": -0.12980555416492934,
      "compression_ratio": 1.6071428571428572,
      "no_speech_prob": 0.018653610721230507
    },
    {
      "id": 172,
      "seek": 81848,
      "start": 827.48,
      "end": 829.48,
      "text": " Just an update on that front.",
      "tokens": [
        50814,
        1449,
        364,
        5623,
        322,
        300,
        1868,
        13,
        50914
      ],
      "temperature": 0.0,
      "avg_logprob": -0.12980555416492934,
      "compression_ratio": 1.6071428571428572,
      "no_speech_prob": 0.018653610721230507
    },
    {
      "id": 173,
      "seek": 81848,
      "start": 829.48,
      "end": 836.48,
      "text": " The other one is I'll be working on these design issue around designing for external jobs.",
      "tokens": [
        50914,
        440,
        661,
        472,
        307,
        286,
        603,
        312,
        1364,
        322,
        613,
        1715,
        2734,
        926,
        14685,
        337,
        8320,
        4782,
        13,
        51264
      ],
      "temperature": 0.0,
      "avg_logprob": -0.12980555416492934,
      "compression_ratio": 1.6071428571428572,
      "no_speech_prob": 0.018653610721230507
    },
    {
      "id": 174,
      "seek": 81848,
      "start": 836.48,
      "end": 843.48,
      "text": " And I believe this touches kind of like the job details page, the job list page and the pipeline details page.",
      "tokens": [
        51264,
        400,
        286,
        1697,
        341,
        17431,
        733,
        295,
        411,
        264,
        1691,
        4365,
        3028,
        11,
        264,
        1691,
        1329,
        3028,
        293,
        264,
        15517,
        4365,
        3028,
        13,
        51614
      ],
      "temperature": 0.0,
      "avg_logprob": -0.12980555416492934,
      "compression_ratio": 1.6071428571428572,
      "no_speech_prob": 0.018653610721230507
    },
    {
      "id": 175,
      "seek": 84348,
      "start": 843.48,
      "end": 859.48,
      "text": " I'm not sure if any of those fell into the verified teams that have designers, but if they do just let me know so I can involve you and kind of feedback for these pages.",
      "tokens": [
        50364,
        286,
        478,
        406,
        988,
        498,
        604,
        295,
        729,
        5696,
        666,
        264,
        31197,
        5491,
        300,
        362,
        16196,
        11,
        457,
        498,
        436,
        360,
        445,
        718,
        385,
        458,
        370,
        286,
        393,
        9494,
        291,
        293,
        733,
        295,
        5824,
        337,
        613,
        7183,
        13,
        51164
      ],
      "temperature": 0.0,
      "avg_logprob": -0.13034689994085402,
      "compression_ratio": 1.3,
      "no_speech_prob": 0.01210870686918497
    },
    {
      "id": 176,
      "seek": 85948,
      "start": 859.48,
      "end": 874.48,
      "text": " I can help because I like I took part in a lot of free searches for those pages so I can help with any feedback, but otherwise there's no designer for a pipeline execution.",
      "tokens": [
        50364,
        286,
        393,
        854,
        570,
        286,
        411,
        286,
        1890,
        644,
        294,
        257,
        688,
        295,
        1737,
        26701,
        337,
        729,
        7183,
        370,
        286,
        393,
        854,
        365,
        604,
        5824,
        11,
        457,
        5911,
        456,
        311,
        572,
        11795,
        337,
        257,
        15517,
        15058,
        13,
        51114
      ],
      "temperature": 0.0,
      "avg_logprob": -0.24429414330459223,
      "compression_ratio": 1.3543307086614174,
      "no_speech_prob": 0.023078909143805504
    },
    {
      "id": 177,
      "seek": 87448,
      "start": 874.48,
      "end": 890.48,
      "text": " I can also help because runner has that very small portion in this, but there is at least a link to the runner that runs the job and then there's a lot of output from runner in the job log that might be helpful.",
      "tokens": [
        50364,
        286,
        393,
        611,
        854,
        570,
        24376,
        575,
        300,
        588,
        1359,
        8044,
        294,
        341,
        11,
        457,
        456,
        307,
        412,
        1935,
        257,
        2113,
        281,
        264,
        24376,
        300,
        6676,
        264,
        1691,
        293,
        550,
        456,
        311,
        257,
        688,
        295,
        5598,
        490,
        24376,
        294,
        264,
        1691,
        3565,
        300,
        1062,
        312,
        4961,
        13,
        51164
      ],
      "temperature": 0.0,
      "avg_logprob": -0.09299955368041993,
      "compression_ratio": 1.6508620689655173,
      "no_speech_prob": 0.04370921477675438
    },
    {
      "id": 178,
      "seek": 87448,
      "start": 890.48,
      "end": 902.48,
      "text": " Sounds good. Yeah, this is mostly just designing it around this new like external job thing and what visuals will show, but I think it'll touch like all of these pages so.",
      "tokens": [
        51164,
        14576,
        665,
        13,
        865,
        11,
        341,
        307,
        5240,
        445,
        14685,
        309,
        926,
        341,
        777,
        411,
        8320,
        1691,
        551,
        293,
        437,
        26035,
        486,
        855,
        11,
        457,
        286,
        519,
        309,
        603,
        2557,
        411,
        439,
        295,
        613,
        7183,
        370,
        13,
        51764
      ],
      "temperature": 0.0,
      "avg_logprob": -0.09299955368041993,
      "compression_ratio": 1.6508620689655173,
      "no_speech_prob": 0.04370921477675438
    },
    {
      "id": 179,
      "seek": 90248,
      "start": 902.48,
      "end": 905.48,
      "text": " Figuring that out as I go forward.",
      "tokens": [
        50364,
        22443,
        1345,
        300,
        484,
        382,
        286,
        352,
        2128,
        13,
        50514
      ],
      "temperature": 0.0,
      "avg_logprob": -0.210646103168356,
      "compression_ratio": 1.36875,
      "no_speech_prob": 0.020309647545218468
    },
    {
      "id": 180,
      "seek": 90248,
      "start": 905.48,
      "end": 919.48,
      "text": " And also please leave me in because pipeline authoring it's not just owning, but we are also touching some pipeline details page and then job log as well, so I'm happy to help.",
      "tokens": [
        50514,
        400,
        611,
        1767,
        1856,
        385,
        294,
        570,
        15517,
        3793,
        278,
        309,
        311,
        406,
        445,
        29820,
        11,
        457,
        321,
        366,
        611,
        11175,
        512,
        15517,
        4365,
        3028,
        293,
        550,
        1691,
        3565,
        382,
        731,
        11,
        370,
        286,
        478,
        2055,
        281,
        854,
        13,
        51214
      ],
      "temperature": 0.0,
      "avg_logprob": -0.210646103168356,
      "compression_ratio": 1.36875,
      "no_speech_prob": 0.020309647545218468
    },
    {
      "id": 181,
      "seek": 90248,
      "start": 919.48,
      "end": 921.48,
      "text": " Thanks.",
      "tokens": [
        51214,
        2561,
        13,
        51314
      ],
      "temperature": 0.0,
      "avg_logprob": -0.210646103168356,
      "compression_ratio": 1.36875,
      "no_speech_prob": 0.020309647545218468
    },
    {
      "id": 182,
      "seek": 92148,
      "start": 921.48,
      "end": 928.48,
      "text": " I'm like, can I ask a question just on like what external job means.",
      "tokens": [
        50364,
        286,
        478,
        411,
        11,
        393,
        286,
        1029,
        257,
        1168,
        445,
        322,
        411,
        437,
        8320,
        1691,
        1355,
        13,
        50714
      ],
      "temperature": 0.0,
      "avg_logprob": -0.20680722103843205,
      "compression_ratio": 1.4919786096256684,
      "no_speech_prob": 0.02770661562681198
    },
    {
      "id": 183,
      "seek": 92148,
      "start": 928.48,
      "end": 941.48,
      "text": " Yeah, so this is one that I'm just starting into, so I have to kind of look for it, but this is as far as I know a say I job that's being executed outside of Git lab.",
      "tokens": [
        50714,
        865,
        11,
        370,
        341,
        307,
        472,
        300,
        286,
        478,
        445,
        2891,
        666,
        11,
        370,
        286,
        362,
        281,
        733,
        295,
        574,
        337,
        309,
        11,
        457,
        341,
        307,
        382,
        1400,
        382,
        286,
        458,
        257,
        584,
        286,
        1691,
        300,
        311,
        885,
        17577,
        2380,
        295,
        16939,
        2715,
        13,
        51364
      ],
      "temperature": 0.0,
      "avg_logprob": -0.20680722103843205,
      "compression_ratio": 1.4919786096256684,
      "no_speech_prob": 0.02770661562681198
    },
    {
      "id": 184,
      "seek": 92148,
      "start": 941.48,
      "end": 947.48,
      "text": " Like on actions, so for example on actions.",
      "tokens": [
        51364,
        1743,
        322,
        5909,
        11,
        370,
        337,
        1365,
        322,
        5909,
        13,
        51664
      ],
      "temperature": 0.0,
      "avg_logprob": -0.20680722103843205,
      "compression_ratio": 1.4919786096256684,
      "no_speech_prob": 0.02770661562681198
    },
    {
      "id": 185,
      "seek": 94748,
      "start": 948.48,
      "end": 950.48,
      "text": " Okay.",
      "tokens": [
        50414,
        1033,
        13,
        50514
      ],
      "temperature": 0.0,
      "avg_logprob": -0.23712289655530774,
      "compression_ratio": 1.5423728813559323,
      "no_speech_prob": 0.002449718536809087
    },
    {
      "id": 186,
      "seek": 94748,
      "start": 950.48,
      "end": 952.48,
      "text": " Like it's.",
      "tokens": [
        50514,
        1743,
        309,
        311,
        13,
        50614
      ],
      "temperature": 0.0,
      "avg_logprob": -0.23712289655530774,
      "compression_ratio": 1.5423728813559323,
      "no_speech_prob": 0.002449718536809087
    },
    {
      "id": 187,
      "seek": 94748,
      "start": 952.48,
      "end": 964.48,
      "text": " No worries. It's so what this is is I'm reading this that is like it's adding a new job status, which will represent CI jobs that are currently being executed by a service external to Git lab.",
      "tokens": [
        50614,
        883,
        16340,
        13,
        467,
        311,
        370,
        437,
        341,
        307,
        307,
        286,
        478,
        3760,
        341,
        300,
        307,
        411,
        309,
        311,
        5127,
        257,
        777,
        1691,
        6558,
        11,
        597,
        486,
        2906,
        37777,
        4782,
        300,
        366,
        4362,
        885,
        17577,
        538,
        257,
        2643,
        8320,
        281,
        16939,
        2715,
        13,
        51214
      ],
      "temperature": 0.0,
      "avg_logprob": -0.23712289655530774,
      "compression_ratio": 1.5423728813559323,
      "no_speech_prob": 0.002449718536809087
    },
    {
      "id": 188,
      "seek": 94748,
      "start": 964.48,
      "end": 972.48,
      "text": " So adding in like appending kind of state as that is being run.",
      "tokens": [
        51214,
        407,
        5127,
        294,
        411,
        724,
        2029,
        733,
        295,
        1785,
        382,
        300,
        307,
        885,
        1190,
        13,
        51614
      ],
      "temperature": 0.0,
      "avg_logprob": -0.23712289655530774,
      "compression_ratio": 1.5423728813559323,
      "no_speech_prob": 0.002449718536809087
    },
    {
      "id": 189,
      "seek": 97248,
      "start": 972.48,
      "end": 984.48,
      "text": " And then yeah, so I'll actually link the engineering issue here that explains it a bit more than the design issue.",
      "tokens": [
        50364,
        400,
        550,
        1338,
        11,
        370,
        286,
        603,
        767,
        2113,
        264,
        7043,
        2734,
        510,
        300,
        13948,
        309,
        257,
        857,
        544,
        813,
        264,
        1715,
        2734,
        13,
        50964
      ],
      "temperature": 0.0,
      "avg_logprob": -0.16266084889896582,
      "compression_ratio": 1.4567901234567902,
      "no_speech_prob": 0.001424371381290257
    },
    {
      "id": 190,
      "seek": 97248,
      "start": 990.48,
      "end": 993.48,
      "text": " That is helpful.",
      "tokens": [
        51264,
        663,
        307,
        4961,
        13,
        51414
      ],
      "temperature": 0.0,
      "avg_logprob": -0.16266084889896582,
      "compression_ratio": 1.4567901234567902,
      "no_speech_prob": 0.001424371381290257
    },
    {
      "id": 191,
      "seek": 97248,
      "start": 993.48,
      "end": 1000.48,
      "text": " I'm also reading the epic and it says external jobs are not picked up by a runner, so it might not even.",
      "tokens": [
        51414,
        286,
        478,
        611,
        3760,
        264,
        13581,
        293,
        309,
        1619,
        8320,
        4782,
        366,
        406,
        6183,
        493,
        538,
        257,
        24376,
        11,
        370,
        309,
        1062,
        406,
        754,
        13,
        51764
      ],
      "temperature": 0.0,
      "avg_logprob": -0.16266084889896582,
      "compression_ratio": 1.4567901234567902,
      "no_speech_prob": 0.001424371381290257
    },
    {
      "id": 192,
      "seek": 100048,
      "start": 1000.48,
      "end": 1009.48,
      "text": " And actually with me and if so, you don't have to work me in.",
      "tokens": [
        50364,
        400,
        767,
        365,
        385,
        293,
        498,
        370,
        11,
        291,
        500,
        380,
        362,
        281,
        589,
        385,
        294,
        13,
        50814
      ],
      "temperature": 0.0,
      "avg_logprob": -0.30454510655896416,
      "compression_ratio": 1.4258064516129032,
      "no_speech_prob": 0.002234810031950474
    },
    {
      "id": 193,
      "seek": 100048,
      "start": 1009.48,
      "end": 1023.48,
      "text": " I heard about this use case and some research is an else in the conference lately, but I don't know I still cannot drop animals if I need to read through this.",
      "tokens": [
        50814,
        286,
        2198,
        466,
        341,
        764,
        1389,
        293,
        512,
        2132,
        307,
        364,
        1646,
        294,
        264,
        7586,
        12881,
        11,
        457,
        286,
        500,
        380,
        458,
        286,
        920,
        2644,
        3270,
        4882,
        498,
        286,
        643,
        281,
        1401,
        807,
        341,
        13,
        51514
      ],
      "temperature": 0.0,
      "avg_logprob": -0.30454510655896416,
      "compression_ratio": 1.4258064516129032,
      "no_speech_prob": 0.002234810031950474
    },
    {
      "id": 194,
      "seek": 102348,
      "start": 1023.48,
      "end": 1030.48,
      "text": " Yeah, this is new for 16 three I just started looking into it. So as I learn more.",
      "tokens": [
        50364,
        865,
        11,
        341,
        307,
        777,
        337,
        3165,
        1045,
        286,
        445,
        1409,
        1237,
        666,
        309,
        13,
        407,
        382,
        286,
        1466,
        544,
        13,
        50714
      ],
      "temperature": 0.0,
      "avg_logprob": -0.15143943900492654,
      "compression_ratio": 1.462962962962963,
      "no_speech_prob": 0.003964429721236229
    },
    {
      "id": 195,
      "seek": 102348,
      "start": 1030.48,
      "end": 1041.48,
      "text": " I'll reach out, but it's a fairly new thing we're looking into. So I don't have all the details right now, but I'm sure over the next week I'll have more.",
      "tokens": [
        50714,
        286,
        603,
        2524,
        484,
        11,
        457,
        309,
        311,
        257,
        6457,
        777,
        551,
        321,
        434,
        1237,
        666,
        13,
        407,
        286,
        500,
        380,
        362,
        439,
        264,
        4365,
        558,
        586,
        11,
        457,
        286,
        478,
        988,
        670,
        264,
        958,
        1243,
        286,
        603,
        362,
        544,
        13,
        51264
      ],
      "temperature": 0.0,
      "avg_logprob": -0.15143943900492654,
      "compression_ratio": 1.462962962962963,
      "no_speech_prob": 0.003964429721236229
    },
    {
      "id": 196,
      "seek": 105348,
      "start": 1053.48,
      "end": 1069.48,
      "text": " So I'm mostly focusing on just one thing besides a couple of community contributions this week, and that's the like finishing the PC prototype.",
      "tokens": [
        50364,
        407,
        286,
        478,
        5240,
        8416,
        322,
        445,
        472,
        551,
        11868,
        257,
        1916,
        295,
        1768,
        15725,
        341,
        1243,
        11,
        293,
        300,
        311,
        264,
        411,
        12693,
        264,
        6465,
        19475,
        13,
        51164
      ],
      "temperature": 0.0,
      "avg_logprob": -0.2980605854707606,
      "compression_ratio": 1.375,
      "no_speech_prob": 0.3797546625137329
    },
    {
      "id": 197,
      "seek": 105348,
      "start": 1069.48,
      "end": 1074.48,
      "text": " So I created the user map and flow on like jam, it looks so good.",
      "tokens": [
        51164,
        407,
        286,
        2942,
        264,
        4195,
        4471,
        293,
        3095,
        322,
        411,
        7872,
        11,
        309,
        1542,
        370,
        665,
        13,
        51414
      ],
      "temperature": 0.0,
      "avg_logprob": -0.2980605854707606,
      "compression_ratio": 1.375,
      "no_speech_prob": 0.3797546625137329
    },
    {
      "id": 198,
      "seek": 107448,
      "start": 1074.48,
      "end": 1089.48,
      "text": " And based on this particular story mapping, I would be like designing the different pages or the different actions for this flow and that's for this.",
      "tokens": [
        50364,
        400,
        2361,
        322,
        341,
        1729,
        1657,
        18350,
        11,
        286,
        576,
        312,
        411,
        14685,
        264,
        819,
        7183,
        420,
        264,
        819,
        5909,
        337,
        341,
        3095,
        293,
        300,
        311,
        337,
        341,
        13,
        51114
      ],
      "temperature": 0.0,
      "avg_logprob": -0.09773449464277788,
      "compression_ratio": 1.5789473684210527,
      "no_speech_prob": 0.4002090096473694
    },
    {
      "id": 199,
      "seek": 107448,
      "start": 1089.48,
      "end": 1101.48,
      "text": " Another thing that I wanted to share was I'm starting to add like I've added a slot to my calendar from starting next week for every Thursday morning.",
      "tokens": [
        51114,
        3996,
        551,
        300,
        286,
        1415,
        281,
        2073,
        390,
        286,
        478,
        2891,
        281,
        909,
        411,
        286,
        600,
        3869,
        257,
        14747,
        281,
        452,
        12183,
        490,
        2891,
        958,
        1243,
        337,
        633,
        10383,
        2446,
        13,
        51714
      ],
      "temperature": 0.0,
      "avg_logprob": -0.09773449464277788,
      "compression_ratio": 1.5789473684210527,
      "no_speech_prob": 0.4002090096473694
    },
    {
      "id": 200,
      "seek": 110148,
      "start": 1101.48,
      "end": 1108.48,
      "text": " And for time that suits all of us, like all the members in the CICD team at least.",
      "tokens": [
        50364,
        400,
        337,
        565,
        300,
        15278,
        439,
        295,
        505,
        11,
        411,
        439,
        264,
        2679,
        294,
        264,
        383,
        2532,
        35,
        1469,
        412,
        1935,
        13,
        50714
      ],
      "temperature": 0.0,
      "avg_logprob": -0.19994172384572584,
      "compression_ratio": 1.665158371040724,
      "no_speech_prob": 0.18689681589603424
    },
    {
      "id": 201,
      "seek": 110148,
      "start": 1108.48,
      "end": 1115.48,
      "text": " And this is for like to have a sync time with me to discuss just anything that relates to public presence.",
      "tokens": [
        50714,
        400,
        341,
        307,
        337,
        411,
        281,
        362,
        257,
        20271,
        565,
        365,
        385,
        281,
        2248,
        445,
        1340,
        300,
        16155,
        281,
        1908,
        6814,
        13,
        51064
      ],
      "temperature": 0.0,
      "avg_logprob": -0.19994172384572584,
      "compression_ratio": 1.665158371040724,
      "no_speech_prob": 0.18689681589603424
    },
    {
      "id": 202,
      "seek": 110148,
      "start": 1115.48,
      "end": 1129.48,
      "text": " We can just like use this sync time to discuss a blog proposal to edit a blog that's already there to like write abstracts, select conferences, designing content representations.",
      "tokens": [
        51064,
        492,
        393,
        445,
        411,
        764,
        341,
        20271,
        565,
        281,
        2248,
        257,
        6968,
        11494,
        281,
        8129,
        257,
        6968,
        300,
        311,
        1217,
        456,
        281,
        411,
        2464,
        12649,
        82,
        11,
        3048,
        22032,
        11,
        14685,
        2701,
        33358,
        13,
        51764
      ],
      "temperature": 0.0,
      "avg_logprob": -0.19994172384572584,
      "compression_ratio": 1.665158371040724,
      "no_speech_prob": 0.18689681589603424
    },
    {
      "id": 203,
      "seek": 112948,
      "start": 1129.48,
      "end": 1141.48,
      "text": " And just anything that comes to your mind if we can use this time for that and to be like how you can add yourself, you can just like go to the link, add it to your calendar.",
      "tokens": [
        50364,
        400,
        445,
        1340,
        300,
        1487,
        281,
        428,
        1575,
        498,
        321,
        393,
        764,
        341,
        565,
        337,
        300,
        293,
        281,
        312,
        411,
        577,
        291,
        393,
        909,
        1803,
        11,
        291,
        393,
        445,
        411,
        352,
        281,
        264,
        2113,
        11,
        909,
        309,
        281,
        428,
        12183,
        13,
        50964
      ],
      "temperature": 0.0,
      "avg_logprob": -0.17520129053216232,
      "compression_ratio": 1.6363636363636365,
      "no_speech_prob": 0.1223374605178833
    },
    {
      "id": 204,
      "seek": 112948,
      "start": 1141.48,
      "end": 1149.48,
      "text": " And for a virtual week, you've booked it others would not book it for that particular week.",
      "tokens": [
        50964,
        400,
        337,
        257,
        6374,
        1243,
        11,
        291,
        600,
        26735,
        309,
        2357,
        576,
        406,
        1446,
        309,
        337,
        300,
        1729,
        1243,
        13,
        51364
      ],
      "temperature": 0.0,
      "avg_logprob": -0.17520129053216232,
      "compression_ratio": 1.6363636363636365,
      "no_speech_prob": 0.1223374605178833
    },
    {
      "id": 205,
      "seek": 112948,
      "start": 1149.48,
      "end": 1152.48,
      "text": " Yeah, and that is it.",
      "tokens": [
        51364,
        865,
        11,
        293,
        300,
        307,
        309,
        13,
        51514
      ],
      "temperature": 0.0,
      "avg_logprob": -0.17520129053216232,
      "compression_ratio": 1.6363636363636365,
      "no_speech_prob": 0.1223374605178833
    },
    {
      "id": 206,
      "seek": 115248,
      "start": 1152.48,
      "end": 1164.48,
      "text": " And just do I like, I noticed it goes to a calendar block. Do you get notified or do we have to like book it with you to let you know, um, let's try.",
      "tokens": [
        50364,
        400,
        445,
        360,
        286,
        411,
        11,
        286,
        5694,
        309,
        1709,
        281,
        257,
        12183,
        3461,
        13,
        1144,
        291,
        483,
        18013,
        420,
        360,
        321,
        362,
        281,
        411,
        1446,
        309,
        365,
        291,
        281,
        718,
        291,
        458,
        11,
        1105,
        11,
        718,
        311,
        853,
        13,
        50964
      ],
      "temperature": 0.0,
      "avg_logprob": -0.17109590695228105,
      "compression_ratio": 1.5363128491620113,
      "no_speech_prob": 0.024574046954512596
    },
    {
      "id": 207,
      "seek": 115248,
      "start": 1164.48,
      "end": 1167.48,
      "text": " I don't know if that happens.",
      "tokens": [
        50964,
        286,
        500,
        380,
        458,
        498,
        300,
        2314,
        13,
        51114
      ],
      "temperature": 0.0,
      "avg_logprob": -0.17109590695228105,
      "compression_ratio": 1.5363128491620113,
      "no_speech_prob": 0.024574046954512596
    },
    {
      "id": 208,
      "seek": 115248,
      "start": 1167.48,
      "end": 1173.48,
      "text": " Because when I clicked on it, it just goes to a calendar event, but no one's invited to it. So.",
      "tokens": [
        51114,
        1436,
        562,
        286,
        23370,
        322,
        309,
        11,
        309,
        445,
        1709,
        281,
        257,
        12183,
        2280,
        11,
        457,
        572,
        472,
        311,
        9185,
        281,
        309,
        13,
        407,
        13,
        51414
      ],
      "temperature": 0.0,
      "avg_logprob": -0.17109590695228105,
      "compression_ratio": 1.5363128491620113,
      "no_speech_prob": 0.024574046954512596
    },
    {
      "id": 209,
      "seek": 117348,
      "start": 1173.48,
      "end": 1179.48,
      "text": " Yeah, okay, I'm also not a part of it. That is amazing.",
      "tokens": [
        50364,
        865,
        11,
        1392,
        11,
        286,
        478,
        611,
        406,
        257,
        644,
        295,
        309,
        13,
        663,
        307,
        2243,
        13,
        50664
      ],
      "temperature": 0.0,
      "avg_logprob": -0.18200761621648615,
      "compression_ratio": 1.2619047619047619,
      "no_speech_prob": 0.1254977136850357
    },
    {
      "id": 210,
      "seek": 117348,
      "start": 1179.48,
      "end": 1188.48,
      "text": " Okay, now I am.",
      "tokens": [
        50664,
        1033,
        11,
        586,
        286,
        669,
        13,
        51114
      ],
      "temperature": 0.0,
      "avg_logprob": -0.18200761621648615,
      "compression_ratio": 1.2619047619047619,
      "no_speech_prob": 0.1254977136850357
    },
    {
      "id": 211,
      "seek": 117348,
      "start": 1188.48,
      "end": 1191.48,
      "text": " I'll work further on this.",
      "tokens": [
        51114,
        286,
        603,
        589,
        3052,
        322,
        341,
        13,
        51264
      ],
      "temperature": 0.0,
      "avg_logprob": -0.18200761621648615,
      "compression_ratio": 1.2619047619047619,
      "no_speech_prob": 0.1254977136850357
    },
    {
      "id": 212,
      "seek": 117348,
      "start": 1191.48,
      "end": 1195.48,
      "text": " Doesn't look very foolproof.",
      "tokens": [
        51264,
        12955,
        380,
        574,
        588,
        7979,
        15690,
        13,
        51464
      ],
      "temperature": 0.0,
      "avg_logprob": -0.18200761621648615,
      "compression_ratio": 1.2619047619047619,
      "no_speech_prob": 0.1254977136850357
    },
    {
      "id": 213,
      "seek": 117348,
      "start": 1195.48,
      "end": 1199.48,
      "text": " Thanks for putting it together.",
      "tokens": [
        51464,
        2561,
        337,
        3372,
        309,
        1214,
        13,
        51664
      ],
      "temperature": 0.0,
      "avg_logprob": -0.18200761621648615,
      "compression_ratio": 1.2619047619047619,
      "no_speech_prob": 0.1254977136850357
    },
    {
      "id": 214,
      "seek": 119948,
      "start": 1199.48,
      "end": 1201.48,
      "text": " We can just like sync.",
      "tokens": [
        50364,
        492,
        393,
        445,
        411,
        20271,
        13,
        50464
      ],
      "temperature": 0.0,
      "avg_logprob": -0.29362051827566965,
      "compression_ratio": 1.2746478873239437,
      "no_speech_prob": 0.0492238849401474
    },
    {
      "id": 215,
      "seek": 119948,
      "start": 1201.48,
      "end": 1214.48,
      "text": " Like catch up over Slack and just books lots manually. This doesn't work. I'm not so good with automating things.",
      "tokens": [
        50464,
        1743,
        3745,
        493,
        670,
        37211,
        293,
        445,
        3642,
        3195,
        16945,
        13,
        639,
        1177,
        380,
        589,
        13,
        286,
        478,
        406,
        370,
        665,
        365,
        3553,
        990,
        721,
        13,
        51114
      ],
      "temperature": 0.0,
      "avg_logprob": -0.29362051827566965,
      "compression_ratio": 1.2746478873239437,
      "no_speech_prob": 0.0492238849401474
    },
    {
      "id": 216,
      "seek": 119948,
      "start": 1214.48,
      "end": 1216.48,
      "text": " And that's it.",
      "tokens": [
        51114,
        400,
        300,
        311,
        309,
        13,
        51214
      ],
      "temperature": 0.0,
      "avg_logprob": -0.29362051827566965,
      "compression_ratio": 1.2746478873239437,
      "no_speech_prob": 0.0492238849401474
    },
    {
      "id": 217,
      "seek": 119948,
      "start": 1216.48,
      "end": 1222.48,
      "text": " Yeah, so you can take it out.",
      "tokens": [
        51214,
        865,
        11,
        370,
        291,
        393,
        747,
        309,
        484,
        13,
        51514
      ],
      "temperature": 0.0,
      "avg_logprob": -0.29362051827566965,
      "compression_ratio": 1.2746478873239437,
      "no_speech_prob": 0.0492238849401474
    },
    {
      "id": 218,
      "seek": 122248,
      "start": 1222.48,
      "end": 1231.48,
      "text": " I just wanted to say plus one, but yeah, it's my turn. Okay, so I'll type it later.",
      "tokens": [
        50364,
        286,
        445,
        1415,
        281,
        584,
        1804,
        472,
        11,
        457,
        1338,
        11,
        309,
        311,
        452,
        1261,
        13,
        1033,
        11,
        370,
        286,
        603,
        2010,
        309,
        1780,
        13,
        50814
      ],
      "temperature": 0.0,
      "avg_logprob": -0.3226380937554863,
      "compression_ratio": 1.4857142857142858,
      "no_speech_prob": 0.016950290650129318
    },
    {
      "id": 219,
      "seek": 122248,
      "start": 1231.48,
      "end": 1233.48,
      "text": " So thanks, Biti.",
      "tokens": [
        50814,
        407,
        3231,
        11,
        363,
        8707,
        13,
        50914
      ],
      "temperature": 0.0,
      "avg_logprob": -0.3226380937554863,
      "compression_ratio": 1.4857142857142858,
      "no_speech_prob": 0.016950290650129318
    },
    {
      "id": 220,
      "seek": 122248,
      "start": 1233.48,
      "end": 1238.48,
      "text": " I have two items that I like to share that.",
      "tokens": [
        50914,
        286,
        362,
        732,
        4754,
        300,
        286,
        411,
        281,
        2073,
        300,
        13,
        51164
      ],
      "temperature": 0.0,
      "avg_logprob": -0.3226380937554863,
      "compression_ratio": 1.4857142857142858,
      "no_speech_prob": 0.016950290650129318
    },
    {
      "id": 221,
      "seek": 122248,
      "start": 1238.48,
      "end": 1251.48,
      "text": " Dove and Mark and myself, we're planning the Q3 that what could be the CICD catalog beta and then what could be included technically and also from the user standpoint.",
      "tokens": [
        51164,
        1144,
        303,
        293,
        3934,
        293,
        2059,
        11,
        321,
        434,
        5038,
        264,
        1249,
        18,
        300,
        437,
        727,
        312,
        264,
        383,
        2532,
        35,
        19746,
        9861,
        293,
        550,
        437,
        727,
        312,
        5556,
        12120,
        293,
        611,
        490,
        264,
        4195,
        15827,
        13,
        51814
      ],
      "temperature": 0.0,
      "avg_logprob": -0.3226380937554863,
      "compression_ratio": 1.4857142857142858,
      "no_speech_prob": 0.016950290650129318
    },
    {
      "id": 222,
      "seek": 125148,
      "start": 1251.48,
      "end": 1258.48,
      "text": " It's almost like defined and then we also announce it to the team and then now team is on board.",
      "tokens": [
        50364,
        467,
        311,
        1920,
        411,
        7642,
        293,
        550,
        321,
        611,
        7478,
        309,
        281,
        264,
        1469,
        293,
        550,
        586,
        1469,
        307,
        322,
        3150,
        13,
        50714
      ],
      "temperature": 0.0,
      "avg_logprob": -0.2505036308651879,
      "compression_ratio": 1.598984771573604,
      "no_speech_prob": 0.007889450527727604
    },
    {
      "id": 223,
      "seek": 125148,
      "start": 1258.48,
      "end": 1261.48,
      "text": " So that's exciting. That means.",
      "tokens": [
        50714,
        407,
        300,
        311,
        4670,
        13,
        663,
        1355,
        13,
        50864
      ],
      "temperature": 0.0,
      "avg_logprob": -0.2505036308651879,
      "compression_ratio": 1.598984771573604,
      "no_speech_prob": 0.007889450527727604
    },
    {
      "id": 224,
      "seek": 125148,
      "start": 1261.48,
      "end": 1276.48,
      "text": " So just to tell you a little bit of more details, we'll gather more and made a data from now on so that we just just don't need to hold like this depend on the repeat.mv that user right.",
      "tokens": [
        50864,
        407,
        445,
        281,
        980,
        291,
        257,
        707,
        857,
        295,
        544,
        4365,
        11,
        321,
        603,
        5448,
        544,
        293,
        1027,
        257,
        1412,
        490,
        586,
        322,
        370,
        300,
        321,
        445,
        445,
        500,
        380,
        643,
        281,
        1797,
        411,
        341,
        5672,
        322,
        264,
        7149,
        13,
        76,
        85,
        300,
        4195,
        558,
        13,
        51614
      ],
      "temperature": 0.0,
      "avg_logprob": -0.2505036308651879,
      "compression_ratio": 1.598984771573604,
      "no_speech_prob": 0.007889450527727604
    },
    {
      "id": 225,
      "seek": 127648,
      "start": 1276.48,
      "end": 1292.48,
      "text": " This is your component a and b and see it like for now, like we have to just let them write everything and then just all the depend on this instruction, but we like to kind of provide a better assistance and suggestion system.",
      "tokens": [
        50364,
        639,
        307,
        428,
        6542,
        257,
        293,
        272,
        293,
        536,
        309,
        411,
        337,
        586,
        11,
        411,
        321,
        362,
        281,
        445,
        718,
        552,
        2464,
        1203,
        293,
        550,
        445,
        439,
        264,
        5672,
        322,
        341,
        10951,
        11,
        457,
        321,
        411,
        281,
        733,
        295,
        2893,
        257,
        1101,
        9683,
        293,
        16541,
        1185,
        13,
        51164
      ],
      "temperature": 0.0,
      "avg_logprob": -0.18785716960956525,
      "compression_ratio": 1.5943396226415094,
      "no_speech_prob": 0.019823040813207626
    },
    {
      "id": 226,
      "seek": 127648,
      "start": 1292.48,
      "end": 1302.48,
      "text": " So this is exciting for me because it'll open the doors for the further UX opportunities. I like to share this.",
      "tokens": [
        51164,
        407,
        341,
        307,
        4670,
        337,
        385,
        570,
        309,
        603,
        1269,
        264,
        8077,
        337,
        264,
        3052,
        40176,
        4786,
        13,
        286,
        411,
        281,
        2073,
        341,
        13,
        51664
      ],
      "temperature": 0.0,
      "avg_logprob": -0.18785716960956525,
      "compression_ratio": 1.5943396226415094,
      "no_speech_prob": 0.019823040813207626
    },
    {
      "id": 227,
      "seek": 130248,
      "start": 1302.48,
      "end": 1316.48,
      "text": " And the other point is probably you might see this in the MR pipeline tap. There is a now failed job number is surfacing in those tap. So that was the idea for this packathon.",
      "tokens": [
        50364,
        400,
        264,
        661,
        935,
        307,
        1391,
        291,
        1062,
        536,
        341,
        294,
        264,
        9808,
        15517,
        5119,
        13,
        821,
        307,
        257,
        586,
        7612,
        1691,
        1230,
        307,
        9684,
        5615,
        294,
        729,
        5119,
        13,
        407,
        300,
        390,
        264,
        1558,
        337,
        341,
        2844,
        18660,
        13,
        51064
      ],
      "temperature": 0.0,
      "avg_logprob": -0.21054175642670178,
      "compression_ratio": 1.4523809523809523,
      "no_speech_prob": 0.03156795725226402
    },
    {
      "id": 228,
      "seek": 130248,
      "start": 1316.48,
      "end": 1321.48,
      "text": " And then luckily we get a lot of positive feedback at the same time.",
      "tokens": [
        51064,
        400,
        550,
        22880,
        321,
        483,
        257,
        688,
        295,
        3353,
        5824,
        412,
        264,
        912,
        565,
        13,
        51314
      ],
      "temperature": 0.0,
      "avg_logprob": -0.21054175642670178,
      "compression_ratio": 1.4523809523809523,
      "no_speech_prob": 0.03156795725226402
    },
    {
      "id": 229,
      "seek": 132148,
      "start": 1321.48,
      "end": 1336.48,
      "text": " So we started to get a lot of bug reports, for example, there's four failed job, but the width is as is zero failed job and with even with the total emoji so like, oh, okay, we should fix it now.",
      "tokens": [
        50364,
        407,
        321,
        1409,
        281,
        483,
        257,
        688,
        295,
        7426,
        7122,
        11,
        337,
        1365,
        11,
        456,
        311,
        1451,
        7612,
        1691,
        11,
        457,
        264,
        11402,
        307,
        382,
        307,
        4018,
        7612,
        1691,
        293,
        365,
        754,
        365,
        264,
        3217,
        31595,
        370,
        411,
        11,
        1954,
        11,
        1392,
        11,
        321,
        820,
        3191,
        309,
        586,
        13,
        51114
      ],
      "temperature": 0.0,
      "avg_logprob": -0.40638563767919,
      "compression_ratio": 1.4028776978417266,
      "no_speech_prob": 0.41625678539276123
    },
    {
      "id": 230,
      "seek": 133648,
      "start": 1336.48,
      "end": 1358.48,
      "text": " We're also working on this. It just popped up, but it's exciting that people kind of coming in and as they and then if you also have any other feedback, please, please let me know I'll also link the app for issues so that you could leave some feedback around UX and also the UI tax as well.",
      "tokens": [
        50364,
        492,
        434,
        611,
        1364,
        322,
        341,
        13,
        467,
        445,
        21545,
        493,
        11,
        457,
        309,
        311,
        4670,
        300,
        561,
        733,
        295,
        1348,
        294,
        293,
        382,
        436,
        293,
        550,
        498,
        291,
        611,
        362,
        604,
        661,
        5824,
        11,
        1767,
        11,
        1767,
        718,
        385,
        458,
        286,
        603,
        611,
        2113,
        264,
        724,
        337,
        2663,
        370,
        300,
        291,
        727,
        1856,
        512,
        5824,
        926,
        40176,
        293,
        611,
        264,
        15682,
        3366,
        382,
        731,
        13,
        51464
      ],
      "temperature": 0.0,
      "avg_logprob": -0.2514478410993304,
      "compression_ratio": 1.5343915343915344,
      "no_speech_prob": 0.579518735408783
    },
    {
      "id": 231,
      "seek": 135848,
      "start": 1358.48,
      "end": 1363.48,
      "text": " This was a really good improvement like a tiny one, but so impactful.",
      "tokens": [
        50364,
        639,
        390,
        257,
        534,
        665,
        10444,
        411,
        257,
        5870,
        472,
        11,
        457,
        370,
        30842,
        13,
        50614
      ],
      "temperature": 0.0,
      "avg_logprob": -0.2678857130162856,
      "compression_ratio": 1.3043478260869565,
      "no_speech_prob": 0.13607224822044373
    },
    {
      "id": 232,
      "seek": 135848,
      "start": 1363.48,
      "end": 1372.48,
      "text": " Yeah, I was excited and then yeah, we will have more issues.",
      "tokens": [
        50614,
        865,
        11,
        286,
        390,
        2919,
        293,
        550,
        1338,
        11,
        321,
        486,
        362,
        544,
        2663,
        13,
        51064
      ],
      "temperature": 0.0,
      "avg_logprob": -0.2678857130162856,
      "compression_ratio": 1.3043478260869565,
      "no_speech_prob": 0.13607224822044373
    },
    {
      "id": 233,
      "seek": 135848,
      "start": 1372.48,
      "end": 1380.48,
      "text": " Thanks Eric is on leave, so move it over to bill.",
      "tokens": [
        51064,
        2561,
        9336,
        307,
        322,
        1856,
        11,
        370,
        1286,
        309,
        670,
        281,
        2961,
        13,
        51464
      ],
      "temperature": 0.0,
      "avg_logprob": -0.2678857130162856,
      "compression_ratio": 1.3043478260869565,
      "no_speech_prob": 0.13607224822044373
    },
    {
      "id": 234,
      "seek": 138048,
      "start": 1380.48,
      "end": 1409.48,
      "text": " Thanks, engine. So I just wanted to quickly touch on a judicial police about the survey calculator that I spoke about a couple of minutes ago added some information earlier on, but I would just recommend, you know, working with your researcher, if you decide to use one of those types of tools to figure out.",
      "tokens": [
        50364,
        2561,
        11,
        2848,
        13,
        407,
        286,
        445,
        1415,
        281,
        2661,
        2557,
        322,
        257,
        26581,
        3804,
        466,
        264,
        8984,
        24993,
        300,
        286,
        7179,
        466,
        257,
        1916,
        295,
        2077,
        2057,
        3869,
        512,
        1589,
        3071,
        322,
        11,
        457,
        286,
        576,
        445,
        2748,
        11,
        291,
        458,
        11,
        1364,
        365,
        428,
        21751,
        11,
        498,
        291,
        4536,
        281,
        764,
        472,
        295,
        729,
        3467,
        295,
        3873,
        281,
        2573,
        484,
        13,
        51814
      ],
      "temperature": 0.0,
      "avg_logprob": -0.2060753694221155,
      "compression_ratio": 1.4951456310679612,
      "no_speech_prob": 0.013203618116676807
    },
    {
      "id": 235,
      "seek": 140948,
      "start": 1409.48,
      "end": 1422.48,
      "text": " What's the appropriate like margin of error and confidence interval because if you leave it as is that's those things are more like academic standard.",
      "tokens": [
        50364,
        708,
        311,
        264,
        6854,
        411,
        10270,
        295,
        6713,
        293,
        6687,
        15035,
        570,
        498,
        291,
        1856,
        309,
        382,
        307,
        300,
        311,
        729,
        721,
        366,
        544,
        411,
        7778,
        3832,
        13,
        51014
      ],
      "temperature": 0.0,
      "avg_logprob": -0.14065251058461714,
      "compression_ratio": 1.469387755102041,
      "no_speech_prob": 0.12289413064718246
    },
    {
      "id": 236,
      "seek": 140948,
      "start": 1422.48,
      "end": 1428.48,
      "text": " So it'll tell you that you need probably more than you really do.",
      "tokens": [
        51014,
        407,
        309,
        603,
        980,
        291,
        300,
        291,
        643,
        1391,
        544,
        813,
        291,
        534,
        360,
        13,
        51314
      ],
      "temperature": 0.0,
      "avg_logprob": -0.14065251058461714,
      "compression_ratio": 1.469387755102041,
      "no_speech_prob": 0.12289413064718246
    },
    {
      "id": 237,
      "seek": 142848,
      "start": 1428.48,
      "end": 1447.48,
      "text": " So if you have like tight deadlines or, you know, whatever it might be, you might have to adjust that a little bit to get a better sense of what's a more reasonable number of people to survey.",
      "tokens": [
        50364,
        407,
        498,
        291,
        362,
        411,
        4524,
        37548,
        420,
        11,
        291,
        458,
        11,
        2035,
        309,
        1062,
        312,
        11,
        291,
        1062,
        362,
        281,
        4369,
        300,
        257,
        707,
        857,
        281,
        483,
        257,
        1101,
        2020,
        295,
        437,
        311,
        257,
        544,
        10585,
        1230,
        295,
        561,
        281,
        8984,
        13,
        51314
      ],
      "temperature": 0.0,
      "avg_logprob": -0.09302587712064703,
      "compression_ratio": 1.4545454545454546,
      "no_speech_prob": 0.07472378760576248
    },
    {
      "id": 238,
      "seek": 144748,
      "start": 1447.48,
      "end": 1457.48,
      "text": " So that's I'm going to be on leave starting next Monday. I'll be out for four weeks. So I'll get back.",
      "tokens": [
        50364,
        407,
        300,
        311,
        286,
        478,
        516,
        281,
        312,
        322,
        1856,
        2891,
        958,
        8138,
        13,
        286,
        603,
        312,
        484,
        337,
        1451,
        3259,
        13,
        407,
        286,
        603,
        483,
        646,
        13,
        50864
      ],
      "temperature": 0.0,
      "avg_logprob": -0.16271531581878662,
      "compression_ratio": 1.296551724137931,
      "no_speech_prob": 0.12859858572483063
    },
    {
      "id": 239,
      "seek": 144748,
      "start": 1457.48,
      "end": 1460.48,
      "text": " Second half of August.",
      "tokens": [
        50864,
        5736,
        1922,
        295,
        6897,
        13,
        51014
      ],
      "temperature": 0.0,
      "avg_logprob": -0.16271531581878662,
      "compression_ratio": 1.296551724137931,
      "no_speech_prob": 0.12859858572483063
    },
    {
      "id": 240,
      "seek": 144748,
      "start": 1460.48,
      "end": 1466.48,
      "text": " I've created my Q3 prioritization issue and link to that here.",
      "tokens": [
        51014,
        286,
        600,
        2942,
        452,
        1249,
        18,
        14846,
        2144,
        2734,
        293,
        2113,
        281,
        300,
        510,
        13,
        51314
      ],
      "temperature": 0.0,
      "avg_logprob": -0.16271531581878662,
      "compression_ratio": 1.296551724137931,
      "no_speech_prob": 0.12859858572483063
    },
    {
      "id": 241,
      "seek": 146648,
      "start": 1466.48,
      "end": 1474.48,
      "text": " So I'm going to be out of the stuff that's represented there's just carrying over from this quarter that's still active.",
      "tokens": [
        50364,
        407,
        286,
        478,
        516,
        281,
        312,
        484,
        295,
        264,
        1507,
        300,
        311,
        10379,
        456,
        311,
        445,
        9792,
        670,
        490,
        341,
        6555,
        300,
        311,
        920,
        4967,
        13,
        50764
      ],
      "temperature": 0.0,
      "avg_logprob": -0.25298116293298195,
      "compression_ratio": 1.6053811659192825,
      "no_speech_prob": 0.3147270381450653
    },
    {
      "id": 242,
      "seek": 146648,
      "start": 1474.48,
      "end": 1478.48,
      "text": " And then I'm going to be picking up some projects that are paused.",
      "tokens": [
        50764,
        400,
        550,
        286,
        478,
        516,
        281,
        312,
        8867,
        493,
        512,
        4455,
        300,
        366,
        46860,
        13,
        50964
      ],
      "temperature": 0.0,
      "avg_logprob": -0.25298116293298195,
      "compression_ratio": 1.6053811659192825,
      "no_speech_prob": 0.3147270381450653
    },
    {
      "id": 243,
      "seek": 146648,
      "start": 1478.48,
      "end": 1495.48,
      "text": " And then since Ali has departed, get lab I'm coordinating with the PM Lauren and then hi, to determine next steps for the solution validation project that he was leading.",
      "tokens": [
        50964,
        400,
        550,
        1670,
        12020,
        575,
        47018,
        11,
        483,
        2715,
        286,
        478,
        37824,
        365,
        264,
        12499,
        18915,
        293,
        550,
        4879,
        11,
        281,
        6997,
        958,
        4439,
        337,
        264,
        3827,
        24071,
        1716,
        300,
        415,
        390,
        5775,
        13,
        51814
      ],
      "temperature": 0.0,
      "avg_logprob": -0.25298116293298195,
      "compression_ratio": 1.6053811659192825,
      "no_speech_prob": 0.3147270381450653
    },
    {
      "id": 244,
      "seek": 149548,
      "start": 1495.48,
      "end": 1503.48,
      "text": " So we're meeting on Thursday to discuss this more sink call, but we're also having a sink discussions about it.",
      "tokens": [
        50364,
        407,
        321,
        434,
        3440,
        322,
        10383,
        281,
        2248,
        341,
        544,
        9500,
        818,
        11,
        457,
        321,
        434,
        611,
        1419,
        257,
        9500,
        11088,
        466,
        309,
        13,
        50764
      ],
      "temperature": 0.0,
      "avg_logprob": -0.15330425175753506,
      "compression_ratio": 1.5971563981042654,
      "no_speech_prob": 0.006144319195300341
    },
    {
      "id": 245,
      "seek": 149548,
      "start": 1503.48,
      "end": 1514.48,
      "text": " Like kind of if high on is going to be able to drive it while I'm out if we're going to just kind of keep it pause for the next couple of weeks until I'm back those sort of things.",
      "tokens": [
        50764,
        1743,
        733,
        295,
        498,
        1090,
        322,
        307,
        516,
        281,
        312,
        1075,
        281,
        3332,
        309,
        1339,
        286,
        478,
        484,
        498,
        321,
        434,
        516,
        281,
        445,
        733,
        295,
        1066,
        309,
        10465,
        337,
        264,
        958,
        1916,
        295,
        3259,
        1826,
        286,
        478,
        646,
        729,
        1333,
        295,
        721,
        13,
        51314
      ],
      "temperature": 0.0,
      "avg_logprob": -0.15330425175753506,
      "compression_ratio": 1.5971563981042654,
      "no_speech_prob": 0.006144319195300341
    },
    {
      "id": 246,
      "seek": 149548,
      "start": 1514.48,
      "end": 1523.48,
      "text": " So that's what's going on with me right now.",
      "tokens": [
        51314,
        407,
        300,
        311,
        437,
        311,
        516,
        322,
        365,
        385,
        558,
        586,
        13,
        51764
      ],
      "temperature": 0.0,
      "avg_logprob": -0.15330425175753506,
      "compression_ratio": 1.5971563981042654,
      "no_speech_prob": 0.006144319195300341
    },
    {
      "id": 247,
      "seek": 152348,
      "start": 1523.48,
      "end": 1526.48,
      "text": " Enjoy your family time.",
      "tokens": [
        50364,
        15411,
        428,
        1605,
        565,
        13,
        50514
      ],
      "temperature": 0.0,
      "avg_logprob": -0.2138058297774371,
      "compression_ratio": 1.3397435897435896,
      "no_speech_prob": 0.0009438026463612914
    },
    {
      "id": 248,
      "seek": 152348,
      "start": 1526.48,
      "end": 1532.48,
      "text": " Yeah. Yeah, I need it.",
      "tokens": [
        50514,
        865,
        13,
        865,
        11,
        286,
        643,
        309,
        13,
        50814
      ],
      "temperature": 0.0,
      "avg_logprob": -0.2138058297774371,
      "compression_ratio": 1.3397435897435896,
      "no_speech_prob": 0.0009438026463612914
    },
    {
      "id": 249,
      "seek": 152348,
      "start": 1532.48,
      "end": 1537.48,
      "text": " Yeah, my wife goes back to work next week for the first time in like 12 weeks.",
      "tokens": [
        50814,
        865,
        11,
        452,
        3836,
        1709,
        646,
        281,
        589,
        958,
        1243,
        337,
        264,
        700,
        565,
        294,
        411,
        2272,
        3259,
        13,
        51064
      ],
      "temperature": 0.0,
      "avg_logprob": -0.2138058297774371,
      "compression_ratio": 1.3397435897435896,
      "no_speech_prob": 0.0009438026463612914
    },
    {
      "id": 250,
      "seek": 152348,
      "start": 1537.48,
      "end": 1547.48,
      "text": " So I'm going to be on full dead duty 24, 7.",
      "tokens": [
        51064,
        407,
        286,
        478,
        516,
        281,
        312,
        322,
        1577,
        3116,
        9776,
        4022,
        11,
        1614,
        13,
        51564
      ],
      "temperature": 0.0,
      "avg_logprob": -0.2138058297774371,
      "compression_ratio": 1.3397435897435896,
      "no_speech_prob": 0.0009438026463612914
    },
    {
      "id": 251,
      "seek": 152348,
      "start": 1547.48,
      "end": 1551.48,
      "text": " So give it her a little bit of a break.",
      "tokens": [
        51564,
        407,
        976,
        309,
        720,
        257,
        707,
        857,
        295,
        257,
        1821,
        13,
        51764
      ],
      "temperature": 0.0,
      "avg_logprob": -0.2138058297774371,
      "compression_ratio": 1.3397435897435896,
      "no_speech_prob": 0.0009438026463612914
    },
    {
      "id": 252,
      "seek": 155348,
      "start": 1553.48,
      "end": 1567.48,
      "text": " Okay, so with that we have at the end of the agenda anything that anyone wants to bring up.",
      "tokens": [
        50364,
        1033,
        11,
        370,
        365,
        300,
        321,
        362,
        412,
        264,
        917,
        295,
        264,
        9829,
        1340,
        300,
        2878,
        2738,
        281,
        1565,
        493,
        13,
        51064
      ],
      "temperature": 0.0,
      "avg_logprob": -0.1698598861694336,
      "compression_ratio": 1.1666666666666667,
      "no_speech_prob": 0.007772353012114763
    },
    {
      "id": 253,
      "seek": 156748,
      "start": 1567.48,
      "end": 1570.48,
      "text": " If not, then let's play more time back.",
      "tokens": [
        50364,
        759,
        406,
        11,
        550,
        718,
        311,
        862,
        544,
        565,
        646,
        13,
        50514
      ],
      "temperature": 0.0,
      "avg_logprob": -0.34680917527940536,
      "compression_ratio": 0.9583333333333334,
      "no_speech_prob": 0.15588386356830597
    },
    {
      "id": 254,
      "seek": 156748,
      "start": 1570.48,
      "end": 1573.48,
      "text": " Yeah, let's meet later.",
      "tokens": [
        50514,
        865,
        11,
        718,
        311,
        1677,
        1780,
        13,
        50664
      ],
      "temperature": 0.0,
      "avg_logprob": -0.34680917527940536,
      "compression_ratio": 0.9583333333333334,
      "no_speech_prob": 0.15588386356830597
    },
    {
      "id": 255,
      "seek": 156748,
      "start": 1573.48,
      "end": 1582.48,
      "text": " Fine.",
      "tokens": [
        50664,
        12024,
        13,
        51114
      ],
      "temperature": 0.0,
      "avg_logprob": -0.34680917527940536,
      "compression_ratio": 0.9583333333333334,
      "no_speech_prob": 0.15588386356830597
    }
  ]
}