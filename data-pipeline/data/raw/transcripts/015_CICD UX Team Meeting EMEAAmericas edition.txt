Video: CI/CD UX Team Meeting (EMEA/Americas edition)
Video ID: MKgCtg0KpDQ
URL: https://www.youtube.com/watch?v=MKgCtg0KpDQ
================================================================================

 Hello, this is the CICD UX team on June 15th, the America's thread, and I'll just start by going through some of the manager announcements. So, hi on a head added if there was, um, okay, ours that are blocked or at risk. And if we need help with them, but it doesn't look like anyone added anything so cool looks like we're on track. Unless does anyone have anything for that. Okay, sweet. We have a friends and family day on June 24th next week. And then hi on a head send out a message about mid year check ins and created a tracking issue for these they are team member led check ins to assess how things are going from both the team member and the manager's point of view. And then share feedback to help inform performance and development plans and just make sure to dedicate an upcoming one on one to do this before July 22nd. And welcome Emily to the team. We're very happy to have you. And then Katie says that she'll be in Europe for around two months starting late next week. She's taking a few days off to deal with jet lag, but that could be a good time to set up coffee chats if your time zone doesn't overlap. I know for me, it's like the last hour of the day. So. Anything else in the announcements that anybody wants to say. Okay, I guess I'll just say thank you for the welcome and excited to be here. So. Okay. Okay, so I'll just go. I have more of like updates on my side for pipeline insights and runner. I'm going to go through them quickly since there's four of us. And the artifacts page research that I've been talking for, I think for weeks now about is finally complete. So we're moving forward with creating the page. It's a project's page for artifacts that list all of your artifacts in that project. And we got really great feedback. And so I'll be making on updating the list view based on that feedback. So we sent out a survey and Erica helped so much with this and so to Caitlin, we sent out a survey to validate artifacts jobs because we have no data really around the jobs related to artifacts. So we just wanted to quickly like get something. So we did the survey route. So we have 80 responses already, which is great. So we're just going to cap it at that and start analyzing the data there. Erica had asked if I saw any differences in self manage versus SAS responses. Because Jackie, who used to be the PM for pipeline insights had asset. I haven't looked into it yet, but my assumption is that self managed folks are focused on storage related and management jobs when it comes to artifacts versus. And so that's what I'm going to ask is probably focused on using artifacts to debug jobs and pipelines. Because we have like automatic cleanup for SAS. Yes, so I was looking into build artifacts as a category and I know that both job and pipeline artifacts are a part of that. But since we had stopped using the world build many different parts of the product, is there a plan to maybe rename this category. Rename it to build artifacts or what do you mean? So the name currently is built artifacts. But is there a plan to stop using the word build because that's confusing to many users. And even internally, it's very it's a very confusing term. So just wanted to ask about that. We don't there's there's no plan, but I completely agree there's been so much confusion about if artifacts means like packages or build artifacts. So we need we need to do some type of clarification there. So we need to do some kind of clarification. I get a lot of we call them job and pipeline artifacts. So there's potential to just call them that. So I think that's a lot of questions. Well, do you want to voice your question? So I think that's a lot of questions. So I think that's a lot of questions. So I think that's a lot of questions. Which I wasn't expecting. I assume we would get more SaaS users. Yeah, usually we've had a little bit more difficulty getting self managed users. So that's that's good that there's more of a balance this time around. Yeah, well, I have a question for you that ideally what is recommended like how much of a balance should we keep the sample size for us versus self managed. I know that in a direction as an organization. We want to later focus more on SaaS. But at the same time, we also want to understand like why self managed users. What would it take for self managed users to become SaaS users. So any recommendations there. It's a good question. I think like I mentioned before we've traditionally had a lot of difficulty finding self managed users. I think at least if you're getting like, you know, 20% self managed when you're specifically targeting a mix. I think that would be fine as long as it's at least 20%. Anything lower. You might like want to like talk with your team about and kind of look more closely at before you. This is what we found from self managed users. So we found a lot of different factors. Okay. Thanks. Yeah. Okay. Last thing was the runner list view. I was also it took on three research projects this milestone, which was a mistake, but. I was the last one. So I did unmoderated research and so on. We got that and it was purely quantitative data. So I wanted. Hold sessions and in holding sessions with customers to get qualitative data to make sure that they can complete their jobs with the new view. And I'll have some updates around that. We added an upgrade icon. If anybody has upgrades within their area. So this one specific to get lab runners, you would update the version. And now we have an icon to represent that. Anybody needs it. Emily. So I don't have too much to update on just like the next few weeks in 15 to my focus will really be on onboarding. So I think I've sent coffee chats to everyone on the team, just getting to know like the release area really well and all of that. So that's going to be my focus for the next little while. So I think that's like a very fast update. Hi, I know as also given me like two tasks to help me with on boardying the first one which I'm going to tackle. And of this week start of next week, which is really deploying a project and creating a release isn't get lab. And I will be I we are chatting, and I and I will be taking like if you notice throughout it, I could capture some like the screens and what the flow looks like for those interested on. getting used to the journey. And I see Will has a question. Yeah, for the first task, I'm definitely interested in whatever you learn. So feel free to share once you have gone through that experience. I'm just connected with so many teams. I haven't like dug in deep to understand like that end to end experience. So I'll be interested to hear how your process goes. And then for the jobs to be done task, is that a plan jobs to be done study? So that one I'm actually going to get into. I have to do task one first. So I ultimately haven't read into this one too much. It'll probably be done in like the second half of 15 to. But I don't actually know. I have to read into this one a bit more. I can link the discussion with Hianana where there's a bit more detail. So you can read into that. OK. There's the link to the conversation around it. And I think the point of this is really just to, I think it was one of Gina's first activities to onboarding onto the team. It's like a good onboarding activity to go through the process. And ultimately in growth, we didn't do a lot of jobs to be done because we were working cross stage. So it will be my first job to be done here at GitLab as well. So kind of just get in process with that. OK. Well, when you're ready to pick up that work, I've done a couple of jobs to be done studies for different teams. So I can help with that. And I did include a link to my planning issue, which I'll talk a little bit more about later. But we use it to essentially just prioritize like how researchers are aligned to different projects and how we prioritize the work. Sounds good. I'll keep you up to date with this. I expect to finish the first task pretty quickly. So I'll let you know, give you a heads up when I'm finishing that up and starting with these. So we can figure out how to go from there. OK. Sounds good. Awesome. I think that's it for mine. OK. I have a lot of follow-up points from this discussion. But I would like mentioned those points in context to what I'm about to share. So I recently wrapped up a foundation to research for piping execution. I documented the jobs to be done for continuous integration when I started off, which was about more than 1.5 years back. And in the course of this time, what I realized was it was very much based on my very initial understanding of what those teams group was all about and what this whole thing is, what verify stands for in the DevOps process. And I understand that the jobs to be done, the core jobs to be done, is not something that evolves, but because my understanding evolved of those jobs, I thought, by not like take up very basic foundational research, which is not very much tied to any specific area in the three categories that we look at, but something which is generic enough from where we can extract the core jobs to be done, the high level ones. So we did that. And because I deleted the history of my browser, I couldn't find the issue. I'll do that and add this here. But in the meanwhile, so will you mention that you have conducted a few JDBD studies? Now, when I went through the whole process, when I read the description and when I wanted to do things from scratch once more, what I figured was in the documentation, we have mentioned that a problem validation is a really good way to go. But from the recent problem validations that we were engaging within the piping execution team, they were too focused. They were very focused on the specific capability or specific requirement. So I was not able to get much from that. And that's why I conducted a whole new research. And with Erica's help, we did this in a very different way, which I have mentioned in the YouTube video that I've added. And this is the outcome from that. I know that that's a lot to consume and provide feedback on in this short while. But in case all of you get time, please look at the video. And I would love to hear your thoughts around the process. Because I'm also planning to write something about this process and document it and publish it as a blog. So any feedback is helpful. Nice. Thanks for sharing. I'll also add going to the issue as I find it. Anyway, that was all from me for the day. Do we cover the AEPA thread? Sorry. How much time do we? We do have a lot of time. Well, Will, do you want to cover your stuff first? And then we'll go back up to the AEPA thread. Yeah, I did have a quick question for Vithika. I opened up your merger class. And it looks like you've added your jobs to be done to a more extensive EML file. I think when I've been a part of some of the other jobs to be done studies, they typically will also add a section of a product direction page or a category page. And add tables with the jobs to be done that have been determined based on the research. So that once you do a category maturity scorecard, you couldn't fill out the scores. Are there any plans to do something like that? Yeah, so the reason I picked it up was I was working on the maturity plan for the categories with our PM. And when we laid down the plan, I figured that for each one of them, we are very heavily relying on the jobs to be done. And the other ones that I don't have enough confidence on. So that wasn't a good place to be. Yeah, so once things are finalized and based on that, we will kind of make a plan with the research. Like what researchers we need to do in the upcoming months, like the very close milestones that we need to hit with the maturity. And we usually, I mean, this is how I have been documenting jobs to be done specifically. And we don't replicate that to the direction page. We only mentioned the maturity plan. So this kind of shows up in the pipeline execution, you were jobs to be done page that I've set up. Shady beauties. And I mean, I myself not aware of there's a different way of doing it. I'm based in the link here for the page where I added. We do the same on the runner and pipeline insights team to document the jobs. I just added that will. Okay. With the links. So I already had a bunch of them. They did not like some of them just never helped us to with anything. They were very hypothetical and they were just one person's. So I trick on it as GTV. So I wanted to get rid of those as well. Okay. So just to clarify like our. The ones that you like the high level jobs to be done are those like mentioned. On this link that you just added. Yeah, so the headers. How they're documented is the high level ones they appear as a once with a more bold title like the shot and one. And following that are like what are the sub GTVDs which are related to it. Okay. Cool. And if you can also point me to the researchers that you have conducted so I can also get inspired by the process. Yeah. Yeah, I can. Link to the. I'll do a reminder. Just so I don't have to try to find that in the middle of this. So I can talk briefly about some of what I'm doing in. Release within the UX research thread below. So Emily for your. You know, information. Erica and I have included our prioritization issues. So minds. Right here. I've had a lot of questions. So minds. Right here. I've highlighted it within the doc. And I basically have like all of the research that's going on in the teams that I cover. So I cover all the enablements and then roughly half of ops and Erica covers the other half of ops. And so I've tried to specify to the best of my abilities. And so I've got to have a look at the. And then I've got a look at these. Like which projects. There's like a column. Kind of in the middle of that particular prioritization issue. That says. If certain work is connected to release. So there's about three or four projects in total that are listed. Within that table. So some of the things that I'm working on currently. I've made a lot of updates to that just in the past couple of days. So I've added a timeline checklist to that. And I'm also going through a set of tasks that I worked on with. Hi, Anna and Chris. And then. I'm also in the process of making a UX cloud sandbox. So I'm following the handbook doc. And that project that will eventually create will be used for data collection. Taking users through very specific tasks for release. And outside of that study. Hi, Anna has closed her solution validation study. I think within the past week or so. And then linked. Actual insights. Actual insight issues to that research issues. So if you're curious to see what came out of that work. And what's on the backlog. It's all there within that link. And then. Finally, Chris, the PM for the release team is wrapping up interviews for his project. It kind of covers a couple of different stage groups, not just release. So it's focused on Kubernetes deployments. And he's conducted about six or seven user interviews in total. And hopes to have insights later this month. So. I'll pause in any questions on any of those things. I know we put some time aside next week to go over the usability benchmarking for release. So in preparation for that, I'll just make sure kind of to read up on this spring. Any like specific questions long like. I'll just say that you will be seeing the same. First, it's the same. I think that's. I'm going to, you know, this is going into like September, probably when I'm going to be fully onboarded. So like which I can need to help in the future. And all of that. But this is great. Thanks for all the links into this. And. I'll definitely take a look. I know Chris has talked about the interview. So he's been doing as well. sink mural activity with Chris and Hyanna. This referenced in the comments section of that epic. And we spent about two or three weeks going through different parts of the study set up, trying to determine what tasks we were going to do, who we were going to recruit, what metrics we were going to look at. So that mural board is linked and available in there. So I think that would may be a good thing to look at ahead of time too. Awesome. Yeah, I just found it's the one that's like part one, part team. Yeah. Yeah. So I just, I just found it. So, awesome. So I think that's all that I had. If we want to go back up to the APEC stuff, yeah, I think so. They also had already, when they had the meeting, they took a recording of it if anybody wants to take a look at that. It's in the channel. Katie completed a solution validation for container registry, cleanup policies, and she linked the dovetail. And it allows the user to set up rules or policies to delete items from the registry to save storage space. Her findings were in line with her assumptions, but she did discover that users want a way to make a template of their policies and apply them to other projects, which sounds cool. Should I read like the whole conversation after that or should I just move to the next point? You can move to the next one. Okay. Okay. In 15 to, they'll be implementing a feature that will release to a small subset of users and monitor with snowplow. The reason is to spite two rounds of validation. We're still not certain if this is disruptive to people's workflows. So we are taking a cautious approach. And this will be Katie's first time at GitLab doing a phased rollout. And she'll keep us posted. And finally, she's, oh, no, that's not true. She's consistently hearing from customers that the integration with package and release could be better. An example of the feedback that's captured in that link. Will and Emily, that might be good for you to look at. Okay. And yeah, she said, I would like to work with Emily to share my findings and see about improving things. Yeah, that would be great. I know I've set up some time with her next week as well. So we can track about this. Yay. Her focus for 15 to will be mostly research heavy and will be improving the package detail page. And then she'll also attempt to implement the missing front end events tracking. So we have better data of how customers are using the product. That was in I don't know if anybody had time to think Katie, either recorded a video or shared in a previous UX meeting, but she found all of the front end events like through the browser and tracked those. And that was really cool. Especially if your team is lacking telemetry right now, that might be a good thing to look into. I know we we are on runner and pipeline insights. Okay. Nadia said that this milestone and in 15th three should be focusing on pipeline components MVC as we're starting to get as we're getting ready to start the implementation 15 for. And we've recently added the guidelines for in product reference information using drawers. I'm two pajamas and that's a good thing to check out as well. I think that's it. Lots of links. So if you have time, it would be good to look at them. Yeah. I'm going to share the issue that's created by by the product manager of package for the snowplough tracking events. I'll be sharing that with my PM as well because we are also facing some issues of making a decision regarding filters and similar functionalities where things are not so black and white. I mean the insights are not so black and white that we've received from the validations. And still we have to make like we have to proceed in some direction. And we have to do it cautiously. Yeah. That definitely would be nice. We're seeing like similar trends on both running runner and testing and also even for purposes of like investigating bugs for developers. Sometimes it's really difficult to recreate the bugs. So having more like insight into what was going on with the event at that time would help them move faster. Well, do you want to read through Erica's stuff? Sure. I didn't notice that we didn't touch on this general update. I think Erica must have added this. But it was like we're seeing a higher overall no-show rate for sessions. So just a heads up. I think this might just be due to like seasonal variants. So it may be related to like you know a lot of people are out or you know have holidays or have you know summer vacation or you know whatever it is depending on where they are in the world. So just want to provide a heads up about that. I don't know if anybody's noticed that with with any of the studies that they've done recently. I haven't noticed a ton. So Erica and I discussed about this I think the previous week that this has been a happening a lot for us. It never happens when we record through platforms like Respondent. But if we rely on the our internal recruiting process, we send out mails and like nobody responds to them for a very long time. Even when they do it's like pretty much after them whole analysis is over. And I am not very sure if that's for that's due to seasonal variants. But I mean that's how I have been trying to reason by telling it's because there's some hindsight bias that's going on here that oh maybe it's because this was going on they didn't turn up because this has been happening throughout the year. And our when we internally try to record users are the reattach which we get a positive reply it's it's always pretty less. So I think we need to figure out what's happening there. Yeah. Yeah. And I know that we talked about it with just like the research team last week. And some of what came out of that was that you know Respondent sends like a lot of reminders and also has you know that panel of users that are more likely to you know apply and attend for the things that they sign up for. So there's been kind of some talk within the research team about ways that we can try to remind people a little bit more. So like maybe getting Kaitlyn involved and reminding her to send out reminders or if you have the ability to like have the list of emails you could reach out to people before sessions and say like hey just a heads up you know the session's going to happen in 24 hours. Can you confirm that you'll be there or you know something like that? Right. Yeah. I definitely don't send reminders so maybe that should be a practice that we should follow. Same. I don't either. And this I can have for the artifacts research that I did get a few no shows. And some of them like if they missed it I would email them and be like oh sorry we missed you. Do you want to sign up again? And then they sign up and then they wouldn't show up to the next one. We're done. I have not heard on a few on-boarding interviews on growth too or someone wouldn't show up. I'd email latery schedule and then they would charge the second one too. Okay. But I can provide some updates once we know a little bit more. I think Kaitlyn and our other new research ops coordinator we're going to try to work on some of that to see if they could build some things into their current workflow to take into account that people need to be reminded ahead of time because I think there's a way to set that up through calendar so that it just automatically does it instead of having to remember to do it. But I'll I'll provide some updates once I know a little bit more. Any other comments or questions for I go to Erica's points. So let's see. She's asking if anyone can link and add the studies that they would like to be included in the verify and package research registry and synthesis in this issue. Number 1738. She is also asking to update studies in her prioritization issue. And then some specific updates related to her prioritization issue. She closed out the ops product direction survey where she went to cubicon. So she's included the link to the report there and has a link to the async discussion issue. And I've looked at that report too. It's very detailed so I highly recommend checking that out. And then she's got a couple of read only updates. Yes. All right. All right, well, we get 10 minutes back. So yeah, have a good rest of your days. Bye everybody.