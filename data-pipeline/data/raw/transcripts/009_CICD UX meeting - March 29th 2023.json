{
  "title": "CI/CD UX meeting - March 29th, 2023",
  "video_id": "PdVV1qeD1fE",
  "url": "https://www.youtube.com/watch?v=PdVV1qeD1fE",
  "transcript": " Hello, this is March 29th, almost April for our CICD UX meeting. And we don't have any standing topics to review right now. Is anything blocked or at risk for anyone? I know. Okay. All right, then I'll just leave it to Emily. Um, so to give some background on this, I think a bunch of you know, I am now working with the configure team and really sweet, come together into the environments team and with part of my onboarding was understanding what Kubernetes was, which is word I've heard quite a bit. During my onboarding, I was reading about it and nothing was sticking until one person told me about Kubernetes as a post office and I just wanted to go through that because I chatted with Gina about this and we were both I go. Everything makes so much more sense now. So I just put together a very, very tiny little presentation that kind of goes over what I learned during this onboarding. So going back to it, I did a lot of reading about Kubernetes. A lot of it was in a very technical jargon and nothing was sticking until I came across the post office metaphor where it kind of subbed in Kubernetes as like a post office management and everything here then started to make sense with me. So this is just like a cute little story about that. So a post office, as we all know, is responsible for processing and delivering thousands of packages each day and the goal being that the packages are delivered to the intended recipients quickly, securely and reliably. Kubernetes kind of does something very similar to that. To help achieve this, there's a team of postal workers are responsible for processing and delivering packages. However, with packages, there's many challenges, such as sometimes there's an influx of packages coming in, there's incorrect addresses that need to be fixed. And this is where the orchestration portion comes into play. So when you think about Kubernetes in the same frame as a post office, you can think about containers, which is a package of code as like the mail packages. And when you get a group of containers together into pods, you can think of this as just a group of mail packages that need to be delivered to the same place. So you have now that group of mail packages needing to get somewhere. And then there was no emoji for a postal worker, so just pretend he is a post worker. Kubernetes makes sure each group of packages known as the pod is assigned to a postal worker known as a node. And that all these groups of packages are evenly distributed across workers. So we're not overwhelming certain workers or certain workers aren't getting any work. It also makes sure workers have the resources necessary to handle each pod such as enough room in their delivery truck, which is the CPU or memory. And then Kubernetes also makes sure that each worker is not overwhelmed by packages by doing the load balancing or adjust the number of workers based on how many packages are coming in, which is the automatic scaling. And if a worker calls in sick or a node becomes unavailable, Kubernetes can help get the new worker on the job, so the delivery goes uninterrupted. So yeah, you can think of Kubernetes as an operating system that manages the processing and delivery of packages in a post office by ensuring they're all signed to a worker. And that the resources are allocated efficiently, so workers can do their job. So those are just a very, very tiny story I put together, but when doing my onboarding, this is the one thing that helped me really, really understand what Kubernetes was. And I thought it was cool to share out that with everyone else. That was so great. I love that. And it makes it like so much more obvious, I think, as to what Kubernetes means. And I have one question for you. Yes, is, do we. The information that you are saying like CPU and memory that is there. And if a node goes down and stuff, do we get that visibility into GitLab or do you only get that when you're using whatever Kubernetes platform you're using. Very good. I'm late on Ellie here because I'm still learning some of the stuff that we're actually showing in GitLab, but is that kind of the work that we're doing around the Kubernetes dashboard currently to get more of that information showing into the UI. Yeah, yeah, that's what we're trying to do with the dashboard. Sweet. I've had to deal with Kubernetes a lot lately with runners as well, because runners are sometimes in the way that you're saying that there's a coordinator it like the runner access that and he's like the head and then tells whatever machines to go work within the cluster. But a lot of people have brought up the fact that CPU or memory could be an issue. And that's why like the runners aren't working as expected. So I think we're going to start exploring that probably within like the next couple of years and there's going to be a lot more connection between the runners and Kubernetes in general. Yeah, I just wanted to point out Emily, like I love the presentation. It's super simple with like the emoji thing. I love it. And then what I would feel like honest feeling I needed this like three years ago when I started to work on the distribution game. I with a lot of peers with a lot of time watching the YouTube developers talking about Kubernetes wasn't easy. Ali is laughing because he knows I know that Ali was reading a book about Kubernetes. So this is what I mean. So thank you for that. And I also really like the point that Gina just mentioned that it's this concept could be work like load balancing and then it's kind of make things more scalable. And then I think it could be also applied to runner and many other places. And I think that's the reason why Kubernetes getting more popular. So truth I'm the for your presentation. Thank you. Yeah, something super simple. I wanted to put together. But when I was talking with Gina during our one on one, I was like, oh, this makes so much sense. I think it'll be helpful to share it to everyone. Erica, I think you're next. And yes, thank you, Emily. I wanted to share like the origin story, which I don't know if you know this, but this really helped me. And as much as I understand Kubernetes, which I don't think anyone ever will. So there's that. But so it started. This is like an Eric a version. So fact, well, don't just we're going with the story origin story. Okay. So Pokemon go was this big app, right, where everyone was trying to catch them all. And it was this huge craze. And it was a Google startup that was in charge of it and spinning it up. So what happened was they didn't anticipate that so many people would be using it so quickly. And within the first three days, everything crashed because they couldn't serve those users. And then they were like, well, how could we scale things more quickly without having a big overhead and cost. And then they bore Kubernetes, which is this idea that you don't have to have all that compute power all over the whole country. But when you need it, you scale it up immediately in this localized way. And I didn't really understand it. And then I had that like from I have I remember when that went down. And but that was like the computing problem that then kind of led to the creation of of Kubernetes. And then just one more note that I've heard it also I think the male the male person perspective is perfect because it's like more relatable to like I can think of a male person like a male man or a male woman person. But also like this idea of shipping containers is another metaphor you'll see a lot. But yeah, I don't I don't relate to a shipping container person as much as a male person. Okay, cool. Anyway. Yeah, there's the person of analogies. I remember when you told me about the hammer and the scooter stuff about secrets management. I was like, wow, it's just made everything seem so simple. It was a great presentation. Thanks, Emily. And it actually like it kind of explained in such a simple way the whole like the environment when you talk about everything related to communities like load balancers workers. I mean, what's the individual goal. I have been through some pretty complex comic books in the past, but they did not do this job. So yeah. But I took the inspiration from one of the courses. I did actually it might have been a documentary. You will just mentioned, but someone said this and then that's how that was really my My light bulb moment. So all the things has to go to I believe it actually was the documentary. I might have linked the wrong thing in my slides, but this documentary will kind of link here has was great in helping me understand Kubernetes. Yeah, and the documentary like briefly touched on what you covered in your slides like maybe the first two or three slides that didn't go like very deep with that metaphor. So it was nice to like have a better understanding of it. And then it also, I think it's like a two part documentary because the whole thing's posted to YouTube. I think there was a section of it that also touched on what you're convention with like Pokemon go. It's kind of that example. I do remember Pokemon go crashing on me that's so I remember being part of that. Thanks for sharing. Yeah. I wish I came across the story when I was still working in configure with yeah, I think in retrospect though, yeah, it's definitely going to stick now. So should we move on to the next items on the agenda. Yeah, I'm starting. So I'm working on the minimal on boarding flow for switchboard. I recorded a video of a walk through the initial concepts that I'm working on. And yeah, it's been a great way to gather information. Also during my conversations, there is some information that customers have to input during the setup process that they can never change again. They lose. They can't access. Get lab. Oh, we can't help them. By the way, is there a way that we communicate this type of info. And then yeah, I appreciate any insights or any if you can point me to something. And then I'm also using that to have conversations with engineers to help me with the scorecard that I'm working on to understand some of the stuff that they do. So, I have one question. So how do you work on the scorecard because I don't think there's existing work flow around switchboard like how would you handle that and how do you approach to work on the scorecard. So today, there's something very minimal. So, we have a switchboard platform and then SRIs kind of paste JSON file in there and kick off a bunch of jobs. And follow the link. Yeah. So, I think it's a good thing to have a switchboard. And we have to do outside switchboard. And because we also building for them would like them to do most of the stuff they do outside switchboard inside switchboard. So, it's kind of a hybrid of mappings, let's scorecard. It's like a bit. Yeah, it's going from the next phase. Okay. I have just a question about the your when you ask if we have an approach for telling the user about information they can never change again. Is this something. Are they selecting like what they're configuring and they can never reconfigure it after. Yeah, so they're bringing encryption keys to encrypt their github instance. And so we're giving them the option of bringing their own keys. And I think once they've put them, we can't see them. And if they lose them, we can't help them either. I see. This sort of reminds me of. Well, actually, no, it doesn't. I was going to say we do a tokens that are only displayed for a certain amount of time in the UI for a runner. And we just tell them like this is going to be displayed only for a short period of time. So copy, we give them the chance to copy it so that they don't lose it. But then if they lose it, like, so on them. So that's how we. Tell them right now. I don't know if it's. Something. I think I'll lean a lot on the right as to kind of help me communicate that. I guess since they isn't. Right? But everything, and this doesn't really relate. Any text and content and content. I don't know if it's supposed to take you to. I'll link that into. Starting to validate runner fleet dashboard, you might have seen I shared it in our UX co-working channel. And I'll just share the issue that I'm doing this through because we have a very hard time finding enterprise customers to meet with. And there's this process and the handbook that you can lean on customer success managers for that. So yeah, if you want to take a look at that issue that applies to you in the future, it might be helpful. Gina, have you reached out to any technical account managers? I think sometimes they're very reactive to our request. Yes. Okay. So they're now called customer success managers. Oh, okay. It's a secret report. It doesn't matter. No, that's okay. That's who I was saying the lean on because they've helped me in the past just like meet with customers just to hear what their pain points are. Yeah. So we have a list and ongoing list that Darren and I are keeping. Yeah. Thanks for clarifying that. Yeah. I'm just. That's it for me, Emily. You want to go? Well, I just had one open question about a list. Well, so I just had one open question about larger designs that are put in through a lot of small MRs that are kept behind a feature flag. I was getting this question from some of the engineers as if they get a UX review for each small MR or larger view at the end, mostly being they were doing small reviews per small MRs, but we were getting people wondering why the experience wasn't complete. Like you couldn't go through everything because it was just one piece of the MR and then the rest seemed broken. So I was just wanting to get other people's opinions on this. Yeah, this was so relevant to the token, like re-architecture that runner was doing and our devs did UX reviews for each small MR and then sometimes our front end dev will include a table in the description being like, this is where you are and these are the other MRs that are adding these other features. Or they'll just add like some description text to tell people when they're bringing up the changes. And then I linked an example there. Sometimes they'll also just ask me to review the MR before they ask like the random UX assigned reviewer. It's been like a mix. Well, thanks. Yeah, I think the problem was we're not like clear enough where it is in the experience because the issue at links to links to the entire future, not just the little portion. So I guess just being a lot more clear about what the MR is changing and what is out of scope for that particular MR. Yes, exactly. We did the exact same thing and linked to the entire feature that that MR was not creating. So I think just like that text that he had added was helpful. Vittika, did you want to? Yeah, it was very simple to what Gina said. So this I experienced when we started to work on the CI job token. I'm trying to find the MR. But like that proposal, it had to be broken down to like a few parts like some preparation by the front end, some preparation by the back end. And then eventually they would like club all of that to form the final to work on the final development and implementation. So for that, what we the process we followed was. Engineers used to invite me first to do a UX review and to make things easier for them. Like once I reviewed, I left a very detailed sort of summary for the next designer who will be assigned by the reviewer to look at. So I explained to them that this is the bigger proposal. This MR only takes care of like this small part of that solution. And eventually this will lead to something else. So yeah, that's how we did it. Now thanks for sharing that'll help us a lot with there's two big initiatives going on on my team right now. And I think we'll have to do this for both of them. So. Oh, okay. Yeah, my final thing is I'm finalizing the group level environment solution validation study this week. So hopefully this will make some progress after being paused during kind of like the team switch time. I'll pass it to you to get. Thanks. I just have this small update like besides all the state group work that I anyway keep sharing and co working and other channels. That in preparation for the UX theme workshop that I thought I would be conducting next month. I figured that we don't have a vision documented for five plans security yet. So like the very first step for me to prepare for the workshop was to meet with Jocelyn and right down our vision and do that keeping in mind the information that we have at hand like all the insights from Erica's research is the existing epics that we have the highly water issues that we have the market inside the jobs to be done. So I created sort of this homegrown template to have that conversation that discussion so that eventually like we make sure that whatever vision we arrive at it is done like after being informed of everything that else that we have in front of us. So the first mural is where we started from I just made a copy when I created that. So that was the very first date. I brought together all the information that we had on different areas and create two sections. One was for each category. The plan was that we create clusters like we'll be bringing together a item that could form one theme and we ended up with four groups of information or of like the work that we want to do and then we just voted on them. One of the options that we had was to maybe take it forward and instead do a like come up with the rice score but we refrain from doing that since we are going to be doing the UX thing workshop anyway like in future. So this we just kept this very simple and once we identified the larger themes the next step we took was put them like use this business model innovation framework to bring together like who are we making who are we creating the solution for how we are planning to do that and to achieve that how what is it that we would be working on in terms of features in terms of changes then why do we think like why are we confident that these sort of features are going to like solve the problem and the good part with this innovation framework is like you change if you make slightest of change in one part of this diagram you will have to like make adjustments all throughout so it's always balanced it's never imbalanced and it worked out pretty well for us we came up with our new vision which is we enable organizations to adopt good practices for secure handling of sensitive information and I just thought like since it worked so well maybe I'll just document this and share with the team I haven't done that yet though. Yeah that's all. So for the pipeline offering we are focusing on the solution validation for the placement library placed this catalog feature is it the left side navigation or is it more embedded version on the pipeline editor and there were another option in the navigation but inside the Explorer tab which is the same level with the organization the new tab but it's slightly behind that so it's the second step. So we just tried to work on another round of research so things are cut to jumping in so we just started this conversation how can we proceed with the new test and then once we got the result probably we can more confidently say okay so we should play this feature to where so I'm also working on that I do work on that also the rest of the week and there is another very critical discussion which is mostly about like how we architecture how we create architecture for the backend which will eventually impact the year's work so here is the MR like I just don't want to dive into the two details but it's more like if we design API this way then there's two there's some impact to the user flow and then if we design API in the be way then it also changed the user behavior so I'm really glad that I'm participating in this discussion and just trying to follow up what they're saying and then if we make this decision then our MVC scope might be slightly changed in terms of technical perspective but not a lot from the UX perspective so this is still ongoing just wanted to share with you all and yeah that was my agenda that do you have any questions feedback? No then I'll pass it over to Erica sorry do you want to verbalize your call? Yeah so I think that this has been like really good work and we're trying to figure out like the way to approach this solution validation but what we're going to end up doing is having an approach that other teams can use and basically like what we've arrived at is having two groups one that has the side nav in a prototype one that doesn't have the side nav in a prototype same tasks different entry points performance comparison maybe a satisfaction question in there and I think that's what we're going to need to make an argument for the side nav so just wanted to like say that this was a hard problem and to give props to say good job being patient and getting through it and then I think like we're going to have a model so that next time it won't require so much thinking as much thinking to get to how we approach it. Yeah thank you for sharing that Erica because it wasn't easy but the good thing is like full the first round of validation we could eliminate like a list one option and now we have two so that's good and then I think since it's a new process like there are some challenges it's just not our team there are some other teams also going for a similar process at the moment and then I think once we design nicely and then come up with the results I think probably we can also add it to our handbook and then provide better guidance on how to test the things around the placement. Yeah I've been thinking about this Erica and like wouldn't it always be the case that whenever we present to users an navigation option that's placed at a higher level it will always end up getting a better score. Yeah and exactly I've been calling it the bagel problem or I'm like we could ask them to search for bagels in the product and if we had bagels on the side nav they would always go to the side nav so yeah so that's why I think having the two groups there where one they just don't even have that option so it's not like bagels on the side nav then we can see if they can still perform or not and that's why it's with it that's why it's two different groups. Yeah I'm just eager to see like if you figure out that let's say without bagels being on the front shelf if this still happened to find it how do you still justify to the team that you know they were still able to find it. Yeah they found it much easier on the higher level so that's why I think it's like if performance is the same with those two groups we don't have evidence to add it to this okay it's only that if they are failing without that side nav option then we can say hey it's important okay and we don't we don't know like with the other approach I was like it's bagels like they'll just go to the where it says bagels but so with this approach we don't feel like we have a sense of which one will work and then that's when we should be doing research. Yeah thanks. Good question. Yeah I just had a question about how has the rest of your team been responding to performing this research and if there's been any learnings that you could share with us if we have to run into the same situation. So first of all I think they're excited that they're validating this because if the input is coming from the user then like we could really be more confident to say like we're placing this menu at this place because of this outcome so I think they like that from that perspective but on the other hand like I think they are still having this sound and more behind the future flag and I am not sure they I don't think they haven't they have merged it so of course there are also some frustration because they are ready to make this change happen but at this point they need to just wait for the results as that was the reason why the timeline and timeframe was a little bit tight because it's working progress so there are kind of like mixed feelings but in the end like they understood like of course like if we have more strong like evidence that's good and then I just try to like illustrate okay what's happening in this version so that they know what's going on for this research so that they could be more patient. Nice yeah thanks for sharing. You're right. So if you don't have any other comments then I'll pass it over to Will. Thanks, Angel. So just a couple quick updates. I was on a customer call Emily led a couple hours ago to learn more about users and questions of the group level environments. View concept that's going to feed into her solution validation study and I thought you did a good job handling you know multiple users or customers on the call and like trying to pivot to get to the questions that you really wanted to know given some of the time constraints that we had on the call so a good job there. I'm also working with Ali and Pedro to brainstorm just be done for switchboard so we've started that in an issue and now we're moving over to a Google doc to put that in there. I've also drafted an issue based on an initial team call that we had with switchboard that group. As I was writing it up after the meeting I was a little bit confused about the request so I'm going to need some context on how this fits into the foundational research plan that Hiana has drafted so I've I'll use to I've tagged you and Hiana for some additional feedback. Eric I see you're ready did you want to elaborate? Well maybe see if the team wants to respond first. Anyone? So I think we just need to see how this fits into what we're doing now and and make a call because yeah we're trying to find out what the customer wants but then also I mean our some of our users are also the general folks so I kind of feel like we're trying to do the same thing and then maybe my not necessarily to have two issues to do that. But that's just my feeling maybe we can get some feedback from Hiana as well. Yeah and that's why I want to like clarify that before like jumping in because if if this is like duplicate or redundant like let's not you know do that in that case. Um so my first note is just that they're I wanted you to know that there's this foundational research happening like in the back burner but we've run some participants on the life cycle of an image and understanding how those are environments and how that relates and kind of whether or not people want one pipeline for them or how they kind of structure that so that's happening and I can just catching up on life but I will put I will link to the dovetail and it might be helpful for you to watch some of those videos. That's my first point but then looking at the foundational research like I think we have a bunch of this just from the the secrets and security related research. Um and I think I'll look I'll take a note to look through all of that but one thing I can just quickly make note of is with the the personas we know that there's this like shift left with security so even developers are that was like a big thing that we keep finding is even developers are getting their hands on security and compliance stuff um and we did in the secret features prioritization survey we asked I'll bring up the finding but we asked them if they knew what compliance requirements they were working under um and surprisingly they had a sense of it even the developers but let me pull up that work um but I can dig in here and help you piece together what we will we know okay because it might feel like this is coming from nowhere but it's actually I didn't really even realize this was a nice summary people to be like oh we know some stuff about this so good job in framing the issue there everyone um but I can be helpful so let's figure out the best way for me to help you guys okay okay well um and then the last point um I'm working with Erica she may speak to it a little bit below but um I'm helping her pull some like customer emails later in the week um from our past us analysis so that I think she and her team can do some like future like follow calls so I think that's it for me yeah and since we have time I can actually just share my screen and show you the benchmark report yeah okay so this is a one side slide summary of the performance and I set up a benchmark findings epic that has like videos of overall like so this is all detailed and in a way like we issueatize this report um but basically there um we're in good we're doing good but with each of our workflows that we tested back to me mouse there's like one sort of really painful task in each of the workflows that we need to address so it's usually includes in tax um identifying the failure the reason for a failure and understanding the desk unit tests and then the first of the fine and fixed pipeline error workflows um and I so this table here kind of gives you a sense for each of those workflows this column gives you two to three two to three word summary of each of them so for the author of pipeline workflow we found a barrier to entry with the includes syntax which is important because that's the entry point for our CI components catalog um and as a companion analysis I looked at all the suspectum for verify for the last six quarters and did an alignment exercise to see how much the pain points for each of the tasks and then each of those UX themes in that table below mapped to the suspectum and and it was kind of astounding so 13% which we're calling a significant amount of those negative suspectum were related to this author of pipeline workflow um and then looking down here at these UX themes one of the themes across all the tasks was just general confusion about YAML and that was 8% of the negative suspectum so we really want to focus here on and making that whole YAML related workflow easier and then it occurred to me that we now actually have all of these people who gave us these responses about the YAML experience so we can actually that's great because they're hard to recruit right it's hard for us to find good matches so here we have this lovely sample of 43 participants who gave us feedback on the YAML experience there's like all these different categories where they made points but basically it bubbles up to having a hard time with the YAML experience so Will is do thank you Will is going to give us the email contact for those and what we can do because we can't really have although people would probably try we can't really schedule 43 calls but what we can do is run them through that CI alpha components program and those assignments and then we'll get a nice read on the satisfaction scores and how those are increasing right now we have like a dedicated really well-rounded sample where we can do deep dive interviews but actually they're pretty satisfied with the current template experience when we ask them so it means that it's harder for us to raise that bar in terms of their feedback and getting them into a really satisfied place so these folks who gave us the substrate back and were negative we should get a good read on if we can move them up to positive then we're really doing a good job so that was what Will was talking about and that's the author of pipeline workflow and then understanding a just unit test two words for that is his hidden treasure so it was amazing I just so one of the things we did at the end was we gave them this job to be done rating question where we had them agree or disagree on a scale of 1 to 7 where anyway so we had them look at their agreement and they gave us really high scores here and they were just they were delighted they were delighted once they understood they didn't understand what was quite happening with the unit test at first so they it wasn't discoverable for them because they went into the logs and they weren't looking at the UI and they did this like circular thing where they were like oh I've revived this artifact don't know what that means and then they would like arrive at the artifact in a different way and a different way and and then like when they then we kind of like pointed out hey the artifact is going to be helpful for this task and then they were like what is this artifact how does it relate to this error but once we did the reveal they were like thanking us at the end of the session like they were like and so this rating here this 6.1 is very high because they were just so pleased about it so I think that there is some small I mean I'm not the designer but if we can make it more discoverable and a little bit more obvious to them how those things are related like an example is so in this in the unit test it showed them a screenshot of what the website would look like if it displayed as per the code and they weren't sure if it was what the website should look like or it does look like so we just need to like connect those dots for them and then they will make them so happy and here we saw only 3% alignment with the sus for beta but that actually like tracks with this hidden treasure idea right where they're not quite understandable that's not discoverable for them and then for the fine and fixed pipeline errors 9% they're waiting too long another way to tell the story there though is that they learned which is really cool so if you look at this so we had fine and fixed pipeline errors in build and then one in test and one in deploy and so you can see here that they kind of bombed that first one but then on the second on the next one they were in green again and basically what it was is if they could step back and look at the patterns in the YAML file they could use that to quickly come up with the solution as opposed to like digging into the logs so if we can figure out how to help them take that step back maybe it's AI I don't know but to like look at the discrepancies in the different stages of the YAML file we could really help them succeed there but here you can see that this jobs to be done rating is not as high and that's because they dinged us because they get really frustrated waiting for the entire pipeline to run after every fix so that's a theme that came out here and it's actually that specific theme is small in terms of the percentage of suspectum I think that's because we logically have a smaller percentage of enterprise respondents and that's a particular enterprise problem that we were getting feedback on so another thing that Will and I yeah we'll know like our working on is to track business size in the SOTS responses now and so I think that will be helpful but yeah so this frustration with running the entire pipeline for every fix I've heard that before but it was really resounding and we see them bringing down our our satisfaction scores or agreement with the job to be done ability and that's because the user behavior in terms of fixing a pipeline is just to like throw a fix at it and see if that worked and throw a fix at it and see if that worked they're not like now with this work you'll have to check on the documentation like they're not doing that kind of thinking and so then it becomes really frustrating they're like I just wanted to try that thing and now I have to sit here and wait for however many minutes um and I asked so I did my like do diligence follow up like tell me about your tell me about your workflow and and like is this really a big thing and um they're like yes yes yes this gets me out of my flow gets me out of context I'll go watch a YouTube video and then I'm totally distracted and not useful um yes so that's the benchmark stuff in a quick summary um and then I wow let's let's go to comments and I'll stop sharing I had a question about how you determined if the suspect back percentage was significant considerable or small yep uh so um if there's documentation also that's already out there I can just look at that no it's totally no I love that question so um we I put that in the report and it's just mapping percentages okay so and it's just the because later on if it just has three percent or not like I put the percentages in that report right like nine percent just that later on and like Jackie's question was like is that good it's like oh so uh so uh yeah so we wanted to be able to characterize it in those ways and so just being transparent about how we're doing that um but I think it's legitimate because if we look at like kind of the other findings 33 percent of them were not categorizable right so like something about inconsistent behavior and CICD pipelines like I don't know how you map that too much um but so and you might think like that's a lot is should we be concerned but actually like it's pretty to me it legitimizes it because it means that there's a bucket of things you can't get forced fit into these benchmark pain points and themes and so like having the a third of it right be a round number is good um yeah to answer your point we just came up with these kind of categories um that's just so we're labeling it well thanks for yeah but they're mutually exclusive so it has to fit into one of the categories and only one of the categories and to do that I ended up having to parse out a lot of the verbatim because they'll talk about all these things and so to get the one to one's pouring um and I think it was 317 verbatim that we had wow okay and uh Erica when you pull those sus verbatim that i'm gonna you know find emails for later did you only focus on people who indicated they were open to follow up conversation yes sir okay cool yeah i just want to make sure i wasn't at first and i was like oh what's this column oh that's important but sorry i did i got it yeah and then i just pulled out just to try to make it easier so it's not lost in the issue verse here are these four pipeline authoring design related tasks that we have at a critical severity level um so those are the ones that are critical so like a way of like prioritizing all of them those would be the most important and then yeah the one that i pulled out i can lead is bringing those people into the um see i have a program Erica i just wanted to say really that this was really great work maybe because it related to things that i was working on so i was like more interested in it but yeah this was this was awesome and i love that we're seeing feedback across like the whole pipeline experience it's not just one section yeah actually when it touches upon the places that we have always like talked about and there there's never been like enough evidence that we should really focus on that work and improve those experiences so i'm very hopeful that the results of the benchmarking would like help us prioritize things which are really going to make um big changes we're good yeah that's such analysis was pretty amazing but i didn't expect to see that much overlap and and some people kind of just did like a one sentence like we found this also in the sess but i'm just extra so i was like i need to know the precise uh precise percentage of self-proclaimed homes that are mapping to each of these things um but in a way it's good because then we oh yeah because then we can make this statement that 45% of the negative self-proclaimed overlap with those like as an aggregate is 45% of them which is more than the 33% of the n-h so i think it's it's good stuff thanks thanks thanks oh yeah and then later this week i'll work on the Q2 research prioritization and so we'll start pinging in that to start getting threads of what we should be prioritizing and then the last but not least is i isn't linking yeah i put i put a deck together um after kind of now having this idea that we are focusing on environments oh i was like oh i can think of a key problem that i've heard so like across these four studies we keep hearing this problem related to um coordinating variables across environments so i just pulled out slides from each of those reports um because i would say that that's a big problem like we're not even asking about that it just keeps coming up so i linked that deck i'm glad to be back i missed everyone i was just only working on that one study so thank you for sharing all that too very excited all right um does anybody have anything else no all right we'll have a good rest of your weeks talk to you later bye bye",
  "segments": [
    {
      "id": 0,
      "seek": 0,
      "start": 0.0,
      "end": 12.0,
      "text": " Hello, this is March 29th, almost April for our CICD UX meeting.",
      "tokens": [
        50364,
        2425,
        11,
        341,
        307,
        6129,
        9413,
        392,
        11,
        1920,
        6929,
        337,
        527,
        37777,
        16508,
        40176,
        3440,
        13,
        50964
      ],
      "temperature": 0.0,
      "avg_logprob": -0.37158135308159723,
      "compression_ratio": 1.1942446043165467,
      "no_speech_prob": 0.04251469671726227
    },
    {
      "id": 1,
      "seek": 0,
      "start": 12.0,
      "end": 24.0,
      "text": " And we don't have any standing topics to review right now. Is anything blocked or at risk for anyone?",
      "tokens": [
        50964,
        400,
        321,
        500,
        380,
        362,
        604,
        4877,
        8378,
        281,
        3131,
        558,
        586,
        13,
        1119,
        1340,
        15470,
        420,
        412,
        3148,
        337,
        2878,
        30,
        51564
      ],
      "temperature": 0.0,
      "avg_logprob": -0.37158135308159723,
      "compression_ratio": 1.1942446043165467,
      "no_speech_prob": 0.04251469671726227
    },
    {
      "id": 2,
      "seek": 2400,
      "start": 24.0,
      "end": 30.0,
      "text": " I know. Okay. All right, then I'll just leave it to Emily.",
      "tokens": [
        50364,
        286,
        458,
        13,
        1033,
        13,
        1057,
        558,
        11,
        550,
        286,
        603,
        445,
        1856,
        309,
        281,
        15034,
        13,
        50664
      ],
      "temperature": 0.0,
      "avg_logprob": -0.2625524474353325,
      "compression_ratio": 1.5022421524663676,
      "no_speech_prob": 0.2798919975757599
    },
    {
      "id": 3,
      "seek": 2400,
      "start": 30.0,
      "end": 49.0,
      "text": " Um, so to give some background on this, I think a bunch of you know, I am now working with the configure team and really sweet, come together into the environments team and with part of my onboarding was understanding what Kubernetes was, which is word I've heard quite a bit.",
      "tokens": [
        50664,
        3301,
        11,
        370,
        281,
        976,
        512,
        3678,
        322,
        341,
        11,
        286,
        519,
        257,
        3840,
        295,
        291,
        458,
        11,
        286,
        669,
        586,
        1364,
        365,
        264,
        22162,
        1469,
        293,
        534,
        3844,
        11,
        808,
        1214,
        666,
        264,
        12388,
        1469,
        293,
        365,
        644,
        295,
        452,
        24033,
        278,
        390,
        3701,
        437,
        23145,
        390,
        11,
        597,
        307,
        1349,
        286,
        600,
        2198,
        1596,
        257,
        857,
        13,
        51614
      ],
      "temperature": 0.0,
      "avg_logprob": -0.2625524474353325,
      "compression_ratio": 1.5022421524663676,
      "no_speech_prob": 0.2798919975757599
    },
    {
      "id": 4,
      "seek": 4900,
      "start": 49.0,
      "end": 62.0,
      "text": " During my onboarding, I was reading about it and nothing was sticking until one person told me about Kubernetes as a post office and I just wanted to go through that because I chatted with Gina about this and we were both I go.",
      "tokens": [
        50364,
        6842,
        452,
        24033,
        278,
        11,
        286,
        390,
        3760,
        466,
        309,
        293,
        1825,
        390,
        13465,
        1826,
        472,
        954,
        1907,
        385,
        466,
        23145,
        382,
        257,
        2183,
        3398,
        293,
        286,
        445,
        1415,
        281,
        352,
        807,
        300,
        570,
        286,
        417,
        32509,
        365,
        34711,
        466,
        341,
        293,
        321,
        645,
        1293,
        286,
        352,
        13,
        51014
      ],
      "temperature": 0.0,
      "avg_logprob": -0.11675953042918238,
      "compression_ratio": 1.6224066390041494,
      "no_speech_prob": 0.0179242305457592
    },
    {
      "id": 5,
      "seek": 4900,
      "start": 62.0,
      "end": 76.0,
      "text": " Everything makes so much more sense now. So I just put together a very, very tiny little presentation that kind of goes over what I learned during this onboarding.",
      "tokens": [
        51014,
        5471,
        1669,
        370,
        709,
        544,
        2020,
        586,
        13,
        407,
        286,
        445,
        829,
        1214,
        257,
        588,
        11,
        588,
        5870,
        707,
        5860,
        300,
        733,
        295,
        1709,
        670,
        437,
        286,
        3264,
        1830,
        341,
        24033,
        278,
        13,
        51714
      ],
      "temperature": 0.0,
      "avg_logprob": -0.11675953042918238,
      "compression_ratio": 1.6224066390041494,
      "no_speech_prob": 0.0179242305457592
    },
    {
      "id": 6,
      "seek": 7600,
      "start": 76.0,
      "end": 96.0,
      "text": " So going back to it, I did a lot of reading about Kubernetes. A lot of it was in a very technical jargon and nothing was sticking until I came across the post office metaphor where it kind of subbed in Kubernetes as like a post office management and everything here then started to make sense with me.",
      "tokens": [
        50364,
        407,
        516,
        646,
        281,
        309,
        11,
        286,
        630,
        257,
        688,
        295,
        3760,
        466,
        23145,
        13,
        316,
        688,
        295,
        309,
        390,
        294,
        257,
        588,
        6191,
        15181,
        10660,
        293,
        1825,
        390,
        13465,
        1826,
        286,
        1361,
        2108,
        264,
        2183,
        3398,
        19157,
        689,
        309,
        733,
        295,
        1422,
        2883,
        294,
        23145,
        382,
        411,
        257,
        2183,
        3398,
        4592,
        293,
        1203,
        510,
        550,
        1409,
        281,
        652,
        2020,
        365,
        385,
        13,
        51364
      ],
      "temperature": 0.0,
      "avg_logprob": -0.09487021147315182,
      "compression_ratio": 1.5677083333333333,
      "no_speech_prob": 0.025051912292838097
    },
    {
      "id": 7,
      "seek": 9600,
      "start": 96.0,
      "end": 101.0,
      "text": " So this is just like a cute little story about that.",
      "tokens": [
        50364,
        407,
        341,
        307,
        445,
        411,
        257,
        4052,
        707,
        1657,
        466,
        300,
        13,
        50614
      ],
      "temperature": 0.0,
      "avg_logprob": -0.06638639995029995,
      "compression_ratio": 1.561904761904762,
      "no_speech_prob": 0.024420058354735374
    },
    {
      "id": 8,
      "seek": 9600,
      "start": 101.0,
      "end": 114.0,
      "text": " So a post office, as we all know, is responsible for processing and delivering thousands of packages each day and the goal being that the packages are delivered to the intended recipients quickly, securely and reliably.",
      "tokens": [
        50614,
        407,
        257,
        2183,
        3398,
        11,
        382,
        321,
        439,
        458,
        11,
        307,
        6250,
        337,
        9007,
        293,
        14666,
        5383,
        295,
        17401,
        1184,
        786,
        293,
        264,
        3387,
        885,
        300,
        264,
        17401,
        366,
        10144,
        281,
        264,
        10226,
        32440,
        2661,
        11,
        38348,
        293,
        49927,
        13,
        51264
      ],
      "temperature": 0.0,
      "avg_logprob": -0.06638639995029995,
      "compression_ratio": 1.561904761904762,
      "no_speech_prob": 0.024420058354735374
    },
    {
      "id": 9,
      "seek": 9600,
      "start": 114.0,
      "end": 118.0,
      "text": " Kubernetes kind of does something very similar to that.",
      "tokens": [
        51264,
        23145,
        733,
        295,
        775,
        746,
        588,
        2531,
        281,
        300,
        13,
        51464
      ],
      "temperature": 0.0,
      "avg_logprob": -0.06638639995029995,
      "compression_ratio": 1.561904761904762,
      "no_speech_prob": 0.024420058354735374
    },
    {
      "id": 10,
      "seek": 11800,
      "start": 118.0,
      "end": 133.0,
      "text": " To help achieve this, there's a team of postal workers are responsible for processing and delivering packages. However, with packages, there's many challenges, such as sometimes there's an influx of packages coming in, there's incorrect addresses that need to be fixed.",
      "tokens": [
        50364,
        1407,
        854,
        4584,
        341,
        11,
        456,
        311,
        257,
        1469,
        295,
        49645,
        5600,
        366,
        6250,
        337,
        9007,
        293,
        14666,
        17401,
        13,
        2908,
        11,
        365,
        17401,
        11,
        456,
        311,
        867,
        4759,
        11,
        1270,
        382,
        2171,
        456,
        311,
        364,
        9922,
        2449,
        295,
        17401,
        1348,
        294,
        11,
        456,
        311,
        18424,
        16862,
        300,
        643,
        281,
        312,
        6806,
        13,
        51114
      ],
      "temperature": 0.0,
      "avg_logprob": -0.08365809077947912,
      "compression_ratio": 1.5942028985507246,
      "no_speech_prob": 0.004053784534335136
    },
    {
      "id": 11,
      "seek": 11800,
      "start": 133.0,
      "end": 138.0,
      "text": " And this is where the orchestration portion comes into play.",
      "tokens": [
        51114,
        400,
        341,
        307,
        689,
        264,
        14161,
        2405,
        8044,
        1487,
        666,
        862,
        13,
        51364
      ],
      "temperature": 0.0,
      "avg_logprob": -0.08365809077947912,
      "compression_ratio": 1.5942028985507246,
      "no_speech_prob": 0.004053784534335136
    },
    {
      "id": 12,
      "seek": 13800,
      "start": 138.0,
      "end": 148.0,
      "text": " So when you think about Kubernetes in the same frame as a post office, you can think about containers, which is a package of code as like the mail packages.",
      "tokens": [
        50364,
        407,
        562,
        291,
        519,
        466,
        23145,
        294,
        264,
        912,
        3920,
        382,
        257,
        2183,
        3398,
        11,
        291,
        393,
        519,
        466,
        17089,
        11,
        597,
        307,
        257,
        7372,
        295,
        3089,
        382,
        411,
        264,
        10071,
        17401,
        13,
        50864
      ],
      "temperature": 0.0,
      "avg_logprob": -0.047333936581666444,
      "compression_ratio": 1.8689320388349515,
      "no_speech_prob": 0.0075953323394060135
    },
    {
      "id": 13,
      "seek": 13800,
      "start": 148.0,
      "end": 163.0,
      "text": " And when you get a group of containers together into pods, you can think of this as just a group of mail packages that need to be delivered to the same place. So you have now that group of mail packages needing to get somewhere.",
      "tokens": [
        50864,
        400,
        562,
        291,
        483,
        257,
        1594,
        295,
        17089,
        1214,
        666,
        31925,
        11,
        291,
        393,
        519,
        295,
        341,
        382,
        445,
        257,
        1594,
        295,
        10071,
        17401,
        300,
        643,
        281,
        312,
        10144,
        281,
        264,
        912,
        1081,
        13,
        407,
        291,
        362,
        586,
        300,
        1594,
        295,
        10071,
        17401,
        18006,
        281,
        483,
        4079,
        13,
        51614
      ],
      "temperature": 0.0,
      "avg_logprob": -0.047333936581666444,
      "compression_ratio": 1.8689320388349515,
      "no_speech_prob": 0.0075953323394060135
    },
    {
      "id": 14,
      "seek": 16300,
      "start": 163.0,
      "end": 169.0,
      "text": " And then there was no emoji for a postal worker, so just pretend he is a post worker.",
      "tokens": [
        50364,
        400,
        550,
        456,
        390,
        572,
        31595,
        337,
        257,
        49645,
        11346,
        11,
        370,
        445,
        11865,
        415,
        307,
        257,
        2183,
        11346,
        13,
        50664
      ],
      "temperature": 0.0,
      "avg_logprob": -0.1086775260635569,
      "compression_ratio": 1.7211538461538463,
      "no_speech_prob": 0.004717318806797266
    },
    {
      "id": 15,
      "seek": 16300,
      "start": 169.0,
      "end": 176.0,
      "text": " Kubernetes makes sure each group of packages known as the pod is assigned to a postal worker known as a node.",
      "tokens": [
        50664,
        23145,
        1669,
        988,
        1184,
        1594,
        295,
        17401,
        2570,
        382,
        264,
        2497,
        307,
        13279,
        281,
        257,
        49645,
        11346,
        2570,
        382,
        257,
        9984,
        13,
        51014
      ],
      "temperature": 0.0,
      "avg_logprob": -0.1086775260635569,
      "compression_ratio": 1.7211538461538463,
      "no_speech_prob": 0.004717318806797266
    },
    {
      "id": 16,
      "seek": 16300,
      "start": 176.0,
      "end": 185.0,
      "text": " And that all these groups of packages are evenly distributed across workers. So we're not overwhelming certain workers or certain workers aren't getting any work.",
      "tokens": [
        51014,
        400,
        300,
        439,
        613,
        3935,
        295,
        17401,
        366,
        17658,
        12631,
        2108,
        5600,
        13,
        407,
        321,
        434,
        406,
        13373,
        1629,
        5600,
        420,
        1629,
        5600,
        3212,
        380,
        1242,
        604,
        589,
        13,
        51464
      ],
      "temperature": 0.0,
      "avg_logprob": -0.1086775260635569,
      "compression_ratio": 1.7211538461538463,
      "no_speech_prob": 0.004717318806797266
    },
    {
      "id": 17,
      "seek": 18500,
      "start": 185.0,
      "end": 195.0,
      "text": " It also makes sure workers have the resources necessary to handle each pod such as enough room in their delivery truck, which is the CPU or memory.",
      "tokens": [
        50364,
        467,
        611,
        1669,
        988,
        5600,
        362,
        264,
        3593,
        4818,
        281,
        4813,
        1184,
        2497,
        1270,
        382,
        1547,
        1808,
        294,
        641,
        8982,
        5898,
        11,
        597,
        307,
        264,
        13199,
        420,
        4675,
        13,
        50864
      ],
      "temperature": 0.0,
      "avg_logprob": -0.06491804758707682,
      "compression_ratio": 1.6367713004484306,
      "no_speech_prob": 0.0041298773139715195
    },
    {
      "id": 18,
      "seek": 18500,
      "start": 195.0,
      "end": 208.0,
      "text": " And then Kubernetes also makes sure that each worker is not overwhelmed by packages by doing the load balancing or adjust the number of workers based on how many packages are coming in, which is the automatic scaling.",
      "tokens": [
        50864,
        400,
        550,
        23145,
        611,
        1669,
        988,
        300,
        1184,
        11346,
        307,
        406,
        19042,
        538,
        17401,
        538,
        884,
        264,
        3677,
        22495,
        420,
        4369,
        264,
        1230,
        295,
        5600,
        2361,
        322,
        577,
        867,
        17401,
        366,
        1348,
        294,
        11,
        597,
        307,
        264,
        12509,
        21589,
        13,
        51514
      ],
      "temperature": 0.0,
      "avg_logprob": -0.06491804758707682,
      "compression_ratio": 1.6367713004484306,
      "no_speech_prob": 0.0041298773139715195
    },
    {
      "id": 19,
      "seek": 20800,
      "start": 208.0,
      "end": 218.0,
      "text": " And if a worker calls in sick or a node becomes unavailable, Kubernetes can help get the new worker on the job, so the delivery goes uninterrupted.",
      "tokens": [
        50364,
        400,
        498,
        257,
        11346,
        5498,
        294,
        4998,
        420,
        257,
        9984,
        3643,
        36541,
        32699,
        11,
        23145,
        393,
        854,
        483,
        264,
        777,
        11346,
        322,
        264,
        1691,
        11,
        370,
        264,
        8982,
        1709,
        49234,
        5428,
        292,
        13,
        50864
      ],
      "temperature": 0.0,
      "avg_logprob": -0.10283587076892592,
      "compression_ratio": 1.5658536585365854,
      "no_speech_prob": 0.0007136969361454248
    },
    {
      "id": 20,
      "seek": 20800,
      "start": 218.0,
      "end": 227.0,
      "text": " So yeah, you can think of Kubernetes as an operating system that manages the processing and delivery of packages in a post office by ensuring they're all signed to a worker.",
      "tokens": [
        50864,
        407,
        1338,
        11,
        291,
        393,
        519,
        295,
        23145,
        382,
        364,
        7447,
        1185,
        300,
        22489,
        264,
        9007,
        293,
        8982,
        295,
        17401,
        294,
        257,
        2183,
        3398,
        538,
        16882,
        436,
        434,
        439,
        8175,
        281,
        257,
        11346,
        13,
        51314
      ],
      "temperature": 0.0,
      "avg_logprob": -0.10283587076892592,
      "compression_ratio": 1.5658536585365854,
      "no_speech_prob": 0.0007136969361454248
    },
    {
      "id": 21,
      "seek": 22700,
      "start": 227.0,
      "end": 232.0,
      "text": " And that the resources are allocated efficiently, so workers can do their job.",
      "tokens": [
        50364,
        400,
        300,
        264,
        3593,
        366,
        29772,
        19621,
        11,
        370,
        5600,
        393,
        360,
        641,
        1691,
        13,
        50614
      ],
      "temperature": 0.0,
      "avg_logprob": -0.08861900038189358,
      "compression_ratio": 1.5392156862745099,
      "no_speech_prob": 0.018643997609615326
    },
    {
      "id": 22,
      "seek": 22700,
      "start": 232.0,
      "end": 249.0,
      "text": " So those are just a very, very tiny story I put together, but when doing my onboarding, this is the one thing that helped me really, really understand what Kubernetes was. And I thought it was cool to share out that with everyone else.",
      "tokens": [
        50614,
        407,
        729,
        366,
        445,
        257,
        588,
        11,
        588,
        5870,
        1657,
        286,
        829,
        1214,
        11,
        457,
        562,
        884,
        452,
        24033,
        278,
        11,
        341,
        307,
        264,
        472,
        551,
        300,
        4254,
        385,
        534,
        11,
        534,
        1223,
        437,
        23145,
        390,
        13,
        400,
        286,
        1194,
        309,
        390,
        1627,
        281,
        2073,
        484,
        300,
        365,
        1518,
        1646,
        13,
        51464
      ],
      "temperature": 0.0,
      "avg_logprob": -0.08861900038189358,
      "compression_ratio": 1.5392156862745099,
      "no_speech_prob": 0.018643997609615326
    },
    {
      "id": 23,
      "seek": 24900,
      "start": 249.0,
      "end": 264.0,
      "text": " That was so great. I love that. And it makes it like so much more obvious, I think, as to what Kubernetes means. And I have one question for you. Yes, is, do we.",
      "tokens": [
        50364,
        663,
        390,
        370,
        869,
        13,
        286,
        959,
        300,
        13,
        400,
        309,
        1669,
        309,
        411,
        370,
        709,
        544,
        6322,
        11,
        286,
        519,
        11,
        382,
        281,
        437,
        23145,
        1355,
        13,
        400,
        286,
        362,
        472,
        1168,
        337,
        291,
        13,
        1079,
        11,
        307,
        11,
        360,
        321,
        13,
        51114
      ],
      "temperature": 0.0,
      "avg_logprob": -0.17288540779276096,
      "compression_ratio": 1.2578125,
      "no_speech_prob": 0.10255797207355499
    },
    {
      "id": 24,
      "seek": 26400,
      "start": 264.0,
      "end": 280.0,
      "text": " The information that you are saying like CPU and memory that is there. And if a node goes down and stuff, do we get that visibility into GitLab or do you only get that when you're using whatever Kubernetes platform you're using.",
      "tokens": [
        50364,
        440,
        1589,
        300,
        291,
        366,
        1566,
        411,
        13199,
        293,
        4675,
        300,
        307,
        456,
        13,
        400,
        498,
        257,
        9984,
        1709,
        760,
        293,
        1507,
        11,
        360,
        321,
        483,
        300,
        19883,
        666,
        16939,
        37880,
        420,
        360,
        291,
        787,
        483,
        300,
        562,
        291,
        434,
        1228,
        2035,
        23145,
        3663,
        291,
        434,
        1228,
        13,
        51164
      ],
      "temperature": 0.0,
      "avg_logprob": -0.1701814761528602,
      "compression_ratio": 1.425,
      "no_speech_prob": 0.18294145166873932
    },
    {
      "id": 25,
      "seek": 28000,
      "start": 281.0,
      "end": 296.0,
      "text": " Very good. I'm late on Ellie here because I'm still learning some of the stuff that we're actually showing in GitLab, but is that kind of the work that we're doing around the Kubernetes dashboard currently to get more of that information showing into the UI.",
      "tokens": [
        50414,
        4372,
        665,
        13,
        286,
        478,
        3469,
        322,
        27151,
        510,
        570,
        286,
        478,
        920,
        2539,
        512,
        295,
        264,
        1507,
        300,
        321,
        434,
        767,
        4099,
        294,
        16939,
        37880,
        11,
        457,
        307,
        300,
        733,
        295,
        264,
        589,
        300,
        321,
        434,
        884,
        926,
        264,
        23145,
        18342,
        4362,
        281,
        483,
        544,
        295,
        300,
        1589,
        4099,
        666,
        264,
        15682,
        13,
        51164
      ],
      "temperature": 0.0,
      "avg_logprob": -0.12782366652237742,
      "compression_ratio": 1.5891089108910892,
      "no_speech_prob": 0.051870692521333694
    },
    {
      "id": 26,
      "seek": 28000,
      "start": 296.0,
      "end": 301.0,
      "text": " Yeah, yeah, that's what we're trying to do with the dashboard.",
      "tokens": [
        51164,
        865,
        11,
        1338,
        11,
        300,
        311,
        437,
        321,
        434,
        1382,
        281,
        360,
        365,
        264,
        18342,
        13,
        51414
      ],
      "temperature": 0.0,
      "avg_logprob": -0.12782366652237742,
      "compression_ratio": 1.5891089108910892,
      "no_speech_prob": 0.051870692521333694
    },
    {
      "id": 27,
      "seek": 30100,
      "start": 302.0,
      "end": 303.0,
      "text": " Sweet.",
      "tokens": [
        50414,
        14653,
        13,
        50464
      ],
      "temperature": 0.0,
      "avg_logprob": -0.15820501721094524,
      "compression_ratio": 1.5674157303370786,
      "no_speech_prob": 0.09601347893476486
    },
    {
      "id": 28,
      "seek": 30100,
      "start": 303.0,
      "end": 324.0,
      "text": " I've had to deal with Kubernetes a lot lately with runners as well, because runners are sometimes in the way that you're saying that there's a coordinator it like the runner access that and he's like the head and then tells whatever machines to go work within the cluster.",
      "tokens": [
        50464,
        286,
        600,
        632,
        281,
        2028,
        365,
        23145,
        257,
        688,
        12881,
        365,
        33892,
        382,
        731,
        11,
        570,
        33892,
        366,
        2171,
        294,
        264,
        636,
        300,
        291,
        434,
        1566,
        300,
        456,
        311,
        257,
        27394,
        309,
        411,
        264,
        24376,
        2105,
        300,
        293,
        415,
        311,
        411,
        264,
        1378,
        293,
        550,
        5112,
        2035,
        8379,
        281,
        352,
        589,
        1951,
        264,
        13630,
        13,
        51514
      ],
      "temperature": 0.0,
      "avg_logprob": -0.15820501721094524,
      "compression_ratio": 1.5674157303370786,
      "no_speech_prob": 0.09601347893476486
    },
    {
      "id": 29,
      "seek": 32400,
      "start": 324.0,
      "end": 345.0,
      "text": " But a lot of people have brought up the fact that CPU or memory could be an issue. And that's why like the runners aren't working as expected. So I think we're going to start exploring that probably within like the next couple of years and there's going to be a lot more connection between the runners and Kubernetes in general.",
      "tokens": [
        50364,
        583,
        257,
        688,
        295,
        561,
        362,
        3038,
        493,
        264,
        1186,
        300,
        13199,
        420,
        4675,
        727,
        312,
        364,
        2734,
        13,
        400,
        300,
        311,
        983,
        411,
        264,
        33892,
        3212,
        380,
        1364,
        382,
        5176,
        13,
        407,
        286,
        519,
        321,
        434,
        516,
        281,
        722,
        12736,
        300,
        1391,
        1951,
        411,
        264,
        958,
        1916,
        295,
        924,
        293,
        456,
        311,
        516,
        281,
        312,
        257,
        688,
        544,
        4984,
        1296,
        264,
        33892,
        293,
        23145,
        294,
        2674,
        13,
        51414
      ],
      "temperature": 0.0,
      "avg_logprob": -0.11540144019656712,
      "compression_ratio": 1.5545023696682465,
      "no_speech_prob": 0.017522769048810005
    },
    {
      "id": 30,
      "seek": 34500,
      "start": 346.0,
      "end": 366.0,
      "text": " Yeah, I just wanted to point out Emily, like I love the presentation. It's super simple with like the emoji thing. I love it. And then what I would feel like honest feeling I needed this like three years ago when I started to work on the distribution game.",
      "tokens": [
        50414,
        865,
        11,
        286,
        445,
        1415,
        281,
        935,
        484,
        15034,
        11,
        411,
        286,
        959,
        264,
        5860,
        13,
        467,
        311,
        1687,
        2199,
        365,
        411,
        264,
        31595,
        551,
        13,
        286,
        959,
        309,
        13,
        400,
        550,
        437,
        286,
        576,
        841,
        411,
        3245,
        2633,
        286,
        2978,
        341,
        411,
        1045,
        924,
        2057,
        562,
        286,
        1409,
        281,
        589,
        322,
        264,
        7316,
        1216,
        13,
        51414
      ],
      "temperature": 0.0,
      "avg_logprob": -0.19168032010396321,
      "compression_ratio": 1.471264367816092,
      "no_speech_prob": 0.09856301546096802
    },
    {
      "id": 31,
      "seek": 36600,
      "start": 366.0,
      "end": 395.0,
      "text": " I with a lot of peers with a lot of time watching the YouTube developers talking about Kubernetes wasn't easy. Ali is laughing because he knows I know that Ali was reading a book about Kubernetes. So this is what I mean. So thank you for that. And I also really like the point that Gina just mentioned that it's this concept could be work like load balancing and then it's kind of make things more scalable.",
      "tokens": [
        50364,
        286,
        365,
        257,
        688,
        295,
        16739,
        365,
        257,
        688,
        295,
        565,
        1976,
        264,
        3088,
        8849,
        1417,
        466,
        23145,
        2067,
        380,
        1858,
        13,
        12020,
        307,
        5059,
        570,
        415,
        3255,
        286,
        458,
        300,
        12020,
        390,
        3760,
        257,
        1446,
        466,
        23145,
        13,
        407,
        341,
        307,
        437,
        286,
        914,
        13,
        407,
        1309,
        291,
        337,
        300,
        13,
        400,
        286,
        611,
        534,
        411,
        264,
        935,
        300,
        34711,
        445,
        2835,
        300,
        309,
        311,
        341,
        3410,
        727,
        312,
        589,
        411,
        3677,
        22495,
        293,
        550,
        309,
        311,
        733,
        295,
        652,
        721,
        544,
        38481,
        13,
        51814
      ],
      "temperature": 0.0,
      "avg_logprob": -0.21288389570257638,
      "compression_ratio": 1.6887966804979253,
      "no_speech_prob": 0.2140817791223526
    },
    {
      "id": 32,
      "seek": 39500,
      "start": 395.0,
      "end": 408.0,
      "text": " And then I think it could be also applied to runner and many other places. And I think that's the reason why Kubernetes getting more popular. So truth I'm the for your presentation. Thank you.",
      "tokens": [
        50364,
        400,
        550,
        286,
        519,
        309,
        727,
        312,
        611,
        6456,
        281,
        24376,
        293,
        867,
        661,
        3190,
        13,
        400,
        286,
        519,
        300,
        311,
        264,
        1778,
        983,
        23145,
        1242,
        544,
        3743,
        13,
        407,
        3494,
        286,
        478,
        264,
        337,
        428,
        5860,
        13,
        1044,
        291,
        13,
        51014
      ],
      "temperature": 0.0,
      "avg_logprob": -0.2188874880472819,
      "compression_ratio": 1.546875,
      "no_speech_prob": 0.00953258853405714
    },
    {
      "id": 33,
      "seek": 39500,
      "start": 408.0,
      "end": 420.0,
      "text": " Yeah, something super simple. I wanted to put together. But when I was talking with Gina during our one on one, I was like, oh, this makes so much sense. I think it'll be helpful to share it to everyone.",
      "tokens": [
        51014,
        865,
        11,
        746,
        1687,
        2199,
        13,
        286,
        1415,
        281,
        829,
        1214,
        13,
        583,
        562,
        286,
        390,
        1417,
        365,
        34711,
        1830,
        527,
        472,
        322,
        472,
        11,
        286,
        390,
        411,
        11,
        1954,
        11,
        341,
        1669,
        370,
        709,
        2020,
        13,
        286,
        519,
        309,
        603,
        312,
        4961,
        281,
        2073,
        309,
        281,
        1518,
        13,
        51614
      ],
      "temperature": 0.0,
      "avg_logprob": -0.2188874880472819,
      "compression_ratio": 1.546875,
      "no_speech_prob": 0.00953258853405714
    },
    {
      "id": 34,
      "seek": 42000,
      "start": 421.0,
      "end": 425.0,
      "text": " Erica, I think you're next.",
      "tokens": [
        50414,
        37429,
        11,
        286,
        519,
        291,
        434,
        958,
        13,
        50614
      ],
      "temperature": 0.0,
      "avg_logprob": -0.1534548617423849,
      "compression_ratio": 1.272,
      "no_speech_prob": 0.03980020806193352
    },
    {
      "id": 35,
      "seek": 42000,
      "start": 425.0,
      "end": 436.0,
      "text": " And yes, thank you, Emily. I wanted to share like the origin story, which I don't know if you know this, but this really helped me.",
      "tokens": [
        50614,
        400,
        2086,
        11,
        1309,
        291,
        11,
        15034,
        13,
        286,
        1415,
        281,
        2073,
        411,
        264,
        4957,
        1657,
        11,
        597,
        286,
        500,
        380,
        458,
        498,
        291,
        458,
        341,
        11,
        457,
        341,
        534,
        4254,
        385,
        13,
        51164
      ],
      "temperature": 0.0,
      "avg_logprob": -0.1534548617423849,
      "compression_ratio": 1.272,
      "no_speech_prob": 0.03980020806193352
    },
    {
      "id": 36,
      "seek": 43600,
      "start": 437.0,
      "end": 465.0,
      "text": " And as much as I understand Kubernetes, which I don't think anyone ever will. So there's that. But so it started. This is like an Eric a version. So fact, well, don't just we're going with the story origin story. Okay. So Pokemon go was this big app, right, where everyone was trying to catch them all. And it was this huge craze.",
      "tokens": [
        50414,
        400,
        382,
        709,
        382,
        286,
        1223,
        23145,
        11,
        597,
        286,
        500,
        380,
        519,
        2878,
        1562,
        486,
        13,
        407,
        456,
        311,
        300,
        13,
        583,
        370,
        309,
        1409,
        13,
        639,
        307,
        411,
        364,
        9336,
        257,
        3037,
        13,
        407,
        1186,
        11,
        731,
        11,
        500,
        380,
        445,
        321,
        434,
        516,
        365,
        264,
        1657,
        4957,
        1657,
        13,
        1033,
        13,
        407,
        13796,
        352,
        390,
        341,
        955,
        724,
        11,
        558,
        11,
        689,
        1518,
        390,
        1382,
        281,
        3745,
        552,
        439,
        13,
        400,
        309,
        390,
        341,
        2603,
        2094,
        1381,
        13,
        51814
      ],
      "temperature": 0.0,
      "avg_logprob": -0.1971944247975069,
      "compression_ratio": 1.4864864864864864,
      "no_speech_prob": 0.03767034411430359
    },
    {
      "id": 37,
      "seek": 46500,
      "start": 465.0,
      "end": 479.0,
      "text": " And it was a Google startup that was in charge of it and spinning it up. So what happened was they didn't anticipate that so many people would be using it so quickly.",
      "tokens": [
        50364,
        400,
        309,
        390,
        257,
        3329,
        18578,
        300,
        390,
        294,
        4602,
        295,
        309,
        293,
        15640,
        309,
        493,
        13,
        407,
        437,
        2011,
        390,
        436,
        994,
        380,
        21685,
        300,
        370,
        867,
        561,
        576,
        312,
        1228,
        309,
        370,
        2661,
        13,
        51064
      ],
      "temperature": 0.0,
      "avg_logprob": -0.06553840637207031,
      "compression_ratio": 1.3387096774193548,
      "no_speech_prob": 0.0028738421387970448
    },
    {
      "id": 38,
      "seek": 47900,
      "start": 479.0,
      "end": 495.0,
      "text": " And within the first three days, everything crashed because they couldn't serve those users. And then they were like, well, how could we scale things more quickly without having a big overhead and cost.",
      "tokens": [
        50364,
        400,
        1951,
        264,
        700,
        1045,
        1708,
        11,
        1203,
        24190,
        570,
        436,
        2809,
        380,
        4596,
        729,
        5022,
        13,
        400,
        550,
        436,
        645,
        411,
        11,
        731,
        11,
        577,
        727,
        321,
        4373,
        721,
        544,
        2661,
        1553,
        1419,
        257,
        955,
        19922,
        293,
        2063,
        13,
        51164
      ],
      "temperature": 0.0,
      "avg_logprob": -0.09678477590734308,
      "compression_ratio": 1.4225352112676057,
      "no_speech_prob": 0.10872118920087814
    },
    {
      "id": 39,
      "seek": 49500,
      "start": 495.0,
      "end": 513.0,
      "text": " And then they bore Kubernetes, which is this idea that you don't have to have all that compute power all over the whole country. But when you need it, you scale it up immediately in this localized way.",
      "tokens": [
        50364,
        400,
        550,
        436,
        26002,
        23145,
        11,
        597,
        307,
        341,
        1558,
        300,
        291,
        500,
        380,
        362,
        281,
        362,
        439,
        300,
        14722,
        1347,
        439,
        670,
        264,
        1379,
        1941,
        13,
        583,
        562,
        291,
        643,
        309,
        11,
        291,
        4373,
        309,
        493,
        4258,
        294,
        341,
        44574,
        636,
        13,
        51264
      ],
      "temperature": 0.0,
      "avg_logprob": -0.07559582527647628,
      "compression_ratio": 1.4154929577464788,
      "no_speech_prob": 0.03984649106860161
    },
    {
      "id": 40,
      "seek": 51300,
      "start": 514.0,
      "end": 521.0,
      "text": " And I didn't really understand it. And then I had that like from I have I remember when that went down.",
      "tokens": [
        50414,
        400,
        286,
        994,
        380,
        534,
        1223,
        309,
        13,
        400,
        550,
        286,
        632,
        300,
        411,
        490,
        286,
        362,
        286,
        1604,
        562,
        300,
        1437,
        760,
        13,
        50764
      ],
      "temperature": 0.0,
      "avg_logprob": -0.13253851890563964,
      "compression_ratio": 1.45,
      "no_speech_prob": 0.12887027859687805
    },
    {
      "id": 41,
      "seek": 51300,
      "start": 523.0,
      "end": 531.0,
      "text": " And but that was like the computing problem that then kind of led to the creation of of Kubernetes.",
      "tokens": [
        50864,
        400,
        457,
        300,
        390,
        411,
        264,
        15866,
        1154,
        300,
        550,
        733,
        295,
        4684,
        281,
        264,
        8016,
        295,
        295,
        23145,
        13,
        51264
      ],
      "temperature": 0.0,
      "avg_logprob": -0.13253851890563964,
      "compression_ratio": 1.45,
      "no_speech_prob": 0.12887027859687805
    },
    {
      "id": 42,
      "seek": 53100,
      "start": 532.0,
      "end": 549.0,
      "text": " And then just one more note that I've heard it also I think the male the male person perspective is perfect because it's like more relatable to like I can think of a male person like a male man or a male woman person.",
      "tokens": [
        50414,
        400,
        550,
        445,
        472,
        544,
        3637,
        300,
        286,
        600,
        2198,
        309,
        611,
        286,
        519,
        264,
        7133,
        264,
        7133,
        954,
        4585,
        307,
        2176,
        570,
        309,
        311,
        411,
        544,
        42355,
        281,
        411,
        286,
        393,
        519,
        295,
        257,
        7133,
        954,
        411,
        257,
        7133,
        587,
        420,
        257,
        7133,
        3059,
        954,
        13,
        51264
      ],
      "temperature": 0.0,
      "avg_logprob": -0.14824540007348155,
      "compression_ratio": 1.631578947368421,
      "no_speech_prob": 0.03645256161689758
    },
    {
      "id": 43,
      "seek": 54900,
      "start": 550.0,
      "end": 555.0,
      "text": " But also like this idea of shipping containers is another metaphor you'll see a lot.",
      "tokens": [
        50414,
        583,
        611,
        411,
        341,
        1558,
        295,
        14122,
        17089,
        307,
        1071,
        19157,
        291,
        603,
        536,
        257,
        688,
        13,
        50664
      ],
      "temperature": 0.0,
      "avg_logprob": -0.1761754015658764,
      "compression_ratio": 1.5975103734439835,
      "no_speech_prob": 0.17649181187152863
    },
    {
      "id": 44,
      "seek": 54900,
      "start": 556.0,
      "end": 564.0,
      "text": " But yeah, I don't I don't relate to a shipping container person as much as a male person. Okay, cool. Anyway.",
      "tokens": [
        50714,
        583,
        1338,
        11,
        286,
        500,
        380,
        286,
        500,
        380,
        10961,
        281,
        257,
        14122,
        10129,
        954,
        382,
        709,
        382,
        257,
        7133,
        954,
        13,
        1033,
        11,
        1627,
        13,
        5684,
        13,
        51114
      ],
      "temperature": 0.0,
      "avg_logprob": -0.1761754015658764,
      "compression_ratio": 1.5975103734439835,
      "no_speech_prob": 0.17649181187152863
    },
    {
      "id": 45,
      "seek": 54900,
      "start": 566.0,
      "end": 578.0,
      "text": " Yeah, there's the person of analogies. I remember when you told me about the hammer and the scooter stuff about secrets management. I was like, wow, it's just made everything seem so simple.",
      "tokens": [
        51214,
        865,
        11,
        456,
        311,
        264,
        954,
        295,
        16660,
        530,
        13,
        286,
        1604,
        562,
        291,
        1907,
        385,
        466,
        264,
        13017,
        293,
        264,
        30441,
        1507,
        466,
        14093,
        4592,
        13,
        286,
        390,
        411,
        11,
        6076,
        11,
        309,
        311,
        445,
        1027,
        1203,
        1643,
        370,
        2199,
        13,
        51814
      ],
      "temperature": 0.0,
      "avg_logprob": -0.1761754015658764,
      "compression_ratio": 1.5975103734439835,
      "no_speech_prob": 0.17649181187152863
    },
    {
      "id": 46,
      "seek": 57900,
      "start": 579.0,
      "end": 607.0,
      "text": " It was a great presentation. Thanks, Emily. And it actually like it kind of explained in such a simple way the whole like the environment when you talk about everything related to communities like load balancers workers. I mean, what's the individual goal. I have been through some pretty complex comic books in the past, but they did not do this job. So yeah.",
      "tokens": [
        50414,
        467,
        390,
        257,
        869,
        5860,
        13,
        2561,
        11,
        15034,
        13,
        400,
        309,
        767,
        411,
        309,
        733,
        295,
        8825,
        294,
        1270,
        257,
        2199,
        636,
        264,
        1379,
        411,
        264,
        2823,
        562,
        291,
        751,
        466,
        1203,
        4077,
        281,
        4456,
        411,
        3677,
        3119,
        4463,
        433,
        5600,
        13,
        286,
        914,
        11,
        437,
        311,
        264,
        2609,
        3387,
        13,
        286,
        362,
        668,
        807,
        512,
        1238,
        3997,
        13900,
        3642,
        294,
        264,
        1791,
        11,
        457,
        436,
        630,
        406,
        360,
        341,
        1691,
        13,
        407,
        1338,
        13,
        51764
      ],
      "temperature": 0.0,
      "avg_logprob": -0.17380104789251014,
      "compression_ratio": 1.5126050420168067,
      "no_speech_prob": 0.005605258513242006
    },
    {
      "id": 47,
      "seek": 60900,
      "start": 609.0,
      "end": 623.0,
      "text": " But I took the inspiration from one of the courses. I did actually it might have been a documentary. You will just mentioned, but someone said this and then that's how that was really my",
      "tokens": [
        50364,
        583,
        286,
        1890,
        264,
        10249,
        490,
        472,
        295,
        264,
        7712,
        13,
        286,
        630,
        767,
        309,
        1062,
        362,
        668,
        257,
        15674,
        13,
        509,
        486,
        445,
        2835,
        11,
        457,
        1580,
        848,
        341,
        293,
        550,
        300,
        311,
        577,
        300,
        390,
        534,
        452,
        51064
      ],
      "temperature": 0.0,
      "avg_logprob": -0.20323367451512536,
      "compression_ratio": 1.3381294964028776,
      "no_speech_prob": 0.00410201633349061
    },
    {
      "id": 48,
      "seek": 62300,
      "start": 624.0,
      "end": 638.0,
      "text": " My light bulb moment. So all the things has to go to I believe it actually was the documentary. I might have linked the wrong thing in my slides, but this documentary will kind of link here has was great in helping me understand Kubernetes.",
      "tokens": [
        50414,
        1222,
        1442,
        21122,
        1623,
        13,
        407,
        439,
        264,
        721,
        575,
        281,
        352,
        281,
        286,
        1697,
        309,
        767,
        390,
        264,
        15674,
        13,
        286,
        1062,
        362,
        9408,
        264,
        2085,
        551,
        294,
        452,
        9788,
        11,
        457,
        341,
        15674,
        486,
        733,
        295,
        2113,
        510,
        575,
        390,
        869,
        294,
        4315,
        385,
        1223,
        23145,
        13,
        51114
      ],
      "temperature": 0.0,
      "avg_logprob": -0.13608028303902103,
      "compression_ratio": 1.4906832298136645,
      "no_speech_prob": 0.2892454266548157
    },
    {
      "id": 49,
      "seek": 63800,
      "start": 639.0,
      "end": 656.0,
      "text": " Yeah, and the documentary like briefly touched on what you covered in your slides like maybe the first two or three slides that didn't go like very deep with that metaphor. So it was nice to like have a better understanding of it.",
      "tokens": [
        50414,
        865,
        11,
        293,
        264,
        15674,
        411,
        10515,
        9828,
        322,
        437,
        291,
        5343,
        294,
        428,
        9788,
        411,
        1310,
        264,
        700,
        732,
        420,
        1045,
        9788,
        300,
        994,
        380,
        352,
        411,
        588,
        2452,
        365,
        300,
        19157,
        13,
        407,
        309,
        390,
        1481,
        281,
        411,
        362,
        257,
        1101,
        3701,
        295,
        309,
        13,
        51264
      ],
      "temperature": 0.0,
      "avg_logprob": -0.14872412588082107,
      "compression_ratio": 1.4743589743589745,
      "no_speech_prob": 0.4451676309108734
    },
    {
      "id": 50,
      "seek": 65600,
      "start": 657.0,
      "end": 670.0,
      "text": " And then it also, I think it's like a two part documentary because the whole thing's posted to YouTube. I think there was a section of it that also touched on what you're convention with",
      "tokens": [
        50414,
        400,
        550,
        309,
        611,
        11,
        286,
        519,
        309,
        311,
        411,
        257,
        732,
        644,
        15674,
        570,
        264,
        1379,
        551,
        311,
        9437,
        281,
        3088,
        13,
        286,
        519,
        456,
        390,
        257,
        3541,
        295,
        309,
        300,
        611,
        9828,
        322,
        437,
        291,
        434,
        10286,
        365,
        51064
      ],
      "temperature": 0.0,
      "avg_logprob": -0.2677464118370643,
      "compression_ratio": 1.61139896373057,
      "no_speech_prob": 0.01408003456890583
    },
    {
      "id": 51,
      "seek": 65600,
      "start": 671.0,
      "end": 673.0,
      "text": " like Pokemon go.",
      "tokens": [
        51114,
        411,
        13796,
        352,
        13,
        51214
      ],
      "temperature": 0.0,
      "avg_logprob": -0.2677464118370643,
      "compression_ratio": 1.61139896373057,
      "no_speech_prob": 0.01408003456890583
    },
    {
      "id": 52,
      "seek": 65600,
      "start": 673.0,
      "end": 675.0,
      "text": " It's kind of that example.",
      "tokens": [
        51214,
        467,
        311,
        733,
        295,
        300,
        1365,
        13,
        51314
      ],
      "temperature": 0.0,
      "avg_logprob": -0.2677464118370643,
      "compression_ratio": 1.61139896373057,
      "no_speech_prob": 0.01408003456890583
    },
    {
      "id": 53,
      "seek": 65600,
      "start": 679.0,
      "end": 684.0,
      "text": " I do remember Pokemon go crashing on me that's so I remember being part of that.",
      "tokens": [
        51514,
        286,
        360,
        1604,
        13796,
        352,
        26900,
        322,
        385,
        300,
        311,
        370,
        286,
        1604,
        885,
        644,
        295,
        300,
        13,
        51764
      ],
      "temperature": 0.0,
      "avg_logprob": -0.2677464118370643,
      "compression_ratio": 1.61139896373057,
      "no_speech_prob": 0.01408003456890583
    },
    {
      "id": 54,
      "seek": 68600,
      "start": 687.0,
      "end": 691.0,
      "text": " Thanks for sharing. Yeah.",
      "tokens": [
        50414,
        2561,
        337,
        5414,
        13,
        865,
        13,
        50614
      ],
      "temperature": 0.0,
      "avg_logprob": -0.3871818118625217,
      "compression_ratio": 1.330827067669173,
      "no_speech_prob": 0.0232467632740736
    },
    {
      "id": 55,
      "seek": 68600,
      "start": 691.0,
      "end": 706.0,
      "text": " I wish I came across the story when I was still working in configure with yeah, I think in retrospect though, yeah, it's definitely going to stick now.",
      "tokens": [
        50614,
        286,
        3172,
        286,
        1361,
        2108,
        264,
        1657,
        562,
        286,
        390,
        920,
        1364,
        294,
        22162,
        365,
        1338,
        11,
        286,
        519,
        294,
        34997,
        1673,
        11,
        1338,
        11,
        309,
        311,
        2138,
        516,
        281,
        2897,
        586,
        13,
        51364
      ],
      "temperature": 0.0,
      "avg_logprob": -0.3871818118625217,
      "compression_ratio": 1.330827067669173,
      "no_speech_prob": 0.0232467632740736
    },
    {
      "id": 56,
      "seek": 70600,
      "start": 707.0,
      "end": 715.0,
      "text": " So should we move on to the next items on the agenda.",
      "tokens": [
        50414,
        407,
        820,
        321,
        1286,
        322,
        281,
        264,
        958,
        4754,
        322,
        264,
        9829,
        13,
        50814
      ],
      "temperature": 0.0,
      "avg_logprob": -0.3661770820617676,
      "compression_ratio": 1.2156862745098038,
      "no_speech_prob": 0.0464191809296608
    },
    {
      "id": 57,
      "seek": 70600,
      "start": 715.0,
      "end": 718.0,
      "text": " Yeah, I'm starting.",
      "tokens": [
        50814,
        865,
        11,
        286,
        478,
        2891,
        13,
        50964
      ],
      "temperature": 0.0,
      "avg_logprob": -0.3661770820617676,
      "compression_ratio": 1.2156862745098038,
      "no_speech_prob": 0.0464191809296608
    },
    {
      "id": 58,
      "seek": 70600,
      "start": 718.0,
      "end": 724.0,
      "text": " So I'm working on the minimal on boarding flow for",
      "tokens": [
        50964,
        407,
        286,
        478,
        1364,
        322,
        264,
        13206,
        322,
        30528,
        3095,
        337,
        51264
      ],
      "temperature": 0.0,
      "avg_logprob": -0.3661770820617676,
      "compression_ratio": 1.2156862745098038,
      "no_speech_prob": 0.0464191809296608
    },
    {
      "id": 59,
      "seek": 72400,
      "start": 725.0,
      "end": 727.0,
      "text": " switchboard.",
      "tokens": [
        50414,
        3679,
        3787,
        13,
        50514
      ],
      "temperature": 0.0,
      "avg_logprob": -0.2259375768549302,
      "compression_ratio": 1.5263157894736843,
      "no_speech_prob": 0.04927189648151398
    },
    {
      "id": 60,
      "seek": 72400,
      "start": 727.0,
      "end": 733.0,
      "text": " I recorded a video of a walk through the initial concepts that I'm working on.",
      "tokens": [
        50514,
        286,
        8287,
        257,
        960,
        295,
        257,
        1792,
        807,
        264,
        5883,
        10392,
        300,
        286,
        478,
        1364,
        322,
        13,
        50814
      ],
      "temperature": 0.0,
      "avg_logprob": -0.2259375768549302,
      "compression_ratio": 1.5263157894736843,
      "no_speech_prob": 0.04927189648151398
    },
    {
      "id": 61,
      "seek": 72400,
      "start": 733.0,
      "end": 740.0,
      "text": " And yeah, it's been a great way to gather information.",
      "tokens": [
        50814,
        400,
        1338,
        11,
        309,
        311,
        668,
        257,
        869,
        636,
        281,
        5448,
        1589,
        13,
        51164
      ],
      "temperature": 0.0,
      "avg_logprob": -0.2259375768549302,
      "compression_ratio": 1.5263157894736843,
      "no_speech_prob": 0.04927189648151398
    },
    {
      "id": 62,
      "seek": 72400,
      "start": 740.0,
      "end": 752.0,
      "text": " Also during my conversations, there is some information that customers have to input during the setup process that they can never change again.",
      "tokens": [
        51164,
        2743,
        1830,
        452,
        7315,
        11,
        456,
        307,
        512,
        1589,
        300,
        4581,
        362,
        281,
        4846,
        1830,
        264,
        8657,
        1399,
        300,
        436,
        393,
        1128,
        1319,
        797,
        13,
        51764
      ],
      "temperature": 0.0,
      "avg_logprob": -0.2259375768549302,
      "compression_ratio": 1.5263157894736843,
      "no_speech_prob": 0.04927189648151398
    },
    {
      "id": 63,
      "seek": 75200,
      "start": 752.0,
      "end": 756.0,
      "text": " They lose. They can't access.",
      "tokens": [
        50364,
        814,
        3624,
        13,
        814,
        393,
        380,
        2105,
        13,
        50564
      ],
      "temperature": 0.0,
      "avg_logprob": -0.272788577609592,
      "compression_ratio": 1.390728476821192,
      "no_speech_prob": 0.006144266109913588
    },
    {
      "id": 64,
      "seek": 75200,
      "start": 756.0,
      "end": 759.0,
      "text": " Get lab. Oh, we can't help them.",
      "tokens": [
        50564,
        3240,
        2715,
        13,
        876,
        11,
        321,
        393,
        380,
        854,
        552,
        13,
        50714
      ],
      "temperature": 0.0,
      "avg_logprob": -0.272788577609592,
      "compression_ratio": 1.390728476821192,
      "no_speech_prob": 0.006144266109913588
    },
    {
      "id": 65,
      "seek": 75200,
      "start": 759.0,
      "end": 770.0,
      "text": " By the way, is there a way that we communicate this type of info.",
      "tokens": [
        50714,
        3146,
        264,
        636,
        11,
        307,
        456,
        257,
        636,
        300,
        321,
        7890,
        341,
        2010,
        295,
        13614,
        13,
        51264
      ],
      "temperature": 0.0,
      "avg_logprob": -0.272788577609592,
      "compression_ratio": 1.390728476821192,
      "no_speech_prob": 0.006144266109913588
    },
    {
      "id": 66,
      "seek": 75200,
      "start": 770.0,
      "end": 779.0,
      "text": " And then yeah, I appreciate any insights or any if you can point me to something.",
      "tokens": [
        51264,
        400,
        550,
        1338,
        11,
        286,
        4449,
        604,
        14310,
        420,
        604,
        498,
        291,
        393,
        935,
        385,
        281,
        746,
        13,
        51714
      ],
      "temperature": 0.0,
      "avg_logprob": -0.272788577609592,
      "compression_ratio": 1.390728476821192,
      "no_speech_prob": 0.006144266109913588
    },
    {
      "id": 67,
      "seek": 77900,
      "start": 780.0,
      "end": 796.0,
      "text": " And then I'm also using that to have conversations with engineers to help me with the scorecard that I'm working on to understand some of the stuff that they do.",
      "tokens": [
        50414,
        400,
        550,
        286,
        478,
        611,
        1228,
        300,
        281,
        362,
        7315,
        365,
        11955,
        281,
        854,
        385,
        365,
        264,
        6175,
        22259,
        300,
        286,
        478,
        1364,
        322,
        281,
        1223,
        512,
        295,
        264,
        1507,
        300,
        436,
        360,
        13,
        51214
      ],
      "temperature": 0.0,
      "avg_logprob": -0.13310503959655762,
      "compression_ratio": 1.376068376068376,
      "no_speech_prob": 0.004215632099658251
    },
    {
      "id": 68,
      "seek": 79600,
      "start": 797.0,
      "end": 814.0,
      "text": " So, I have one question. So how do you work on the scorecard because I don't think there's existing work flow around switchboard like how would you handle that and how do you approach to work on the scorecard.",
      "tokens": [
        50414,
        407,
        11,
        286,
        362,
        472,
        1168,
        13,
        407,
        577,
        360,
        291,
        589,
        322,
        264,
        6175,
        22259,
        570,
        286,
        500,
        380,
        519,
        456,
        311,
        6741,
        589,
        3095,
        926,
        3679,
        3787,
        411,
        577,
        576,
        291,
        4813,
        300,
        293,
        577,
        360,
        291,
        3109,
        281,
        589,
        322,
        264,
        6175,
        22259,
        13,
        51264
      ],
      "temperature": 0.0,
      "avg_logprob": -0.38725105408699284,
      "compression_ratio": 1.5493827160493827,
      "no_speech_prob": 0.150652214884758
    },
    {
      "id": 69,
      "seek": 79600,
      "start": 814.0,
      "end": 817.0,
      "text": " So today, there's something very minimal.",
      "tokens": [
        51264,
        407,
        965,
        11,
        456,
        311,
        746,
        588,
        13206,
        13,
        51414
      ],
      "temperature": 0.0,
      "avg_logprob": -0.38725105408699284,
      "compression_ratio": 1.5493827160493827,
      "no_speech_prob": 0.150652214884758
    },
    {
      "id": 70,
      "seek": 81700,
      "start": 817.0,
      "end": 829.0,
      "text": " So, we have a switchboard platform and then SRIs kind of paste JSON file in there and kick off a bunch of jobs.",
      "tokens": [
        50364,
        407,
        11,
        321,
        362,
        257,
        3679,
        3787,
        3663,
        293,
        550,
        318,
        5577,
        82,
        733,
        295,
        9163,
        31828,
        3991,
        294,
        456,
        293,
        4437,
        766,
        257,
        3840,
        295,
        4782,
        13,
        50964
      ],
      "temperature": 0.4,
      "avg_logprob": -0.8062728172124818,
      "compression_ratio": 1.1794871794871795,
      "no_speech_prob": 0.10890666395425797
    },
    {
      "id": 71,
      "seek": 81700,
      "start": 829.0,
      "end": 831.0,
      "text": " And follow the link.",
      "tokens": [
        50964,
        400,
        1524,
        264,
        2113,
        13,
        51064
      ],
      "temperature": 0.4,
      "avg_logprob": -0.8062728172124818,
      "compression_ratio": 1.1794871794871795,
      "no_speech_prob": 0.10890666395425797
    },
    {
      "id": 72,
      "seek": 81700,
      "start": 831.0,
      "end": 834.0,
      "text": " Yeah.",
      "tokens": [
        51064,
        865,
        13,
        51214
      ],
      "temperature": 0.4,
      "avg_logprob": -0.8062728172124818,
      "compression_ratio": 1.1794871794871795,
      "no_speech_prob": 0.10890666395425797
    },
    {
      "id": 73,
      "seek": 83400,
      "start": 834.0,
      "end": 837.0,
      "text": " So, I think it's a good thing to have a switchboard.",
      "tokens": [
        50364,
        407,
        11,
        286,
        519,
        309,
        311,
        257,
        665,
        551,
        281,
        362,
        257,
        3679,
        3787,
        13,
        50514
      ],
      "temperature": 0.0,
      "avg_logprob": -0.5538363265991211,
      "compression_ratio": 1.7305699481865284,
      "no_speech_prob": 0.13130569458007812
    },
    {
      "id": 74,
      "seek": 83400,
      "start": 837.0,
      "end": 840.0,
      "text": " And we have to do outside switchboard.",
      "tokens": [
        50514,
        400,
        321,
        362,
        281,
        360,
        2380,
        3679,
        3787,
        13,
        50664
      ],
      "temperature": 0.0,
      "avg_logprob": -0.5538363265991211,
      "compression_ratio": 1.7305699481865284,
      "no_speech_prob": 0.13130569458007812
    },
    {
      "id": 75,
      "seek": 83400,
      "start": 840.0,
      "end": 847.0,
      "text": " And because we also building for them would like them to do most of the stuff they do outside switchboard inside switchboard.",
      "tokens": [
        50664,
        400,
        570,
        321,
        611,
        2390,
        337,
        552,
        576,
        411,
        552,
        281,
        360,
        881,
        295,
        264,
        1507,
        436,
        360,
        2380,
        3679,
        3787,
        1854,
        3679,
        3787,
        13,
        51014
      ],
      "temperature": 0.0,
      "avg_logprob": -0.5538363265991211,
      "compression_ratio": 1.7305699481865284,
      "no_speech_prob": 0.13130569458007812
    },
    {
      "id": 76,
      "seek": 83400,
      "start": 847.0,
      "end": 852.0,
      "text": " So, it's kind of a hybrid of mappings, let's scorecard.",
      "tokens": [
        51014,
        407,
        11,
        309,
        311,
        733,
        295,
        257,
        13051,
        295,
        463,
        28968,
        11,
        718,
        311,
        6175,
        22259,
        13,
        51264
      ],
      "temperature": 0.0,
      "avg_logprob": -0.5538363265991211,
      "compression_ratio": 1.7305699481865284,
      "no_speech_prob": 0.13130569458007812
    },
    {
      "id": 77,
      "seek": 83400,
      "start": 852.0,
      "end": 855.0,
      "text": " It's like a bit.",
      "tokens": [
        51264,
        467,
        311,
        411,
        257,
        857,
        13,
        51414
      ],
      "temperature": 0.0,
      "avg_logprob": -0.5538363265991211,
      "compression_ratio": 1.7305699481865284,
      "no_speech_prob": 0.13130569458007812
    },
    {
      "id": 78,
      "seek": 83400,
      "start": 855.0,
      "end": 859.0,
      "text": " Yeah, it's going from the next phase.",
      "tokens": [
        51414,
        865,
        11,
        309,
        311,
        516,
        490,
        264,
        958,
        5574,
        13,
        51614
      ],
      "temperature": 0.0,
      "avg_logprob": -0.5538363265991211,
      "compression_ratio": 1.7305699481865284,
      "no_speech_prob": 0.13130569458007812
    },
    {
      "id": 79,
      "seek": 83400,
      "start": 859.0,
      "end": 861.0,
      "text": " Okay.",
      "tokens": [
        51614,
        1033,
        13,
        51714
      ],
      "temperature": 0.0,
      "avg_logprob": -0.5538363265991211,
      "compression_ratio": 1.7305699481865284,
      "no_speech_prob": 0.13130569458007812
    },
    {
      "id": 80,
      "seek": 86100,
      "start": 862.0,
      "end": 872.0,
      "text": " I have just a question about the your when you ask if we have an approach for telling the user about information they can never change again.",
      "tokens": [
        50414,
        286,
        362,
        445,
        257,
        1168,
        466,
        264,
        428,
        562,
        291,
        1029,
        498,
        321,
        362,
        364,
        3109,
        337,
        3585,
        264,
        4195,
        466,
        1589,
        436,
        393,
        1128,
        1319,
        797,
        13,
        50914
      ],
      "temperature": 0.0,
      "avg_logprob": -0.25625398603536315,
      "compression_ratio": 1.5527950310559007,
      "no_speech_prob": 0.013579951599240303
    },
    {
      "id": 81,
      "seek": 86100,
      "start": 872.0,
      "end": 874.0,
      "text": " Is this something.",
      "tokens": [
        50914,
        1119,
        341,
        746,
        13,
        51014
      ],
      "temperature": 0.0,
      "avg_logprob": -0.25625398603536315,
      "compression_ratio": 1.5527950310559007,
      "no_speech_prob": 0.013579951599240303
    },
    {
      "id": 82,
      "seek": 86100,
      "start": 874.0,
      "end": 883.0,
      "text": " Are they selecting like what they're configuring and they can never reconfigure it after.",
      "tokens": [
        51014,
        2014,
        436,
        18182,
        411,
        437,
        436,
        434,
        6662,
        1345,
        293,
        436,
        393,
        1128,
        9993,
        20646,
        540,
        309,
        934,
        13,
        51464
      ],
      "temperature": 0.0,
      "avg_logprob": -0.25625398603536315,
      "compression_ratio": 1.5527950310559007,
      "no_speech_prob": 0.013579951599240303
    },
    {
      "id": 83,
      "seek": 88300,
      "start": 883.0,
      "end": 893.0,
      "text": " Yeah, so they're bringing encryption keys to encrypt their github instance.",
      "tokens": [
        50364,
        865,
        11,
        370,
        436,
        434,
        5062,
        29575,
        9317,
        281,
        17972,
        662,
        641,
        290,
        355,
        836,
        5197,
        13,
        50864
      ],
      "temperature": 0.0,
      "avg_logprob": -0.1463342611340509,
      "compression_ratio": 1.62,
      "no_speech_prob": 0.021242855116724968
    },
    {
      "id": 84,
      "seek": 88300,
      "start": 893.0,
      "end": 900.0,
      "text": " And so we're giving them the option of bringing their own keys.",
      "tokens": [
        50864,
        400,
        370,
        321,
        434,
        2902,
        552,
        264,
        3614,
        295,
        5062,
        641,
        1065,
        9317,
        13,
        51214
      ],
      "temperature": 0.0,
      "avg_logprob": -0.1463342611340509,
      "compression_ratio": 1.62,
      "no_speech_prob": 0.021242855116724968
    },
    {
      "id": 85,
      "seek": 88300,
      "start": 900.0,
      "end": 905.0,
      "text": " And I think once they've put them, we can't see them.",
      "tokens": [
        51214,
        400,
        286,
        519,
        1564,
        436,
        600,
        829,
        552,
        11,
        321,
        393,
        380,
        536,
        552,
        13,
        51464
      ],
      "temperature": 0.0,
      "avg_logprob": -0.1463342611340509,
      "compression_ratio": 1.62,
      "no_speech_prob": 0.021242855116724968
    },
    {
      "id": 86,
      "seek": 88300,
      "start": 905.0,
      "end": 910.0,
      "text": " And if they lose them, we can't help them either.",
      "tokens": [
        51464,
        400,
        498,
        436,
        3624,
        552,
        11,
        321,
        393,
        380,
        854,
        552,
        2139,
        13,
        51714
      ],
      "temperature": 0.0,
      "avg_logprob": -0.1463342611340509,
      "compression_ratio": 1.62,
      "no_speech_prob": 0.021242855116724968
    },
    {
      "id": 87,
      "seek": 91000,
      "start": 910.0,
      "end": 913.0,
      "text": " I see.",
      "tokens": [
        50364,
        286,
        536,
        13,
        50514
      ],
      "temperature": 0.0,
      "avg_logprob": -0.1308864535707416,
      "compression_ratio": 1.5841121495327102,
      "no_speech_prob": 0.004017098341137171
    },
    {
      "id": 88,
      "seek": 91000,
      "start": 913.0,
      "end": 919.0,
      "text": " This sort of reminds me of.",
      "tokens": [
        50514,
        639,
        1333,
        295,
        12025,
        385,
        295,
        13,
        50814
      ],
      "temperature": 0.0,
      "avg_logprob": -0.1308864535707416,
      "compression_ratio": 1.5841121495327102,
      "no_speech_prob": 0.004017098341137171
    },
    {
      "id": 89,
      "seek": 91000,
      "start": 919.0,
      "end": 921.0,
      "text": " Well, actually, no, it doesn't.",
      "tokens": [
        50814,
        1042,
        11,
        767,
        11,
        572,
        11,
        309,
        1177,
        380,
        13,
        50914
      ],
      "temperature": 0.0,
      "avg_logprob": -0.1308864535707416,
      "compression_ratio": 1.5841121495327102,
      "no_speech_prob": 0.004017098341137171
    },
    {
      "id": 90,
      "seek": 91000,
      "start": 921.0,
      "end": 927.0,
      "text": " I was going to say we do a tokens that are only displayed for a certain amount of time in the UI for a runner.",
      "tokens": [
        50914,
        286,
        390,
        516,
        281,
        584,
        321,
        360,
        257,
        22667,
        300,
        366,
        787,
        16372,
        337,
        257,
        1629,
        2372,
        295,
        565,
        294,
        264,
        15682,
        337,
        257,
        24376,
        13,
        51214
      ],
      "temperature": 0.0,
      "avg_logprob": -0.1308864535707416,
      "compression_ratio": 1.5841121495327102,
      "no_speech_prob": 0.004017098341137171
    },
    {
      "id": 91,
      "seek": 91000,
      "start": 927.0,
      "end": 933.0,
      "text": " And we just tell them like this is going to be displayed only for a short period of time.",
      "tokens": [
        51214,
        400,
        321,
        445,
        980,
        552,
        411,
        341,
        307,
        516,
        281,
        312,
        16372,
        787,
        337,
        257,
        2099,
        2896,
        295,
        565,
        13,
        51514
      ],
      "temperature": 0.0,
      "avg_logprob": -0.1308864535707416,
      "compression_ratio": 1.5841121495327102,
      "no_speech_prob": 0.004017098341137171
    },
    {
      "id": 92,
      "seek": 91000,
      "start": 933.0,
      "end": 937.0,
      "text": " So copy, we give them the chance to copy it so that they don't lose it.",
      "tokens": [
        51514,
        407,
        5055,
        11,
        321,
        976,
        552,
        264,
        2931,
        281,
        5055,
        309,
        370,
        300,
        436,
        500,
        380,
        3624,
        309,
        13,
        51714
      ],
      "temperature": 0.0,
      "avg_logprob": -0.1308864535707416,
      "compression_ratio": 1.5841121495327102,
      "no_speech_prob": 0.004017098341137171
    },
    {
      "id": 93,
      "seek": 93700,
      "start": 937.0,
      "end": 941.0,
      "text": " But then if they lose it, like, so on them.",
      "tokens": [
        50364,
        583,
        550,
        498,
        436,
        3624,
        309,
        11,
        411,
        11,
        370,
        322,
        552,
        13,
        50564
      ],
      "temperature": 0.0,
      "avg_logprob": -0.29323964369924443,
      "compression_ratio": 1.4533333333333334,
      "no_speech_prob": 0.0016472192946821451
    },
    {
      "id": 94,
      "seek": 93700,
      "start": 941.0,
      "end": 943.0,
      "text": " So that's how we.",
      "tokens": [
        50564,
        407,
        300,
        311,
        577,
        321,
        13,
        50664
      ],
      "temperature": 0.0,
      "avg_logprob": -0.29323964369924443,
      "compression_ratio": 1.4533333333333334,
      "no_speech_prob": 0.0016472192946821451
    },
    {
      "id": 95,
      "seek": 93700,
      "start": 943.0,
      "end": 946.0,
      "text": " Tell them right now.",
      "tokens": [
        50664,
        5115,
        552,
        558,
        586,
        13,
        50814
      ],
      "temperature": 0.0,
      "avg_logprob": -0.29323964369924443,
      "compression_ratio": 1.4533333333333334,
      "no_speech_prob": 0.0016472192946821451
    },
    {
      "id": 96,
      "seek": 93700,
      "start": 946.0,
      "end": 954.0,
      "text": " I don't know if it's.",
      "tokens": [
        50814,
        286,
        500,
        380,
        458,
        498,
        309,
        311,
        13,
        51214
      ],
      "temperature": 0.0,
      "avg_logprob": -0.29323964369924443,
      "compression_ratio": 1.4533333333333334,
      "no_speech_prob": 0.0016472192946821451
    },
    {
      "id": 97,
      "seek": 93700,
      "start": 954.0,
      "end": 956.0,
      "text": " Something.",
      "tokens": [
        51214,
        6595,
        13,
        51314
      ],
      "temperature": 0.0,
      "avg_logprob": -0.29323964369924443,
      "compression_ratio": 1.4533333333333334,
      "no_speech_prob": 0.0016472192946821451
    },
    {
      "id": 98,
      "seek": 93700,
      "start": 956.0,
      "end": 963.0,
      "text": " I think I'll lean a lot on the right as to kind of help me communicate that.",
      "tokens": [
        51314,
        286,
        519,
        286,
        603,
        11659,
        257,
        688,
        322,
        264,
        558,
        382,
        281,
        733,
        295,
        854,
        385,
        7890,
        300,
        13,
        51664
      ],
      "temperature": 0.0,
      "avg_logprob": -0.29323964369924443,
      "compression_ratio": 1.4533333333333334,
      "no_speech_prob": 0.0016472192946821451
    },
    {
      "id": 99,
      "seek": 93700,
      "start": 963.0,
      "end": 965.0,
      "text": " I guess since they isn't.",
      "tokens": [
        51664,
        286,
        2041,
        1670,
        436,
        1943,
        380,
        13,
        51764
      ],
      "temperature": 0.0,
      "avg_logprob": -0.29323964369924443,
      "compression_ratio": 1.4533333333333334,
      "no_speech_prob": 0.0016472192946821451
    },
    {
      "id": 100,
      "seek": 96500,
      "start": 965.0,
      "end": 966.0,
      "text": " Right?",
      "tokens": [
        50364,
        1779,
        30,
        50414
      ],
      "temperature": 1.0,
      "avg_logprob": -2.500183410644531,
      "compression_ratio": 1.2916666666666667,
      "no_speech_prob": 0.0048688482493162155
    },
    {
      "id": 101,
      "seek": 96500,
      "start": 966.0,
      "end": 969.0,
      "text": " But everything, and this doesn't really relate.",
      "tokens": [
        50414,
        583,
        1203,
        11,
        293,
        341,
        1177,
        380,
        534,
        10961,
        13,
        50564
      ],
      "temperature": 1.0,
      "avg_logprob": -2.500183410644531,
      "compression_ratio": 1.2916666666666667,
      "no_speech_prob": 0.0048688482493162155
    },
    {
      "id": 102,
      "seek": 96500,
      "start": 969.0,
      "end": 971.0,
      "text": " Any text and content and content.",
      "tokens": [
        50564,
        2639,
        2487,
        293,
        2701,
        293,
        2701,
        13,
        50664
      ],
      "temperature": 1.0,
      "avg_logprob": -2.500183410644531,
      "compression_ratio": 1.2916666666666667,
      "no_speech_prob": 0.0048688482493162155
    },
    {
      "id": 103,
      "seek": 96500,
      "start": 971.0,
      "end": 976.0,
      "text": " I don't know if it's supposed to take you to.",
      "tokens": [
        50664,
        286,
        500,
        380,
        458,
        498,
        309,
        311,
        3442,
        281,
        747,
        291,
        281,
        13,
        50914
      ],
      "temperature": 1.0,
      "avg_logprob": -2.500183410644531,
      "compression_ratio": 1.2916666666666667,
      "no_speech_prob": 0.0048688482493162155
    },
    {
      "id": 104,
      "seek": 96500,
      "start": 976.0,
      "end": 986.0,
      "text": " I'll link that into.",
      "tokens": [
        50914,
        286,
        603,
        2113,
        300,
        666,
        13,
        51414
      ],
      "temperature": 1.0,
      "avg_logprob": -2.500183410644531,
      "compression_ratio": 1.2916666666666667,
      "no_speech_prob": 0.0048688482493162155
    },
    {
      "id": 105,
      "seek": 98600,
      "start": 986.0,
      "end": 990.24,
      "text": " Starting to validate runner fleet dashboard, you might have seen I shared it in our",
      "tokens": [
        50364,
        16217,
        281,
        29562,
        24376,
        19396,
        18342,
        11,
        291,
        1062,
        362,
        1612,
        286,
        5507,
        309,
        294,
        527,
        50576
      ],
      "temperature": 0.0,
      "avg_logprob": -0.2636206983083702,
      "compression_ratio": 1.555045871559633,
      "no_speech_prob": 0.05441531538963318
    },
    {
      "id": 106,
      "seek": 98600,
      "start": 990.24,
      "end": 992.44,
      "text": " UX co-working channel.",
      "tokens": [
        50576,
        40176,
        598,
        12,
        22475,
        2269,
        13,
        50686
      ],
      "temperature": 0.0,
      "avg_logprob": -0.2636206983083702,
      "compression_ratio": 1.555045871559633,
      "no_speech_prob": 0.05441531538963318
    },
    {
      "id": 107,
      "seek": 98600,
      "start": 992.44,
      "end": 999.8,
      "text": " And I'll just share the issue that I'm doing this through because we have a very hard time",
      "tokens": [
        50686,
        400,
        286,
        603,
        445,
        2073,
        264,
        2734,
        300,
        286,
        478,
        884,
        341,
        807,
        570,
        321,
        362,
        257,
        588,
        1152,
        565,
        51054
      ],
      "temperature": 0.0,
      "avg_logprob": -0.2636206983083702,
      "compression_ratio": 1.555045871559633,
      "no_speech_prob": 0.05441531538963318
    },
    {
      "id": 108,
      "seek": 98600,
      "start": 999.8,
      "end": 1003.64,
      "text": " finding enterprise customers to meet with.",
      "tokens": [
        51054,
        5006,
        14132,
        4581,
        281,
        1677,
        365,
        13,
        51246
      ],
      "temperature": 0.0,
      "avg_logprob": -0.2636206983083702,
      "compression_ratio": 1.555045871559633,
      "no_speech_prob": 0.05441531538963318
    },
    {
      "id": 109,
      "seek": 98600,
      "start": 1003.64,
      "end": 1011.12,
      "text": " And there's this process and the handbook that you can lean on customer success managers",
      "tokens": [
        51246,
        400,
        456,
        311,
        341,
        1399,
        293,
        264,
        1011,
        2939,
        300,
        291,
        393,
        11659,
        322,
        5474,
        2245,
        14084,
        51620
      ],
      "temperature": 0.0,
      "avg_logprob": -0.2636206983083702,
      "compression_ratio": 1.555045871559633,
      "no_speech_prob": 0.05441531538963318
    },
    {
      "id": 110,
      "seek": 98600,
      "start": 1011.12,
      "end": 1012.12,
      "text": " for that.",
      "tokens": [
        51620,
        337,
        300,
        13,
        51670
      ],
      "temperature": 0.0,
      "avg_logprob": -0.2636206983083702,
      "compression_ratio": 1.555045871559633,
      "no_speech_prob": 0.05441531538963318
    },
    {
      "id": 111,
      "seek": 101212,
      "start": 1012.12,
      "end": 1016.2,
      "text": " So yeah, if you want to take a look at that issue that applies to you in the future, it",
      "tokens": [
        50364,
        407,
        1338,
        11,
        498,
        291,
        528,
        281,
        747,
        257,
        574,
        412,
        300,
        2734,
        300,
        13165,
        281,
        291,
        294,
        264,
        2027,
        11,
        309,
        50568
      ],
      "temperature": 0.0,
      "avg_logprob": -0.3995127811610142,
      "compression_ratio": 1.5260869565217392,
      "no_speech_prob": 0.07456688582897186
    },
    {
      "id": 112,
      "seek": 101212,
      "start": 1016.2,
      "end": 1017.2,
      "text": " might be helpful.",
      "tokens": [
        50568,
        1062,
        312,
        4961,
        13,
        50618
      ],
      "temperature": 0.0,
      "avg_logprob": -0.3995127811610142,
      "compression_ratio": 1.5260869565217392,
      "no_speech_prob": 0.07456688582897186
    },
    {
      "id": 113,
      "seek": 101212,
      "start": 1017.2,
      "end": 1025.24,
      "text": " Gina, have you reached out to any technical account managers?",
      "tokens": [
        50618,
        34711,
        11,
        362,
        291,
        6488,
        484,
        281,
        604,
        6191,
        2696,
        14084,
        30,
        51020
      ],
      "temperature": 0.0,
      "avg_logprob": -0.3995127811610142,
      "compression_ratio": 1.5260869565217392,
      "no_speech_prob": 0.07456688582897186
    },
    {
      "id": 114,
      "seek": 101212,
      "start": 1025.24,
      "end": 1029.56,
      "text": " I think sometimes they're very reactive to our request.",
      "tokens": [
        51020,
        286,
        519,
        2171,
        436,
        434,
        588,
        28897,
        281,
        527,
        5308,
        13,
        51236
      ],
      "temperature": 0.0,
      "avg_logprob": -0.3995127811610142,
      "compression_ratio": 1.5260869565217392,
      "no_speech_prob": 0.07456688582897186
    },
    {
      "id": 115,
      "seek": 101212,
      "start": 1029.56,
      "end": 1030.56,
      "text": " Yes.",
      "tokens": [
        51236,
        1079,
        13,
        51286
      ],
      "temperature": 0.0,
      "avg_logprob": -0.3995127811610142,
      "compression_ratio": 1.5260869565217392,
      "no_speech_prob": 0.07456688582897186
    },
    {
      "id": 116,
      "seek": 101212,
      "start": 1030.56,
      "end": 1031.56,
      "text": " Okay.",
      "tokens": [
        51286,
        1033,
        13,
        51336
      ],
      "temperature": 0.0,
      "avg_logprob": -0.3995127811610142,
      "compression_ratio": 1.5260869565217392,
      "no_speech_prob": 0.07456688582897186
    },
    {
      "id": 117,
      "seek": 101212,
      "start": 1031.56,
      "end": 1035.44,
      "text": " So they're now called customer success managers.",
      "tokens": [
        51336,
        407,
        436,
        434,
        586,
        1219,
        5474,
        2245,
        14084,
        13,
        51530
      ],
      "temperature": 0.0,
      "avg_logprob": -0.3995127811610142,
      "compression_ratio": 1.5260869565217392,
      "no_speech_prob": 0.07456688582897186
    },
    {
      "id": 118,
      "seek": 101212,
      "start": 1035.44,
      "end": 1036.44,
      "text": " Oh, okay.",
      "tokens": [
        51530,
        876,
        11,
        1392,
        13,
        51580
      ],
      "temperature": 0.0,
      "avg_logprob": -0.3995127811610142,
      "compression_ratio": 1.5260869565217392,
      "no_speech_prob": 0.07456688582897186
    },
    {
      "id": 119,
      "seek": 101212,
      "start": 1036.44,
      "end": 1037.44,
      "text": " It's a secret report.",
      "tokens": [
        51580,
        467,
        311,
        257,
        4054,
        2275,
        13,
        51630
      ],
      "temperature": 0.0,
      "avg_logprob": -0.3995127811610142,
      "compression_ratio": 1.5260869565217392,
      "no_speech_prob": 0.07456688582897186
    },
    {
      "id": 120,
      "seek": 101212,
      "start": 1037.44,
      "end": 1038.44,
      "text": " It doesn't matter.",
      "tokens": [
        51630,
        467,
        1177,
        380,
        1871,
        13,
        51680
      ],
      "temperature": 0.0,
      "avg_logprob": -0.3995127811610142,
      "compression_ratio": 1.5260869565217392,
      "no_speech_prob": 0.07456688582897186
    },
    {
      "id": 121,
      "seek": 101212,
      "start": 1038.44,
      "end": 1039.44,
      "text": " No, that's okay.",
      "tokens": [
        51680,
        883,
        11,
        300,
        311,
        1392,
        13,
        51730
      ],
      "temperature": 0.0,
      "avg_logprob": -0.3995127811610142,
      "compression_ratio": 1.5260869565217392,
      "no_speech_prob": 0.07456688582897186
    },
    {
      "id": 122,
      "seek": 103944,
      "start": 1039.44,
      "end": 1045.76,
      "text": " That's who I was saying the lean on because they've helped me in the past just like meet",
      "tokens": [
        50364,
        663,
        311,
        567,
        286,
        390,
        1566,
        264,
        11659,
        322,
        570,
        436,
        600,
        4254,
        385,
        294,
        264,
        1791,
        445,
        411,
        1677,
        50680
      ],
      "temperature": 0.0,
      "avg_logprob": -0.45419998856278154,
      "compression_ratio": 1.5614035087719298,
      "no_speech_prob": 0.0022969243582338095
    },
    {
      "id": 123,
      "seek": 103944,
      "start": 1045.76,
      "end": 1048.76,
      "text": " with customers just to hear what their pain points are.",
      "tokens": [
        50680,
        365,
        4581,
        445,
        281,
        1568,
        437,
        641,
        1822,
        2793,
        366,
        13,
        50830
      ],
      "temperature": 0.0,
      "avg_logprob": -0.45419998856278154,
      "compression_ratio": 1.5614035087719298,
      "no_speech_prob": 0.0022969243582338095
    },
    {
      "id": 124,
      "seek": 103944,
      "start": 1048.76,
      "end": 1049.76,
      "text": " Yeah.",
      "tokens": [
        50830,
        865,
        13,
        50880
      ],
      "temperature": 0.0,
      "avg_logprob": -0.45419998856278154,
      "compression_ratio": 1.5614035087719298,
      "no_speech_prob": 0.0022969243582338095
    },
    {
      "id": 125,
      "seek": 103944,
      "start": 1049.76,
      "end": 1053.28,
      "text": " So we have a list and ongoing list that Darren and I are keeping.",
      "tokens": [
        50880,
        407,
        321,
        362,
        257,
        1329,
        293,
        10452,
        1329,
        300,
        36691,
        293,
        286,
        366,
        5145,
        13,
        51056
      ],
      "temperature": 0.0,
      "avg_logprob": -0.45419998856278154,
      "compression_ratio": 1.5614035087719298,
      "no_speech_prob": 0.0022969243582338095
    },
    {
      "id": 126,
      "seek": 103944,
      "start": 1053.28,
      "end": 1054.28,
      "text": " Yeah.",
      "tokens": [
        51056,
        865,
        13,
        51106
      ],
      "temperature": 0.0,
      "avg_logprob": -0.45419998856278154,
      "compression_ratio": 1.5614035087719298,
      "no_speech_prob": 0.0022969243582338095
    },
    {
      "id": 127,
      "seek": 103944,
      "start": 1054.28,
      "end": 1055.76,
      "text": " Thanks for clarifying that.",
      "tokens": [
        51106,
        2561,
        337,
        6093,
        5489,
        300,
        13,
        51180
      ],
      "temperature": 0.0,
      "avg_logprob": -0.45419998856278154,
      "compression_ratio": 1.5614035087719298,
      "no_speech_prob": 0.0022969243582338095
    },
    {
      "id": 128,
      "seek": 103944,
      "start": 1055.76,
      "end": 1056.76,
      "text": " Yeah.",
      "tokens": [
        51180,
        865,
        13,
        51230
      ],
      "temperature": 0.0,
      "avg_logprob": -0.45419998856278154,
      "compression_ratio": 1.5614035087719298,
      "no_speech_prob": 0.0022969243582338095
    },
    {
      "id": 129,
      "seek": 103944,
      "start": 1056.76,
      "end": 1057.76,
      "text": " I'm just.",
      "tokens": [
        51230,
        286,
        478,
        445,
        13,
        51280
      ],
      "temperature": 0.0,
      "avg_logprob": -0.45419998856278154,
      "compression_ratio": 1.5614035087719298,
      "no_speech_prob": 0.0022969243582338095
    },
    {
      "id": 130,
      "seek": 103944,
      "start": 1057.76,
      "end": 1058.76,
      "text": " That's it for me, Emily.",
      "tokens": [
        51280,
        663,
        311,
        309,
        337,
        385,
        11,
        15034,
        13,
        51330
      ],
      "temperature": 0.0,
      "avg_logprob": -0.45419998856278154,
      "compression_ratio": 1.5614035087719298,
      "no_speech_prob": 0.0022969243582338095
    },
    {
      "id": 131,
      "seek": 103944,
      "start": 1058.76,
      "end": 1059.76,
      "text": " You want to go?",
      "tokens": [
        51330,
        509,
        528,
        281,
        352,
        30,
        51380
      ],
      "temperature": 0.0,
      "avg_logprob": -0.45419998856278154,
      "compression_ratio": 1.5614035087719298,
      "no_speech_prob": 0.0022969243582338095
    },
    {
      "id": 132,
      "seek": 103944,
      "start": 1059.76,
      "end": 1063.76,
      "text": " Well, I just had one open question about a list.",
      "tokens": [
        51380,
        1042,
        11,
        286,
        445,
        632,
        472,
        1269,
        1168,
        466,
        257,
        1329,
        13,
        51580
      ],
      "temperature": 0.0,
      "avg_logprob": -0.45419998856278154,
      "compression_ratio": 1.5614035087719298,
      "no_speech_prob": 0.0022969243582338095
    },
    {
      "id": 133,
      "seek": 106376,
      "start": 1064.76,
      "end": 1072.8799999999999,
      "text": " Well, so I just had one open question about larger designs that are put in through a lot",
      "tokens": [
        50414,
        1042,
        11,
        370,
        286,
        445,
        632,
        472,
        1269,
        1168,
        466,
        4833,
        11347,
        300,
        366,
        829,
        294,
        807,
        257,
        688,
        50820
      ],
      "temperature": 0.0,
      "avg_logprob": -0.20118586222330728,
      "compression_ratio": 1.6779661016949152,
      "no_speech_prob": 0.007368758320808411
    },
    {
      "id": 134,
      "seek": 106376,
      "start": 1072.8799999999999,
      "end": 1075.76,
      "text": " of small MRs that are kept behind a feature flag.",
      "tokens": [
        50820,
        295,
        1359,
        9808,
        82,
        300,
        366,
        4305,
        2261,
        257,
        4111,
        7166,
        13,
        50964
      ],
      "temperature": 0.0,
      "avg_logprob": -0.20118586222330728,
      "compression_ratio": 1.6779661016949152,
      "no_speech_prob": 0.007368758320808411
    },
    {
      "id": 135,
      "seek": 106376,
      "start": 1075.76,
      "end": 1081.36,
      "text": " I was getting this question from some of the engineers as if they get a UX review for",
      "tokens": [
        50964,
        286,
        390,
        1242,
        341,
        1168,
        490,
        512,
        295,
        264,
        11955,
        382,
        498,
        436,
        483,
        257,
        40176,
        3131,
        337,
        51244
      ],
      "temperature": 0.0,
      "avg_logprob": -0.20118586222330728,
      "compression_ratio": 1.6779661016949152,
      "no_speech_prob": 0.007368758320808411
    },
    {
      "id": 136,
      "seek": 106376,
      "start": 1081.36,
      "end": 1087.68,
      "text": " each small MR or larger view at the end, mostly being they were doing small reviews per",
      "tokens": [
        51244,
        1184,
        1359,
        9808,
        420,
        4833,
        1910,
        412,
        264,
        917,
        11,
        5240,
        885,
        436,
        645,
        884,
        1359,
        10229,
        680,
        51560
      ],
      "temperature": 0.0,
      "avg_logprob": -0.20118586222330728,
      "compression_ratio": 1.6779661016949152,
      "no_speech_prob": 0.007368758320808411
    },
    {
      "id": 137,
      "seek": 106376,
      "start": 1087.68,
      "end": 1092.76,
      "text": " small MRs, but we were getting people wondering why the experience wasn't complete.",
      "tokens": [
        51560,
        1359,
        9808,
        82,
        11,
        457,
        321,
        645,
        1242,
        561,
        6359,
        983,
        264,
        1752,
        2067,
        380,
        3566,
        13,
        51814
      ],
      "temperature": 0.0,
      "avg_logprob": -0.20118586222330728,
      "compression_ratio": 1.6779661016949152,
      "no_speech_prob": 0.007368758320808411
    },
    {
      "id": 138,
      "seek": 109276,
      "start": 1092.76,
      "end": 1097.16,
      "text": " Like you couldn't go through everything because it was just one piece of the MR and then",
      "tokens": [
        50364,
        1743,
        291,
        2809,
        380,
        352,
        807,
        1203,
        570,
        309,
        390,
        445,
        472,
        2522,
        295,
        264,
        9808,
        293,
        550,
        50584
      ],
      "temperature": 0.0,
      "avg_logprob": -0.2140849895691604,
      "compression_ratio": 1.5526315789473684,
      "no_speech_prob": 0.00019977260672021657
    },
    {
      "id": 139,
      "seek": 109276,
      "start": 1097.16,
      "end": 1098.76,
      "text": " the rest seemed broken.",
      "tokens": [
        50584,
        264,
        1472,
        6576,
        5463,
        13,
        50664
      ],
      "temperature": 0.0,
      "avg_logprob": -0.2140849895691604,
      "compression_ratio": 1.5526315789473684,
      "no_speech_prob": 0.00019977260672021657
    },
    {
      "id": 140,
      "seek": 109276,
      "start": 1098.76,
      "end": 1102.76,
      "text": " So I was just wanting to get other people's opinions on this.",
      "tokens": [
        50664,
        407,
        286,
        390,
        445,
        7935,
        281,
        483,
        661,
        561,
        311,
        11819,
        322,
        341,
        13,
        50864
      ],
      "temperature": 0.0,
      "avg_logprob": -0.2140849895691604,
      "compression_ratio": 1.5526315789473684,
      "no_speech_prob": 0.00019977260672021657
    },
    {
      "id": 141,
      "seek": 109276,
      "start": 1102.76,
      "end": 1112.6,
      "text": " Yeah, this was so relevant to the token, like re-architecture that runner was doing and",
      "tokens": [
        50864,
        865,
        11,
        341,
        390,
        370,
        7340,
        281,
        264,
        14862,
        11,
        411,
        319,
        12,
        1178,
        5739,
        540,
        300,
        24376,
        390,
        884,
        293,
        51356
      ],
      "temperature": 0.0,
      "avg_logprob": -0.2140849895691604,
      "compression_ratio": 1.5526315789473684,
      "no_speech_prob": 0.00019977260672021657
    },
    {
      "id": 142,
      "seek": 109276,
      "start": 1112.6,
      "end": 1120.76,
      "text": " our devs did UX reviews for each small MR and then sometimes our front end dev will include",
      "tokens": [
        51356,
        527,
        1905,
        82,
        630,
        40176,
        10229,
        337,
        1184,
        1359,
        9808,
        293,
        550,
        2171,
        527,
        1868,
        917,
        1905,
        486,
        4090,
        51764
      ],
      "temperature": 0.0,
      "avg_logprob": -0.2140849895691604,
      "compression_ratio": 1.5526315789473684,
      "no_speech_prob": 0.00019977260672021657
    },
    {
      "id": 143,
      "seek": 112076,
      "start": 1120.76,
      "end": 1125.4,
      "text": " a table in the description being like, this is where you are and these are the other MRs",
      "tokens": [
        50364,
        257,
        3199,
        294,
        264,
        3855,
        885,
        411,
        11,
        341,
        307,
        689,
        291,
        366,
        293,
        613,
        366,
        264,
        661,
        9808,
        82,
        50596
      ],
      "temperature": 0.0,
      "avg_logprob": -0.19503355538973244,
      "compression_ratio": 1.6355555555555557,
      "no_speech_prob": 0.0662352442741394
    },
    {
      "id": 144,
      "seek": 112076,
      "start": 1125.4,
      "end": 1127.76,
      "text": " that are adding these other features.",
      "tokens": [
        50596,
        300,
        366,
        5127,
        613,
        661,
        4122,
        13,
        50714
      ],
      "temperature": 0.0,
      "avg_logprob": -0.19503355538973244,
      "compression_ratio": 1.6355555555555557,
      "no_speech_prob": 0.0662352442741394
    },
    {
      "id": 145,
      "seek": 112076,
      "start": 1127.76,
      "end": 1132.12,
      "text": " Or they'll just add like some description text to tell people when they're bringing up",
      "tokens": [
        50714,
        1610,
        436,
        603,
        445,
        909,
        411,
        512,
        3855,
        2487,
        281,
        980,
        561,
        562,
        436,
        434,
        5062,
        493,
        50932
      ],
      "temperature": 0.0,
      "avg_logprob": -0.19503355538973244,
      "compression_ratio": 1.6355555555555557,
      "no_speech_prob": 0.0662352442741394
    },
    {
      "id": 146,
      "seek": 112076,
      "start": 1132.12,
      "end": 1133.12,
      "text": " the changes.",
      "tokens": [
        50932,
        264,
        2962,
        13,
        50982
      ],
      "temperature": 0.0,
      "avg_logprob": -0.19503355538973244,
      "compression_ratio": 1.6355555555555557,
      "no_speech_prob": 0.0662352442741394
    },
    {
      "id": 147,
      "seek": 112076,
      "start": 1133.12,
      "end": 1136.76,
      "text": " And then I linked an example there.",
      "tokens": [
        50982,
        400,
        550,
        286,
        9408,
        364,
        1365,
        456,
        13,
        51164
      ],
      "temperature": 0.0,
      "avg_logprob": -0.19503355538973244,
      "compression_ratio": 1.6355555555555557,
      "no_speech_prob": 0.0662352442741394
    },
    {
      "id": 148,
      "seek": 112076,
      "start": 1136.76,
      "end": 1145.4,
      "text": " Sometimes they'll also just ask me to review the MR before they ask like the random UX",
      "tokens": [
        51164,
        4803,
        436,
        603,
        611,
        445,
        1029,
        385,
        281,
        3131,
        264,
        9808,
        949,
        436,
        1029,
        411,
        264,
        4974,
        40176,
        51596
      ],
      "temperature": 0.0,
      "avg_logprob": -0.19503355538973244,
      "compression_ratio": 1.6355555555555557,
      "no_speech_prob": 0.0662352442741394
    },
    {
      "id": 149,
      "seek": 112076,
      "start": 1145.4,
      "end": 1147.76,
      "text": " assigned reviewer.",
      "tokens": [
        51596,
        13279,
        3131,
        260,
        13,
        51714
      ],
      "temperature": 0.0,
      "avg_logprob": -0.19503355538973244,
      "compression_ratio": 1.6355555555555557,
      "no_speech_prob": 0.0662352442741394
    },
    {
      "id": 150,
      "seek": 114776,
      "start": 1147.76,
      "end": 1150.76,
      "text": " It's been like a mix.",
      "tokens": [
        50364,
        467,
        311,
        668,
        411,
        257,
        2890,
        13,
        50514
      ],
      "temperature": 0.0,
      "avg_logprob": -0.1981382121210513,
      "compression_ratio": 1.6953125,
      "no_speech_prob": 0.041228070855140686
    },
    {
      "id": 151,
      "seek": 114776,
      "start": 1150.76,
      "end": 1151.76,
      "text": " Well, thanks.",
      "tokens": [
        50514,
        1042,
        11,
        3231,
        13,
        50564
      ],
      "temperature": 0.0,
      "avg_logprob": -0.1981382121210513,
      "compression_ratio": 1.6953125,
      "no_speech_prob": 0.041228070855140686
    },
    {
      "id": 152,
      "seek": 114776,
      "start": 1151.76,
      "end": 1155.52,
      "text": " Yeah, I think the problem was we're not like clear enough where it is in the experience",
      "tokens": [
        50564,
        865,
        11,
        286,
        519,
        264,
        1154,
        390,
        321,
        434,
        406,
        411,
        1850,
        1547,
        689,
        309,
        307,
        294,
        264,
        1752,
        50752
      ],
      "temperature": 0.0,
      "avg_logprob": -0.1981382121210513,
      "compression_ratio": 1.6953125,
      "no_speech_prob": 0.041228070855140686
    },
    {
      "id": 153,
      "seek": 114776,
      "start": 1155.52,
      "end": 1161.16,
      "text": " because the issue at links to links to the entire future, not just the little portion.",
      "tokens": [
        50752,
        570,
        264,
        2734,
        412,
        6123,
        281,
        6123,
        281,
        264,
        2302,
        2027,
        11,
        406,
        445,
        264,
        707,
        8044,
        13,
        51034
      ],
      "temperature": 0.0,
      "avg_logprob": -0.1981382121210513,
      "compression_ratio": 1.6953125,
      "no_speech_prob": 0.041228070855140686
    },
    {
      "id": 154,
      "seek": 114776,
      "start": 1161.16,
      "end": 1166.6,
      "text": " So I guess just being a lot more clear about what the MR is changing and what is out of scope",
      "tokens": [
        51034,
        407,
        286,
        2041,
        445,
        885,
        257,
        688,
        544,
        1850,
        466,
        437,
        264,
        9808,
        307,
        4473,
        293,
        437,
        307,
        484,
        295,
        11923,
        51306
      ],
      "temperature": 0.0,
      "avg_logprob": -0.1981382121210513,
      "compression_ratio": 1.6953125,
      "no_speech_prob": 0.041228070855140686
    },
    {
      "id": 155,
      "seek": 114776,
      "start": 1166.6,
      "end": 1168.08,
      "text": " for that particular MR.",
      "tokens": [
        51306,
        337,
        300,
        1729,
        9808,
        13,
        51380
      ],
      "temperature": 0.0,
      "avg_logprob": -0.1981382121210513,
      "compression_ratio": 1.6953125,
      "no_speech_prob": 0.041228070855140686
    },
    {
      "id": 156,
      "seek": 114776,
      "start": 1168.08,
      "end": 1169.6,
      "text": " Yes, exactly.",
      "tokens": [
        51380,
        1079,
        11,
        2293,
        13,
        51456
      ],
      "temperature": 0.0,
      "avg_logprob": -0.1981382121210513,
      "compression_ratio": 1.6953125,
      "no_speech_prob": 0.041228070855140686
    },
    {
      "id": 157,
      "seek": 114776,
      "start": 1169.6,
      "end": 1175.2,
      "text": " We did the exact same thing and linked to the entire feature that that MR was not creating.",
      "tokens": [
        51456,
        492,
        630,
        264,
        1900,
        912,
        551,
        293,
        9408,
        281,
        264,
        2302,
        4111,
        300,
        300,
        9808,
        390,
        406,
        4084,
        13,
        51736
      ],
      "temperature": 0.0,
      "avg_logprob": -0.1981382121210513,
      "compression_ratio": 1.6953125,
      "no_speech_prob": 0.041228070855140686
    },
    {
      "id": 158,
      "seek": 117520,
      "start": 1175.2,
      "end": 1182.2,
      "text": " So I think just like that text that he had added was helpful.",
      "tokens": [
        50364,
        407,
        286,
        519,
        445,
        411,
        300,
        2487,
        300,
        415,
        632,
        3869,
        390,
        4961,
        13,
        50714
      ],
      "temperature": 0.0,
      "avg_logprob": -0.3269984563191732,
      "compression_ratio": 1.483568075117371,
      "no_speech_prob": 0.07591690123081207
    },
    {
      "id": 159,
      "seek": 117520,
      "start": 1182.2,
      "end": 1185.2,
      "text": " Vittika, did you want to?",
      "tokens": [
        50714,
        691,
        593,
        5439,
        11,
        630,
        291,
        528,
        281,
        30,
        50864
      ],
      "temperature": 0.0,
      "avg_logprob": -0.3269984563191732,
      "compression_ratio": 1.483568075117371,
      "no_speech_prob": 0.07591690123081207
    },
    {
      "id": 160,
      "seek": 117520,
      "start": 1185.2,
      "end": 1187.52,
      "text": " Yeah, it was very simple to what Gina said.",
      "tokens": [
        50864,
        865,
        11,
        309,
        390,
        588,
        2199,
        281,
        437,
        34711,
        848,
        13,
        50980
      ],
      "temperature": 0.0,
      "avg_logprob": -0.3269984563191732,
      "compression_ratio": 1.483568075117371,
      "no_speech_prob": 0.07591690123081207
    },
    {
      "id": 161,
      "seek": 117520,
      "start": 1187.52,
      "end": 1191.6000000000001,
      "text": " So this I experienced when we started to work on the CI job token.",
      "tokens": [
        50980,
        407,
        341,
        286,
        6751,
        562,
        321,
        1409,
        281,
        589,
        322,
        264,
        37777,
        1691,
        14862,
        13,
        51184
      ],
      "temperature": 0.0,
      "avg_logprob": -0.3269984563191732,
      "compression_ratio": 1.483568075117371,
      "no_speech_prob": 0.07591690123081207
    },
    {
      "id": 162,
      "seek": 117520,
      "start": 1191.6000000000001,
      "end": 1193.76,
      "text": " I'm trying to find the MR.",
      "tokens": [
        51184,
        286,
        478,
        1382,
        281,
        915,
        264,
        9808,
        13,
        51292
      ],
      "temperature": 0.0,
      "avg_logprob": -0.3269984563191732,
      "compression_ratio": 1.483568075117371,
      "no_speech_prob": 0.07591690123081207
    },
    {
      "id": 163,
      "seek": 117520,
      "start": 1193.76,
      "end": 1201.2,
      "text": " But like that proposal, it had to be broken down to like a few parts like some preparation",
      "tokens": [
        51292,
        583,
        411,
        300,
        11494,
        11,
        309,
        632,
        281,
        312,
        5463,
        760,
        281,
        411,
        257,
        1326,
        3166,
        411,
        512,
        13081,
        51664
      ],
      "temperature": 0.0,
      "avg_logprob": -0.3269984563191732,
      "compression_ratio": 1.483568075117371,
      "no_speech_prob": 0.07591690123081207
    },
    {
      "id": 164,
      "seek": 120120,
      "start": 1201.2,
      "end": 1204.2,
      "text": " by the front end, some preparation by the back end.",
      "tokens": [
        50364,
        538,
        264,
        1868,
        917,
        11,
        512,
        13081,
        538,
        264,
        646,
        917,
        13,
        50514
      ],
      "temperature": 0.0,
      "avg_logprob": -0.18069818068523796,
      "compression_ratio": 1.6129032258064515,
      "no_speech_prob": 0.0012424468295648694
    },
    {
      "id": 165,
      "seek": 120120,
      "start": 1204.2,
      "end": 1210.16,
      "text": " And then eventually they would like club all of that to form the final to work on the final",
      "tokens": [
        50514,
        400,
        550,
        4728,
        436,
        576,
        411,
        6482,
        439,
        295,
        300,
        281,
        1254,
        264,
        2572,
        281,
        589,
        322,
        264,
        2572,
        50812
      ],
      "temperature": 0.0,
      "avg_logprob": -0.18069818068523796,
      "compression_ratio": 1.6129032258064515,
      "no_speech_prob": 0.0012424468295648694
    },
    {
      "id": 166,
      "seek": 120120,
      "start": 1210.16,
      "end": 1213.2,
      "text": " development and implementation.",
      "tokens": [
        50812,
        3250,
        293,
        11420,
        13,
        50964
      ],
      "temperature": 0.0,
      "avg_logprob": -0.18069818068523796,
      "compression_ratio": 1.6129032258064515,
      "no_speech_prob": 0.0012424468295648694
    },
    {
      "id": 167,
      "seek": 120120,
      "start": 1213.2,
      "end": 1217.8,
      "text": " So for that, what we the process we followed was.",
      "tokens": [
        50964,
        407,
        337,
        300,
        11,
        437,
        321,
        264,
        1399,
        321,
        6263,
        390,
        13,
        51194
      ],
      "temperature": 0.0,
      "avg_logprob": -0.18069818068523796,
      "compression_ratio": 1.6129032258064515,
      "no_speech_prob": 0.0012424468295648694
    },
    {
      "id": 168,
      "seek": 120120,
      "start": 1217.8,
      "end": 1222.88,
      "text": " Engineers used to invite me first to do a UX review and to make things easier for them.",
      "tokens": [
        51194,
        43950,
        1143,
        281,
        7980,
        385,
        700,
        281,
        360,
        257,
        40176,
        3131,
        293,
        281,
        652,
        721,
        3571,
        337,
        552,
        13,
        51448
      ],
      "temperature": 0.0,
      "avg_logprob": -0.18069818068523796,
      "compression_ratio": 1.6129032258064515,
      "no_speech_prob": 0.0012424468295648694
    },
    {
      "id": 169,
      "seek": 120120,
      "start": 1222.88,
      "end": 1229.44,
      "text": " Like once I reviewed, I left a very detailed sort of summary for the next designer who",
      "tokens": [
        51448,
        1743,
        1564,
        286,
        18429,
        11,
        286,
        1411,
        257,
        588,
        9942,
        1333,
        295,
        12691,
        337,
        264,
        958,
        11795,
        567,
        51776
      ],
      "temperature": 0.0,
      "avg_logprob": -0.18069818068523796,
      "compression_ratio": 1.6129032258064515,
      "no_speech_prob": 0.0012424468295648694
    },
    {
      "id": 170,
      "seek": 122944,
      "start": 1229.44,
      "end": 1232.2,
      "text": " will be assigned by the reviewer to look at.",
      "tokens": [
        50364,
        486,
        312,
        13279,
        538,
        264,
        3131,
        260,
        281,
        574,
        412,
        13,
        50502
      ],
      "temperature": 0.0,
      "avg_logprob": -0.19102063672295932,
      "compression_ratio": 1.5324074074074074,
      "no_speech_prob": 0.023646557703614235
    },
    {
      "id": 171,
      "seek": 122944,
      "start": 1232.2,
      "end": 1236.1200000000001,
      "text": " So I explained to them that this is the bigger proposal.",
      "tokens": [
        50502,
        407,
        286,
        8825,
        281,
        552,
        300,
        341,
        307,
        264,
        3801,
        11494,
        13,
        50698
      ],
      "temperature": 0.0,
      "avg_logprob": -0.19102063672295932,
      "compression_ratio": 1.5324074074074074,
      "no_speech_prob": 0.023646557703614235
    },
    {
      "id": 172,
      "seek": 122944,
      "start": 1236.1200000000001,
      "end": 1242.48,
      "text": " This MR only takes care of like this small part of that solution.",
      "tokens": [
        50698,
        639,
        9808,
        787,
        2516,
        1127,
        295,
        411,
        341,
        1359,
        644,
        295,
        300,
        3827,
        13,
        51016
      ],
      "temperature": 0.0,
      "avg_logprob": -0.19102063672295932,
      "compression_ratio": 1.5324074074074074,
      "no_speech_prob": 0.023646557703614235
    },
    {
      "id": 173,
      "seek": 122944,
      "start": 1242.48,
      "end": 1245.24,
      "text": " And eventually this will lead to something else.",
      "tokens": [
        51016,
        400,
        4728,
        341,
        486,
        1477,
        281,
        746,
        1646,
        13,
        51154
      ],
      "temperature": 0.0,
      "avg_logprob": -0.19102063672295932,
      "compression_ratio": 1.5324074074074074,
      "no_speech_prob": 0.023646557703614235
    },
    {
      "id": 174,
      "seek": 122944,
      "start": 1245.24,
      "end": 1250.16,
      "text": " So yeah, that's how we did it.",
      "tokens": [
        51154,
        407,
        1338,
        11,
        300,
        311,
        577,
        321,
        630,
        309,
        13,
        51400
      ],
      "temperature": 0.0,
      "avg_logprob": -0.19102063672295932,
      "compression_ratio": 1.5324074074074074,
      "no_speech_prob": 0.023646557703614235
    },
    {
      "id": 175,
      "seek": 122944,
      "start": 1250.16,
      "end": 1254.6000000000001,
      "text": " Now thanks for sharing that'll help us a lot with there's two big initiatives going",
      "tokens": [
        51400,
        823,
        3231,
        337,
        5414,
        300,
        603,
        854,
        505,
        257,
        688,
        365,
        456,
        311,
        732,
        955,
        16194,
        516,
        51622
      ],
      "temperature": 0.0,
      "avg_logprob": -0.19102063672295932,
      "compression_ratio": 1.5324074074074074,
      "no_speech_prob": 0.023646557703614235
    },
    {
      "id": 176,
      "seek": 125460,
      "start": 1254.6,
      "end": 1255.8,
      "text": " on on my team right now.",
      "tokens": [
        50364,
        322,
        322,
        452,
        1469,
        558,
        586,
        13,
        50424
      ],
      "temperature": 0.0,
      "avg_logprob": -0.3541531032986111,
      "compression_ratio": 1.493273542600897,
      "no_speech_prob": 0.010122065432369709
    },
    {
      "id": 177,
      "seek": 125460,
      "start": 1255.8,
      "end": 1258.0,
      "text": " And I think we'll have to do this for both of them.",
      "tokens": [
        50424,
        400,
        286,
        519,
        321,
        603,
        362,
        281,
        360,
        341,
        337,
        1293,
        295,
        552,
        13,
        50534
      ],
      "temperature": 0.0,
      "avg_logprob": -0.3541531032986111,
      "compression_ratio": 1.493273542600897,
      "no_speech_prob": 0.010122065432369709
    },
    {
      "id": 178,
      "seek": 125460,
      "start": 1258.0,
      "end": 1259.0,
      "text": " So.",
      "tokens": [
        50534,
        407,
        13,
        50584
      ],
      "temperature": 0.0,
      "avg_logprob": -0.3541531032986111,
      "compression_ratio": 1.493273542600897,
      "no_speech_prob": 0.010122065432369709
    },
    {
      "id": 179,
      "seek": 125460,
      "start": 1259.0,
      "end": 1261.0,
      "text": " Oh, okay.",
      "tokens": [
        50584,
        876,
        11,
        1392,
        13,
        50684
      ],
      "temperature": 0.0,
      "avg_logprob": -0.3541531032986111,
      "compression_ratio": 1.493273542600897,
      "no_speech_prob": 0.010122065432369709
    },
    {
      "id": 180,
      "seek": 125460,
      "start": 1261.0,
      "end": 1267.84,
      "text": " Yeah, my final thing is I'm finalizing the group level environment solution validation study",
      "tokens": [
        50684,
        865,
        11,
        452,
        2572,
        551,
        307,
        286,
        478,
        2572,
        3319,
        264,
        1594,
        1496,
        2823,
        3827,
        24071,
        2979,
        51026
      ],
      "temperature": 0.0,
      "avg_logprob": -0.3541531032986111,
      "compression_ratio": 1.493273542600897,
      "no_speech_prob": 0.010122065432369709
    },
    {
      "id": 181,
      "seek": 125460,
      "start": 1267.84,
      "end": 1268.84,
      "text": " this week.",
      "tokens": [
        51026,
        341,
        1243,
        13,
        51076
      ],
      "temperature": 0.0,
      "avg_logprob": -0.3541531032986111,
      "compression_ratio": 1.493273542600897,
      "no_speech_prob": 0.010122065432369709
    },
    {
      "id": 182,
      "seek": 125460,
      "start": 1268.84,
      "end": 1275.1599999999999,
      "text": " So hopefully this will make some progress after being paused during kind of like the team switch",
      "tokens": [
        51076,
        407,
        4696,
        341,
        486,
        652,
        512,
        4205,
        934,
        885,
        46860,
        1830,
        733,
        295,
        411,
        264,
        1469,
        3679,
        51392
      ],
      "temperature": 0.0,
      "avg_logprob": -0.3541531032986111,
      "compression_ratio": 1.493273542600897,
      "no_speech_prob": 0.010122065432369709
    },
    {
      "id": 183,
      "seek": 125460,
      "start": 1275.1599999999999,
      "end": 1276.1599999999999,
      "text": " time.",
      "tokens": [
        51392,
        565,
        13,
        51442
      ],
      "temperature": 0.0,
      "avg_logprob": -0.3541531032986111,
      "compression_ratio": 1.493273542600897,
      "no_speech_prob": 0.010122065432369709
    },
    {
      "id": 184,
      "seek": 125460,
      "start": 1276.1599999999999,
      "end": 1280.9199999999998,
      "text": " I'll pass it to you to get.",
      "tokens": [
        51442,
        286,
        603,
        1320,
        309,
        281,
        291,
        281,
        483,
        13,
        51680
      ],
      "temperature": 0.0,
      "avg_logprob": -0.3541531032986111,
      "compression_ratio": 1.493273542600897,
      "no_speech_prob": 0.010122065432369709
    },
    {
      "id": 185,
      "seek": 125460,
      "start": 1280.9199999999998,
      "end": 1281.9199999999998,
      "text": " Thanks.",
      "tokens": [
        51680,
        2561,
        13,
        51730
      ],
      "temperature": 0.0,
      "avg_logprob": -0.3541531032986111,
      "compression_ratio": 1.493273542600897,
      "no_speech_prob": 0.010122065432369709
    },
    {
      "id": 186,
      "seek": 128192,
      "start": 1282.64,
      "end": 1287.3600000000001,
      "text": " I just have this small update like besides all the state group work that I anyway keep",
      "tokens": [
        50400,
        286,
        445,
        362,
        341,
        1359,
        5623,
        411,
        11868,
        439,
        264,
        1785,
        1594,
        589,
        300,
        286,
        4033,
        1066,
        50636
      ],
      "temperature": 0.0,
      "avg_logprob": -0.3021968519183951,
      "compression_ratio": 1.5252525252525253,
      "no_speech_prob": 0.10036982595920563
    },
    {
      "id": 187,
      "seek": 128192,
      "start": 1287.3600000000001,
      "end": 1290.88,
      "text": " sharing and co working and other channels.",
      "tokens": [
        50636,
        5414,
        293,
        598,
        1364,
        293,
        661,
        9235,
        13,
        50812
      ],
      "temperature": 0.0,
      "avg_logprob": -0.3021968519183951,
      "compression_ratio": 1.5252525252525253,
      "no_speech_prob": 0.10036982595920563
    },
    {
      "id": 188,
      "seek": 128192,
      "start": 1290.88,
      "end": 1299.4,
      "text": " That in preparation for the UX theme workshop that I thought I would be conducting next",
      "tokens": [
        50812,
        663,
        294,
        13081,
        337,
        264,
        40176,
        6314,
        13541,
        300,
        286,
        1194,
        286,
        576,
        312,
        21749,
        958,
        51238
      ],
      "temperature": 0.0,
      "avg_logprob": -0.3021968519183951,
      "compression_ratio": 1.5252525252525253,
      "no_speech_prob": 0.10036982595920563
    },
    {
      "id": 189,
      "seek": 128192,
      "start": 1299.4,
      "end": 1300.88,
      "text": " month.",
      "tokens": [
        51238,
        1618,
        13,
        51312
      ],
      "temperature": 0.0,
      "avg_logprob": -0.3021968519183951,
      "compression_ratio": 1.5252525252525253,
      "no_speech_prob": 0.10036982595920563
    },
    {
      "id": 190,
      "seek": 128192,
      "start": 1300.88,
      "end": 1305.96,
      "text": " I figured that we don't have a vision documented for five plans security yet.",
      "tokens": [
        51312,
        286,
        8932,
        300,
        321,
        500,
        380,
        362,
        257,
        5201,
        23007,
        337,
        1732,
        5482,
        3825,
        1939,
        13,
        51566
      ],
      "temperature": 0.0,
      "avg_logprob": -0.3021968519183951,
      "compression_ratio": 1.5252525252525253,
      "no_speech_prob": 0.10036982595920563
    },
    {
      "id": 191,
      "seek": 130596,
      "start": 1305.96,
      "end": 1312.96,
      "text": " So like the very first step for me to prepare for the workshop was to meet with Jocelyn",
      "tokens": [
        50364,
        407,
        411,
        264,
        588,
        700,
        1823,
        337,
        385,
        281,
        5940,
        337,
        264,
        13541,
        390,
        281,
        1677,
        365,
        508,
        905,
        49449,
        50714
      ],
      "temperature": 0.0,
      "avg_logprob": -0.1923915147781372,
      "compression_ratio": 1.665,
      "no_speech_prob": 0.019918294623494148
    },
    {
      "id": 192,
      "seek": 130596,
      "start": 1312.96,
      "end": 1319.68,
      "text": " and right down our vision and do that keeping in mind the information that we have at hand",
      "tokens": [
        50714,
        293,
        558,
        760,
        527,
        5201,
        293,
        360,
        300,
        5145,
        294,
        1575,
        264,
        1589,
        300,
        321,
        362,
        412,
        1011,
        51050
      ],
      "temperature": 0.0,
      "avg_logprob": -0.1923915147781372,
      "compression_ratio": 1.665,
      "no_speech_prob": 0.019918294623494148
    },
    {
      "id": 193,
      "seek": 130596,
      "start": 1319.68,
      "end": 1325.4,
      "text": " like all the insights from Erica's research is the existing epics that we have the highly",
      "tokens": [
        51050,
        411,
        439,
        264,
        14310,
        490,
        37429,
        311,
        2132,
        307,
        264,
        6741,
        2388,
        1167,
        300,
        321,
        362,
        264,
        5405,
        51336
      ],
      "temperature": 0.0,
      "avg_logprob": -0.1923915147781372,
      "compression_ratio": 1.665,
      "no_speech_prob": 0.019918294623494148
    },
    {
      "id": 194,
      "seek": 130596,
      "start": 1325.4,
      "end": 1330.56,
      "text": " water issues that we have the market inside the jobs to be done.",
      "tokens": [
        51336,
        1281,
        2663,
        300,
        321,
        362,
        264,
        2142,
        1854,
        264,
        4782,
        281,
        312,
        1096,
        13,
        51594
      ],
      "temperature": 0.0,
      "avg_logprob": -0.1923915147781372,
      "compression_ratio": 1.665,
      "no_speech_prob": 0.019918294623494148
    },
    {
      "id": 195,
      "seek": 133056,
      "start": 1330.56,
      "end": 1337.36,
      "text": " So I created sort of this homegrown template to have that conversation that discussion",
      "tokens": [
        50364,
        407,
        286,
        2942,
        1333,
        295,
        341,
        1280,
        38413,
        12379,
        281,
        362,
        300,
        3761,
        300,
        5017,
        50704
      ],
      "temperature": 0.0,
      "avg_logprob": -0.15917881145033724,
      "compression_ratio": 1.6930232558139535,
      "no_speech_prob": 0.05395087972283363
    },
    {
      "id": 196,
      "seek": 133056,
      "start": 1337.36,
      "end": 1345.6399999999999,
      "text": " so that eventually like we make sure that whatever vision we arrive at it is done like after",
      "tokens": [
        50704,
        370,
        300,
        4728,
        411,
        321,
        652,
        988,
        300,
        2035,
        5201,
        321,
        8881,
        412,
        309,
        307,
        1096,
        411,
        934,
        51118
      ],
      "temperature": 0.0,
      "avg_logprob": -0.15917881145033724,
      "compression_ratio": 1.6930232558139535,
      "no_speech_prob": 0.05395087972283363
    },
    {
      "id": 197,
      "seek": 133056,
      "start": 1345.6399999999999,
      "end": 1350.56,
      "text": " being informed of everything that else that we have in front of us.",
      "tokens": [
        51118,
        885,
        11740,
        295,
        1203,
        300,
        1646,
        300,
        321,
        362,
        294,
        1868,
        295,
        505,
        13,
        51364
      ],
      "temperature": 0.0,
      "avg_logprob": -0.15917881145033724,
      "compression_ratio": 1.6930232558139535,
      "no_speech_prob": 0.05395087972283363
    },
    {
      "id": 198,
      "seek": 133056,
      "start": 1350.56,
      "end": 1357.04,
      "text": " So the first mural is where we started from I just made a copy when I created that.",
      "tokens": [
        51364,
        407,
        264,
        700,
        40595,
        307,
        689,
        321,
        1409,
        490,
        286,
        445,
        1027,
        257,
        5055,
        562,
        286,
        2942,
        300,
        13,
        51688
      ],
      "temperature": 0.0,
      "avg_logprob": -0.15917881145033724,
      "compression_ratio": 1.6930232558139535,
      "no_speech_prob": 0.05395087972283363
    },
    {
      "id": 199,
      "seek": 133056,
      "start": 1357.04,
      "end": 1359.56,
      "text": " So that was the very first date.",
      "tokens": [
        51688,
        407,
        300,
        390,
        264,
        588,
        700,
        4002,
        13,
        51814
      ],
      "temperature": 0.0,
      "avg_logprob": -0.15917881145033724,
      "compression_ratio": 1.6930232558139535,
      "no_speech_prob": 0.05395087972283363
    },
    {
      "id": 200,
      "seek": 135956,
      "start": 1359.56,
      "end": 1368.48,
      "text": " I brought together all the information that we had on different areas and create two sections.",
      "tokens": [
        50364,
        286,
        3038,
        1214,
        439,
        264,
        1589,
        300,
        321,
        632,
        322,
        819,
        3179,
        293,
        1884,
        732,
        10863,
        13,
        50810
      ],
      "temperature": 0.0,
      "avg_logprob": -0.15753713776083553,
      "compression_ratio": 1.574468085106383,
      "no_speech_prob": 0.0009927725186571479
    },
    {
      "id": 201,
      "seek": 135956,
      "start": 1368.48,
      "end": 1372.04,
      "text": " One was for each category.",
      "tokens": [
        50810,
        1485,
        390,
        337,
        1184,
        7719,
        13,
        50988
      ],
      "temperature": 0.0,
      "avg_logprob": -0.15753713776083553,
      "compression_ratio": 1.574468085106383,
      "no_speech_prob": 0.0009927725186571479
    },
    {
      "id": 202,
      "seek": 135956,
      "start": 1372.04,
      "end": 1378.3999999999999,
      "text": " The plan was that we create clusters like we'll be bringing together a item that could",
      "tokens": [
        50988,
        440,
        1393,
        390,
        300,
        321,
        1884,
        23313,
        411,
        321,
        603,
        312,
        5062,
        1214,
        257,
        3174,
        300,
        727,
        51306
      ],
      "temperature": 0.0,
      "avg_logprob": -0.15753713776083553,
      "compression_ratio": 1.574468085106383,
      "no_speech_prob": 0.0009927725186571479
    },
    {
      "id": 203,
      "seek": 135956,
      "start": 1378.3999999999999,
      "end": 1387.36,
      "text": " form one theme and we ended up with four groups of information or of like the work that",
      "tokens": [
        51306,
        1254,
        472,
        6314,
        293,
        321,
        4590,
        493,
        365,
        1451,
        3935,
        295,
        1589,
        420,
        295,
        411,
        264,
        589,
        300,
        51754
      ],
      "temperature": 0.0,
      "avg_logprob": -0.15753713776083553,
      "compression_ratio": 1.574468085106383,
      "no_speech_prob": 0.0009927725186571479
    },
    {
      "id": 204,
      "seek": 138736,
      "start": 1387.36,
      "end": 1391.52,
      "text": " we want to do and then we just voted on them.",
      "tokens": [
        50364,
        321,
        528,
        281,
        360,
        293,
        550,
        321,
        445,
        13415,
        322,
        552,
        13,
        50572
      ],
      "temperature": 0.0,
      "avg_logprob": -0.2294091849491514,
      "compression_ratio": 1.650943396226415,
      "no_speech_prob": 0.014446144923567772
    },
    {
      "id": 205,
      "seek": 138736,
      "start": 1391.52,
      "end": 1398.3999999999999,
      "text": " One of the options that we had was to maybe take it forward and instead do a like come",
      "tokens": [
        50572,
        1485,
        295,
        264,
        3956,
        300,
        321,
        632,
        390,
        281,
        1310,
        747,
        309,
        2128,
        293,
        2602,
        360,
        257,
        411,
        808,
        50916
      ],
      "temperature": 0.0,
      "avg_logprob": -0.2294091849491514,
      "compression_ratio": 1.650943396226415,
      "no_speech_prob": 0.014446144923567772
    },
    {
      "id": 206,
      "seek": 138736,
      "start": 1398.3999999999999,
      "end": 1403.76,
      "text": " up with the rice score but we refrain from doing that since we are going to be doing",
      "tokens": [
        50916,
        493,
        365,
        264,
        5090,
        6175,
        457,
        321,
        46177,
        490,
        884,
        300,
        1670,
        321,
        366,
        516,
        281,
        312,
        884,
        51184
      ],
      "temperature": 0.0,
      "avg_logprob": -0.2294091849491514,
      "compression_ratio": 1.650943396226415,
      "no_speech_prob": 0.014446144923567772
    },
    {
      "id": 207,
      "seek": 138736,
      "start": 1403.76,
      "end": 1407.1599999999999,
      "text": " the UX thing workshop anyway like in future.",
      "tokens": [
        51184,
        264,
        40176,
        551,
        13541,
        4033,
        411,
        294,
        2027,
        13,
        51354
      ],
      "temperature": 0.0,
      "avg_logprob": -0.2294091849491514,
      "compression_ratio": 1.650943396226415,
      "no_speech_prob": 0.014446144923567772
    },
    {
      "id": 208,
      "seek": 138736,
      "start": 1407.1599999999999,
      "end": 1413.3999999999999,
      "text": " So this we just kept this very simple and once we identified the larger themes the next",
      "tokens": [
        51354,
        407,
        341,
        321,
        445,
        4305,
        341,
        588,
        2199,
        293,
        1564,
        321,
        9234,
        264,
        4833,
        13544,
        264,
        958,
        51666
      ],
      "temperature": 0.0,
      "avg_logprob": -0.2294091849491514,
      "compression_ratio": 1.650943396226415,
      "no_speech_prob": 0.014446144923567772
    },
    {
      "id": 209,
      "seek": 141340,
      "start": 1413.4,
      "end": 1423.8000000000002,
      "text": " step we took was put them like use this business model innovation framework to bring together",
      "tokens": [
        50364,
        1823,
        321,
        1890,
        390,
        829,
        552,
        411,
        764,
        341,
        1606,
        2316,
        8504,
        8388,
        281,
        1565,
        1214,
        50884
      ],
      "temperature": 0.0,
      "avg_logprob": -0.16985301029534988,
      "compression_ratio": 1.7878787878787878,
      "no_speech_prob": 0.03635245934128761
    },
    {
      "id": 210,
      "seek": 141340,
      "start": 1423.8000000000002,
      "end": 1431.8400000000001,
      "text": " like who are we making who are we creating the solution for how we are planning to do",
      "tokens": [
        50884,
        411,
        567,
        366,
        321,
        1455,
        567,
        366,
        321,
        4084,
        264,
        3827,
        337,
        577,
        321,
        366,
        5038,
        281,
        360,
        51286
      ],
      "temperature": 0.0,
      "avg_logprob": -0.16985301029534988,
      "compression_ratio": 1.7878787878787878,
      "no_speech_prob": 0.03635245934128761
    },
    {
      "id": 211,
      "seek": 141340,
      "start": 1431.8400000000001,
      "end": 1435.8400000000001,
      "text": " that and to achieve that how what is it that we would be working on in terms of features",
      "tokens": [
        51286,
        300,
        293,
        281,
        4584,
        300,
        577,
        437,
        307,
        309,
        300,
        321,
        576,
        312,
        1364,
        322,
        294,
        2115,
        295,
        4122,
        51486
      ],
      "temperature": 0.0,
      "avg_logprob": -0.16985301029534988,
      "compression_ratio": 1.7878787878787878,
      "no_speech_prob": 0.03635245934128761
    },
    {
      "id": 212,
      "seek": 141340,
      "start": 1435.8400000000001,
      "end": 1442.64,
      "text": " in terms of changes then why do we think like why are we confident that these sort of",
      "tokens": [
        51486,
        294,
        2115,
        295,
        2962,
        550,
        983,
        360,
        321,
        519,
        411,
        983,
        366,
        321,
        6679,
        300,
        613,
        1333,
        295,
        51826
      ],
      "temperature": 0.0,
      "avg_logprob": -0.16985301029534988,
      "compression_ratio": 1.7878787878787878,
      "no_speech_prob": 0.03635245934128761
    },
    {
      "id": 213,
      "seek": 144264,
      "start": 1442.64,
      "end": 1448.8400000000001,
      "text": " features are going to like solve the problem and the good part with this innovation framework",
      "tokens": [
        50364,
        4122,
        366,
        516,
        281,
        411,
        5039,
        264,
        1154,
        293,
        264,
        665,
        644,
        365,
        341,
        8504,
        8388,
        50674
      ],
      "temperature": 0.0,
      "avg_logprob": -0.17801952362060547,
      "compression_ratio": 1.6854460093896713,
      "no_speech_prob": 0.015715111047029495
    },
    {
      "id": 214,
      "seek": 144264,
      "start": 1448.8400000000001,
      "end": 1455.76,
      "text": " is like you change if you make slightest of change in one part of this diagram you will",
      "tokens": [
        50674,
        307,
        411,
        291,
        1319,
        498,
        291,
        652,
        41040,
        295,
        1319,
        294,
        472,
        644,
        295,
        341,
        10686,
        291,
        486,
        51020
      ],
      "temperature": 0.0,
      "avg_logprob": -0.17801952362060547,
      "compression_ratio": 1.6854460093896713,
      "no_speech_prob": 0.015715111047029495
    },
    {
      "id": 215,
      "seek": 144264,
      "start": 1455.76,
      "end": 1460.92,
      "text": " have to like make adjustments all throughout so it's always balanced it's never imbalanced",
      "tokens": [
        51020,
        362,
        281,
        411,
        652,
        18624,
        439,
        3710,
        370,
        309,
        311,
        1009,
        13902,
        309,
        311,
        1128,
        566,
        40251,
        51278
      ],
      "temperature": 0.0,
      "avg_logprob": -0.17801952362060547,
      "compression_ratio": 1.6854460093896713,
      "no_speech_prob": 0.015715111047029495
    },
    {
      "id": 216,
      "seek": 144264,
      "start": 1460.92,
      "end": 1466.6000000000001,
      "text": " and it worked out pretty well for us we came up with our new vision which is we enable",
      "tokens": [
        51278,
        293,
        309,
        2732,
        484,
        1238,
        731,
        337,
        505,
        321,
        1361,
        493,
        365,
        527,
        777,
        5201,
        597,
        307,
        321,
        9528,
        51562
      ],
      "temperature": 0.0,
      "avg_logprob": -0.17801952362060547,
      "compression_ratio": 1.6854460093896713,
      "no_speech_prob": 0.015715111047029495
    },
    {
      "id": 217,
      "seek": 146660,
      "start": 1466.6,
      "end": 1473.6,
      "text": " organizations to adopt good practices for secure handling of sensitive information and",
      "tokens": [
        50364,
        6150,
        281,
        6878,
        665,
        7525,
        337,
        7144,
        13175,
        295,
        9477,
        1589,
        293,
        50714
      ],
      "temperature": 0.0,
      "avg_logprob": -0.4264351931485263,
      "compression_ratio": 1.4472049689440993,
      "no_speech_prob": 0.19591853022575378
    },
    {
      "id": 218,
      "seek": 146660,
      "start": 1473.6,
      "end": 1478.6399999999999,
      "text": " I just thought like since it worked so well maybe I'll just document this and share with",
      "tokens": [
        50714,
        286,
        445,
        1194,
        411,
        1670,
        309,
        2732,
        370,
        731,
        1310,
        286,
        603,
        445,
        4166,
        341,
        293,
        2073,
        365,
        50966
      ],
      "temperature": 0.0,
      "avg_logprob": -0.4264351931485263,
      "compression_ratio": 1.4472049689440993,
      "no_speech_prob": 0.19591853022575378
    },
    {
      "id": 219,
      "seek": 146660,
      "start": 1478.6399999999999,
      "end": 1484.24,
      "text": " the team I haven't done that yet though.",
      "tokens": [
        50966,
        264,
        1469,
        286,
        2378,
        380,
        1096,
        300,
        1939,
        1673,
        13,
        51246
      ],
      "temperature": 0.0,
      "avg_logprob": -0.4264351931485263,
      "compression_ratio": 1.4472049689440993,
      "no_speech_prob": 0.19591853022575378
    },
    {
      "id": 220,
      "seek": 146660,
      "start": 1484.24,
      "end": 1494.84,
      "text": " Yeah that's all.",
      "tokens": [
        51246,
        865,
        300,
        311,
        439,
        13,
        51776
      ],
      "temperature": 0.0,
      "avg_logprob": -0.4264351931485263,
      "compression_ratio": 1.4472049689440993,
      "no_speech_prob": 0.19591853022575378
    },
    {
      "id": 221,
      "seek": 149484,
      "start": 1494.84,
      "end": 1502.84,
      "text": " So for the pipeline offering we are focusing on the solution validation for the placement",
      "tokens": [
        50364,
        407,
        337,
        264,
        15517,
        8745,
        321,
        366,
        8416,
        322,
        264,
        3827,
        24071,
        337,
        264,
        17257,
        50764
      ],
      "temperature": 0.0,
      "avg_logprob": -0.23758411407470703,
      "compression_ratio": 1.7572815533980584,
      "no_speech_prob": 0.14300233125686646
    },
    {
      "id": 222,
      "seek": 149484,
      "start": 1502.84,
      "end": 1509.84,
      "text": " library placed this catalog feature is it the left side navigation or is it more embedded",
      "tokens": [
        50764,
        6405,
        7074,
        341,
        19746,
        4111,
        307,
        309,
        264,
        1411,
        1252,
        17346,
        420,
        307,
        309,
        544,
        16741,
        51114
      ],
      "temperature": 0.0,
      "avg_logprob": -0.23758411407470703,
      "compression_ratio": 1.7572815533980584,
      "no_speech_prob": 0.14300233125686646
    },
    {
      "id": 223,
      "seek": 149484,
      "start": 1509.84,
      "end": 1516.3999999999999,
      "text": " version on the pipeline editor and there were another option in the navigation but inside",
      "tokens": [
        51114,
        3037,
        322,
        264,
        15517,
        9839,
        293,
        456,
        645,
        1071,
        3614,
        294,
        264,
        17346,
        457,
        1854,
        51442
      ],
      "temperature": 0.0,
      "avg_logprob": -0.23758411407470703,
      "compression_ratio": 1.7572815533980584,
      "no_speech_prob": 0.14300233125686646
    },
    {
      "id": 224,
      "seek": 149484,
      "start": 1516.3999999999999,
      "end": 1522.76,
      "text": " the Explorer tab which is the same level with the organization the new tab but it's slightly",
      "tokens": [
        51442,
        264,
        31895,
        4421,
        597,
        307,
        264,
        912,
        1496,
        365,
        264,
        4475,
        264,
        777,
        4421,
        457,
        309,
        311,
        4748,
        51760
      ],
      "temperature": 0.0,
      "avg_logprob": -0.23758411407470703,
      "compression_ratio": 1.7572815533980584,
      "no_speech_prob": 0.14300233125686646
    },
    {
      "id": 225,
      "seek": 152276,
      "start": 1522.76,
      "end": 1525.16,
      "text": " behind that so it's the second step.",
      "tokens": [
        50364,
        2261,
        300,
        370,
        309,
        311,
        264,
        1150,
        1823,
        13,
        50484
      ],
      "temperature": 0.0,
      "avg_logprob": -0.21949918646561473,
      "compression_ratio": 1.7043478260869565,
      "no_speech_prob": 0.12234380841255188
    },
    {
      "id": 226,
      "seek": 152276,
      "start": 1525.16,
      "end": 1531.2,
      "text": " So we just tried to work on another round of research so things are cut to jumping in so",
      "tokens": [
        50484,
        407,
        321,
        445,
        3031,
        281,
        589,
        322,
        1071,
        3098,
        295,
        2132,
        370,
        721,
        366,
        1723,
        281,
        11233,
        294,
        370,
        50786
      ],
      "temperature": 0.0,
      "avg_logprob": -0.21949918646561473,
      "compression_ratio": 1.7043478260869565,
      "no_speech_prob": 0.12234380841255188
    },
    {
      "id": 227,
      "seek": 152276,
      "start": 1531.2,
      "end": 1536.28,
      "text": " we just started this conversation how can we proceed with the new test and then once we",
      "tokens": [
        50786,
        321,
        445,
        1409,
        341,
        3761,
        577,
        393,
        321,
        8991,
        365,
        264,
        777,
        1500,
        293,
        550,
        1564,
        321,
        51040
      ],
      "temperature": 0.0,
      "avg_logprob": -0.21949918646561473,
      "compression_ratio": 1.7043478260869565,
      "no_speech_prob": 0.12234380841255188
    },
    {
      "id": 228,
      "seek": 152276,
      "start": 1536.28,
      "end": 1542.32,
      "text": " got the result probably we can more confidently say okay so we should play this feature to",
      "tokens": [
        51040,
        658,
        264,
        1874,
        1391,
        321,
        393,
        544,
        41956,
        584,
        1392,
        370,
        321,
        820,
        862,
        341,
        4111,
        281,
        51342
      ],
      "temperature": 0.0,
      "avg_logprob": -0.21949918646561473,
      "compression_ratio": 1.7043478260869565,
      "no_speech_prob": 0.12234380841255188
    },
    {
      "id": 229,
      "seek": 152276,
      "start": 1542.32,
      "end": 1550.48,
      "text": " where so I'm also working on that I do work on that also the rest of the week and there",
      "tokens": [
        51342,
        689,
        370,
        286,
        478,
        611,
        1364,
        322,
        300,
        286,
        360,
        589,
        322,
        300,
        611,
        264,
        1472,
        295,
        264,
        1243,
        293,
        456,
        51750
      ],
      "temperature": 0.0,
      "avg_logprob": -0.21949918646561473,
      "compression_ratio": 1.7043478260869565,
      "no_speech_prob": 0.12234380841255188
    },
    {
      "id": 230,
      "seek": 155048,
      "start": 1550.48,
      "end": 1558.3600000000001,
      "text": " is another very critical discussion which is mostly about like how we architecture how",
      "tokens": [
        50364,
        307,
        1071,
        588,
        4924,
        5017,
        597,
        307,
        5240,
        466,
        411,
        577,
        321,
        9482,
        577,
        50758
      ],
      "temperature": 0.0,
      "avg_logprob": -0.2535620927810669,
      "compression_ratio": 1.6186046511627907,
      "no_speech_prob": 0.00703061930835247
    },
    {
      "id": 231,
      "seek": 155048,
      "start": 1558.3600000000001,
      "end": 1564.1200000000001,
      "text": " we create architecture for the backend which will eventually impact the year's work so",
      "tokens": [
        50758,
        321,
        1884,
        9482,
        337,
        264,
        38087,
        597,
        486,
        4728,
        2712,
        264,
        1064,
        311,
        589,
        370,
        51046
      ],
      "temperature": 0.0,
      "avg_logprob": -0.2535620927810669,
      "compression_ratio": 1.6186046511627907,
      "no_speech_prob": 0.00703061930835247
    },
    {
      "id": 232,
      "seek": 155048,
      "start": 1564.1200000000001,
      "end": 1570.64,
      "text": " here is the MR like I just don't want to dive into the two details but it's more like",
      "tokens": [
        51046,
        510,
        307,
        264,
        9808,
        411,
        286,
        445,
        500,
        380,
        528,
        281,
        9192,
        666,
        264,
        732,
        4365,
        457,
        309,
        311,
        544,
        411,
        51372
      ],
      "temperature": 0.0,
      "avg_logprob": -0.2535620927810669,
      "compression_ratio": 1.6186046511627907,
      "no_speech_prob": 0.00703061930835247
    },
    {
      "id": 233,
      "seek": 155048,
      "start": 1570.64,
      "end": 1577.92,
      "text": " if we design API this way then there's two there's some impact to the user flow and then",
      "tokens": [
        51372,
        498,
        321,
        1715,
        9362,
        341,
        636,
        550,
        456,
        311,
        732,
        456,
        311,
        512,
        2712,
        281,
        264,
        4195,
        3095,
        293,
        550,
        51736
      ],
      "temperature": 0.0,
      "avg_logprob": -0.2535620927810669,
      "compression_ratio": 1.6186046511627907,
      "no_speech_prob": 0.00703061930835247
    },
    {
      "id": 234,
      "seek": 157792,
      "start": 1577.92,
      "end": 1585.48,
      "text": " if we design API in the be way then it also changed the user behavior so I'm really glad",
      "tokens": [
        50364,
        498,
        321,
        1715,
        9362,
        294,
        264,
        312,
        636,
        550,
        309,
        611,
        3105,
        264,
        4195,
        5223,
        370,
        286,
        478,
        534,
        5404,
        50742
      ],
      "temperature": 0.0,
      "avg_logprob": -0.20684409141540527,
      "compression_ratio": 1.5844748858447488,
      "no_speech_prob": 0.004106607753783464
    },
    {
      "id": 235,
      "seek": 157792,
      "start": 1585.48,
      "end": 1590.76,
      "text": " that I'm participating in this discussion and just trying to follow up what they're",
      "tokens": [
        50742,
        300,
        286,
        478,
        13950,
        294,
        341,
        5017,
        293,
        445,
        1382,
        281,
        1524,
        493,
        437,
        436,
        434,
        51006
      ],
      "temperature": 0.0,
      "avg_logprob": -0.20684409141540527,
      "compression_ratio": 1.5844748858447488,
      "no_speech_prob": 0.004106607753783464
    },
    {
      "id": 236,
      "seek": 157792,
      "start": 1590.76,
      "end": 1598.76,
      "text": " saying and then if we make this decision then our MVC scope might be slightly changed",
      "tokens": [
        51006,
        1566,
        293,
        550,
        498,
        321,
        652,
        341,
        3537,
        550,
        527,
        17663,
        34,
        11923,
        1062,
        312,
        4748,
        3105,
        51406
      ],
      "temperature": 0.0,
      "avg_logprob": -0.20684409141540527,
      "compression_ratio": 1.5844748858447488,
      "no_speech_prob": 0.004106607753783464
    },
    {
      "id": 237,
      "seek": 157792,
      "start": 1598.76,
      "end": 1604.1200000000001,
      "text": " in terms of technical perspective but not a lot from the UX perspective so this is still",
      "tokens": [
        51406,
        294,
        2115,
        295,
        6191,
        4585,
        457,
        406,
        257,
        688,
        490,
        264,
        40176,
        4585,
        370,
        341,
        307,
        920,
        51674
      ],
      "temperature": 0.0,
      "avg_logprob": -0.20684409141540527,
      "compression_ratio": 1.5844748858447488,
      "no_speech_prob": 0.004106607753783464
    },
    {
      "id": 238,
      "seek": 160412,
      "start": 1604.12,
      "end": 1610.1999999999998,
      "text": " ongoing just wanted to share with you all and yeah that was my agenda that do you have any",
      "tokens": [
        50364,
        10452,
        445,
        1415,
        281,
        2073,
        365,
        291,
        439,
        293,
        1338,
        300,
        390,
        452,
        9829,
        300,
        360,
        291,
        362,
        604,
        50668
      ],
      "temperature": 0.0,
      "avg_logprob": -0.24724562738982725,
      "compression_ratio": 1.5,
      "no_speech_prob": 0.15000613033771515
    },
    {
      "id": 239,
      "seek": 160412,
      "start": 1610.1999999999998,
      "end": 1620.36,
      "text": " questions feedback? No then I'll pass it over to Erica sorry do you want to verbalize your call?",
      "tokens": [
        50668,
        1651,
        5824,
        30,
        883,
        550,
        286,
        603,
        1320,
        309,
        670,
        281,
        37429,
        2597,
        360,
        291,
        528,
        281,
        24781,
        1125,
        428,
        818,
        30,
        51176
      ],
      "temperature": 0.0,
      "avg_logprob": -0.24724562738982725,
      "compression_ratio": 1.5,
      "no_speech_prob": 0.15000613033771515
    },
    {
      "id": 240,
      "seek": 160412,
      "start": 1620.36,
      "end": 1629.7199999999998,
      "text": " Yeah so I think that this has been like really good work and we're trying to figure out like the way",
      "tokens": [
        51176,
        865,
        370,
        286,
        519,
        300,
        341,
        575,
        668,
        411,
        534,
        665,
        589,
        293,
        321,
        434,
        1382,
        281,
        2573,
        484,
        411,
        264,
        636,
        51644
      ],
      "temperature": 0.0,
      "avg_logprob": -0.24724562738982725,
      "compression_ratio": 1.5,
      "no_speech_prob": 0.15000613033771515
    },
    {
      "id": 241,
      "seek": 162972,
      "start": 1629.72,
      "end": 1636.76,
      "text": " to approach this solution validation but what we're going to end up doing is having an approach",
      "tokens": [
        50364,
        281,
        3109,
        341,
        3827,
        24071,
        457,
        437,
        321,
        434,
        516,
        281,
        917,
        493,
        884,
        307,
        1419,
        364,
        3109,
        50716
      ],
      "temperature": 0.0,
      "avg_logprob": -0.12671176298165027,
      "compression_ratio": 1.7720930232558139,
      "no_speech_prob": 0.02081291563808918
    },
    {
      "id": 242,
      "seek": 162972,
      "start": 1636.76,
      "end": 1643.56,
      "text": " that other teams can use and basically like what we've arrived at is having two groups one that",
      "tokens": [
        50716,
        300,
        661,
        5491,
        393,
        764,
        293,
        1936,
        411,
        437,
        321,
        600,
        6678,
        412,
        307,
        1419,
        732,
        3935,
        472,
        300,
        51056
      ],
      "temperature": 0.0,
      "avg_logprob": -0.12671176298165027,
      "compression_ratio": 1.7720930232558139,
      "no_speech_prob": 0.02081291563808918
    },
    {
      "id": 243,
      "seek": 162972,
      "start": 1643.56,
      "end": 1649.96,
      "text": " has the side nav in a prototype one that doesn't have the side nav in a prototype same tasks",
      "tokens": [
        51056,
        575,
        264,
        1252,
        5947,
        294,
        257,
        19475,
        472,
        300,
        1177,
        380,
        362,
        264,
        1252,
        5947,
        294,
        257,
        19475,
        912,
        9608,
        51376
      ],
      "temperature": 0.0,
      "avg_logprob": -0.12671176298165027,
      "compression_ratio": 1.7720930232558139,
      "no_speech_prob": 0.02081291563808918
    },
    {
      "id": 244,
      "seek": 162972,
      "start": 1649.96,
      "end": 1658.04,
      "text": " different entry points performance comparison maybe a satisfaction question in there and I think",
      "tokens": [
        51376,
        819,
        8729,
        2793,
        3389,
        9660,
        1310,
        257,
        18715,
        1168,
        294,
        456,
        293,
        286,
        519,
        51780
      ],
      "temperature": 0.0,
      "avg_logprob": -0.12671176298165027,
      "compression_ratio": 1.7720930232558139,
      "no_speech_prob": 0.02081291563808918
    },
    {
      "id": 245,
      "seek": 165804,
      "start": 1658.04,
      "end": 1666.76,
      "text": " that's what we're going to need to make an argument for the side nav so just wanted to like say",
      "tokens": [
        50364,
        300,
        311,
        437,
        321,
        434,
        516,
        281,
        643,
        281,
        652,
        364,
        6770,
        337,
        264,
        1252,
        5947,
        370,
        445,
        1415,
        281,
        411,
        584,
        50800
      ],
      "temperature": 0.0,
      "avg_logprob": -0.06471469667222765,
      "compression_ratio": 1.6179775280898876,
      "no_speech_prob": 0.00038338094600476325
    },
    {
      "id": 246,
      "seek": 165804,
      "start": 1666.76,
      "end": 1675.1599999999999,
      "text": " that this was a hard problem and to give props to say good job being patient and getting through it",
      "tokens": [
        50800,
        300,
        341,
        390,
        257,
        1152,
        1154,
        293,
        281,
        976,
        26173,
        281,
        584,
        665,
        1691,
        885,
        4537,
        293,
        1242,
        807,
        309,
        51220
      ],
      "temperature": 0.0,
      "avg_logprob": -0.06471469667222765,
      "compression_ratio": 1.6179775280898876,
      "no_speech_prob": 0.00038338094600476325
    },
    {
      "id": 247,
      "seek": 165804,
      "start": 1675.1599999999999,
      "end": 1681.1599999999999,
      "text": " and then I think like we're going to have a model so that next time it won't require so much",
      "tokens": [
        51220,
        293,
        550,
        286,
        519,
        411,
        321,
        434,
        516,
        281,
        362,
        257,
        2316,
        370,
        300,
        958,
        565,
        309,
        1582,
        380,
        3651,
        370,
        709,
        51520
      ],
      "temperature": 0.0,
      "avg_logprob": -0.06471469667222765,
      "compression_ratio": 1.6179775280898876,
      "no_speech_prob": 0.00038338094600476325
    },
    {
      "id": 248,
      "seek": 168116,
      "start": 1681.3200000000002,
      "end": 1689.64,
      "text": " thinking as much thinking to get to how we approach it. Yeah thank you for sharing that Erica because",
      "tokens": [
        50372,
        1953,
        382,
        709,
        1953,
        281,
        483,
        281,
        577,
        321,
        3109,
        309,
        13,
        865,
        1309,
        291,
        337,
        5414,
        300,
        37429,
        570,
        50788
      ],
      "temperature": 0.0,
      "avg_logprob": -0.15262919122522528,
      "compression_ratio": 1.6756756756756757,
      "no_speech_prob": 0.003730928059667349
    },
    {
      "id": 249,
      "seek": 168116,
      "start": 1689.64,
      "end": 1694.8400000000001,
      "text": " it wasn't easy but the good thing is like full the first round of validation we could",
      "tokens": [
        50788,
        309,
        2067,
        380,
        1858,
        457,
        264,
        665,
        551,
        307,
        411,
        1577,
        264,
        700,
        3098,
        295,
        24071,
        321,
        727,
        51048
      ],
      "temperature": 0.0,
      "avg_logprob": -0.15262919122522528,
      "compression_ratio": 1.6756756756756757,
      "no_speech_prob": 0.003730928059667349
    },
    {
      "id": 250,
      "seek": 168116,
      "start": 1695.72,
      "end": 1702.28,
      "text": " eliminate like a list one option and now we have two so that's good and then I think since it's",
      "tokens": [
        51092,
        13819,
        411,
        257,
        1329,
        472,
        3614,
        293,
        586,
        321,
        362,
        732,
        370,
        300,
        311,
        665,
        293,
        550,
        286,
        519,
        1670,
        309,
        311,
        51420
      ],
      "temperature": 0.0,
      "avg_logprob": -0.15262919122522528,
      "compression_ratio": 1.6756756756756757,
      "no_speech_prob": 0.003730928059667349
    },
    {
      "id": 251,
      "seek": 168116,
      "start": 1702.28,
      "end": 1706.6000000000001,
      "text": " a new process like there are some challenges it's just not our team there are some other",
      "tokens": [
        51420,
        257,
        777,
        1399,
        411,
        456,
        366,
        512,
        4759,
        309,
        311,
        445,
        406,
        527,
        1469,
        456,
        366,
        512,
        661,
        51636
      ],
      "temperature": 0.0,
      "avg_logprob": -0.15262919122522528,
      "compression_ratio": 1.6756756756756757,
      "no_speech_prob": 0.003730928059667349
    },
    {
      "id": 252,
      "seek": 170660,
      "start": 1706.6,
      "end": 1713.08,
      "text": " teams also going for a similar process at the moment and then I think once we design nicely and",
      "tokens": [
        50364,
        5491,
        611,
        516,
        337,
        257,
        2531,
        1399,
        412,
        264,
        1623,
        293,
        550,
        286,
        519,
        1564,
        321,
        1715,
        9594,
        293,
        50688
      ],
      "temperature": 0.0,
      "avg_logprob": -0.10953869036774137,
      "compression_ratio": 1.60989010989011,
      "no_speech_prob": 0.0031122996006160975
    },
    {
      "id": 253,
      "seek": 170660,
      "start": 1713.08,
      "end": 1719.0,
      "text": " then come up with the results I think probably we can also add it to our handbook and then provide",
      "tokens": [
        50688,
        550,
        808,
        493,
        365,
        264,
        3542,
        286,
        519,
        1391,
        321,
        393,
        611,
        909,
        309,
        281,
        527,
        1011,
        2939,
        293,
        550,
        2893,
        50984
      ],
      "temperature": 0.0,
      "avg_logprob": -0.10953869036774137,
      "compression_ratio": 1.60989010989011,
      "no_speech_prob": 0.0031122996006160975
    },
    {
      "id": 254,
      "seek": 170660,
      "start": 1719.0,
      "end": 1728.1999999999998,
      "text": " better guidance on how to test the things around the placement. Yeah I've been thinking about this",
      "tokens": [
        50984,
        1101,
        10056,
        322,
        577,
        281,
        1500,
        264,
        721,
        926,
        264,
        17257,
        13,
        865,
        286,
        600,
        668,
        1953,
        466,
        341,
        51444
      ],
      "temperature": 0.0,
      "avg_logprob": -0.10953869036774137,
      "compression_ratio": 1.60989010989011,
      "no_speech_prob": 0.0031122996006160975
    },
    {
      "id": 255,
      "seek": 172820,
      "start": 1728.2,
      "end": 1736.92,
      "text": " Erica and like wouldn't it always be the case that whenever we present to users an navigation",
      "tokens": [
        50364,
        37429,
        293,
        411,
        2759,
        380,
        309,
        1009,
        312,
        264,
        1389,
        300,
        5699,
        321,
        1974,
        281,
        5022,
        364,
        17346,
        50800
      ],
      "temperature": 0.0,
      "avg_logprob": -0.11640281283978335,
      "compression_ratio": 1.696969696969697,
      "no_speech_prob": 0.004863633308559656
    },
    {
      "id": 256,
      "seek": 172820,
      "start": 1736.92,
      "end": 1743.88,
      "text": " option that's placed at a higher level it will always end up getting a better score. Yeah and exactly",
      "tokens": [
        50800,
        3614,
        300,
        311,
        7074,
        412,
        257,
        2946,
        1496,
        309,
        486,
        1009,
        917,
        493,
        1242,
        257,
        1101,
        6175,
        13,
        865,
        293,
        2293,
        51148
      ],
      "temperature": 0.0,
      "avg_logprob": -0.11640281283978335,
      "compression_ratio": 1.696969696969697,
      "no_speech_prob": 0.004863633308559656
    },
    {
      "id": 257,
      "seek": 172820,
      "start": 1743.88,
      "end": 1749.0,
      "text": " I've been calling it the bagel problem or I'm like we could ask them to search for bagels in the",
      "tokens": [
        51148,
        286,
        600,
        668,
        5141,
        309,
        264,
        3411,
        338,
        1154,
        420,
        286,
        478,
        411,
        321,
        727,
        1029,
        552,
        281,
        3164,
        337,
        3411,
        1625,
        294,
        264,
        51404
      ],
      "temperature": 0.0,
      "avg_logprob": -0.11640281283978335,
      "compression_ratio": 1.696969696969697,
      "no_speech_prob": 0.004863633308559656
    },
    {
      "id": 258,
      "seek": 172820,
      "start": 1749.0,
      "end": 1756.28,
      "text": " product and if we had bagels on the side nav they would always go to the side nav so yeah so that's",
      "tokens": [
        51404,
        1674,
        293,
        498,
        321,
        632,
        3411,
        1625,
        322,
        264,
        1252,
        5947,
        436,
        576,
        1009,
        352,
        281,
        264,
        1252,
        5947,
        370,
        1338,
        370,
        300,
        311,
        51768
      ],
      "temperature": 0.0,
      "avg_logprob": -0.11640281283978335,
      "compression_ratio": 1.696969696969697,
      "no_speech_prob": 0.004863633308559656
    },
    {
      "id": 259,
      "seek": 175628,
      "start": 1756.28,
      "end": 1762.52,
      "text": " why I think having the two groups there where one they just don't even have that option so it's not",
      "tokens": [
        50364,
        983,
        286,
        519,
        1419,
        264,
        732,
        3935,
        456,
        689,
        472,
        436,
        445,
        500,
        380,
        754,
        362,
        300,
        3614,
        370,
        309,
        311,
        406,
        50676
      ],
      "temperature": 0.0,
      "avg_logprob": -0.0772995668299058,
      "compression_ratio": 1.768888888888889,
      "no_speech_prob": 0.0007603909471072257
    },
    {
      "id": 260,
      "seek": 175628,
      "start": 1762.52,
      "end": 1768.76,
      "text": " like bagels on the side nav then we can see if they can still perform or not and that's why it's",
      "tokens": [
        50676,
        411,
        3411,
        1625,
        322,
        264,
        1252,
        5947,
        550,
        321,
        393,
        536,
        498,
        436,
        393,
        920,
        2042,
        420,
        406,
        293,
        300,
        311,
        983,
        309,
        311,
        50988
      ],
      "temperature": 0.0,
      "avg_logprob": -0.0772995668299058,
      "compression_ratio": 1.768888888888889,
      "no_speech_prob": 0.0007603909471072257
    },
    {
      "id": 261,
      "seek": 175628,
      "start": 1768.76,
      "end": 1774.84,
      "text": " with it that's why it's two different groups. Yeah I'm just eager to see like if you figure out that",
      "tokens": [
        50988,
        365,
        309,
        300,
        311,
        983,
        309,
        311,
        732,
        819,
        3935,
        13,
        865,
        286,
        478,
        445,
        18259,
        281,
        536,
        411,
        498,
        291,
        2573,
        484,
        300,
        51292
      ],
      "temperature": 0.0,
      "avg_logprob": -0.0772995668299058,
      "compression_ratio": 1.768888888888889,
      "no_speech_prob": 0.0007603909471072257
    },
    {
      "id": 262,
      "seek": 175628,
      "start": 1774.84,
      "end": 1781.8799999999999,
      "text": " let's say without bagels being on the front shelf if this still happened to find it how do you still",
      "tokens": [
        51292,
        718,
        311,
        584,
        1553,
        3411,
        1625,
        885,
        322,
        264,
        1868,
        15222,
        498,
        341,
        920,
        2011,
        281,
        915,
        309,
        577,
        360,
        291,
        920,
        51644
      ],
      "temperature": 0.0,
      "avg_logprob": -0.0772995668299058,
      "compression_ratio": 1.768888888888889,
      "no_speech_prob": 0.0007603909471072257
    },
    {
      "id": 263,
      "seek": 178188,
      "start": 1781.88,
      "end": 1788.7600000000002,
      "text": " justify to the team that you know they were still able to find it. Yeah they found it much",
      "tokens": [
        50364,
        20833,
        281,
        264,
        1469,
        300,
        291,
        458,
        436,
        645,
        920,
        1075,
        281,
        915,
        309,
        13,
        865,
        436,
        1352,
        309,
        709,
        50708
      ],
      "temperature": 0.0,
      "avg_logprob": -0.11982626716295879,
      "compression_ratio": 1.711111111111111,
      "no_speech_prob": 0.0007397782173939049
    },
    {
      "id": 264,
      "seek": 178188,
      "start": 1788.7600000000002,
      "end": 1794.1200000000001,
      "text": " easier on the higher level so that's why I think it's like if performance is the same with those",
      "tokens": [
        50708,
        3571,
        322,
        264,
        2946,
        1496,
        370,
        300,
        311,
        983,
        286,
        519,
        309,
        311,
        411,
        498,
        3389,
        307,
        264,
        912,
        365,
        729,
        50976
      ],
      "temperature": 0.0,
      "avg_logprob": -0.11982626716295879,
      "compression_ratio": 1.711111111111111,
      "no_speech_prob": 0.0007397782173939049
    },
    {
      "id": 265,
      "seek": 178188,
      "start": 1794.1200000000001,
      "end": 1801.24,
      "text": " two groups we don't have evidence to add it to this okay it's only that if they are failing without",
      "tokens": [
        50976,
        732,
        3935,
        321,
        500,
        380,
        362,
        4467,
        281,
        909,
        309,
        281,
        341,
        1392,
        309,
        311,
        787,
        300,
        498,
        436,
        366,
        18223,
        1553,
        51332
      ],
      "temperature": 0.0,
      "avg_logprob": -0.11982626716295879,
      "compression_ratio": 1.711111111111111,
      "no_speech_prob": 0.0007397782173939049
    },
    {
      "id": 266,
      "seek": 178188,
      "start": 1801.24,
      "end": 1809.72,
      "text": " that side nav option then we can say hey it's important okay and we don't we don't know like with",
      "tokens": [
        51332,
        300,
        1252,
        5947,
        3614,
        550,
        321,
        393,
        584,
        4177,
        309,
        311,
        1021,
        1392,
        293,
        321,
        500,
        380,
        321,
        500,
        380,
        458,
        411,
        365,
        51756
      ],
      "temperature": 0.0,
      "avg_logprob": -0.11982626716295879,
      "compression_ratio": 1.711111111111111,
      "no_speech_prob": 0.0007397782173939049
    },
    {
      "id": 267,
      "seek": 180972,
      "start": 1809.72,
      "end": 1815.48,
      "text": " the other approach I was like it's bagels like they'll just go to the where it says bagels but so",
      "tokens": [
        50364,
        264,
        661,
        3109,
        286,
        390,
        411,
        309,
        311,
        3411,
        1625,
        411,
        436,
        603,
        445,
        352,
        281,
        264,
        689,
        309,
        1619,
        3411,
        1625,
        457,
        370,
        50652
      ],
      "temperature": 0.0,
      "avg_logprob": -0.11318155045204974,
      "compression_ratio": 1.6798245614035088,
      "no_speech_prob": 0.00017799856141209602
    },
    {
      "id": 268,
      "seek": 180972,
      "start": 1815.48,
      "end": 1821.48,
      "text": " with this approach we don't feel like we have a sense of which one will work and then that's when",
      "tokens": [
        50652,
        365,
        341,
        3109,
        321,
        500,
        380,
        841,
        411,
        321,
        362,
        257,
        2020,
        295,
        597,
        472,
        486,
        589,
        293,
        550,
        300,
        311,
        562,
        50952
      ],
      "temperature": 0.0,
      "avg_logprob": -0.11318155045204974,
      "compression_ratio": 1.6798245614035088,
      "no_speech_prob": 0.00017799856141209602
    },
    {
      "id": 269,
      "seek": 180972,
      "start": 1821.48,
      "end": 1833.8,
      "text": " we should be doing research. Yeah thanks. Good question. Yeah I just had a question about",
      "tokens": [
        50952,
        321,
        820,
        312,
        884,
        2132,
        13,
        865,
        3231,
        13,
        2205,
        1168,
        13,
        865,
        286,
        445,
        632,
        257,
        1168,
        466,
        51568
      ],
      "temperature": 0.0,
      "avg_logprob": -0.11318155045204974,
      "compression_ratio": 1.6798245614035088,
      "no_speech_prob": 0.00017799856141209602
    },
    {
      "id": 270,
      "seek": 180972,
      "start": 1834.68,
      "end": 1839.0,
      "text": " how has the rest of your team been responding to performing this research and if there's been any",
      "tokens": [
        51612,
        577,
        575,
        264,
        1472,
        295,
        428,
        1469,
        668,
        16670,
        281,
        10205,
        341,
        2132,
        293,
        498,
        456,
        311,
        668,
        604,
        51828
      ],
      "temperature": 0.0,
      "avg_logprob": -0.11318155045204974,
      "compression_ratio": 1.6798245614035088,
      "no_speech_prob": 0.00017799856141209602
    },
    {
      "id": 271,
      "seek": 183900,
      "start": 1839.0,
      "end": 1844.52,
      "text": " learnings that you could share with us if we have to run into the same situation.",
      "tokens": [
        50364,
        2539,
        82,
        300,
        291,
        727,
        2073,
        365,
        505,
        498,
        321,
        362,
        281,
        1190,
        666,
        264,
        912,
        2590,
        13,
        50640
      ],
      "temperature": 0.0,
      "avg_logprob": -0.11408298317043261,
      "compression_ratio": 1.7302325581395348,
      "no_speech_prob": 0.0019028802635148168
    },
    {
      "id": 272,
      "seek": 183900,
      "start": 1846.44,
      "end": 1853.64,
      "text": " So first of all I think they're excited that they're validating this because if the input is coming",
      "tokens": [
        50736,
        407,
        700,
        295,
        439,
        286,
        519,
        436,
        434,
        2919,
        300,
        436,
        434,
        7363,
        990,
        341,
        570,
        498,
        264,
        4846,
        307,
        1348,
        51096
      ],
      "temperature": 0.0,
      "avg_logprob": -0.11408298317043261,
      "compression_ratio": 1.7302325581395348,
      "no_speech_prob": 0.0019028802635148168
    },
    {
      "id": 273,
      "seek": 183900,
      "start": 1853.64,
      "end": 1859.64,
      "text": " from the user then like we could really be more confident to say like we're placing this menu",
      "tokens": [
        51096,
        490,
        264,
        4195,
        550,
        411,
        321,
        727,
        534,
        312,
        544,
        6679,
        281,
        584,
        411,
        321,
        434,
        17221,
        341,
        6510,
        51396
      ],
      "temperature": 0.0,
      "avg_logprob": -0.11408298317043261,
      "compression_ratio": 1.7302325581395348,
      "no_speech_prob": 0.0019028802635148168
    },
    {
      "id": 274,
      "seek": 183900,
      "start": 1859.64,
      "end": 1867.56,
      "text": " at this place because of this outcome so I think they like that from that perspective but on the",
      "tokens": [
        51396,
        412,
        341,
        1081,
        570,
        295,
        341,
        9700,
        370,
        286,
        519,
        436,
        411,
        300,
        490,
        300,
        4585,
        457,
        322,
        264,
        51792
      ],
      "temperature": 0.0,
      "avg_logprob": -0.11408298317043261,
      "compression_ratio": 1.7302325581395348,
      "no_speech_prob": 0.0019028802635148168
    },
    {
      "id": 275,
      "seek": 186756,
      "start": 1867.56,
      "end": 1873.08,
      "text": " other hand like I think they are still having this sound and more behind the future flag and I",
      "tokens": [
        50364,
        661,
        1011,
        411,
        286,
        519,
        436,
        366,
        920,
        1419,
        341,
        1626,
        293,
        544,
        2261,
        264,
        2027,
        7166,
        293,
        286,
        50640
      ],
      "temperature": 0.0,
      "avg_logprob": -0.14077769288229286,
      "compression_ratio": 1.8295454545454546,
      "no_speech_prob": 0.006253651808947325
    },
    {
      "id": 276,
      "seek": 186756,
      "start": 1873.08,
      "end": 1880.04,
      "text": " am not sure they I don't think they haven't they have merged it so of course there are also some",
      "tokens": [
        50640,
        669,
        406,
        988,
        436,
        286,
        500,
        380,
        519,
        436,
        2378,
        380,
        436,
        362,
        36427,
        309,
        370,
        295,
        1164,
        456,
        366,
        611,
        512,
        50988
      ],
      "temperature": 0.0,
      "avg_logprob": -0.14077769288229286,
      "compression_ratio": 1.8295454545454546,
      "no_speech_prob": 0.006253651808947325
    },
    {
      "id": 277,
      "seek": 186756,
      "start": 1880.04,
      "end": 1886.36,
      "text": " frustration because they are ready to make this change happen but at this point they need to just",
      "tokens": [
        50988,
        20491,
        570,
        436,
        366,
        1919,
        281,
        652,
        341,
        1319,
        1051,
        457,
        412,
        341,
        935,
        436,
        643,
        281,
        445,
        51304
      ],
      "temperature": 0.0,
      "avg_logprob": -0.14077769288229286,
      "compression_ratio": 1.8295454545454546,
      "no_speech_prob": 0.006253651808947325
    },
    {
      "id": 278,
      "seek": 186756,
      "start": 1886.9199999999998,
      "end": 1891.72,
      "text": " wait for the results as that was the reason why the timeline and timeframe was a little bit tight",
      "tokens": [
        51332,
        1699,
        337,
        264,
        3542,
        382,
        300,
        390,
        264,
        1778,
        983,
        264,
        12933,
        293,
        34830,
        390,
        257,
        707,
        857,
        4524,
        51572
      ],
      "temperature": 0.0,
      "avg_logprob": -0.14077769288229286,
      "compression_ratio": 1.8295454545454546,
      "no_speech_prob": 0.006253651808947325
    },
    {
      "id": 279,
      "seek": 186756,
      "start": 1891.72,
      "end": 1896.76,
      "text": " because it's working progress so there are kind of like mixed feelings but in the end like they",
      "tokens": [
        51572,
        570,
        309,
        311,
        1364,
        4205,
        370,
        456,
        366,
        733,
        295,
        411,
        7467,
        6640,
        457,
        294,
        264,
        917,
        411,
        436,
        51824
      ],
      "temperature": 0.0,
      "avg_logprob": -0.14077769288229286,
      "compression_ratio": 1.8295454545454546,
      "no_speech_prob": 0.006253651808947325
    },
    {
      "id": 280,
      "seek": 189676,
      "start": 1896.76,
      "end": 1902.84,
      "text": " understood like of course like if we have more strong like evidence that's good and then I just",
      "tokens": [
        50364,
        7320,
        411,
        295,
        1164,
        411,
        498,
        321,
        362,
        544,
        2068,
        411,
        4467,
        300,
        311,
        665,
        293,
        550,
        286,
        445,
        50668
      ],
      "temperature": 0.0,
      "avg_logprob": -0.19005238063751706,
      "compression_ratio": 1.6023391812865497,
      "no_speech_prob": 0.0008392276940867305
    },
    {
      "id": 281,
      "seek": 189676,
      "start": 1902.84,
      "end": 1908.84,
      "text": " try to like illustrate okay what's happening in this version so that they know what's going on",
      "tokens": [
        50668,
        853,
        281,
        411,
        23221,
        1392,
        437,
        311,
        2737,
        294,
        341,
        3037,
        370,
        300,
        436,
        458,
        437,
        311,
        516,
        322,
        50968
      ],
      "temperature": 0.0,
      "avg_logprob": -0.19005238063751706,
      "compression_ratio": 1.6023391812865497,
      "no_speech_prob": 0.0008392276940867305
    },
    {
      "id": 282,
      "seek": 189676,
      "start": 1908.84,
      "end": 1914.76,
      "text": " for this research so that they could be more patient. Nice yeah thanks for sharing.",
      "tokens": [
        50968,
        337,
        341,
        2132,
        370,
        300,
        436,
        727,
        312,
        544,
        4537,
        13,
        5490,
        1338,
        3231,
        337,
        5414,
        13,
        51264
      ],
      "temperature": 0.0,
      "avg_logprob": -0.19005238063751706,
      "compression_ratio": 1.6023391812865497,
      "no_speech_prob": 0.0008392276940867305
    },
    {
      "id": 283,
      "seek": 191476,
      "start": 1915.16,
      "end": 1924.36,
      "text": " You're right. So if you don't have any other comments then I'll pass it over to Will.",
      "tokens": [
        50384,
        509,
        434,
        558,
        13,
        407,
        498,
        291,
        500,
        380,
        362,
        604,
        661,
        3053,
        550,
        286,
        603,
        1320,
        309,
        670,
        281,
        3099,
        13,
        50844
      ],
      "temperature": 0.0,
      "avg_logprob": -0.3331173694494999,
      "compression_ratio": 1.4,
      "no_speech_prob": 0.016705222427845
    },
    {
      "id": 284,
      "seek": 191476,
      "start": 1927.4,
      "end": 1935.4,
      "text": " Thanks, Angel. So just a couple quick updates. I was on a customer call Emily",
      "tokens": [
        50996,
        2561,
        11,
        14902,
        13,
        407,
        445,
        257,
        1916,
        1702,
        9205,
        13,
        286,
        390,
        322,
        257,
        5474,
        818,
        15034,
        51396
      ],
      "temperature": 0.0,
      "avg_logprob": -0.3331173694494999,
      "compression_ratio": 1.4,
      "no_speech_prob": 0.016705222427845
    },
    {
      "id": 285,
      "seek": 191476,
      "start": 1936.36,
      "end": 1943.48,
      "text": " led a couple hours ago to learn more about users and questions of the group level environments.",
      "tokens": [
        51444,
        4684,
        257,
        1916,
        2496,
        2057,
        281,
        1466,
        544,
        466,
        5022,
        293,
        1651,
        295,
        264,
        1594,
        1496,
        12388,
        13,
        51800
      ],
      "temperature": 0.0,
      "avg_logprob": -0.3331173694494999,
      "compression_ratio": 1.4,
      "no_speech_prob": 0.016705222427845
    },
    {
      "id": 286,
      "seek": 194348,
      "start": 1943.48,
      "end": 1950.84,
      "text": " View concept that's going to feed into her solution validation study and I thought you did a good",
      "tokens": [
        50364,
        13909,
        3410,
        300,
        311,
        516,
        281,
        3154,
        666,
        720,
        3827,
        24071,
        2979,
        293,
        286,
        1194,
        291,
        630,
        257,
        665,
        50732
      ],
      "temperature": 0.0,
      "avg_logprob": -0.1301988748403696,
      "compression_ratio": 1.60989010989011,
      "no_speech_prob": 0.0019198448862880468
    },
    {
      "id": 287,
      "seek": 194348,
      "start": 1950.84,
      "end": 1960.2,
      "text": " job handling you know multiple users or customers on the call and like trying to pivot to get to",
      "tokens": [
        50732,
        1691,
        13175,
        291,
        458,
        3866,
        5022,
        420,
        4581,
        322,
        264,
        818,
        293,
        411,
        1382,
        281,
        14538,
        281,
        483,
        281,
        51200
      ],
      "temperature": 0.0,
      "avg_logprob": -0.1301988748403696,
      "compression_ratio": 1.60989010989011,
      "no_speech_prob": 0.0019198448862880468
    },
    {
      "id": 288,
      "seek": 194348,
      "start": 1960.2,
      "end": 1965.4,
      "text": " the questions that you really wanted to know given some of the time constraints that we had on the",
      "tokens": [
        51200,
        264,
        1651,
        300,
        291,
        534,
        1415,
        281,
        458,
        2212,
        512,
        295,
        264,
        565,
        18491,
        300,
        321,
        632,
        322,
        264,
        51460
      ],
      "temperature": 0.0,
      "avg_logprob": -0.1301988748403696,
      "compression_ratio": 1.60989010989011,
      "no_speech_prob": 0.0019198448862880468
    },
    {
      "id": 289,
      "seek": 196540,
      "start": 1965.4,
      "end": 1974.68,
      "text": " call so a good job there. I'm also working with Ali and Pedro to brainstorm just be done for",
      "tokens": [
        50364,
        818,
        370,
        257,
        665,
        1691,
        456,
        13,
        286,
        478,
        611,
        1364,
        365,
        12020,
        293,
        26662,
        281,
        35245,
        445,
        312,
        1096,
        337,
        50828
      ],
      "temperature": 0.0,
      "avg_logprob": -0.18655778283942237,
      "compression_ratio": 1.6089385474860336,
      "no_speech_prob": 0.0008214809349738061
    },
    {
      "id": 290,
      "seek": 196540,
      "start": 1974.68,
      "end": 1983.88,
      "text": " switchboard so we've started that in an issue and now we're moving over to a Google doc to put",
      "tokens": [
        50828,
        3679,
        3787,
        370,
        321,
        600,
        1409,
        300,
        294,
        364,
        2734,
        293,
        586,
        321,
        434,
        2684,
        670,
        281,
        257,
        3329,
        3211,
        281,
        829,
        51288
      ],
      "temperature": 0.0,
      "avg_logprob": -0.18655778283942237,
      "compression_ratio": 1.6089385474860336,
      "no_speech_prob": 0.0008214809349738061
    },
    {
      "id": 291,
      "seek": 196540,
      "start": 1983.88,
      "end": 1994.0400000000002,
      "text": " that in there. I've also drafted an issue based on an initial team call that we had with switchboard",
      "tokens": [
        51288,
        300,
        294,
        456,
        13,
        286,
        600,
        611,
        36288,
        364,
        2734,
        2361,
        322,
        364,
        5883,
        1469,
        818,
        300,
        321,
        632,
        365,
        3679,
        3787,
        51796
      ],
      "temperature": 0.0,
      "avg_logprob": -0.18655778283942237,
      "compression_ratio": 1.6089385474860336,
      "no_speech_prob": 0.0008214809349738061
    },
    {
      "id": 292,
      "seek": 199404,
      "start": 1994.68,
      "end": 2001.48,
      "text": " that group. As I was writing it up after the meeting I was a little bit confused about the request",
      "tokens": [
        50396,
        300,
        1594,
        13,
        1018,
        286,
        390,
        3579,
        309,
        493,
        934,
        264,
        3440,
        286,
        390,
        257,
        707,
        857,
        9019,
        466,
        264,
        5308,
        50736
      ],
      "temperature": 0.0,
      "avg_logprob": -0.2774239139280457,
      "compression_ratio": 1.5434782608695652,
      "no_speech_prob": 0.0005815762560814619
    },
    {
      "id": 293,
      "seek": 199404,
      "start": 2003.56,
      "end": 2010.04,
      "text": " so I'm going to need some context on how this fits into the foundational research plan",
      "tokens": [
        50840,
        370,
        286,
        478,
        516,
        281,
        643,
        512,
        4319,
        322,
        577,
        341,
        9001,
        666,
        264,
        32195,
        2132,
        1393,
        51164
      ],
      "temperature": 0.0,
      "avg_logprob": -0.2774239139280457,
      "compression_ratio": 1.5434782608695652,
      "no_speech_prob": 0.0005815762560814619
    },
    {
      "id": 294,
      "seek": 199404,
      "start": 2011.8,
      "end": 2021.96,
      "text": " that Hiana has drafted so I've I'll use to I've tagged you and Hiana for some additional feedback.",
      "tokens": [
        51252,
        300,
        389,
        8497,
        575,
        36288,
        370,
        286,
        600,
        286,
        603,
        764,
        281,
        286,
        600,
        40239,
        291,
        293,
        389,
        8497,
        337,
        512,
        4497,
        5824,
        13,
        51760
      ],
      "temperature": 0.0,
      "avg_logprob": -0.2774239139280457,
      "compression_ratio": 1.5434782608695652,
      "no_speech_prob": 0.0005815762560814619
    },
    {
      "id": 295,
      "seek": 202404,
      "start": 2024.92,
      "end": 2030.84,
      "text": " Eric I see you're ready did you want to elaborate?",
      "tokens": [
        50408,
        9336,
        286,
        536,
        291,
        434,
        1919,
        630,
        291,
        528,
        281,
        20945,
        30,
        50704
      ],
      "temperature": 0.0,
      "avg_logprob": -0.2579375963944655,
      "compression_ratio": 1.3333333333333333,
      "no_speech_prob": 0.003393994178622961
    },
    {
      "id": 296,
      "seek": 202404,
      "start": 2033.8799999999999,
      "end": 2038.04,
      "text": " Well maybe see if the team wants to respond first.",
      "tokens": [
        50856,
        1042,
        1310,
        536,
        498,
        264,
        1469,
        2738,
        281,
        4196,
        700,
        13,
        51064
      ],
      "temperature": 0.0,
      "avg_logprob": -0.2579375963944655,
      "compression_ratio": 1.3333333333333333,
      "no_speech_prob": 0.003393994178622961
    },
    {
      "id": 297,
      "seek": 202404,
      "start": 2041.3999999999999,
      "end": 2041.8,
      "text": " Anyone?",
      "tokens": [
        51232,
        14643,
        30,
        51252
      ],
      "temperature": 0.0,
      "avg_logprob": -0.2579375963944655,
      "compression_ratio": 1.3333333333333333,
      "no_speech_prob": 0.003393994178622961
    },
    {
      "id": 298,
      "seek": 202404,
      "start": 2044.04,
      "end": 2049.48,
      "text": " So I think we just need to see how this fits into what we're doing now",
      "tokens": [
        51364,
        407,
        286,
        519,
        321,
        445,
        643,
        281,
        536,
        577,
        341,
        9001,
        666,
        437,
        321,
        434,
        884,
        586,
        51636
      ],
      "temperature": 0.0,
      "avg_logprob": -0.2579375963944655,
      "compression_ratio": 1.3333333333333333,
      "no_speech_prob": 0.003393994178622961
    },
    {
      "id": 299,
      "seek": 204948,
      "start": 2049.56,
      "end": 2063.8,
      "text": " and and make a call because yeah we're trying to find out what the customer wants but then also",
      "tokens": [
        50368,
        293,
        293,
        652,
        257,
        818,
        570,
        1338,
        321,
        434,
        1382,
        281,
        915,
        484,
        437,
        264,
        5474,
        2738,
        457,
        550,
        611,
        51080
      ],
      "temperature": 0.0,
      "avg_logprob": -0.24994388978872725,
      "compression_ratio": 1.6058823529411765,
      "no_speech_prob": 0.009923680685460567
    },
    {
      "id": 300,
      "seek": 204948,
      "start": 2064.44,
      "end": 2071.0,
      "text": " I mean our some of our users are also the general folks so I kind of feel like we're trying to",
      "tokens": [
        51112,
        286,
        914,
        527,
        512,
        295,
        527,
        5022,
        366,
        611,
        264,
        2674,
        4024,
        370,
        286,
        733,
        295,
        841,
        411,
        321,
        434,
        1382,
        281,
        51440
      ],
      "temperature": 0.0,
      "avg_logprob": -0.24994388978872725,
      "compression_ratio": 1.6058823529411765,
      "no_speech_prob": 0.009923680685460567
    },
    {
      "id": 301,
      "seek": 204948,
      "start": 2071.0,
      "end": 2078.36,
      "text": " do the same thing and then maybe my not necessarily to have two issues to do that.",
      "tokens": [
        51440,
        360,
        264,
        912,
        551,
        293,
        550,
        1310,
        452,
        406,
        4725,
        281,
        362,
        732,
        2663,
        281,
        360,
        300,
        13,
        51808
      ],
      "temperature": 0.0,
      "avg_logprob": -0.24994388978872725,
      "compression_ratio": 1.6058823529411765,
      "no_speech_prob": 0.009923680685460567
    },
    {
      "id": 302,
      "seek": 207948,
      "start": 2079.8,
      "end": 2085.64,
      "text": " But that's just my feeling maybe we can get some feedback from Hiana as well.",
      "tokens": [
        50380,
        583,
        300,
        311,
        445,
        452,
        2633,
        1310,
        321,
        393,
        483,
        512,
        5824,
        490,
        389,
        8497,
        382,
        731,
        13,
        50672
      ],
      "temperature": 0.0,
      "avg_logprob": -0.16151506392682186,
      "compression_ratio": 1.4698795180722892,
      "no_speech_prob": 0.0010637228842824697
    },
    {
      "id": 303,
      "seek": 207948,
      "start": 2086.84,
      "end": 2091.32,
      "text": " Yeah and that's why I want to like clarify that before like jumping in because if",
      "tokens": [
        50732,
        865,
        293,
        300,
        311,
        983,
        286,
        528,
        281,
        411,
        17594,
        300,
        949,
        411,
        11233,
        294,
        570,
        498,
        50956
      ],
      "temperature": 0.0,
      "avg_logprob": -0.16151506392682186,
      "compression_ratio": 1.4698795180722892,
      "no_speech_prob": 0.0010637228842824697
    },
    {
      "id": 304,
      "seek": 207948,
      "start": 2092.28,
      "end": 2099.32,
      "text": " if this is like duplicate or redundant like let's not you know do that in that case.",
      "tokens": [
        51004,
        498,
        341,
        307,
        411,
        23976,
        420,
        40997,
        411,
        718,
        311,
        406,
        291,
        458,
        360,
        300,
        294,
        300,
        1389,
        13,
        51356
      ],
      "temperature": 0.0,
      "avg_logprob": -0.16151506392682186,
      "compression_ratio": 1.4698795180722892,
      "no_speech_prob": 0.0010637228842824697
    },
    {
      "id": 305,
      "seek": 210948,
      "start": 2110.2,
      "end": 2117.56,
      "text": " Um so my first note is just that they're I wanted you to know that there's this",
      "tokens": [
        50400,
        3301,
        370,
        452,
        700,
        3637,
        307,
        445,
        300,
        436,
        434,
        286,
        1415,
        291,
        281,
        458,
        300,
        456,
        311,
        341,
        50768
      ],
      "temperature": 0.0,
      "avg_logprob": -0.11421165218600979,
      "compression_ratio": 1.638095238095238,
      "no_speech_prob": 0.0010672303615137935
    },
    {
      "id": 306,
      "seek": 210948,
      "start": 2118.12,
      "end": 2124.36,
      "text": " foundational research happening like in the back burner but we've run some participants",
      "tokens": [
        50796,
        32195,
        2132,
        2737,
        411,
        294,
        264,
        646,
        36116,
        457,
        321,
        600,
        1190,
        512,
        10503,
        51108
      ],
      "temperature": 0.0,
      "avg_logprob": -0.11421165218600979,
      "compression_ratio": 1.638095238095238,
      "no_speech_prob": 0.0010672303615137935
    },
    {
      "id": 307,
      "seek": 210948,
      "start": 2125.2400000000002,
      "end": 2132.76,
      "text": " on the life cycle of an image and understanding how those are environments and how that relates",
      "tokens": [
        51152,
        322,
        264,
        993,
        6586,
        295,
        364,
        3256,
        293,
        3701,
        577,
        729,
        366,
        12388,
        293,
        577,
        300,
        16155,
        51528
      ],
      "temperature": 0.0,
      "avg_logprob": -0.11421165218600979,
      "compression_ratio": 1.638095238095238,
      "no_speech_prob": 0.0010672303615137935
    },
    {
      "id": 308,
      "seek": 210948,
      "start": 2132.76,
      "end": 2138.28,
      "text": " and kind of whether or not people want one pipeline for them or how they kind of",
      "tokens": [
        51528,
        293,
        733,
        295,
        1968,
        420,
        406,
        561,
        528,
        472,
        15517,
        337,
        552,
        420,
        577,
        436,
        733,
        295,
        51804
      ],
      "temperature": 0.0,
      "avg_logprob": -0.11421165218600979,
      "compression_ratio": 1.638095238095238,
      "no_speech_prob": 0.0010672303615137935
    },
    {
      "id": 309,
      "seek": 213828,
      "start": 2139.2400000000002,
      "end": 2147.4,
      "text": " structure that so that's happening and I can just catching up on life but I will put",
      "tokens": [
        50412,
        3877,
        300,
        370,
        300,
        311,
        2737,
        293,
        286,
        393,
        445,
        16124,
        493,
        322,
        993,
        457,
        286,
        486,
        829,
        50820
      ],
      "temperature": 0.0,
      "avg_logprob": -0.1504205185690044,
      "compression_ratio": 1.6201923076923077,
      "no_speech_prob": 0.00016276538372039795
    },
    {
      "id": 310,
      "seek": 213828,
      "start": 2148.6800000000003,
      "end": 2152.76,
      "text": " I will link to the dovetail and it might be helpful for you to watch some of those videos.",
      "tokens": [
        50884,
        286,
        486,
        2113,
        281,
        264,
        360,
        9771,
        864,
        293,
        309,
        1062,
        312,
        4961,
        337,
        291,
        281,
        1159,
        512,
        295,
        729,
        2145,
        13,
        51088
      ],
      "temperature": 0.0,
      "avg_logprob": -0.1504205185690044,
      "compression_ratio": 1.6201923076923077,
      "no_speech_prob": 0.00016276538372039795
    },
    {
      "id": 311,
      "seek": 213828,
      "start": 2153.88,
      "end": 2159.2400000000002,
      "text": " That's my first point but then looking at the foundational research like I think we have a bunch",
      "tokens": [
        51144,
        663,
        311,
        452,
        700,
        935,
        457,
        550,
        1237,
        412,
        264,
        32195,
        2132,
        411,
        286,
        519,
        321,
        362,
        257,
        3840,
        51412
      ],
      "temperature": 0.0,
      "avg_logprob": -0.1504205185690044,
      "compression_ratio": 1.6201923076923077,
      "no_speech_prob": 0.00016276538372039795
    },
    {
      "id": 312,
      "seek": 213828,
      "start": 2159.2400000000002,
      "end": 2163.2400000000002,
      "text": " of this just from the the secrets and security related research.",
      "tokens": [
        51412,
        295,
        341,
        445,
        490,
        264,
        264,
        14093,
        293,
        3825,
        4077,
        2132,
        13,
        51612
      ],
      "temperature": 0.0,
      "avg_logprob": -0.1504205185690044,
      "compression_ratio": 1.6201923076923077,
      "no_speech_prob": 0.00016276538372039795
    },
    {
      "id": 313,
      "seek": 216324,
      "start": 2164.12,
      "end": 2175.9599999999996,
      "text": " Um and I think I'll look I'll take a note to look through all of that but one thing I can just",
      "tokens": [
        50408,
        3301,
        293,
        286,
        519,
        286,
        603,
        574,
        286,
        603,
        747,
        257,
        3637,
        281,
        574,
        807,
        439,
        295,
        300,
        457,
        472,
        551,
        286,
        393,
        445,
        51000
      ],
      "temperature": 0.0,
      "avg_logprob": -0.14955342156546456,
      "compression_ratio": 1.7,
      "no_speech_prob": 0.0007048473344184458
    },
    {
      "id": 314,
      "seek": 216324,
      "start": 2175.9599999999996,
      "end": 2183.3199999999997,
      "text": " quickly make note of is with the the personas we know that there's this like shift left with",
      "tokens": [
        51000,
        2661,
        652,
        3637,
        295,
        307,
        365,
        264,
        264,
        12019,
        321,
        458,
        300,
        456,
        311,
        341,
        411,
        5513,
        1411,
        365,
        51368
      ],
      "temperature": 0.0,
      "avg_logprob": -0.14955342156546456,
      "compression_ratio": 1.7,
      "no_speech_prob": 0.0007048473344184458
    },
    {
      "id": 315,
      "seek": 216324,
      "start": 2183.3199999999997,
      "end": 2191.3999999999996,
      "text": " security so even developers are that was like a big thing that we keep finding is even developers are",
      "tokens": [
        51368,
        3825,
        370,
        754,
        8849,
        366,
        300,
        390,
        411,
        257,
        955,
        551,
        300,
        321,
        1066,
        5006,
        307,
        754,
        8849,
        366,
        51772
      ],
      "temperature": 0.0,
      "avg_logprob": -0.14955342156546456,
      "compression_ratio": 1.7,
      "no_speech_prob": 0.0007048473344184458
    },
    {
      "id": 316,
      "seek": 219140,
      "start": 2192.04,
      "end": 2200.76,
      "text": " getting their hands on security and compliance stuff um and we did in the secret features prioritization",
      "tokens": [
        50396,
        1242,
        641,
        2377,
        322,
        3825,
        293,
        15882,
        1507,
        1105,
        293,
        321,
        630,
        294,
        264,
        4054,
        4122,
        14846,
        2144,
        50832
      ],
      "temperature": 0.0,
      "avg_logprob": -0.13207325378021637,
      "compression_ratio": 1.6602870813397128,
      "no_speech_prob": 0.00020895458874292672
    },
    {
      "id": 317,
      "seek": 219140,
      "start": 2200.76,
      "end": 2206.84,
      "text": " survey we asked I'll bring up the finding but we asked them if they knew what compliance",
      "tokens": [
        50832,
        8984,
        321,
        2351,
        286,
        603,
        1565,
        493,
        264,
        5006,
        457,
        321,
        2351,
        552,
        498,
        436,
        2586,
        437,
        15882,
        51136
      ],
      "temperature": 0.0,
      "avg_logprob": -0.13207325378021637,
      "compression_ratio": 1.6602870813397128,
      "no_speech_prob": 0.00020895458874292672
    },
    {
      "id": 318,
      "seek": 219140,
      "start": 2206.84,
      "end": 2214.04,
      "text": " requirements they were working under um and surprisingly they had a sense of it",
      "tokens": [
        51136,
        7728,
        436,
        645,
        1364,
        833,
        1105,
        293,
        17600,
        436,
        632,
        257,
        2020,
        295,
        309,
        51496
      ],
      "temperature": 0.0,
      "avg_logprob": -0.13207325378021637,
      "compression_ratio": 1.6602870813397128,
      "no_speech_prob": 0.00020895458874292672
    },
    {
      "id": 319,
      "seek": 219140,
      "start": 2214.76,
      "end": 2220.52,
      "text": " even the developers but let me pull up that work um but I can dig in here",
      "tokens": [
        51532,
        754,
        264,
        8849,
        457,
        718,
        385,
        2235,
        493,
        300,
        589,
        1105,
        457,
        286,
        393,
        2528,
        294,
        510,
        51820
      ],
      "temperature": 0.0,
      "avg_logprob": -0.13207325378021637,
      "compression_ratio": 1.6602870813397128,
      "no_speech_prob": 0.00020895458874292672
    },
    {
      "id": 320,
      "seek": 222052,
      "start": 2221.16,
      "end": 2227.72,
      "text": " and help you piece together what we will we know okay because it might feel like this is coming from",
      "tokens": [
        50396,
        293,
        854,
        291,
        2522,
        1214,
        437,
        321,
        486,
        321,
        458,
        1392,
        570,
        309,
        1062,
        841,
        411,
        341,
        307,
        1348,
        490,
        50724
      ],
      "temperature": 0.0,
      "avg_logprob": -0.15082982529041378,
      "compression_ratio": 1.6318181818181818,
      "no_speech_prob": 0.0004914223682135344
    },
    {
      "id": 321,
      "seek": 222052,
      "start": 2227.72,
      "end": 2233.16,
      "text": " nowhere but it's actually I didn't really even realize this was a nice summary",
      "tokens": [
        50724,
        11159,
        457,
        309,
        311,
        767,
        286,
        994,
        380,
        534,
        754,
        4325,
        341,
        390,
        257,
        1481,
        12691,
        50996
      ],
      "temperature": 0.0,
      "avg_logprob": -0.15082982529041378,
      "compression_ratio": 1.6318181818181818,
      "no_speech_prob": 0.0004914223682135344
    },
    {
      "id": 322,
      "seek": 222052,
      "start": 2236.04,
      "end": 2241.96,
      "text": " people to be like oh we know some stuff about this so good job in framing the issue there everyone",
      "tokens": [
        51140,
        561,
        281,
        312,
        411,
        1954,
        321,
        458,
        512,
        1507,
        466,
        341,
        370,
        665,
        1691,
        294,
        28971,
        264,
        2734,
        456,
        1518,
        51436
      ],
      "temperature": 0.0,
      "avg_logprob": -0.15082982529041378,
      "compression_ratio": 1.6318181818181818,
      "no_speech_prob": 0.0004914223682135344
    },
    {
      "id": 323,
      "seek": 222052,
      "start": 2242.7599999999998,
      "end": 2248.2,
      "text": " um but I can be helpful so let's figure out the best way for me to help you guys",
      "tokens": [
        51476,
        1105,
        457,
        286,
        393,
        312,
        4961,
        370,
        718,
        311,
        2573,
        484,
        264,
        1151,
        636,
        337,
        385,
        281,
        854,
        291,
        1074,
        51748
      ],
      "temperature": 0.0,
      "avg_logprob": -0.15082982529041378,
      "compression_ratio": 1.6318181818181818,
      "no_speech_prob": 0.0004914223682135344
    },
    {
      "id": 324,
      "seek": 224820,
      "start": 2249.16,
      "end": 2259.8799999999997,
      "text": " okay okay well um and then the last point um I'm working with Erica she may speak to it a little",
      "tokens": [
        50412,
        1392,
        1392,
        731,
        1105,
        293,
        550,
        264,
        1036,
        935,
        1105,
        286,
        478,
        1364,
        365,
        37429,
        750,
        815,
        1710,
        281,
        309,
        257,
        707,
        50948
      ],
      "temperature": 0.0,
      "avg_logprob": -0.18342327585025708,
      "compression_ratio": 1.4338235294117647,
      "no_speech_prob": 0.0014722717460244894
    },
    {
      "id": 325,
      "seek": 224820,
      "start": 2259.8799999999997,
      "end": 2271.3999999999996,
      "text": " bit below but um I'm helping her pull some like customer emails later in the week um from our past",
      "tokens": [
        50948,
        857,
        2507,
        457,
        1105,
        286,
        478,
        4315,
        720,
        2235,
        512,
        411,
        5474,
        12524,
        1780,
        294,
        264,
        1243,
        1105,
        490,
        527,
        1791,
        51524
      ],
      "temperature": 0.0,
      "avg_logprob": -0.18342327585025708,
      "compression_ratio": 1.4338235294117647,
      "no_speech_prob": 0.0014722717460244894
    },
    {
      "id": 326,
      "seek": 227140,
      "start": 2271.4,
      "end": 2278.12,
      "text": " us analysis so that I think she and her team can do some like future like follow calls",
      "tokens": [
        50364,
        505,
        5215,
        370,
        300,
        286,
        519,
        750,
        293,
        720,
        1469,
        393,
        360,
        512,
        411,
        2027,
        411,
        1524,
        5498,
        50700
      ],
      "temperature": 0.0,
      "avg_logprob": -0.18972816107408055,
      "compression_ratio": 1.50354609929078,
      "no_speech_prob": 0.0006196793983690441
    },
    {
      "id": 327,
      "seek": 227140,
      "start": 2282.12,
      "end": 2283.2400000000002,
      "text": " so I think that's it for me",
      "tokens": [
        50900,
        370,
        286,
        519,
        300,
        311,
        309,
        337,
        385,
        50956
      ],
      "temperature": 0.0,
      "avg_logprob": -0.18972816107408055,
      "compression_ratio": 1.50354609929078,
      "no_speech_prob": 0.0006196793983690441
    },
    {
      "id": 328,
      "seek": 227140,
      "start": 2288.2000000000003,
      "end": 2294.2000000000003,
      "text": " yeah and since we have time I can actually just share my screen and show you the benchmark report",
      "tokens": [
        51204,
        1338,
        293,
        1670,
        321,
        362,
        565,
        286,
        393,
        767,
        445,
        2073,
        452,
        2568,
        293,
        855,
        291,
        264,
        18927,
        2275,
        51504
      ],
      "temperature": 0.0,
      "avg_logprob": -0.18972816107408055,
      "compression_ratio": 1.50354609929078,
      "no_speech_prob": 0.0006196793983690441
    },
    {
      "id": 329,
      "seek": 229420,
      "start": 2294.3599999999997,
      "end": 2308.2799999999997,
      "text": " yeah okay so this is a one side slide summary of the performance and I set up a benchmark findings",
      "tokens": [
        50372,
        1338,
        1392,
        370,
        341,
        307,
        257,
        472,
        1252,
        4137,
        12691,
        295,
        264,
        3389,
        293,
        286,
        992,
        493,
        257,
        18927,
        16483,
        51068
      ],
      "temperature": 0.0,
      "avg_logprob": -0.13377774556477864,
      "compression_ratio": 1.4566929133858268,
      "no_speech_prob": 0.0006324345013126731
    },
    {
      "id": 330,
      "seek": 229420,
      "start": 2308.2799999999997,
      "end": 2315.7999999999997,
      "text": " epic that has like videos of overall like so this is all detailed and in a way like we",
      "tokens": [
        51068,
        13581,
        300,
        575,
        411,
        2145,
        295,
        4787,
        411,
        370,
        341,
        307,
        439,
        9942,
        293,
        294,
        257,
        636,
        411,
        321,
        51444
      ],
      "temperature": 0.0,
      "avg_logprob": -0.13377774556477864,
      "compression_ratio": 1.4566929133858268,
      "no_speech_prob": 0.0006324345013126731
    },
    {
      "id": 331,
      "seek": 231580,
      "start": 2315.8,
      "end": 2325.8,
      "text": " issueatize this report um but basically there um we're in good we're doing good",
      "tokens": [
        50364,
        2734,
        267,
        1125,
        341,
        2275,
        1105,
        457,
        1936,
        456,
        1105,
        321,
        434,
        294,
        665,
        321,
        434,
        884,
        665,
        50864
      ],
      "temperature": 0.0,
      "avg_logprob": -0.13224972784519196,
      "compression_ratio": 1.639751552795031,
      "no_speech_prob": 0.00012537339353002608
    },
    {
      "id": 332,
      "seek": 231580,
      "start": 2326.92,
      "end": 2336.44,
      "text": " but with each of our workflows that we tested back to me mouse there's like one sort of really",
      "tokens": [
        50920,
        457,
        365,
        1184,
        295,
        527,
        43461,
        300,
        321,
        8246,
        646,
        281,
        385,
        9719,
        456,
        311,
        411,
        472,
        1333,
        295,
        534,
        51396
      ],
      "temperature": 0.0,
      "avg_logprob": -0.13224972784519196,
      "compression_ratio": 1.639751552795031,
      "no_speech_prob": 0.00012537339353002608
    },
    {
      "id": 333,
      "seek": 231580,
      "start": 2336.44,
      "end": 2342.84,
      "text": " painful task in each of the workflows that we need to address so it's usually includes in",
      "tokens": [
        51396,
        11697,
        5633,
        294,
        1184,
        295,
        264,
        43461,
        300,
        321,
        643,
        281,
        2985,
        370,
        309,
        311,
        2673,
        5974,
        294,
        51716
      ],
      "temperature": 0.0,
      "avg_logprob": -0.13224972784519196,
      "compression_ratio": 1.639751552795031,
      "no_speech_prob": 0.00012537339353002608
    },
    {
      "id": 334,
      "seek": 234284,
      "start": 2342.84,
      "end": 2348.84,
      "text": " tax um identifying the failure the reason for a failure and understanding the desk unit tests",
      "tokens": [
        50364,
        3366,
        1105,
        16696,
        264,
        7763,
        264,
        1778,
        337,
        257,
        7763,
        293,
        3701,
        264,
        10026,
        4985,
        6921,
        50664
      ],
      "temperature": 0.0,
      "avg_logprob": -0.21629313171887007,
      "compression_ratio": 1.7142857142857142,
      "no_speech_prob": 0.0003197437908966094
    },
    {
      "id": 335,
      "seek": 234284,
      "start": 2348.84,
      "end": 2358.76,
      "text": " and then the first of the fine and fixed pipeline error workflows um and I so this",
      "tokens": [
        50664,
        293,
        550,
        264,
        700,
        295,
        264,
        2489,
        293,
        6806,
        15517,
        6713,
        43461,
        1105,
        293,
        286,
        370,
        341,
        51160
      ],
      "temperature": 0.0,
      "avg_logprob": -0.21629313171887007,
      "compression_ratio": 1.7142857142857142,
      "no_speech_prob": 0.0003197437908966094
    },
    {
      "id": 336,
      "seek": 234284,
      "start": 2359.56,
      "end": 2367.6400000000003,
      "text": " table here kind of gives you a sense for each of those workflows this column gives you two to three",
      "tokens": [
        51200,
        3199,
        510,
        733,
        295,
        2709,
        291,
        257,
        2020,
        337,
        1184,
        295,
        729,
        43461,
        341,
        7738,
        2709,
        291,
        732,
        281,
        1045,
        51604
      ],
      "temperature": 0.0,
      "avg_logprob": -0.21629313171887007,
      "compression_ratio": 1.7142857142857142,
      "no_speech_prob": 0.0003197437908966094
    },
    {
      "id": 337,
      "seek": 236764,
      "start": 2367.96,
      "end": 2375.0,
      "text": " two to three word summary of each of them so for the author of pipeline workflow we found a barrier",
      "tokens": [
        50380,
        732,
        281,
        1045,
        1349,
        12691,
        295,
        1184,
        295,
        552,
        370,
        337,
        264,
        3793,
        295,
        15517,
        20993,
        321,
        1352,
        257,
        13357,
        50732
      ],
      "temperature": 0.0,
      "avg_logprob": -0.14103147718641493,
      "compression_ratio": 1.532258064516129,
      "no_speech_prob": 9.074669651454315e-05
    },
    {
      "id": 338,
      "seek": 236764,
      "start": 2375.0,
      "end": 2380.7599999999998,
      "text": " to entry with the includes syntax which is important because that's the entry point for our",
      "tokens": [
        50732,
        281,
        8729,
        365,
        264,
        5974,
        28431,
        597,
        307,
        1021,
        570,
        300,
        311,
        264,
        8729,
        935,
        337,
        527,
        51020
      ],
      "temperature": 0.0,
      "avg_logprob": -0.14103147718641493,
      "compression_ratio": 1.532258064516129,
      "no_speech_prob": 9.074669651454315e-05
    },
    {
      "id": 339,
      "seek": 236764,
      "start": 2381.3199999999997,
      "end": 2391.8799999999997,
      "text": " CI components catalog um and as a companion analysis I looked at all the suspectum for verify",
      "tokens": [
        51048,
        37777,
        6677,
        19746,
        1105,
        293,
        382,
        257,
        22363,
        5215,
        286,
        2956,
        412,
        439,
        264,
        9091,
        449,
        337,
        16888,
        51576
      ],
      "temperature": 0.0,
      "avg_logprob": -0.14103147718641493,
      "compression_ratio": 1.532258064516129,
      "no_speech_prob": 9.074669651454315e-05
    },
    {
      "id": 340,
      "seek": 239188,
      "start": 2391.88,
      "end": 2401.0,
      "text": " for the last six quarters and did an alignment exercise to see how much the pain points for each",
      "tokens": [
        50364,
        337,
        264,
        1036,
        2309,
        20612,
        293,
        630,
        364,
        18515,
        5380,
        281,
        536,
        577,
        709,
        264,
        1822,
        2793,
        337,
        1184,
        50820
      ],
      "temperature": 0.0,
      "avg_logprob": -0.11934686429572827,
      "compression_ratio": 1.5303867403314917,
      "no_speech_prob": 0.0001717353443382308
    },
    {
      "id": 341,
      "seek": 239188,
      "start": 2401.0,
      "end": 2407.56,
      "text": " of the tasks and then each of those UX themes in that table below mapped to the suspectum",
      "tokens": [
        50820,
        295,
        264,
        9608,
        293,
        550,
        1184,
        295,
        729,
        40176,
        13544,
        294,
        300,
        3199,
        2507,
        33318,
        281,
        264,
        9091,
        449,
        51148
      ],
      "temperature": 0.0,
      "avg_logprob": -0.11934686429572827,
      "compression_ratio": 1.5303867403314917,
      "no_speech_prob": 0.0001717353443382308
    },
    {
      "id": 342,
      "seek": 239188,
      "start": 2408.6,
      "end": 2416.6,
      "text": " and and it was kind of astounding so 13% which we're calling a significant amount of those",
      "tokens": [
        51200,
        293,
        293,
        309,
        390,
        733,
        295,
        5357,
        24625,
        370,
        3705,
        4,
        597,
        321,
        434,
        5141,
        257,
        4776,
        2372,
        295,
        729,
        51600
      ],
      "temperature": 0.0,
      "avg_logprob": -0.11934686429572827,
      "compression_ratio": 1.5303867403314917,
      "no_speech_prob": 0.0001717353443382308
    },
    {
      "id": 343,
      "seek": 241660,
      "start": 2417.24,
      "end": 2425.7999999999997,
      "text": " negative suspectum were related to this author of pipeline workflow um and then looking down here",
      "tokens": [
        50396,
        3671,
        9091,
        449,
        645,
        4077,
        281,
        341,
        3793,
        295,
        15517,
        20993,
        1105,
        293,
        550,
        1237,
        760,
        510,
        50824
      ],
      "temperature": 0.0,
      "avg_logprob": -0.12187799574836852,
      "compression_ratio": 1.569767441860465,
      "no_speech_prob": 8.973827061709017e-05
    },
    {
      "id": 344,
      "seek": 241660,
      "start": 2425.7999999999997,
      "end": 2431.4,
      "text": " at these UX themes one of the themes across all the tasks was just general confusion about YAML",
      "tokens": [
        50824,
        412,
        613,
        40176,
        13544,
        472,
        295,
        264,
        13544,
        2108,
        439,
        264,
        9608,
        390,
        445,
        2674,
        15075,
        466,
        398,
        2865,
        43,
        51104
      ],
      "temperature": 0.0,
      "avg_logprob": -0.12187799574836852,
      "compression_ratio": 1.569767441860465,
      "no_speech_prob": 8.973827061709017e-05
    },
    {
      "id": 345,
      "seek": 241660,
      "start": 2432.04,
      "end": 2439.64,
      "text": " and that was 8% of the negative suspectum so we really want to focus here on",
      "tokens": [
        51136,
        293,
        300,
        390,
        1649,
        4,
        295,
        264,
        3671,
        9091,
        449,
        370,
        321,
        534,
        528,
        281,
        1879,
        510,
        322,
        51516
      ],
      "temperature": 0.0,
      "avg_logprob": -0.12187799574836852,
      "compression_ratio": 1.569767441860465,
      "no_speech_prob": 8.973827061709017e-05
    },
    {
      "id": 346,
      "seek": 243964,
      "start": 2439.7999999999997,
      "end": 2448.2799999999997,
      "text": " and making that whole YAML related workflow easier and then it occurred to me that we now actually",
      "tokens": [
        50372,
        293,
        1455,
        300,
        1379,
        398,
        2865,
        43,
        4077,
        20993,
        3571,
        293,
        550,
        309,
        11068,
        281,
        385,
        300,
        321,
        586,
        767,
        50796
      ],
      "temperature": 0.0,
      "avg_logprob": -0.06771827006078028,
      "compression_ratio": 1.6781115879828326,
      "no_speech_prob": 0.000953317154198885
    },
    {
      "id": 347,
      "seek": 243964,
      "start": 2448.2799999999997,
      "end": 2454.92,
      "text": " have all of these people who gave us these responses about the YAML experience so we can actually",
      "tokens": [
        50796,
        362,
        439,
        295,
        613,
        561,
        567,
        2729,
        505,
        613,
        13019,
        466,
        264,
        398,
        2865,
        43,
        1752,
        370,
        321,
        393,
        767,
        51128
      ],
      "temperature": 0.0,
      "avg_logprob": -0.06771827006078028,
      "compression_ratio": 1.6781115879828326,
      "no_speech_prob": 0.000953317154198885
    },
    {
      "id": 348,
      "seek": 243964,
      "start": 2454.92,
      "end": 2460.3599999999997,
      "text": " that's great because they're hard to recruit right it's hard for us to find good matches so here we",
      "tokens": [
        51128,
        300,
        311,
        869,
        570,
        436,
        434,
        1152,
        281,
        15119,
        558,
        309,
        311,
        1152,
        337,
        505,
        281,
        915,
        665,
        10676,
        370,
        510,
        321,
        51400
      ],
      "temperature": 0.0,
      "avg_logprob": -0.06771827006078028,
      "compression_ratio": 1.6781115879828326,
      "no_speech_prob": 0.000953317154198885
    },
    {
      "id": 349,
      "seek": 243964,
      "start": 2460.3599999999997,
      "end": 2467.8799999999997,
      "text": " have this lovely sample of 43 participants who gave us feedback on the YAML experience there's",
      "tokens": [
        51400,
        362,
        341,
        7496,
        6889,
        295,
        17914,
        10503,
        567,
        2729,
        505,
        5824,
        322,
        264,
        398,
        2865,
        43,
        1752,
        456,
        311,
        51776
      ],
      "temperature": 0.0,
      "avg_logprob": -0.06771827006078028,
      "compression_ratio": 1.6781115879828326,
      "no_speech_prob": 0.000953317154198885
    },
    {
      "id": 350,
      "seek": 246788,
      "start": 2467.88,
      "end": 2473.56,
      "text": " like all these different categories where they made points but basically it bubbles up to having",
      "tokens": [
        50364,
        411,
        439,
        613,
        819,
        10479,
        689,
        436,
        1027,
        2793,
        457,
        1936,
        309,
        16295,
        493,
        281,
        1419,
        50648
      ],
      "temperature": 0.0,
      "avg_logprob": -0.13035982305353339,
      "compression_ratio": 1.6224066390041494,
      "no_speech_prob": 0.00018141423061024398
    },
    {
      "id": 351,
      "seek": 246788,
      "start": 2473.56,
      "end": 2479.96,
      "text": " a hard time with the YAML experience so Will is do thank you Will is going to give us the email",
      "tokens": [
        50648,
        257,
        1152,
        565,
        365,
        264,
        398,
        2865,
        43,
        1752,
        370,
        3099,
        307,
        360,
        1309,
        291,
        3099,
        307,
        516,
        281,
        976,
        505,
        264,
        3796,
        50968
      ],
      "temperature": 0.0,
      "avg_logprob": -0.13035982305353339,
      "compression_ratio": 1.6224066390041494,
      "no_speech_prob": 0.00018141423061024398
    },
    {
      "id": 352,
      "seek": 246788,
      "start": 2479.96,
      "end": 2486.12,
      "text": " contact for those and what we can do because we can't really have although people would probably try",
      "tokens": [
        50968,
        3385,
        337,
        729,
        293,
        437,
        321,
        393,
        360,
        570,
        321,
        393,
        380,
        534,
        362,
        4878,
        561,
        576,
        1391,
        853,
        51276
      ],
      "temperature": 0.0,
      "avg_logprob": -0.13035982305353339,
      "compression_ratio": 1.6224066390041494,
      "no_speech_prob": 0.00018141423061024398
    },
    {
      "id": 353,
      "seek": 246788,
      "start": 2486.12,
      "end": 2492.92,
      "text": " we can't really schedule 43 calls but what we can do is run them through that CI alpha components",
      "tokens": [
        51276,
        321,
        393,
        380,
        534,
        7567,
        17914,
        5498,
        457,
        437,
        321,
        393,
        360,
        307,
        1190,
        552,
        807,
        300,
        37777,
        8961,
        6677,
        51616
      ],
      "temperature": 0.0,
      "avg_logprob": -0.13035982305353339,
      "compression_ratio": 1.6224066390041494,
      "no_speech_prob": 0.00018141423061024398
    },
    {
      "id": 354,
      "seek": 249292,
      "start": 2492.92,
      "end": 2499.56,
      "text": " program and those assignments and then we'll get a nice read on the satisfaction scores",
      "tokens": [
        50364,
        1461,
        293,
        729,
        22546,
        293,
        550,
        321,
        603,
        483,
        257,
        1481,
        1401,
        322,
        264,
        18715,
        13444,
        50696
      ],
      "temperature": 0.0,
      "avg_logprob": -0.06072713093585279,
      "compression_ratio": 1.6581196581196582,
      "no_speech_prob": 0.0006904125330038369
    },
    {
      "id": 355,
      "seek": 249292,
      "start": 2499.56,
      "end": 2505.64,
      "text": " and how those are increasing right now we have like a dedicated really well-rounded sample where we can",
      "tokens": [
        50696,
        293,
        577,
        729,
        366,
        5662,
        558,
        586,
        321,
        362,
        411,
        257,
        8374,
        534,
        731,
        12,
        50167,
        6889,
        689,
        321,
        393,
        51000
      ],
      "temperature": 0.0,
      "avg_logprob": -0.06072713093585279,
      "compression_ratio": 1.6581196581196582,
      "no_speech_prob": 0.0006904125330038369
    },
    {
      "id": 356,
      "seek": 249292,
      "start": 2505.64,
      "end": 2513.2400000000002,
      "text": " do deep dive interviews but actually they're pretty satisfied with the current template experience",
      "tokens": [
        51000,
        360,
        2452,
        9192,
        12318,
        457,
        767,
        436,
        434,
        1238,
        11239,
        365,
        264,
        2190,
        12379,
        1752,
        51380
      ],
      "temperature": 0.0,
      "avg_logprob": -0.06072713093585279,
      "compression_ratio": 1.6581196581196582,
      "no_speech_prob": 0.0006904125330038369
    },
    {
      "id": 357,
      "seek": 249292,
      "start": 2513.2400000000002,
      "end": 2518.6,
      "text": " when we ask them so it means that it's harder for us to raise that bar in terms of their feedback",
      "tokens": [
        51380,
        562,
        321,
        1029,
        552,
        370,
        309,
        1355,
        300,
        309,
        311,
        6081,
        337,
        505,
        281,
        5300,
        300,
        2159,
        294,
        2115,
        295,
        641,
        5824,
        51648
      ],
      "temperature": 0.0,
      "avg_logprob": -0.06072713093585279,
      "compression_ratio": 1.6581196581196582,
      "no_speech_prob": 0.0006904125330038369
    },
    {
      "id": 358,
      "seek": 251860,
      "start": 2519.56,
      "end": 2524.7599999999998,
      "text": " and getting them into a really satisfied place so these folks who gave us the",
      "tokens": [
        50412,
        293,
        1242,
        552,
        666,
        257,
        534,
        11239,
        1081,
        370,
        613,
        4024,
        567,
        2729,
        505,
        264,
        50672
      ],
      "temperature": 0.0,
      "avg_logprob": -0.10861589038182819,
      "compression_ratio": 1.6196319018404908,
      "no_speech_prob": 0.0004280857101548463
    },
    {
      "id": 359,
      "seek": 251860,
      "start": 2524.7599999999998,
      "end": 2534.2799999999997,
      "text": " substrate back and were negative we should get a good read on if we can move them up to positive",
      "tokens": [
        50672,
        27585,
        646,
        293,
        645,
        3671,
        321,
        820,
        483,
        257,
        665,
        1401,
        322,
        498,
        321,
        393,
        1286,
        552,
        493,
        281,
        3353,
        51148
      ],
      "temperature": 0.0,
      "avg_logprob": -0.10861589038182819,
      "compression_ratio": 1.6196319018404908,
      "no_speech_prob": 0.0004280857101548463
    },
    {
      "id": 360,
      "seek": 251860,
      "start": 2534.8399999999997,
      "end": 2541.96,
      "text": " then we're really doing a good job so that was what Will was talking about and that's the",
      "tokens": [
        51176,
        550,
        321,
        434,
        534,
        884,
        257,
        665,
        1691,
        370,
        300,
        390,
        437,
        3099,
        390,
        1417,
        466,
        293,
        300,
        311,
        264,
        51532
      ],
      "temperature": 0.0,
      "avg_logprob": -0.10861589038182819,
      "compression_ratio": 1.6196319018404908,
      "no_speech_prob": 0.0004280857101548463
    },
    {
      "id": 361,
      "seek": 254196,
      "start": 2541.96,
      "end": 2549.64,
      "text": " author of pipeline workflow and then understanding a just unit test two words for that is his hidden",
      "tokens": [
        50364,
        3793,
        295,
        15517,
        20993,
        293,
        550,
        3701,
        257,
        445,
        4985,
        1500,
        732,
        2283,
        337,
        300,
        307,
        702,
        7633,
        50748
      ],
      "temperature": 0.0,
      "avg_logprob": -0.11431005631370106,
      "compression_ratio": 1.6936936936936937,
      "no_speech_prob": 0.000246596202487126
    },
    {
      "id": 362,
      "seek": 254196,
      "start": 2549.64,
      "end": 2556.36,
      "text": " treasure so it was amazing I just so one of the things we did at the end was we gave them this job",
      "tokens": [
        50748,
        12985,
        370,
        309,
        390,
        2243,
        286,
        445,
        370,
        472,
        295,
        264,
        721,
        321,
        630,
        412,
        264,
        917,
        390,
        321,
        2729,
        552,
        341,
        1691,
        51084
      ],
      "temperature": 0.0,
      "avg_logprob": -0.11431005631370106,
      "compression_ratio": 1.6936936936936937,
      "no_speech_prob": 0.000246596202487126
    },
    {
      "id": 363,
      "seek": 254196,
      "start": 2556.36,
      "end": 2564.52,
      "text": " to be done rating question where we had them agree or disagree on a scale of 1 to 7 where",
      "tokens": [
        51084,
        281,
        312,
        1096,
        10990,
        1168,
        689,
        321,
        632,
        552,
        3986,
        420,
        14091,
        322,
        257,
        4373,
        295,
        502,
        281,
        1614,
        689,
        51492
      ],
      "temperature": 0.0,
      "avg_logprob": -0.11431005631370106,
      "compression_ratio": 1.6936936936936937,
      "no_speech_prob": 0.000246596202487126
    },
    {
      "id": 364,
      "seek": 254196,
      "start": 2565.8,
      "end": 2571.16,
      "text": " anyway so we had them look at their agreement and they gave us really high scores here",
      "tokens": [
        51556,
        4033,
        370,
        321,
        632,
        552,
        574,
        412,
        641,
        8106,
        293,
        436,
        2729,
        505,
        534,
        1090,
        13444,
        510,
        51824
      ],
      "temperature": 0.0,
      "avg_logprob": -0.11431005631370106,
      "compression_ratio": 1.6936936936936937,
      "no_speech_prob": 0.000246596202487126
    },
    {
      "id": 365,
      "seek": 257196,
      "start": 2571.96,
      "end": 2578.52,
      "text": " and they were just they were delighted they were delighted once they understood they didn't understand",
      "tokens": [
        50364,
        293,
        436,
        645,
        445,
        436,
        645,
        18783,
        436,
        645,
        18783,
        1564,
        436,
        7320,
        436,
        994,
        380,
        1223,
        50692
      ],
      "temperature": 0.0,
      "avg_logprob": -0.11612031039069681,
      "compression_ratio": 1.9787234042553192,
      "no_speech_prob": 0.00017898943042382598
    },
    {
      "id": 366,
      "seek": 257196,
      "start": 2578.52,
      "end": 2584.12,
      "text": " what was quite happening with the unit test at first so they it wasn't discoverable for them",
      "tokens": [
        50692,
        437,
        390,
        1596,
        2737,
        365,
        264,
        4985,
        1500,
        412,
        700,
        370,
        436,
        309,
        2067,
        380,
        4411,
        712,
        337,
        552,
        50972
      ],
      "temperature": 0.0,
      "avg_logprob": -0.11612031039069681,
      "compression_ratio": 1.9787234042553192,
      "no_speech_prob": 0.00017898943042382598
    },
    {
      "id": 367,
      "seek": 257196,
      "start": 2584.12,
      "end": 2588.6,
      "text": " because they went into the logs and they weren't looking at the UI and they did this like",
      "tokens": [
        50972,
        570,
        436,
        1437,
        666,
        264,
        20820,
        293,
        436,
        4999,
        380,
        1237,
        412,
        264,
        15682,
        293,
        436,
        630,
        341,
        411,
        51196
      ],
      "temperature": 0.0,
      "avg_logprob": -0.11612031039069681,
      "compression_ratio": 1.9787234042553192,
      "no_speech_prob": 0.00017898943042382598
    },
    {
      "id": 368,
      "seek": 257196,
      "start": 2588.6,
      "end": 2592.84,
      "text": " circular thing where they were like oh I've revived this artifact don't know what that means",
      "tokens": [
        51196,
        16476,
        551,
        689,
        436,
        645,
        411,
        1954,
        286,
        600,
        48358,
        341,
        34806,
        500,
        380,
        458,
        437,
        300,
        1355,
        51408
      ],
      "temperature": 0.0,
      "avg_logprob": -0.11612031039069681,
      "compression_ratio": 1.9787234042553192,
      "no_speech_prob": 0.00017898943042382598
    },
    {
      "id": 369,
      "seek": 257196,
      "start": 2592.84,
      "end": 2596.6,
      "text": " and then they would like arrive at the artifact in a different way and a different way",
      "tokens": [
        51408,
        293,
        550,
        436,
        576,
        411,
        8881,
        412,
        264,
        34806,
        294,
        257,
        819,
        636,
        293,
        257,
        819,
        636,
        51596
      ],
      "temperature": 0.0,
      "avg_logprob": -0.11612031039069681,
      "compression_ratio": 1.9787234042553192,
      "no_speech_prob": 0.00017898943042382598
    },
    {
      "id": 370,
      "seek": 259660,
      "start": 2597.24,
      "end": 2602.12,
      "text": " and and then like when they then we kind of like pointed out hey the artifact is going to be",
      "tokens": [
        50396,
        293,
        293,
        550,
        411,
        562,
        436,
        550,
        321,
        733,
        295,
        411,
        10932,
        484,
        4177,
        264,
        34806,
        307,
        516,
        281,
        312,
        50640
      ],
      "temperature": 0.0,
      "avg_logprob": -0.08314615556563454,
      "compression_ratio": 1.8952879581151831,
      "no_speech_prob": 0.0007548726862296462
    },
    {
      "id": 371,
      "seek": 259660,
      "start": 2602.12,
      "end": 2607.88,
      "text": " helpful for this task and then they were like what is this artifact how does it relate to this error",
      "tokens": [
        50640,
        4961,
        337,
        341,
        5633,
        293,
        550,
        436,
        645,
        411,
        437,
        307,
        341,
        34806,
        577,
        775,
        309,
        10961,
        281,
        341,
        6713,
        50928
      ],
      "temperature": 0.0,
      "avg_logprob": -0.08314615556563454,
      "compression_ratio": 1.8952879581151831,
      "no_speech_prob": 0.0007548726862296462
    },
    {
      "id": 372,
      "seek": 259660,
      "start": 2607.88,
      "end": 2613.64,
      "text": " but once we did the reveal they were like thanking us at the end of the session",
      "tokens": [
        50928,
        457,
        1564,
        321,
        630,
        264,
        10658,
        436,
        645,
        411,
        30830,
        505,
        412,
        264,
        917,
        295,
        264,
        5481,
        51216
      ],
      "temperature": 0.0,
      "avg_logprob": -0.08314615556563454,
      "compression_ratio": 1.8952879581151831,
      "no_speech_prob": 0.0007548726862296462
    },
    {
      "id": 373,
      "seek": 259660,
      "start": 2614.44,
      "end": 2620.52,
      "text": " like they were like and so this rating here this 6.1 is very high because they were just",
      "tokens": [
        51256,
        411,
        436,
        645,
        411,
        293,
        370,
        341,
        10990,
        510,
        341,
        1386,
        13,
        16,
        307,
        588,
        1090,
        570,
        436,
        645,
        445,
        51560
      ],
      "temperature": 0.0,
      "avg_logprob": -0.08314615556563454,
      "compression_ratio": 1.8952879581151831,
      "no_speech_prob": 0.0007548726862296462
    },
    {
      "id": 374,
      "seek": 262052,
      "start": 2620.84,
      "end": 2629.88,
      "text": " so pleased about it so I think that there is some small I mean I'm not the designer but if we can",
      "tokens": [
        50380,
        370,
        10587,
        466,
        309,
        370,
        286,
        519,
        300,
        456,
        307,
        512,
        1359,
        286,
        914,
        286,
        478,
        406,
        264,
        11795,
        457,
        498,
        321,
        393,
        50832
      ],
      "temperature": 0.0,
      "avg_logprob": -0.09291174100792926,
      "compression_ratio": 1.7442922374429224,
      "no_speech_prob": 0.0008582405280321836
    },
    {
      "id": 375,
      "seek": 262052,
      "start": 2629.88,
      "end": 2636.52,
      "text": " make it more discoverable and a little bit more obvious to them how those things are related",
      "tokens": [
        50832,
        652,
        309,
        544,
        4411,
        712,
        293,
        257,
        707,
        857,
        544,
        6322,
        281,
        552,
        577,
        729,
        721,
        366,
        4077,
        51164
      ],
      "temperature": 0.0,
      "avg_logprob": -0.09291174100792926,
      "compression_ratio": 1.7442922374429224,
      "no_speech_prob": 0.0008582405280321836
    },
    {
      "id": 376,
      "seek": 262052,
      "start": 2636.52,
      "end": 2641.88,
      "text": " like an example is so in this in the unit test it showed them a screenshot of what the website",
      "tokens": [
        51164,
        411,
        364,
        1365,
        307,
        370,
        294,
        341,
        294,
        264,
        4985,
        1500,
        309,
        4712,
        552,
        257,
        27712,
        295,
        437,
        264,
        3144,
        51432
      ],
      "temperature": 0.0,
      "avg_logprob": -0.09291174100792926,
      "compression_ratio": 1.7442922374429224,
      "no_speech_prob": 0.0008582405280321836
    },
    {
      "id": 377,
      "seek": 262052,
      "start": 2643.08,
      "end": 2648.84,
      "text": " would look like if it displayed as per the code and they weren't sure if it was what the website",
      "tokens": [
        51492,
        576,
        574,
        411,
        498,
        309,
        16372,
        382,
        680,
        264,
        3089,
        293,
        436,
        4999,
        380,
        988,
        498,
        309,
        390,
        437,
        264,
        3144,
        51780
      ],
      "temperature": 0.0,
      "avg_logprob": -0.09291174100792926,
      "compression_ratio": 1.7442922374429224,
      "no_speech_prob": 0.0008582405280321836
    },
    {
      "id": 378,
      "seek": 264884,
      "start": 2648.84,
      "end": 2653.32,
      "text": " should look like or it does look like so we just need to like connect those dots for them",
      "tokens": [
        50364,
        820,
        574,
        411,
        420,
        309,
        775,
        574,
        411,
        370,
        321,
        445,
        643,
        281,
        411,
        1745,
        729,
        15026,
        337,
        552,
        50588
      ],
      "temperature": 0.0,
      "avg_logprob": -0.14088734984397888,
      "compression_ratio": 1.583815028901734,
      "no_speech_prob": 0.0005070965853519738
    },
    {
      "id": 379,
      "seek": 264884,
      "start": 2653.96,
      "end": 2663.1600000000003,
      "text": " and then they will make them so happy and here we saw only 3% alignment with the sus for beta",
      "tokens": [
        50620,
        293,
        550,
        436,
        486,
        652,
        552,
        370,
        2055,
        293,
        510,
        321,
        1866,
        787,
        805,
        4,
        18515,
        365,
        264,
        3291,
        337,
        9861,
        51080
      ],
      "temperature": 0.0,
      "avg_logprob": -0.14088734984397888,
      "compression_ratio": 1.583815028901734,
      "no_speech_prob": 0.0005070965853519738
    },
    {
      "id": 380,
      "seek": 264884,
      "start": 2663.7200000000003,
      "end": 2669.8,
      "text": " but that actually like tracks with this hidden treasure idea right where they're not quite",
      "tokens": [
        51108,
        457,
        300,
        767,
        411,
        10218,
        365,
        341,
        7633,
        12985,
        1558,
        558,
        689,
        436,
        434,
        406,
        1596,
        51412
      ],
      "temperature": 0.0,
      "avg_logprob": -0.14088734984397888,
      "compression_ratio": 1.583815028901734,
      "no_speech_prob": 0.0005070965853519738
    },
    {
      "id": 381,
      "seek": 266980,
      "start": 2670.1200000000003,
      "end": 2679.1600000000003,
      "text": " understandable that's not discoverable for them and then for the fine and fixed pipeline errors",
      "tokens": [
        50380,
        25648,
        300,
        311,
        406,
        4411,
        712,
        337,
        552,
        293,
        550,
        337,
        264,
        2489,
        293,
        6806,
        15517,
        13603,
        50832
      ],
      "temperature": 0.0,
      "avg_logprob": -0.2017270748431866,
      "compression_ratio": 1.7272727272727273,
      "no_speech_prob": 0.0012422908330336213
    },
    {
      "id": 382,
      "seek": 266980,
      "start": 2680.2000000000003,
      "end": 2688.04,
      "text": " 9% they're waiting too long another way to tell the story there though is that they learned",
      "tokens": [
        50884,
        1722,
        4,
        436,
        434,
        3806,
        886,
        938,
        1071,
        636,
        281,
        980,
        264,
        1657,
        456,
        1673,
        307,
        300,
        436,
        3264,
        51276
      ],
      "temperature": 0.0,
      "avg_logprob": -0.2017270748431866,
      "compression_ratio": 1.7272727272727273,
      "no_speech_prob": 0.0012422908330336213
    },
    {
      "id": 383,
      "seek": 266980,
      "start": 2689.32,
      "end": 2695.96,
      "text": " which is really cool so if you look at this so we had fine and fixed pipeline errors in build and",
      "tokens": [
        51340,
        597,
        307,
        534,
        1627,
        370,
        498,
        291,
        574,
        412,
        341,
        370,
        321,
        632,
        2489,
        293,
        6806,
        15517,
        13603,
        294,
        1322,
        293,
        51672
      ],
      "temperature": 0.0,
      "avg_logprob": -0.2017270748431866,
      "compression_ratio": 1.7272727272727273,
      "no_speech_prob": 0.0012422908330336213
    },
    {
      "id": 384,
      "seek": 269596,
      "start": 2695.96,
      "end": 2702.36,
      "text": " then one in test and one in deploy and so you can see here that they kind of bombed that first one",
      "tokens": [
        50364,
        550,
        472,
        294,
        1500,
        293,
        472,
        294,
        7274,
        293,
        370,
        291,
        393,
        536,
        510,
        300,
        436,
        733,
        295,
        7957,
        2883,
        300,
        700,
        472,
        50684
      ],
      "temperature": 0.0,
      "avg_logprob": -0.07650724210237202,
      "compression_ratio": 1.7232142857142858,
      "no_speech_prob": 0.00013545677938964218
    },
    {
      "id": 385,
      "seek": 269596,
      "start": 2703.2400000000002,
      "end": 2709.88,
      "text": " but then on the second on the next one they were in green again and basically what it was is",
      "tokens": [
        50728,
        457,
        550,
        322,
        264,
        1150,
        322,
        264,
        958,
        472,
        436,
        645,
        294,
        3092,
        797,
        293,
        1936,
        437,
        309,
        390,
        307,
        51060
      ],
      "temperature": 0.0,
      "avg_logprob": -0.07650724210237202,
      "compression_ratio": 1.7232142857142858,
      "no_speech_prob": 0.00013545677938964218
    },
    {
      "id": 386,
      "seek": 269596,
      "start": 2710.52,
      "end": 2718.52,
      "text": " if they could step back and look at the patterns in the YAML file they could use that to quickly",
      "tokens": [
        51092,
        498,
        436,
        727,
        1823,
        646,
        293,
        574,
        412,
        264,
        8294,
        294,
        264,
        398,
        2865,
        43,
        3991,
        436,
        727,
        764,
        300,
        281,
        2661,
        51492
      ],
      "temperature": 0.0,
      "avg_logprob": -0.07650724210237202,
      "compression_ratio": 1.7232142857142858,
      "no_speech_prob": 0.00013545677938964218
    },
    {
      "id": 387,
      "seek": 269596,
      "start": 2718.52,
      "end": 2725.48,
      "text": " come up with the solution as opposed to like digging into the logs so if we can figure out how to",
      "tokens": [
        51492,
        808,
        493,
        365,
        264,
        3827,
        382,
        8851,
        281,
        411,
        17343,
        666,
        264,
        20820,
        370,
        498,
        321,
        393,
        2573,
        484,
        577,
        281,
        51840
      ],
      "temperature": 0.0,
      "avg_logprob": -0.07650724210237202,
      "compression_ratio": 1.7232142857142858,
      "no_speech_prob": 0.00013545677938964218
    },
    {
      "id": 388,
      "seek": 272596,
      "start": 2726.28,
      "end": 2733.4,
      "text": " help them take that step back maybe it's AI I don't know but to like look at the discrepancies",
      "tokens": [
        50380,
        854,
        552,
        747,
        300,
        1823,
        646,
        1310,
        309,
        311,
        7318,
        286,
        500,
        380,
        458,
        457,
        281,
        411,
        574,
        412,
        264,
        2983,
        19919,
        32286,
        50736
      ],
      "temperature": 0.0,
      "avg_logprob": -0.0722830463463152,
      "compression_ratio": 1.5027322404371584,
      "no_speech_prob": 0.00018286009435541928
    },
    {
      "id": 389,
      "seek": 272596,
      "start": 2733.4,
      "end": 2739.08,
      "text": " in the different stages of the YAML file we could really help them succeed there",
      "tokens": [
        50736,
        294,
        264,
        819,
        10232,
        295,
        264,
        398,
        2865,
        43,
        3991,
        321,
        727,
        534,
        854,
        552,
        7754,
        456,
        51020
      ],
      "temperature": 0.0,
      "avg_logprob": -0.0722830463463152,
      "compression_ratio": 1.5027322404371584,
      "no_speech_prob": 0.00018286009435541928
    },
    {
      "id": 390,
      "seek": 272596,
      "start": 2740.76,
      "end": 2749.64,
      "text": " but here you can see that this jobs to be done rating is not as high and that's because they dinged",
      "tokens": [
        51104,
        457,
        510,
        291,
        393,
        536,
        300,
        341,
        4782,
        281,
        312,
        1096,
        10990,
        307,
        406,
        382,
        1090,
        293,
        300,
        311,
        570,
        436,
        21211,
        292,
        51548
      ],
      "temperature": 0.0,
      "avg_logprob": -0.0722830463463152,
      "compression_ratio": 1.5027322404371584,
      "no_speech_prob": 0.00018286009435541928
    },
    {
      "id": 391,
      "seek": 274964,
      "start": 2749.64,
      "end": 2756.7599999999998,
      "text": " us because they get really frustrated waiting for the entire pipeline to run after every fix",
      "tokens": [
        50364,
        505,
        570,
        436,
        483,
        534,
        15751,
        3806,
        337,
        264,
        2302,
        15517,
        281,
        1190,
        934,
        633,
        3191,
        50720
      ],
      "temperature": 0.0,
      "avg_logprob": -0.1474089395432245,
      "compression_ratio": 1.6590909090909092,
      "no_speech_prob": 0.000263844383880496
    },
    {
      "id": 392,
      "seek": 274964,
      "start": 2757.56,
      "end": 2765.24,
      "text": " so that's a theme that came out here and it's actually that specific theme is small in terms of",
      "tokens": [
        50760,
        370,
        300,
        311,
        257,
        6314,
        300,
        1361,
        484,
        510,
        293,
        309,
        311,
        767,
        300,
        2685,
        6314,
        307,
        1359,
        294,
        2115,
        295,
        51144
      ],
      "temperature": 0.0,
      "avg_logprob": -0.1474089395432245,
      "compression_ratio": 1.6590909090909092,
      "no_speech_prob": 0.000263844383880496
    },
    {
      "id": 393,
      "seek": 274964,
      "start": 2765.24,
      "end": 2773.72,
      "text": " the percentage of suspectum I think that's because we logically have a smaller percentage of enterprise",
      "tokens": [
        51144,
        264,
        9668,
        295,
        9091,
        449,
        286,
        519,
        300,
        311,
        570,
        321,
        38887,
        362,
        257,
        4356,
        9668,
        295,
        14132,
        51568
      ],
      "temperature": 0.0,
      "avg_logprob": -0.1474089395432245,
      "compression_ratio": 1.6590909090909092,
      "no_speech_prob": 0.000263844383880496
    },
    {
      "id": 394,
      "seek": 277372,
      "start": 2773.72,
      "end": 2779.56,
      "text": " respondents and that's a particular enterprise problem that we were getting feedback on so another",
      "tokens": [
        50364,
        48275,
        293,
        300,
        311,
        257,
        1729,
        14132,
        1154,
        300,
        321,
        645,
        1242,
        5824,
        322,
        370,
        1071,
        50656
      ],
      "temperature": 0.0,
      "avg_logprob": -0.162240372147671,
      "compression_ratio": 1.6092436974789917,
      "no_speech_prob": 0.00018152990378439426
    },
    {
      "id": 395,
      "seek": 277372,
      "start": 2779.56,
      "end": 2785.3999999999996,
      "text": " thing that Will and I yeah we'll know like our working on is to track business size in the",
      "tokens": [
        50656,
        551,
        300,
        3099,
        293,
        286,
        1338,
        321,
        603,
        458,
        411,
        527,
        1364,
        322,
        307,
        281,
        2837,
        1606,
        2744,
        294,
        264,
        50948
      ],
      "temperature": 0.0,
      "avg_logprob": -0.162240372147671,
      "compression_ratio": 1.6092436974789917,
      "no_speech_prob": 0.00018152990378439426
    },
    {
      "id": 396,
      "seek": 277372,
      "start": 2785.3999999999996,
      "end": 2793.7999999999997,
      "text": " SOTS responses now and so I think that will be helpful but yeah so this frustration with running",
      "tokens": [
        50948,
        318,
        5068,
        50,
        13019,
        586,
        293,
        370,
        286,
        519,
        300,
        486,
        312,
        4961,
        457,
        1338,
        370,
        341,
        20491,
        365,
        2614,
        51368
      ],
      "temperature": 0.0,
      "avg_logprob": -0.162240372147671,
      "compression_ratio": 1.6092436974789917,
      "no_speech_prob": 0.00018152990378439426
    },
    {
      "id": 397,
      "seek": 277372,
      "start": 2793.7999999999997,
      "end": 2800.52,
      "text": " the entire pipeline for every fix I've heard that before but it was really resounding and we see",
      "tokens": [
        51368,
        264,
        2302,
        15517,
        337,
        633,
        3191,
        286,
        600,
        2198,
        300,
        949,
        457,
        309,
        390,
        534,
        725,
        24625,
        293,
        321,
        536,
        51704
      ],
      "temperature": 0.0,
      "avg_logprob": -0.162240372147671,
      "compression_ratio": 1.6092436974789917,
      "no_speech_prob": 0.00018152990378439426
    },
    {
      "id": 398,
      "seek": 280052,
      "start": 2800.6,
      "end": 2806.36,
      "text": " them bringing down our our satisfaction scores or agreement with the job to be done",
      "tokens": [
        50368,
        552,
        5062,
        760,
        527,
        527,
        18715,
        13444,
        420,
        8106,
        365,
        264,
        1691,
        281,
        312,
        1096,
        50656
      ],
      "temperature": 0.0,
      "avg_logprob": -0.10607135842699524,
      "compression_ratio": 1.880952380952381,
      "no_speech_prob": 0.00042277673492208123
    },
    {
      "id": 399,
      "seek": 280052,
      "start": 2807.0,
      "end": 2813.48,
      "text": " ability and that's because the user behavior in terms of fixing a pipeline is just to like throw",
      "tokens": [
        50688,
        3485,
        293,
        300,
        311,
        570,
        264,
        4195,
        5223,
        294,
        2115,
        295,
        19442,
        257,
        15517,
        307,
        445,
        281,
        411,
        3507,
        51012
      ],
      "temperature": 0.0,
      "avg_logprob": -0.10607135842699524,
      "compression_ratio": 1.880952380952381,
      "no_speech_prob": 0.00042277673492208123
    },
    {
      "id": 400,
      "seek": 280052,
      "start": 2813.48,
      "end": 2818.44,
      "text": " a fix at it and see if that worked and throw a fix at it and see if that worked they're not like",
      "tokens": [
        51012,
        257,
        3191,
        412,
        309,
        293,
        536,
        498,
        300,
        2732,
        293,
        3507,
        257,
        3191,
        412,
        309,
        293,
        536,
        498,
        300,
        2732,
        436,
        434,
        406,
        411,
        51260
      ],
      "temperature": 0.0,
      "avg_logprob": -0.10607135842699524,
      "compression_ratio": 1.880952380952381,
      "no_speech_prob": 0.00042277673492208123
    },
    {
      "id": 401,
      "seek": 280052,
      "start": 2818.44,
      "end": 2824.2,
      "text": " now with this work you'll have to check on the documentation like they're not doing that kind of",
      "tokens": [
        51260,
        586,
        365,
        341,
        589,
        291,
        603,
        362,
        281,
        1520,
        322,
        264,
        14333,
        411,
        436,
        434,
        406,
        884,
        300,
        733,
        295,
        51548
      ],
      "temperature": 0.0,
      "avg_logprob": -0.10607135842699524,
      "compression_ratio": 1.880952380952381,
      "no_speech_prob": 0.00042277673492208123
    },
    {
      "id": 402,
      "seek": 280052,
      "start": 2824.2,
      "end": 2829.16,
      "text": " thinking and so then it becomes really frustrating they're like I just wanted to try that thing and",
      "tokens": [
        51548,
        1953,
        293,
        370,
        550,
        309,
        3643,
        534,
        16522,
        436,
        434,
        411,
        286,
        445,
        1415,
        281,
        853,
        300,
        551,
        293,
        51796
      ],
      "temperature": 0.0,
      "avg_logprob": -0.10607135842699524,
      "compression_ratio": 1.880952380952381,
      "no_speech_prob": 0.00042277673492208123
    },
    {
      "id": 403,
      "seek": 282916,
      "start": 2829.16,
      "end": 2837.16,
      "text": " now I have to sit here and wait for however many minutes um and I asked so I did my like do",
      "tokens": [
        50364,
        586,
        286,
        362,
        281,
        1394,
        510,
        293,
        1699,
        337,
        4461,
        867,
        2077,
        1105,
        293,
        286,
        2351,
        370,
        286,
        630,
        452,
        411,
        360,
        50764
      ],
      "temperature": 0.0,
      "avg_logprob": -0.11615884173047411,
      "compression_ratio": 1.7183098591549295,
      "no_speech_prob": 0.00011278251622570679
    },
    {
      "id": 404,
      "seek": 282916,
      "start": 2837.16,
      "end": 2842.92,
      "text": " diligence follow up like tell me about your tell me about your workflow and and like is this really",
      "tokens": [
        50764,
        40046,
        1524,
        493,
        411,
        980,
        385,
        466,
        428,
        980,
        385,
        466,
        428,
        20993,
        293,
        293,
        411,
        307,
        341,
        534,
        51052
      ],
      "temperature": 0.0,
      "avg_logprob": -0.11615884173047411,
      "compression_ratio": 1.7183098591549295,
      "no_speech_prob": 0.00011278251622570679
    },
    {
      "id": 405,
      "seek": 282916,
      "start": 2842.92,
      "end": 2850.12,
      "text": " a big thing and um they're like yes yes yes this gets me out of my flow gets me out of context",
      "tokens": [
        51052,
        257,
        955,
        551,
        293,
        1105,
        436,
        434,
        411,
        2086,
        2086,
        2086,
        341,
        2170,
        385,
        484,
        295,
        452,
        3095,
        2170,
        385,
        484,
        295,
        4319,
        51412
      ],
      "temperature": 0.0,
      "avg_logprob": -0.11615884173047411,
      "compression_ratio": 1.7183098591549295,
      "no_speech_prob": 0.00011278251622570679
    },
    {
      "id": 406,
      "seek": 282916,
      "start": 2850.68,
      "end": 2856.52,
      "text": " I'll go watch a YouTube video and then I'm totally distracted and not useful um",
      "tokens": [
        51440,
        286,
        603,
        352,
        1159,
        257,
        3088,
        960,
        293,
        550,
        286,
        478,
        3879,
        21658,
        293,
        406,
        4420,
        1105,
        51732
      ],
      "temperature": 0.0,
      "avg_logprob": -0.11615884173047411,
      "compression_ratio": 1.7183098591549295,
      "no_speech_prob": 0.00011278251622570679
    },
    {
      "id": 407,
      "seek": 285652,
      "start": 2857.4,
      "end": 2866.52,
      "text": " yes so that's the benchmark stuff in a quick summary um and then I",
      "tokens": [
        50408,
        2086,
        370,
        300,
        311,
        264,
        18927,
        1507,
        294,
        257,
        1702,
        12691,
        1105,
        293,
        550,
        286,
        50864
      ],
      "temperature": 0.0,
      "avg_logprob": -0.23296122457466872,
      "compression_ratio": 1.4246575342465753,
      "no_speech_prob": 0.00026276896824128926
    },
    {
      "id": 408,
      "seek": 285652,
      "start": 2866.52,
      "end": 2872.7599999999998,
      "text": " wow let's let's go to comments and I'll stop sharing",
      "tokens": [
        50864,
        6076,
        718,
        311,
        718,
        311,
        352,
        281,
        3053,
        293,
        286,
        603,
        1590,
        5414,
        51176
      ],
      "temperature": 0.0,
      "avg_logprob": -0.23296122457466872,
      "compression_ratio": 1.4246575342465753,
      "no_speech_prob": 0.00026276896824128926
    },
    {
      "id": 409,
      "seek": 285652,
      "start": 2874.36,
      "end": 2881.64,
      "text": " I had a question about how you determined if the suspect back percentage was significant",
      "tokens": [
        51256,
        286,
        632,
        257,
        1168,
        466,
        577,
        291,
        9540,
        498,
        264,
        9091,
        646,
        9668,
        390,
        4776,
        51620
      ],
      "temperature": 0.0,
      "avg_logprob": -0.23296122457466872,
      "compression_ratio": 1.4246575342465753,
      "no_speech_prob": 0.00026276896824128926
    },
    {
      "id": 410,
      "seek": 288164,
      "start": 2881.64,
      "end": 2891.16,
      "text": " considerable or small yep uh so um if there's documentation also that's already out there I",
      "tokens": [
        50364,
        24167,
        420,
        1359,
        18633,
        2232,
        370,
        1105,
        498,
        456,
        311,
        14333,
        611,
        300,
        311,
        1217,
        484,
        456,
        286,
        50840
      ],
      "temperature": 0.0,
      "avg_logprob": -0.1320292569588924,
      "compression_ratio": 1.6607142857142858,
      "no_speech_prob": 8.32872829050757e-05
    },
    {
      "id": 411,
      "seek": 288164,
      "start": 2891.16,
      "end": 2897.3199999999997,
      "text": " can just look at that no it's totally no I love that question so um we I put that in the report",
      "tokens": [
        50840,
        393,
        445,
        574,
        412,
        300,
        572,
        309,
        311,
        3879,
        572,
        286,
        959,
        300,
        1168,
        370,
        1105,
        321,
        286,
        829,
        300,
        294,
        264,
        2275,
        51148
      ],
      "temperature": 0.0,
      "avg_logprob": -0.1320292569588924,
      "compression_ratio": 1.6607142857142858,
      "no_speech_prob": 8.32872829050757e-05
    },
    {
      "id": 412,
      "seek": 288164,
      "start": 2897.3199999999997,
      "end": 2906.2,
      "text": " and it's just mapping percentages okay so and it's just the because later on if it just has",
      "tokens": [
        51148,
        293,
        309,
        311,
        445,
        18350,
        42270,
        1392,
        370,
        293,
        309,
        311,
        445,
        264,
        570,
        1780,
        322,
        498,
        309,
        445,
        575,
        51592
      ],
      "temperature": 0.0,
      "avg_logprob": -0.1320292569588924,
      "compression_ratio": 1.6607142857142858,
      "no_speech_prob": 8.32872829050757e-05
    },
    {
      "id": 413,
      "seek": 290620,
      "start": 2906.2,
      "end": 2911.16,
      "text": " three percent or not like I put the percentages in that report right like nine percent",
      "tokens": [
        50364,
        1045,
        3043,
        420,
        406,
        411,
        286,
        829,
        264,
        42270,
        294,
        300,
        2275,
        558,
        411,
        4949,
        3043,
        50612
      ],
      "temperature": 0.0,
      "avg_logprob": -0.17348080589657738,
      "compression_ratio": 1.7121951219512195,
      "no_speech_prob": 0.00038516975473612547
    },
    {
      "id": 414,
      "seek": 290620,
      "start": 2911.16,
      "end": 2917.64,
      "text": " just that later on and like Jackie's question was like is that good it's like oh",
      "tokens": [
        50612,
        445,
        300,
        1780,
        322,
        293,
        411,
        23402,
        311,
        1168,
        390,
        411,
        307,
        300,
        665,
        309,
        311,
        411,
        1954,
        50936
      ],
      "temperature": 0.0,
      "avg_logprob": -0.17348080589657738,
      "compression_ratio": 1.7121951219512195,
      "no_speech_prob": 0.00038516975473612547
    },
    {
      "id": 415,
      "seek": 290620,
      "start": 2919.3999999999996,
      "end": 2925.08,
      "text": " so uh so uh yeah so we wanted to be able to characterize it in those ways and so just being",
      "tokens": [
        51024,
        370,
        2232,
        370,
        2232,
        1338,
        370,
        321,
        1415,
        281,
        312,
        1075,
        281,
        38463,
        309,
        294,
        729,
        2098,
        293,
        370,
        445,
        885,
        51308
      ],
      "temperature": 0.0,
      "avg_logprob": -0.17348080589657738,
      "compression_ratio": 1.7121951219512195,
      "no_speech_prob": 0.00038516975473612547
    },
    {
      "id": 416,
      "seek": 290620,
      "start": 2925.08,
      "end": 2932.04,
      "text": " transparent about how we're doing that um but I think it's legitimate because if we look at",
      "tokens": [
        51308,
        12737,
        466,
        577,
        321,
        434,
        884,
        300,
        1105,
        457,
        286,
        519,
        309,
        311,
        17956,
        570,
        498,
        321,
        574,
        412,
        51656
      ],
      "temperature": 0.0,
      "avg_logprob": -0.17348080589657738,
      "compression_ratio": 1.7121951219512195,
      "no_speech_prob": 0.00038516975473612547
    },
    {
      "id": 417,
      "seek": 293204,
      "start": 2932.12,
      "end": 2939.16,
      "text": " like kind of the other findings 33 percent of them were not categorizable right so like",
      "tokens": [
        50368,
        411,
        733,
        295,
        264,
        661,
        16483,
        11816,
        3043,
        295,
        552,
        645,
        406,
        19250,
        22395,
        558,
        370,
        411,
        50720
      ],
      "temperature": 0.0,
      "avg_logprob": -0.0915580385186699,
      "compression_ratio": 1.6233766233766234,
      "no_speech_prob": 0.0003108158416580409
    },
    {
      "id": 418,
      "seek": 293204,
      "start": 2939.16,
      "end": 2944.52,
      "text": " something about inconsistent behavior and CICD pipelines like I don't know how you map that too",
      "tokens": [
        50720,
        746,
        466,
        36891,
        5223,
        293,
        383,
        2532,
        35,
        40168,
        411,
        286,
        500,
        380,
        458,
        577,
        291,
        4471,
        300,
        886,
        50988
      ],
      "temperature": 0.0,
      "avg_logprob": -0.0915580385186699,
      "compression_ratio": 1.6233766233766234,
      "no_speech_prob": 0.0003108158416580409
    },
    {
      "id": 419,
      "seek": 293204,
      "start": 2945.64,
      "end": 2951.96,
      "text": " much um but so and you might think like that's a lot is should we be concerned but actually like",
      "tokens": [
        51044,
        709,
        1105,
        457,
        370,
        293,
        291,
        1062,
        519,
        411,
        300,
        311,
        257,
        688,
        307,
        820,
        321,
        312,
        5922,
        457,
        767,
        411,
        51360
      ],
      "temperature": 0.0,
      "avg_logprob": -0.0915580385186699,
      "compression_ratio": 1.6233766233766234,
      "no_speech_prob": 0.0003108158416580409
    },
    {
      "id": 420,
      "seek": 293204,
      "start": 2951.96,
      "end": 2957.16,
      "text": " it's pretty to me it legitimizes it because it means that there's a bucket of things you can't",
      "tokens": [
        51360,
        309,
        311,
        1238,
        281,
        385,
        309,
        29754,
        5660,
        309,
        570,
        309,
        1355,
        300,
        456,
        311,
        257,
        13058,
        295,
        721,
        291,
        393,
        380,
        51620
      ],
      "temperature": 0.0,
      "avg_logprob": -0.0915580385186699,
      "compression_ratio": 1.6233766233766234,
      "no_speech_prob": 0.0003108158416580409
    },
    {
      "id": 421,
      "seek": 295716,
      "start": 2957.3199999999997,
      "end": 2964.3599999999997,
      "text": " get forced fit into these benchmark pain points and themes and so like having the a third of it",
      "tokens": [
        50372,
        483,
        7579,
        3318,
        666,
        613,
        18927,
        1822,
        2793,
        293,
        13544,
        293,
        370,
        411,
        1419,
        264,
        257,
        2636,
        295,
        309,
        50724
      ],
      "temperature": 0.0,
      "avg_logprob": -0.23027577119715073,
      "compression_ratio": 1.7298578199052133,
      "no_speech_prob": 0.00025691205519251525
    },
    {
      "id": 422,
      "seek": 295716,
      "start": 2964.3599999999997,
      "end": 2970.6,
      "text": " right be a round number is good um yeah to answer your point we just came up with these",
      "tokens": [
        50724,
        558,
        312,
        257,
        3098,
        1230,
        307,
        665,
        1105,
        1338,
        281,
        1867,
        428,
        935,
        321,
        445,
        1361,
        493,
        365,
        613,
        51036
      ],
      "temperature": 0.0,
      "avg_logprob": -0.23027577119715073,
      "compression_ratio": 1.7298578199052133,
      "no_speech_prob": 0.00025691205519251525
    },
    {
      "id": 423,
      "seek": 295716,
      "start": 2971.7999999999997,
      "end": 2979.48,
      "text": " kind of categories um that's just so we're labeling it well thanks for yeah but they're",
      "tokens": [
        51096,
        733,
        295,
        10479,
        1105,
        300,
        311,
        445,
        370,
        321,
        434,
        40244,
        309,
        731,
        3231,
        337,
        1338,
        457,
        436,
        434,
        51480
      ],
      "temperature": 0.0,
      "avg_logprob": -0.23027577119715073,
      "compression_ratio": 1.7298578199052133,
      "no_speech_prob": 0.00025691205519251525
    },
    {
      "id": 424,
      "seek": 295716,
      "start": 2979.48,
      "end": 2985.24,
      "text": " mutually exclusive so it has to fit into one of the categories and only one of the categories",
      "tokens": [
        51480,
        39144,
        13005,
        370,
        309,
        575,
        281,
        3318,
        666,
        472,
        295,
        264,
        10479,
        293,
        787,
        472,
        295,
        264,
        10479,
        51768
      ],
      "temperature": 0.0,
      "avg_logprob": -0.23027577119715073,
      "compression_ratio": 1.7298578199052133,
      "no_speech_prob": 0.00025691205519251525
    },
    {
      "id": 425,
      "seek": 298524,
      "start": 2985.4799999999996,
      "end": 2992.68,
      "text": " and to do that I ended up having to parse out a lot of the verbatim because they'll talk about all",
      "tokens": [
        50376,
        293,
        281,
        360,
        300,
        286,
        4590,
        493,
        1419,
        281,
        48377,
        484,
        257,
        688,
        295,
        264,
        9595,
        267,
        332,
        570,
        436,
        603,
        751,
        466,
        439,
        50736
      ],
      "temperature": 0.0,
      "avg_logprob": -0.21616880577730846,
      "compression_ratio": 1.5851063829787233,
      "no_speech_prob": 0.0004064303357154131
    },
    {
      "id": 426,
      "seek": 298524,
      "start": 2992.68,
      "end": 3000.52,
      "text": " these things and so to get the one to one's pouring um and I think it was 317 verbatim that we had",
      "tokens": [
        50736,
        613,
        721,
        293,
        370,
        281,
        483,
        264,
        472,
        281,
        472,
        311,
        20450,
        1105,
        293,
        286,
        519,
        309,
        390,
        805,
        7773,
        9595,
        267,
        332,
        300,
        321,
        632,
        51128
      ],
      "temperature": 0.0,
      "avg_logprob": -0.21616880577730846,
      "compression_ratio": 1.5851063829787233,
      "no_speech_prob": 0.0004064303357154131
    },
    {
      "id": 427,
      "seek": 298524,
      "start": 3001.64,
      "end": 3002.4399999999996,
      "text": " wow okay",
      "tokens": [
        51184,
        6076,
        1392,
        51224
      ],
      "temperature": 0.0,
      "avg_logprob": -0.21616880577730846,
      "compression_ratio": 1.5851063829787233,
      "no_speech_prob": 0.0004064303357154131
    },
    {
      "id": 428,
      "seek": 298524,
      "start": 3006.9199999999996,
      "end": 3013.72,
      "text": " and uh Erica when you pull those sus verbatim that i'm gonna you know find emails for later",
      "tokens": [
        51448,
        293,
        2232,
        37429,
        562,
        291,
        2235,
        729,
        3291,
        9595,
        267,
        332,
        300,
        741,
        478,
        799,
        291,
        458,
        915,
        12524,
        337,
        1780,
        51788
      ],
      "temperature": 0.0,
      "avg_logprob": -0.21616880577730846,
      "compression_ratio": 1.5851063829787233,
      "no_speech_prob": 0.0004064303357154131
    },
    {
      "id": 429,
      "seek": 301372,
      "start": 3014.52,
      "end": 3020.2799999999997,
      "text": " did you only focus on people who indicated they were open to follow up conversation yes sir",
      "tokens": [
        50404,
        630,
        291,
        787,
        1879,
        322,
        561,
        567,
        16176,
        436,
        645,
        1269,
        281,
        1524,
        493,
        3761,
        2086,
        4735,
        50692
      ],
      "temperature": 0.0,
      "avg_logprob": -0.17638095437663875,
      "compression_ratio": 1.6187845303867403,
      "no_speech_prob": 0.0005541966529563069
    },
    {
      "id": 430,
      "seek": 301372,
      "start": 3020.9199999999996,
      "end": 3026.9199999999996,
      "text": " okay cool yeah i just want to make sure i wasn't at first and i was like oh what's this column oh that's",
      "tokens": [
        50724,
        1392,
        1627,
        1338,
        741,
        445,
        528,
        281,
        652,
        988,
        741,
        2067,
        380,
        412,
        700,
        293,
        741,
        390,
        411,
        1954,
        437,
        311,
        341,
        7738,
        1954,
        300,
        311,
        51024
      ],
      "temperature": 0.0,
      "avg_logprob": -0.17638095437663875,
      "compression_ratio": 1.6187845303867403,
      "no_speech_prob": 0.0005541966529563069
    },
    {
      "id": 431,
      "seek": 301372,
      "start": 3026.9199999999996,
      "end": 3038.12,
      "text": " important but sorry i did i got it yeah and then i just pulled out just to try to make it easier",
      "tokens": [
        51024,
        1021,
        457,
        2597,
        741,
        630,
        741,
        658,
        309,
        1338,
        293,
        550,
        741,
        445,
        7373,
        484,
        445,
        281,
        853,
        281,
        652,
        309,
        3571,
        51584
      ],
      "temperature": 0.0,
      "avg_logprob": -0.17638095437663875,
      "compression_ratio": 1.6187845303867403,
      "no_speech_prob": 0.0005541966529563069
    },
    {
      "id": 432,
      "seek": 303812,
      "start": 3038.12,
      "end": 3044.44,
      "text": " so it's not lost in the issue verse here are these four pipeline authoring design related",
      "tokens": [
        50364,
        370,
        309,
        311,
        406,
        2731,
        294,
        264,
        2734,
        7996,
        510,
        366,
        613,
        1451,
        15517,
        3793,
        278,
        1715,
        4077,
        50680
      ],
      "temperature": 0.0,
      "avg_logprob": -0.08724669159435836,
      "compression_ratio": 1.63125,
      "no_speech_prob": 0.0002792488958220929
    },
    {
      "id": 433,
      "seek": 303812,
      "start": 3044.44,
      "end": 3054.2799999999997,
      "text": " tasks that we have at a critical severity level um so those are the ones that are critical",
      "tokens": [
        50680,
        9608,
        300,
        321,
        362,
        412,
        257,
        4924,
        35179,
        1496,
        1105,
        370,
        729,
        366,
        264,
        2306,
        300,
        366,
        4924,
        51172
      ],
      "temperature": 0.0,
      "avg_logprob": -0.08724669159435836,
      "compression_ratio": 1.63125,
      "no_speech_prob": 0.0002792488958220929
    },
    {
      "id": 434,
      "seek": 303812,
      "start": 3055.08,
      "end": 3061.48,
      "text": " so like a way of like prioritizing all of them those would be the most important",
      "tokens": [
        51212,
        370,
        411,
        257,
        636,
        295,
        411,
        14846,
        3319,
        439,
        295,
        552,
        729,
        576,
        312,
        264,
        881,
        1021,
        51532
      ],
      "temperature": 0.0,
      "avg_logprob": -0.08724669159435836,
      "compression_ratio": 1.63125,
      "no_speech_prob": 0.0002792488958220929
    },
    {
      "id": 435,
      "seek": 306148,
      "start": 3062.36,
      "end": 3067.96,
      "text": " and then yeah the one that i pulled out i can lead is bringing those people into the",
      "tokens": [
        50408,
        293,
        550,
        1338,
        264,
        472,
        300,
        741,
        7373,
        484,
        741,
        393,
        1477,
        307,
        5062,
        729,
        561,
        666,
        264,
        50688
      ],
      "temperature": 0.0,
      "avg_logprob": -0.1392420677289571,
      "compression_ratio": 1.7159090909090908,
      "no_speech_prob": 0.0003207960107829422
    },
    {
      "id": 436,
      "seek": 306148,
      "start": 3068.92,
      "end": 3070.52,
      "text": " um see i have a program",
      "tokens": [
        50736,
        1105,
        536,
        741,
        362,
        257,
        1461,
        50816
      ],
      "temperature": 0.0,
      "avg_logprob": -0.1392420677289571,
      "compression_ratio": 1.7159090909090908,
      "no_speech_prob": 0.0003207960107829422
    },
    {
      "id": 437,
      "seek": 306148,
      "start": 3075.2400000000002,
      "end": 3081.48,
      "text": " Erica i just wanted to say really that this was really great work maybe because it related to",
      "tokens": [
        51052,
        37429,
        741,
        445,
        1415,
        281,
        584,
        534,
        300,
        341,
        390,
        534,
        869,
        589,
        1310,
        570,
        309,
        4077,
        281,
        51364
      ],
      "temperature": 0.0,
      "avg_logprob": -0.1392420677289571,
      "compression_ratio": 1.7159090909090908,
      "no_speech_prob": 0.0003207960107829422
    },
    {
      "id": 438,
      "seek": 306148,
      "start": 3081.48,
      "end": 3086.84,
      "text": " things that i was working on so i was like more interested in it but yeah this was this was awesome",
      "tokens": [
        51364,
        721,
        300,
        741,
        390,
        1364,
        322,
        370,
        741,
        390,
        411,
        544,
        3102,
        294,
        309,
        457,
        1338,
        341,
        390,
        341,
        390,
        3476,
        51632
      ],
      "temperature": 0.0,
      "avg_logprob": -0.1392420677289571,
      "compression_ratio": 1.7159090909090908,
      "no_speech_prob": 0.0003207960107829422
    },
    {
      "id": 439,
      "seek": 308684,
      "start": 3086.84,
      "end": 3093.48,
      "text": " and i love that we're seeing feedback across like the whole pipeline experience it's not just one",
      "tokens": [
        50364,
        293,
        741,
        959,
        300,
        321,
        434,
        2577,
        5824,
        2108,
        411,
        264,
        1379,
        15517,
        1752,
        309,
        311,
        406,
        445,
        472,
        50696
      ],
      "temperature": 0.0,
      "avg_logprob": -0.15096604069576988,
      "compression_ratio": 1.6954545454545455,
      "no_speech_prob": 0.0007680928683839738
    },
    {
      "id": 440,
      "seek": 308684,
      "start": 3093.48,
      "end": 3099.4,
      "text": " section yeah actually when it touches upon the places that we have always like talked about",
      "tokens": [
        50696,
        3541,
        1338,
        767,
        562,
        309,
        17431,
        3564,
        264,
        3190,
        300,
        321,
        362,
        1009,
        411,
        2825,
        466,
        50992
      ],
      "temperature": 0.0,
      "avg_logprob": -0.15096604069576988,
      "compression_ratio": 1.6954545454545455,
      "no_speech_prob": 0.0007680928683839738
    },
    {
      "id": 441,
      "seek": 308684,
      "start": 3100.04,
      "end": 3105.7200000000003,
      "text": " and there there's never been like enough evidence that we should really focus on that work and",
      "tokens": [
        51024,
        293,
        456,
        456,
        311,
        1128,
        668,
        411,
        1547,
        4467,
        300,
        321,
        820,
        534,
        1879,
        322,
        300,
        589,
        293,
        51308
      ],
      "temperature": 0.0,
      "avg_logprob": -0.15096604069576988,
      "compression_ratio": 1.6954545454545455,
      "no_speech_prob": 0.0007680928683839738
    },
    {
      "id": 442,
      "seek": 308684,
      "start": 3105.7200000000003,
      "end": 3112.2000000000003,
      "text": " improve those experiences so i'm very hopeful that the results of the benchmarking would",
      "tokens": [
        51308,
        3470,
        729,
        5235,
        370,
        741,
        478,
        588,
        20531,
        300,
        264,
        3542,
        295,
        264,
        18927,
        278,
        576,
        51632
      ],
      "temperature": 0.0,
      "avg_logprob": -0.15096604069576988,
      "compression_ratio": 1.6954545454545455,
      "no_speech_prob": 0.0007680928683839738
    },
    {
      "id": 443,
      "seek": 311220,
      "start": 3113.0,
      "end": 3118.6,
      "text": " like help us prioritize things which are really going to make um big changes",
      "tokens": [
        50404,
        411,
        854,
        505,
        25164,
        721,
        597,
        366,
        534,
        516,
        281,
        652,
        1105,
        955,
        2962,
        50684
      ],
      "temperature": 0.0,
      "avg_logprob": -0.18082956426283892,
      "compression_ratio": 1.6359447004608294,
      "no_speech_prob": 0.0006221070070751011
    },
    {
      "id": 444,
      "seek": 311220,
      "start": 3119.7999999999997,
      "end": 3128.12,
      "text": " we're good yeah that's such analysis was pretty amazing but i didn't expect to see that much overlap",
      "tokens": [
        50744,
        321,
        434,
        665,
        1338,
        300,
        311,
        1270,
        5215,
        390,
        1238,
        2243,
        457,
        741,
        994,
        380,
        2066,
        281,
        536,
        300,
        709,
        19959,
        51160
      ],
      "temperature": 0.0,
      "avg_logprob": -0.18082956426283892,
      "compression_ratio": 1.6359447004608294,
      "no_speech_prob": 0.0006221070070751011
    },
    {
      "id": 445,
      "seek": 311220,
      "start": 3128.7599999999998,
      "end": 3134.52,
      "text": " and and some people kind of just did like a one sentence like we found this also in the sess",
      "tokens": [
        51192,
        293,
        293,
        512,
        561,
        733,
        295,
        445,
        630,
        411,
        257,
        472,
        8174,
        411,
        321,
        1352,
        341,
        611,
        294,
        264,
        262,
        442,
        51480
      ],
      "temperature": 0.0,
      "avg_logprob": -0.18082956426283892,
      "compression_ratio": 1.6359447004608294,
      "no_speech_prob": 0.0006221070070751011
    },
    {
      "id": 446,
      "seek": 311220,
      "start": 3135.3199999999997,
      "end": 3140.8399999999997,
      "text": " but i'm just extra so i was like i need to know the precise uh precise percentage of",
      "tokens": [
        51520,
        457,
        741,
        478,
        445,
        2857,
        370,
        741,
        390,
        411,
        741,
        643,
        281,
        458,
        264,
        13600,
        2232,
        13600,
        9668,
        295,
        51796
      ],
      "temperature": 0.0,
      "avg_logprob": -0.18082956426283892,
      "compression_ratio": 1.6359447004608294,
      "no_speech_prob": 0.0006221070070751011
    },
    {
      "id": 447,
      "seek": 314084,
      "start": 3140.84,
      "end": 3146.2000000000003,
      "text": " self-proclaimed homes that are mapping to each of these things um but in a way it's good because then we",
      "tokens": [
        50364,
        2698,
        12,
        4318,
        66,
        22642,
        7388,
        300,
        366,
        18350,
        281,
        1184,
        295,
        613,
        721,
        1105,
        457,
        294,
        257,
        636,
        309,
        311,
        665,
        570,
        550,
        321,
        50632
      ],
      "temperature": 0.0,
      "avg_logprob": -0.28690302502977977,
      "compression_ratio": 1.7842105263157895,
      "no_speech_prob": 0.0004149079613853246
    },
    {
      "id": 448,
      "seek": 314084,
      "start": 3146.2000000000003,
      "end": 3152.28,
      "text": " oh yeah because then we can make this statement that 45% of the negative self-proclaimed",
      "tokens": [
        50632,
        1954,
        1338,
        570,
        550,
        321,
        393,
        652,
        341,
        5629,
        300,
        6905,
        4,
        295,
        264,
        3671,
        2698,
        12,
        4318,
        66,
        22642,
        50936
      ],
      "temperature": 0.0,
      "avg_logprob": -0.28690302502977977,
      "compression_ratio": 1.7842105263157895,
      "no_speech_prob": 0.0004149079613853246
    },
    {
      "id": 449,
      "seek": 314084,
      "start": 3152.28,
      "end": 3158.6800000000003,
      "text": " overlap with those like as an aggregate is 45% of them which is more than the 33%",
      "tokens": [
        50936,
        19959,
        365,
        729,
        411,
        382,
        364,
        26118,
        307,
        6905,
        4,
        295,
        552,
        597,
        307,
        544,
        813,
        264,
        11816,
        4,
        51256
      ],
      "temperature": 0.0,
      "avg_logprob": -0.28690302502977977,
      "compression_ratio": 1.7842105263157895,
      "no_speech_prob": 0.0004149079613853246
    },
    {
      "id": 450,
      "seek": 314084,
      "start": 3159.6400000000003,
      "end": 3164.76,
      "text": " of the n-h so i think it's it's good stuff thanks thanks thanks",
      "tokens": [
        51304,
        295,
        264,
        297,
        12,
        71,
        370,
        741,
        519,
        309,
        311,
        309,
        311,
        665,
        1507,
        3231,
        3231,
        3231,
        51560
      ],
      "temperature": 0.0,
      "avg_logprob": -0.28690302502977977,
      "compression_ratio": 1.7842105263157895,
      "no_speech_prob": 0.0004149079613853246
    },
    {
      "id": 451,
      "seek": 316476,
      "start": 3164.92,
      "end": 3177.0,
      "text": " oh yeah and then later this week i'll work on the Q2 research prioritization and so we'll",
      "tokens": [
        50372,
        1954,
        1338,
        293,
        550,
        1780,
        341,
        1243,
        741,
        603,
        589,
        322,
        264,
        1249,
        17,
        2132,
        14846,
        2144,
        293,
        370,
        321,
        603,
        50976
      ],
      "temperature": 0.0,
      "avg_logprob": -0.1196706664394325,
      "compression_ratio": 1.6114285714285714,
      "no_speech_prob": 0.0019917571917176247
    },
    {
      "id": 452,
      "seek": 316476,
      "start": 3177.0,
      "end": 3183.1600000000003,
      "text": " start pinging in that to start getting threads of what we should be prioritizing and then the last",
      "tokens": [
        50976,
        722,
        280,
        8716,
        294,
        300,
        281,
        722,
        1242,
        19314,
        295,
        437,
        321,
        820,
        312,
        14846,
        3319,
        293,
        550,
        264,
        1036,
        51284
      ],
      "temperature": 0.0,
      "avg_logprob": -0.1196706664394325,
      "compression_ratio": 1.6114285714285714,
      "no_speech_prob": 0.0019917571917176247
    },
    {
      "id": 453,
      "seek": 316476,
      "start": 3183.1600000000003,
      "end": 3192.28,
      "text": " but not least is i isn't linking yeah i put i put a deck together um after kind of now having",
      "tokens": [
        51284,
        457,
        406,
        1935,
        307,
        741,
        1943,
        380,
        25775,
        1338,
        741,
        829,
        741,
        829,
        257,
        9341,
        1214,
        1105,
        934,
        733,
        295,
        586,
        1419,
        51740
      ],
      "temperature": 0.0,
      "avg_logprob": -0.1196706664394325,
      "compression_ratio": 1.6114285714285714,
      "no_speech_prob": 0.0019917571917176247
    },
    {
      "id": 454,
      "seek": 319228,
      "start": 3192.36,
      "end": 3198.92,
      "text": " this idea that we are focusing on environments oh i was like oh i can think of a key problem that i've",
      "tokens": [
        50368,
        341,
        1558,
        300,
        321,
        366,
        8416,
        322,
        12388,
        1954,
        741,
        390,
        411,
        1954,
        741,
        393,
        519,
        295,
        257,
        2141,
        1154,
        300,
        741,
        600,
        50696
      ],
      "temperature": 0.0,
      "avg_logprob": -0.11875587770308571,
      "compression_ratio": 1.7356828193832599,
      "no_speech_prob": 0.00045510378549806774
    },
    {
      "id": 455,
      "seek": 319228,
      "start": 3198.92,
      "end": 3206.76,
      "text": " heard so like across these four studies we keep hearing this problem related to um coordinating variables",
      "tokens": [
        50696,
        2198,
        370,
        411,
        2108,
        613,
        1451,
        5313,
        321,
        1066,
        4763,
        341,
        1154,
        4077,
        281,
        1105,
        37824,
        9102,
        51088
      ],
      "temperature": 0.0,
      "avg_logprob": -0.11875587770308571,
      "compression_ratio": 1.7356828193832599,
      "no_speech_prob": 0.00045510378549806774
    },
    {
      "id": 456,
      "seek": 319228,
      "start": 3206.76,
      "end": 3213.32,
      "text": " across environments so i just pulled out slides from each of those reports um because i would say",
      "tokens": [
        51088,
        2108,
        12388,
        370,
        741,
        445,
        7373,
        484,
        9788,
        490,
        1184,
        295,
        729,
        7122,
        1105,
        570,
        741,
        576,
        584,
        51416
      ],
      "temperature": 0.0,
      "avg_logprob": -0.11875587770308571,
      "compression_ratio": 1.7356828193832599,
      "no_speech_prob": 0.00045510378549806774
    },
    {
      "id": 457,
      "seek": 319228,
      "start": 3213.32,
      "end": 3219.0,
      "text": " that that's a big problem like we're not even asking about that it just keeps coming up",
      "tokens": [
        51416,
        300,
        300,
        311,
        257,
        955,
        1154,
        411,
        321,
        434,
        406,
        754,
        3365,
        466,
        300,
        309,
        445,
        5965,
        1348,
        493,
        51700
      ],
      "temperature": 0.0,
      "avg_logprob": -0.11875587770308571,
      "compression_ratio": 1.7356828193832599,
      "no_speech_prob": 0.00045510378549806774
    },
    {
      "id": 458,
      "seek": 321900,
      "start": 3219.72,
      "end": 3221.8,
      "text": " so i linked that deck",
      "tokens": [
        50400,
        370,
        741,
        9408,
        300,
        9341,
        50504
      ],
      "temperature": 0.0,
      "avg_logprob": -0.21963475674999003,
      "compression_ratio": 1.4924242424242424,
      "no_speech_prob": 0.00037214846815913916
    },
    {
      "id": 459,
      "seek": 321900,
      "start": 3227.96,
      "end": 3232.76,
      "text": " i'm glad to be back i missed everyone i was just only working on that one study",
      "tokens": [
        50812,
        741,
        478,
        5404,
        281,
        312,
        646,
        741,
        6721,
        1518,
        741,
        390,
        445,
        787,
        1364,
        322,
        300,
        472,
        2979,
        51052
      ],
      "temperature": 0.0,
      "avg_logprob": -0.21963475674999003,
      "compression_ratio": 1.4924242424242424,
      "no_speech_prob": 0.00037214846815913916
    },
    {
      "id": 460,
      "seek": 321900,
      "start": 3235.64,
      "end": 3245.4,
      "text": " so thank you for sharing all that too very excited all right um does anybody have anything else",
      "tokens": [
        51196,
        370,
        1309,
        291,
        337,
        5414,
        439,
        300,
        886,
        588,
        2919,
        439,
        558,
        1105,
        775,
        4472,
        362,
        1340,
        1646,
        51684
      ],
      "temperature": 0.0,
      "avg_logprob": -0.21963475674999003,
      "compression_ratio": 1.4924242424242424,
      "no_speech_prob": 0.00037214846815913916
    },
    {
      "id": 461,
      "seek": 324540,
      "start": 3245.48,
      "end": 3253.0,
      "text": " no all right we'll have a good rest of your weeks talk to you later",
      "tokens": [
        50368,
        572,
        439,
        558,
        321,
        603,
        362,
        257,
        665,
        1472,
        295,
        428,
        3259,
        751,
        281,
        291,
        1780,
        50744
      ],
      "temperature": 0.0,
      "avg_logprob": -0.5680486844933551,
      "compression_ratio": 1.0869565217391304,
      "no_speech_prob": 0.005618272814899683
    },
    {
      "id": 462,
      "seek": 324540,
      "start": 3253.0,
      "end": 3255.0,
      "text": " bye bye",
      "tokens": [
        50744,
        6543,
        6543,
        50844
      ],
      "temperature": 0.0,
      "avg_logprob": -0.5680486844933551,
      "compression_ratio": 1.0869565217391304,
      "no_speech_prob": 0.005618272814899683
    }
  ]
}