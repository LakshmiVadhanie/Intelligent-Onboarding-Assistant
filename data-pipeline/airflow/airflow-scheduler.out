[[34m2025-10-27T11:47:41.087-0400[0m] {[34mscheduler_job_runner.py:[0m798} INFO[0m - Starting the scheduler[0m
[[34m2025-10-27T11:47:41.089-0400[0m] {[34mscheduler_job_runner.py:[0m805} INFO[0m - Processing each file at most -1 times[0m
[[34m2025-10-27T11:47:41.091-0400[0m] {[34mmanager.py:[0m166} INFO[0m - Launched DagFileProcessorManager with pid: 73364[0m
[[34m2025-10-27T11:47:41.093-0400[0m] {[34mscheduler_job_runner.py:[0m1586} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2025-10-27T11:47:41.448-0400[0m] {[34msettings.py:[0m60} INFO[0m - Configured default timezone Timezone('UTC')[0m
[2025-10-27T11:47:41.454-0400] {manager.py:410} WARNING - Because we cannot use more than 1 thread (parsing_processes = 2) when using sqlite. So we set parallelism to 1.
[[34m2025-10-27T11:52:42.015-0400[0m] {[34mscheduler_job_runner.py:[0m1586} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2025-10-27T11:57:42.455-0400[0m] {[34mscheduler_job_runner.py:[0m1586} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2025-10-27T12:02:42.530-0400[0m] {[34mscheduler_job_runner.py:[0m1586} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2025-10-27T12:07:42.969-0400[0m] {[34mscheduler_job_runner.py:[0m1586} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2025-10-27T12:12:43.489-0400[0m] {[34mscheduler_job_runner.py:[0m1586} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2025-10-27T12:17:43.696-0400[0m] {[34mscheduler_job_runner.py:[0m1586} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2025-10-27T12:22:43.754-0400[0m] {[34mscheduler_job_runner.py:[0m1586} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2025-10-27T12:27:44.230-0400[0m] {[34mscheduler_job_runner.py:[0m1586} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2025-10-27T12:32:44.756-0400[0m] {[34mscheduler_job_runner.py:[0m1586} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2025-10-27T12:37:45.289-0400[0m] {[34mscheduler_job_runner.py:[0m1586} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2025-10-27T12:42:45.725-0400[0m] {[34mscheduler_job_runner.py:[0m1586} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2025-10-27T12:47:46.190-0400[0m] {[34mscheduler_job_runner.py:[0m1586} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2025-10-27T12:52:46.667-0400[0m] {[34mscheduler_job_runner.py:[0m1586} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2025-10-27T12:57:47.135-0400[0m] {[34mscheduler_job_runner.py:[0m1586} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2025-10-27T13:02:47.532-0400[0m] {[34mscheduler_job_runner.py:[0m1586} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2025-10-27T13:07:48.095-0400[0m] {[34mscheduler_job_runner.py:[0m1586} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2025-10-27T13:12:48.575-0400[0m] {[34mscheduler_job_runner.py:[0m1586} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2025-10-27T13:17:49.075-0400[0m] {[34mscheduler_job_runner.py:[0m1586} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2025-10-27T13:22:49.130-0400[0m] {[34mscheduler_job_runner.py:[0m1586} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2025-10-27T13:27:49.643-0400[0m] {[34mscheduler_job_runner.py:[0m1586} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2025-10-27T13:32:50.185-0400[0m] {[34mscheduler_job_runner.py:[0m1586} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2025-10-27T13:37:50.233-0400[0m] {[34mscheduler_job_runner.py:[0m1586} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2025-10-27T13:42:50.703-0400[0m] {[34mscheduler_job_runner.py:[0m1586} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2025-10-27T13:47:51.248-0400[0m] {[34mscheduler_job_runner.py:[0m1586} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2025-10-27T13:52:51.737-0400[0m] {[34mscheduler_job_runner.py:[0m1586} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2025-10-27T13:57:52.279-0400[0m] {[34mscheduler_job_runner.py:[0m1586} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2025-10-27T14:02:52.605-0400[0m] {[34mscheduler_job_runner.py:[0m1586} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2025-10-27T14:07:52.951-0400[0m] {[34mscheduler_job_runner.py:[0m1586} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2025-10-27T14:12:53.483-0400[0m] {[34mscheduler_job_runner.py:[0m1586} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2025-10-27T14:17:54.024-0400[0m] {[34mscheduler_job_runner.py:[0m1586} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2025-10-27T14:22:54.164-0400[0m] {[34mscheduler_job_runner.py:[0m1586} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2025-10-27T14:27:54.300-0400[0m] {[34mscheduler_job_runner.py:[0m1586} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2025-10-27T14:32:54.872-0400[0m] {[34mscheduler_job_runner.py:[0m1586} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2025-10-27T14:37:55.327-0400[0m] {[34mscheduler_job_runner.py:[0m1586} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2025-10-27T14:42:55.840-0400[0m] {[34mscheduler_job_runner.py:[0m1586} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2025-10-27T14:47:56.332-0400[0m] {[34mscheduler_job_runner.py:[0m1586} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2025-10-27T14:52:56.802-0400[0m] {[34mscheduler_job_runner.py:[0m1586} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2025-10-27T14:57:57.286-0400[0m] {[34mscheduler_job_runner.py:[0m1586} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2025-10-27T15:02:57.375-0400[0m] {[34mscheduler_job_runner.py:[0m1586} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2025-10-27T15:07:57.436-0400[0m] {[34mscheduler_job_runner.py:[0m1586} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2025-10-27T15:12:57.922-0400[0m] {[34mscheduler_job_runner.py:[0m1586} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2025-10-27T15:17:58.417-0400[0m] {[34mscheduler_job_runner.py:[0m1586} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2025-10-27T15:22:58.993-0400[0m] {[34mscheduler_job_runner.py:[0m1586} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2025-10-27T15:27:59.980-0400[0m] {[34mscheduler_job_runner.py:[0m1586} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2025-10-27T15:33:00.057-0400[0m] {[34mscheduler_job_runner.py:[0m1586} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2025-10-27T15:38:00.525-0400[0m] {[34mscheduler_job_runner.py:[0m1586} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2025-10-27T15:43:01.006-0400[0m] {[34mscheduler_job_runner.py:[0m1586} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2025-10-27T15:48:01.582-0400[0m] {[34mscheduler_job_runner.py:[0m1586} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2025-10-27T15:53:01.631-0400[0m] {[34mscheduler_job_runner.py:[0m1586} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2025-10-27T15:58:02.275-0400[0m] {[34mscheduler_job_runner.py:[0m1586} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2025-10-27T16:03:02.739-0400[0m] {[34mscheduler_job_runner.py:[0m1586} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2025-10-27T16:08:03.183-0400[0m] {[34mscheduler_job_runner.py:[0m1586} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2025-10-27T16:13:03.724-0400[0m] {[34mscheduler_job_runner.py:[0m1586} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2025-10-27T16:18:03.971-0400[0m] {[34mscheduler_job_runner.py:[0m1586} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2025-10-27T16:23:04.455-0400[0m] {[34mscheduler_job_runner.py:[0m1586} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2025-10-27T16:28:04.990-0400[0m] {[34mscheduler_job_runner.py:[0m1586} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2025-10-27T16:36:08.050-0400[0m] {[34mscheduler_job_runner.py:[0m1586} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2025-10-27T16:41:08.519-0400[0m] {[34mscheduler_job_runner.py:[0m1586} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2025-10-27T16:46:08.912-0400[0m] {[34mscheduler_job_runner.py:[0m1586} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2025-10-27T16:51:08.985-0400[0m] {[34mscheduler_job_runner.py:[0m1586} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2025-10-27T16:56:09.514-0400[0m] {[34mscheduler_job_runner.py:[0m1586} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[2025-10-27T16:56:59.104-0400] {manager.py:543} INFO - DAG onboarding_data_pipeline is missing and will be deactivated.
[2025-10-27T16:56:59.125-0400] {manager.py:553} INFO - Deactivated 1 DAGs which are no longer present in file.
[2025-10-27T16:56:59.127-0400] {manager.py:557} INFO - Deleted DAG onboarding_data_pipeline in serialized_dag table
[[34m2025-10-27T17:01:09.996-0400[0m] {[34mscheduler_job_runner.py:[0m1586} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2025-10-27T17:02:20.983-0400[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: onboarding_data_pipeline.check_environment manual__2025-10-27T21:02:18.591873+00:00 [scheduled]>[0m
[[34m2025-10-27T17:02:20.984-0400[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG onboarding_data_pipeline has 0/16 running and queued tasks[0m
[[34m2025-10-27T17:02:20.984-0400[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: onboarding_data_pipeline.check_environment manual__2025-10-27T21:02:18.591873+00:00 [scheduled]>[0m
[[34m2025-10-27T17:02:20.985-0400[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task check_environment because previous state change time has not been saved[0m
[[34m2025-10-27T17:02:20.986-0400[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='onboarding_data_pipeline', task_id='check_environment', run_id='manual__2025-10-27T21:02:18.591873+00:00', try_number=1, map_index=-1) to executor with priority 11 and queue default[0m
[[34m2025-10-27T17:02:20.986-0400[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'onboarding_data_pipeline', 'check_environment', 'manual__2025-10-27T21:02:18.591873+00:00', '--local', '--subdir', 'DAGS_FOLDER/main_pipeline_dag.py'][0m
[[34m2025-10-27T17:02:20.987-0400[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'onboarding_data_pipeline', 'check_environment', 'manual__2025-10-27T21:02:18.591873+00:00', '--local', '--subdir', 'DAGS_FOLDER/main_pipeline_dag.py'][0m
[[34m2025-10-27T17:02:21.331-0400[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /Users/mithundineshkumar/Documents/Development/MLOps/Project/MLOPS-Test1/data-pipeline/airflow/dags/main_pipeline_dag.py[0m
[[34m2025-10-27T17:02:21.561-0400[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2025-10-27T17:02:21.561-0400[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2025-10-27T17:02:22.072-0400[0m] {[34mutils.py:[0m161} INFO[0m - NumExpr defaulting to 10 threads.[0m
Changing /Users/mithundineshkumar/Documents/Development/MLOps/Project/MLOPS-Test1/data-pipeline/airflow/logs/dag_id=onboarding_data_pipeline/run_id=manual__2025-10-27T21:02:18.591873+00:00/task_id=check_environment permission to 509
[[34m2025-10-27T17:02:22.278-0400[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: onboarding_data_pipeline.check_environment manual__2025-10-27T21:02:18.591873+00:00 [queued]> on host mithuns-macbook-pro.local[0m
[[34m2025-10-27T17:02:22.499-0400[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='onboarding_data_pipeline', task_id='check_environment', run_id='manual__2025-10-27T21:02:18.591873+00:00', try_number=1, map_index=-1)[0m
[[34m2025-10-27T17:02:22.504-0400[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=onboarding_data_pipeline, task_id=check_environment, run_id=manual__2025-10-27T21:02:18.591873+00:00, map_index=-1, run_start_date=2025-10-27 21:02:22.303025+00:00, run_end_date=2025-10-27 21:02:22.350846+00:00, run_duration=0.047821, state=up_for_retry, executor_state=success, try_number=1, max_tries=2, job_id=2, pool=default_pool, queue=default, priority_weight=11, operator=PythonOperator, queued_dttm=2025-10-27 21:02:20.985284+00:00, queued_by_job_id=1, pid=55361[0m
[[34m2025-10-27T17:06:10.285-0400[0m] {[34mscheduler_job_runner.py:[0m1586} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2025-10-27T17:07:23.158-0400[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: onboarding_data_pipeline.check_environment manual__2025-10-27T21:02:18.591873+00:00 [scheduled]>[0m
[[34m2025-10-27T17:07:23.159-0400[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG onboarding_data_pipeline has 0/16 running and queued tasks[0m
[[34m2025-10-27T17:07:23.159-0400[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: onboarding_data_pipeline.check_environment manual__2025-10-27T21:02:18.591873+00:00 [scheduled]>[0m
[[34m2025-10-27T17:07:23.160-0400[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='onboarding_data_pipeline', task_id='check_environment', run_id='manual__2025-10-27T21:02:18.591873+00:00', try_number=2, map_index=-1) to executor with priority 11 and queue default[0m
[[34m2025-10-27T17:07:23.160-0400[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'onboarding_data_pipeline', 'check_environment', 'manual__2025-10-27T21:02:18.591873+00:00', '--local', '--subdir', 'DAGS_FOLDER/main_pipeline_dag.py'][0m
[[34m2025-10-27T17:07:23.161-0400[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'onboarding_data_pipeline', 'check_environment', 'manual__2025-10-27T21:02:18.591873+00:00', '--local', '--subdir', 'DAGS_FOLDER/main_pipeline_dag.py'][0m
[[34m2025-10-27T17:07:23.497-0400[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /Users/mithundineshkumar/Documents/Development/MLOps/Project/MLOPS-Test1/data-pipeline/airflow/dags/main_pipeline_dag.py[0m
[[34m2025-10-27T17:07:23.691-0400[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2025-10-27T17:07:23.691-0400[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2025-10-27T17:07:24.109-0400[0m] {[34mutils.py:[0m161} INFO[0m - NumExpr defaulting to 10 threads.[0m
[[34m2025-10-27T17:07:24.285-0400[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: onboarding_data_pipeline.check_environment manual__2025-10-27T21:02:18.591873+00:00 [queued]> on host mithuns-macbook-pro.local[0m
[[34m2025-10-27T17:07:24.520-0400[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='onboarding_data_pipeline', task_id='check_environment', run_id='manual__2025-10-27T21:02:18.591873+00:00', try_number=2, map_index=-1)[0m
[[34m2025-10-27T17:07:24.524-0400[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=onboarding_data_pipeline, task_id=check_environment, run_id=manual__2025-10-27T21:02:18.591873+00:00, map_index=-1, run_start_date=2025-10-27 21:07:24.310301+00:00, run_end_date=2025-10-27 21:07:24.356996+00:00, run_duration=0.046695, state=up_for_retry, executor_state=success, try_number=2, max_tries=2, job_id=3, pool=default_pool, queue=default, priority_weight=11, operator=PythonOperator, queued_dttm=2025-10-27 21:07:23.160116+00:00, queued_by_job_id=1, pid=58123[0m
[[34m2025-10-27T17:11:10.846-0400[0m] {[34mscheduler_job_runner.py:[0m1586} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[2025-10-27T17:12:08.022-0400] {manager.py:543} INFO - DAG onboarding_data_pipeline is missing and will be deactivated.
[2025-10-27T17:12:08.026-0400] {manager.py:553} INFO - Deactivated 1 DAGs which are no longer present in file.
[2025-10-27T17:12:08.028-0400] {manager.py:557} INFO - Deleted DAG onboarding_data_pipeline in serialized_dag table
[[34m2025-10-27T17:13:28.013-0400[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: onboarding_data_pipeline.check_environment manual__2025-10-27T21:02:18.591873+00:00 [scheduled]>[0m
[[34m2025-10-27T17:13:28.015-0400[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG onboarding_data_pipeline has 0/16 running and queued tasks[0m
[[34m2025-10-27T17:13:28.015-0400[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: onboarding_data_pipeline.check_environment manual__2025-10-27T21:02:18.591873+00:00 [scheduled]>[0m
[[34m2025-10-27T17:13:28.016-0400[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='onboarding_data_pipeline', task_id='check_environment', run_id='manual__2025-10-27T21:02:18.591873+00:00', try_number=3, map_index=-1) to executor with priority 11 and queue default[0m
[[34m2025-10-27T17:13:28.017-0400[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'onboarding_data_pipeline', 'check_environment', 'manual__2025-10-27T21:02:18.591873+00:00', '--local', '--subdir', 'DAGS_FOLDER/main_pipeline_dag.py'][0m
[[34m2025-10-27T17:13:28.018-0400[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'onboarding_data_pipeline', 'check_environment', 'manual__2025-10-27T21:02:18.591873+00:00', '--local', '--subdir', 'DAGS_FOLDER/main_pipeline_dag.py'][0m
[[34m2025-10-27T17:13:28.358-0400[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /Users/mithundineshkumar/Documents/Development/MLOps/Project/MLOPS-Test1/data-pipeline/airflow/dags/main_pipeline_dag.py[0m
[[34m2025-10-27T17:13:28.619-0400[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2025-10-27T17:13:28.619-0400[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2025-10-27T17:13:29.014-0400[0m] {[34mutils.py:[0m161} INFO[0m - NumExpr defaulting to 10 threads.[0m
[[34m2025-10-27T17:13:29.200-0400[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: onboarding_data_pipeline.check_environment manual__2025-10-27T21:02:18.591873+00:00 [queued]> on host mithuns-macbook-pro.local[0m
[[34m2025-10-27T17:13:29.415-0400[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='onboarding_data_pipeline', task_id='check_environment', run_id='manual__2025-10-27T21:02:18.591873+00:00', try_number=3, map_index=-1)[0m
[[34m2025-10-27T17:13:29.418-0400[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=onboarding_data_pipeline, task_id=check_environment, run_id=manual__2025-10-27T21:02:18.591873+00:00, map_index=-1, run_start_date=2025-10-27 21:13:29.224069+00:00, run_end_date=2025-10-27 21:13:29.272086+00:00, run_duration=0.048017, state=failed, executor_state=success, try_number=3, max_tries=2, job_id=4, pool=default_pool, queue=default, priority_weight=11, operator=PythonOperator, queued_dttm=2025-10-27 21:13:28.016393+00:00, queued_by_job_id=1, pid=61527[0m
[[34m2025-10-27T17:13:38.741-0400[0m] {[34mdagrun.py:[0m632} ERROR[0m - Marking run <DagRun onboarding_data_pipeline @ 2025-10-27 21:02:18.591873+00:00: manual__2025-10-27T21:02:18.591873+00:00, state:running, queued_at: 2025-10-27 21:02:18.610881+00:00. externally triggered: True> failed[0m
[[34m2025-10-27T17:13:38.742-0400[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=onboarding_data_pipeline, execution_date=2025-10-27 21:02:18.591873+00:00, run_id=manual__2025-10-27T21:02:18.591873+00:00, run_start_date=2025-10-27 21:02:19.422785+00:00, run_end_date=2025-10-27 21:13:38.742517+00:00, run_duration=679.319732, state=failed, external_trigger=True, run_type=manual, data_interval_start=2025-10-19 00:00:00+00:00, data_interval_end=2025-10-26 00:00:00+00:00, dag_hash=a2a5969c036fa370a7e462a5f535c04b[0m
[[34m2025-10-27T17:13:38.744-0400[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for onboarding_data_pipeline to 2025-10-26T00:00:00+00:00, run_after=2025-11-02T00:00:00+00:00[0m
[[34m2025-10-27T17:15:26.259-0400[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: onboarding_data_pipeline.check_environment manual__2025-10-27T21:15:23.789244+00:00 [scheduled]>[0m
[[34m2025-10-27T17:15:26.260-0400[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG onboarding_data_pipeline has 0/16 running and queued tasks[0m
[[34m2025-10-27T17:15:26.261-0400[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: onboarding_data_pipeline.check_environment manual__2025-10-27T21:15:23.789244+00:00 [scheduled]>[0m
[[34m2025-10-27T17:15:26.261-0400[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task check_environment because previous state change time has not been saved[0m
[[34m2025-10-27T17:15:26.262-0400[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='onboarding_data_pipeline', task_id='check_environment', run_id='manual__2025-10-27T21:15:23.789244+00:00', try_number=1, map_index=-1) to executor with priority 11 and queue default[0m
[[34m2025-10-27T17:15:26.262-0400[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'onboarding_data_pipeline', 'check_environment', 'manual__2025-10-27T21:15:23.789244+00:00', '--local', '--subdir', 'DAGS_FOLDER/main_pipeline_dag.py'][0m
[[34m2025-10-27T17:15:26.263-0400[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'onboarding_data_pipeline', 'check_environment', 'manual__2025-10-27T21:15:23.789244+00:00', '--local', '--subdir', 'DAGS_FOLDER/main_pipeline_dag.py'][0m
[[34m2025-10-27T17:15:26.596-0400[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /Users/mithundineshkumar/Documents/Development/MLOps/Project/MLOPS-Test1/data-pipeline/airflow/dags/main_pipeline_dag.py[0m
[[34m2025-10-27T17:15:26.843-0400[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2025-10-27T17:15:26.843-0400[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2025-10-27T17:15:27.265-0400[0m] {[34mutils.py:[0m161} INFO[0m - NumExpr defaulting to 10 threads.[0m
Changing /Users/mithundineshkumar/Documents/Development/MLOps/Project/MLOPS-Test1/data-pipeline/airflow/logs/dag_id=onboarding_data_pipeline/run_id=manual__2025-10-27T21:15:23.789244+00:00/task_id=check_environment permission to 509
[[34m2025-10-27T17:15:27.454-0400[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: onboarding_data_pipeline.check_environment manual__2025-10-27T21:15:23.789244+00:00 [queued]> on host mithuns-macbook-pro.local[0m
[[34m2025-10-27T17:15:27.711-0400[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='onboarding_data_pipeline', task_id='check_environment', run_id='manual__2025-10-27T21:15:23.789244+00:00', try_number=1, map_index=-1)[0m
[[34m2025-10-27T17:15:27.715-0400[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=onboarding_data_pipeline, task_id=check_environment, run_id=manual__2025-10-27T21:15:23.789244+00:00, map_index=-1, run_start_date=2025-10-27 21:15:27.478851+00:00, run_end_date=2025-10-27 21:15:27.533374+00:00, run_duration=0.054523, state=up_for_retry, executor_state=success, try_number=1, max_tries=2, job_id=5, pool=default_pool, queue=default, priority_weight=11, operator=PythonOperator, queued_dttm=2025-10-27 21:15:26.261589+00:00, queued_by_job_id=1, pid=62616[0m
[[34m2025-10-27T17:16:11.317-0400[0m] {[34mscheduler_job_runner.py:[0m1586} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2025-10-27T17:18:34.024-0400[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: onboarding_data_pipeline.check_environment manual__2025-10-27T21:18:32.053379+00:00 [scheduled]>[0m
[[34m2025-10-27T17:18:34.025-0400[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG onboarding_data_pipeline has 0/16 running and queued tasks[0m
[[34m2025-10-27T17:18:34.026-0400[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: onboarding_data_pipeline.check_environment manual__2025-10-27T21:18:32.053379+00:00 [scheduled]>[0m
[[34m2025-10-27T17:18:34.026-0400[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task check_environment because previous state change time has not been saved[0m
[[34m2025-10-27T17:18:34.027-0400[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='onboarding_data_pipeline', task_id='check_environment', run_id='manual__2025-10-27T21:18:32.053379+00:00', try_number=1, map_index=-1) to executor with priority 11 and queue default[0m
[[34m2025-10-27T17:18:34.027-0400[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'onboarding_data_pipeline', 'check_environment', 'manual__2025-10-27T21:18:32.053379+00:00', '--local', '--subdir', 'DAGS_FOLDER/main_pipeline_dag.py'][0m
[[34m2025-10-27T17:18:34.028-0400[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'onboarding_data_pipeline', 'check_environment', 'manual__2025-10-27T21:18:32.053379+00:00', '--local', '--subdir', 'DAGS_FOLDER/main_pipeline_dag.py'][0m
[[34m2025-10-27T17:18:34.373-0400[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /Users/mithundineshkumar/Documents/Development/MLOps/Project/MLOPS-Test1/data-pipeline/airflow/dags/main_pipeline_dag.py[0m
[[34m2025-10-27T17:18:34.644-0400[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2025-10-27T17:18:34.644-0400[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2025-10-27T17:18:35.249-0400[0m] {[34mutils.py:[0m161} INFO[0m - NumExpr defaulting to 10 threads.[0m
Changing /Users/mithundineshkumar/Documents/Development/MLOps/Project/MLOPS-Test1/data-pipeline/airflow/logs/dag_id=onboarding_data_pipeline/run_id=manual__2025-10-27T21:18:32.053379+00:00/task_id=check_environment permission to 509
[[34m2025-10-27T17:18:35.485-0400[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: onboarding_data_pipeline.check_environment manual__2025-10-27T21:18:32.053379+00:00 [queued]> on host mithuns-macbook-pro.local[0m
[[34m2025-10-27T17:18:35.704-0400[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='onboarding_data_pipeline', task_id='check_environment', run_id='manual__2025-10-27T21:18:32.053379+00:00', try_number=1, map_index=-1)[0m
[[34m2025-10-27T17:18:35.707-0400[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=onboarding_data_pipeline, task_id=check_environment, run_id=manual__2025-10-27T21:18:32.053379+00:00, map_index=-1, run_start_date=2025-10-27 21:18:35.510609+00:00, run_end_date=2025-10-27 21:18:35.564826+00:00, run_duration=0.054217, state=success, executor_state=success, try_number=1, max_tries=2, job_id=2, pool=default_pool, queue=default, priority_weight=11, operator=PythonOperator, queued_dttm=2025-10-27 21:18:34.026547+00:00, queued_by_job_id=1, pid=64372[0m
[[34m2025-10-27T17:18:36.198-0400[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 3 tasks up for execution:
	<TaskInstance: onboarding_data_pipeline.ingestion_tasks.ingest_gitlab manual__2025-10-27T21:18:32.053379+00:00 [scheduled]>
	<TaskInstance: onboarding_data_pipeline.ingestion_tasks.ingest_youtube manual__2025-10-27T21:18:32.053379+00:00 [scheduled]>
	<TaskInstance: onboarding_data_pipeline.ingestion_tasks.ingest_blogs manual__2025-10-27T21:18:32.053379+00:00 [scheduled]>[0m
[[34m2025-10-27T17:18:36.199-0400[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG onboarding_data_pipeline has 0/16 running and queued tasks[0m
[[34m2025-10-27T17:18:36.199-0400[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG onboarding_data_pipeline has 1/16 running and queued tasks[0m
[[34m2025-10-27T17:18:36.199-0400[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG onboarding_data_pipeline has 2/16 running and queued tasks[0m
[[34m2025-10-27T17:18:36.200-0400[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: onboarding_data_pipeline.ingestion_tasks.ingest_gitlab manual__2025-10-27T21:18:32.053379+00:00 [scheduled]>
	<TaskInstance: onboarding_data_pipeline.ingestion_tasks.ingest_youtube manual__2025-10-27T21:18:32.053379+00:00 [scheduled]>
	<TaskInstance: onboarding_data_pipeline.ingestion_tasks.ingest_blogs manual__2025-10-27T21:18:32.053379+00:00 [scheduled]>[0m
[[34m2025-10-27T17:18:36.200-0400[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task ingestion_tasks.ingest_gitlab because previous state change time has not been saved[0m
[[34m2025-10-27T17:18:36.201-0400[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task ingestion_tasks.ingest_youtube because previous state change time has not been saved[0m
[[34m2025-10-27T17:18:36.201-0400[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task ingestion_tasks.ingest_blogs because previous state change time has not been saved[0m
[[34m2025-10-27T17:18:36.202-0400[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='onboarding_data_pipeline', task_id='ingestion_tasks.ingest_gitlab', run_id='manual__2025-10-27T21:18:32.053379+00:00', try_number=1, map_index=-1) to executor with priority 8 and queue default[0m
[[34m2025-10-27T17:18:36.202-0400[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'onboarding_data_pipeline', 'ingestion_tasks.ingest_gitlab', 'manual__2025-10-27T21:18:32.053379+00:00', '--local', '--subdir', 'DAGS_FOLDER/main_pipeline_dag.py'][0m
[[34m2025-10-27T17:18:36.202-0400[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='onboarding_data_pipeline', task_id='ingestion_tasks.ingest_youtube', run_id='manual__2025-10-27T21:18:32.053379+00:00', try_number=1, map_index=-1) to executor with priority 8 and queue default[0m
[[34m2025-10-27T17:18:36.203-0400[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'onboarding_data_pipeline', 'ingestion_tasks.ingest_youtube', 'manual__2025-10-27T21:18:32.053379+00:00', '--local', '--subdir', 'DAGS_FOLDER/main_pipeline_dag.py'][0m
[[34m2025-10-27T17:18:36.203-0400[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='onboarding_data_pipeline', task_id='ingestion_tasks.ingest_blogs', run_id='manual__2025-10-27T21:18:32.053379+00:00', try_number=1, map_index=-1) to executor with priority 8 and queue default[0m
[[34m2025-10-27T17:18:36.203-0400[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'onboarding_data_pipeline', 'ingestion_tasks.ingest_blogs', 'manual__2025-10-27T21:18:32.053379+00:00', '--local', '--subdir', 'DAGS_FOLDER/main_pipeline_dag.py'][0m
[[34m2025-10-27T17:18:36.204-0400[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'onboarding_data_pipeline', 'ingestion_tasks.ingest_gitlab', 'manual__2025-10-27T21:18:32.053379+00:00', '--local', '--subdir', 'DAGS_FOLDER/main_pipeline_dag.py'][0m
[[34m2025-10-27T17:18:36.536-0400[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /Users/mithundineshkumar/Documents/Development/MLOps/Project/MLOPS-Test1/data-pipeline/airflow/dags/main_pipeline_dag.py[0m
[[34m2025-10-27T17:18:36.665-0400[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2025-10-27T17:18:36.665-0400[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2025-10-27T17:18:36.970-0400[0m] {[34mutils.py:[0m161} INFO[0m - NumExpr defaulting to 10 threads.[0m
Changing /Users/mithundineshkumar/Documents/Development/MLOps/Project/MLOPS-Test1/data-pipeline/airflow/logs/dag_id=onboarding_data_pipeline/run_id=manual__2025-10-27T21:18:32.053379+00:00/task_id=ingestion_tasks.ingest_gitlab permission to 509
[[34m2025-10-27T17:18:37.137-0400[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: onboarding_data_pipeline.ingestion_tasks.ingest_gitlab manual__2025-10-27T21:18:32.053379+00:00 [queued]> on host mithuns-macbook-pro.local[0m
[[34m2025-10-27T17:18:37.398-0400[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'onboarding_data_pipeline', 'ingestion_tasks.ingest_youtube', 'manual__2025-10-27T21:18:32.053379+00:00', '--local', '--subdir', 'DAGS_FOLDER/main_pipeline_dag.py'][0m
[[34m2025-10-27T17:18:37.751-0400[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /Users/mithundineshkumar/Documents/Development/MLOps/Project/MLOPS-Test1/data-pipeline/airflow/dags/main_pipeline_dag.py[0m
[[34m2025-10-27T17:18:37.874-0400[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2025-10-27T17:18:37.874-0400[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2025-10-27T17:18:38.187-0400[0m] {[34mutils.py:[0m161} INFO[0m - NumExpr defaulting to 10 threads.[0m
Changing /Users/mithundineshkumar/Documents/Development/MLOps/Project/MLOPS-Test1/data-pipeline/airflow/logs/dag_id=onboarding_data_pipeline/run_id=manual__2025-10-27T21:18:32.053379+00:00/task_id=ingestion_tasks.ingest_youtube permission to 509
[[34m2025-10-27T17:18:38.328-0400[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: onboarding_data_pipeline.ingestion_tasks.ingest_youtube manual__2025-10-27T21:18:32.053379+00:00 [queued]> on host mithuns-macbook-pro.local[0m
[[34m2025-10-27T17:18:38.542-0400[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'onboarding_data_pipeline', 'ingestion_tasks.ingest_blogs', 'manual__2025-10-27T21:18:32.053379+00:00', '--local', '--subdir', 'DAGS_FOLDER/main_pipeline_dag.py'][0m
[[34m2025-10-27T17:18:38.918-0400[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /Users/mithundineshkumar/Documents/Development/MLOps/Project/MLOPS-Test1/data-pipeline/airflow/dags/main_pipeline_dag.py[0m
[[34m2025-10-27T17:18:39.077-0400[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2025-10-27T17:18:39.078-0400[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2025-10-27T17:18:39.443-0400[0m] {[34mutils.py:[0m161} INFO[0m - NumExpr defaulting to 10 threads.[0m
Changing /Users/mithundineshkumar/Documents/Development/MLOps/Project/MLOPS-Test1/data-pipeline/airflow/logs/dag_id=onboarding_data_pipeline/run_id=manual__2025-10-27T21:18:32.053379+00:00/task_id=ingestion_tasks.ingest_blogs permission to 509
[[34m2025-10-27T17:18:39.600-0400[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: onboarding_data_pipeline.ingestion_tasks.ingest_blogs manual__2025-10-27T21:18:32.053379+00:00 [queued]> on host mithuns-macbook-pro.local[0m
[[34m2025-10-27T17:18:39.873-0400[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='onboarding_data_pipeline', task_id='ingestion_tasks.ingest_gitlab', run_id='manual__2025-10-27T21:18:32.053379+00:00', try_number=1, map_index=-1)[0m
[[34m2025-10-27T17:18:39.874-0400[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='onboarding_data_pipeline', task_id='ingestion_tasks.ingest_youtube', run_id='manual__2025-10-27T21:18:32.053379+00:00', try_number=1, map_index=-1)[0m
[[34m2025-10-27T17:18:39.874-0400[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='onboarding_data_pipeline', task_id='ingestion_tasks.ingest_blogs', run_id='manual__2025-10-27T21:18:32.053379+00:00', try_number=1, map_index=-1)[0m
[[34m2025-10-27T17:18:39.877-0400[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=onboarding_data_pipeline, task_id=ingestion_tasks.ingest_blogs, run_id=manual__2025-10-27T21:18:32.053379+00:00, map_index=-1, run_start_date=2025-10-27 21:18:39.627616+00:00, run_end_date=2025-10-27 21:18:39.683368+00:00, run_duration=0.055752, state=success, executor_state=success, try_number=1, max_tries=2, job_id=5, pool=default_pool, queue=default, priority_weight=8, operator=PythonOperator, queued_dttm=2025-10-27 21:18:36.200588+00:00, queued_by_job_id=1, pid=64412[0m
[[34m2025-10-27T17:18:39.878-0400[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=onboarding_data_pipeline, task_id=ingestion_tasks.ingest_gitlab, run_id=manual__2025-10-27T21:18:32.053379+00:00, map_index=-1, run_start_date=2025-10-27 21:18:37.167067+00:00, run_end_date=2025-10-27 21:18:37.219519+00:00, run_duration=0.052452, state=success, executor_state=success, try_number=1, max_tries=2, job_id=3, pool=default_pool, queue=default, priority_weight=8, operator=PythonOperator, queued_dttm=2025-10-27 21:18:36.200588+00:00, queued_by_job_id=1, pid=64391[0m
[[34m2025-10-27T17:18:39.879-0400[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=onboarding_data_pipeline, task_id=ingestion_tasks.ingest_youtube, run_id=manual__2025-10-27T21:18:32.053379+00:00, map_index=-1, run_start_date=2025-10-27 21:18:38.352174+00:00, run_end_date=2025-10-27 21:18:38.399774+00:00, run_duration=0.0476, state=success, executor_state=success, try_number=1, max_tries=2, job_id=4, pool=default_pool, queue=default, priority_weight=8, operator=PythonOperator, queued_dttm=2025-10-27 21:18:36.200588+00:00, queued_by_job_id=1, pid=64408[0m
[[34m2025-10-27T17:18:40.535-0400[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: onboarding_data_pipeline.preprocessing_tasks.clean_text manual__2025-10-27T21:18:32.053379+00:00 [scheduled]>[0m
[[34m2025-10-27T17:18:40.536-0400[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG onboarding_data_pipeline has 0/16 running and queued tasks[0m
[[34m2025-10-27T17:18:40.536-0400[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: onboarding_data_pipeline.preprocessing_tasks.clean_text manual__2025-10-27T21:18:32.053379+00:00 [scheduled]>[0m
[[34m2025-10-27T17:18:40.537-0400[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task preprocessing_tasks.clean_text because previous state change time has not been saved[0m
[[34m2025-10-27T17:18:40.537-0400[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='onboarding_data_pipeline', task_id='preprocessing_tasks.clean_text', run_id='manual__2025-10-27T21:18:32.053379+00:00', try_number=1, map_index=-1) to executor with priority 7 and queue default[0m
[[34m2025-10-27T17:18:40.538-0400[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'onboarding_data_pipeline', 'preprocessing_tasks.clean_text', 'manual__2025-10-27T21:18:32.053379+00:00', '--local', '--subdir', 'DAGS_FOLDER/main_pipeline_dag.py'][0m
[[34m2025-10-27T17:18:40.539-0400[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'onboarding_data_pipeline', 'preprocessing_tasks.clean_text', 'manual__2025-10-27T21:18:32.053379+00:00', '--local', '--subdir', 'DAGS_FOLDER/main_pipeline_dag.py'][0m
[[34m2025-10-27T17:18:40.876-0400[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /Users/mithundineshkumar/Documents/Development/MLOps/Project/MLOPS-Test1/data-pipeline/airflow/dags/main_pipeline_dag.py[0m
[[34m2025-10-27T17:18:40.999-0400[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2025-10-27T17:18:41.000-0400[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2025-10-27T17:18:41.292-0400[0m] {[34mutils.py:[0m161} INFO[0m - NumExpr defaulting to 10 threads.[0m
Changing /Users/mithundineshkumar/Documents/Development/MLOps/Project/MLOPS-Test1/data-pipeline/airflow/logs/dag_id=onboarding_data_pipeline/run_id=manual__2025-10-27T21:18:32.053379+00:00/task_id=preprocessing_tasks.clean_text permission to 509
[[34m2025-10-27T17:18:41.446-0400[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: onboarding_data_pipeline.preprocessing_tasks.clean_text manual__2025-10-27T21:18:32.053379+00:00 [queued]> on host mithuns-macbook-pro.local[0m
[[34m2025-10-27T17:18:41.665-0400[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='onboarding_data_pipeline', task_id='preprocessing_tasks.clean_text', run_id='manual__2025-10-27T21:18:32.053379+00:00', try_number=1, map_index=-1)[0m
[[34m2025-10-27T17:18:41.669-0400[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=onboarding_data_pipeline, task_id=preprocessing_tasks.clean_text, run_id=manual__2025-10-27T21:18:32.053379+00:00, map_index=-1, run_start_date=2025-10-27 21:18:41.470885+00:00, run_end_date=2025-10-27 21:18:41.524030+00:00, run_duration=0.053145, state=success, executor_state=success, try_number=1, max_tries=2, job_id=6, pool=default_pool, queue=default, priority_weight=7, operator=PythonOperator, queued_dttm=2025-10-27 21:18:40.536914+00:00, queued_by_job_id=1, pid=64436[0m
[[34m2025-10-27T17:18:42.158-0400[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: onboarding_data_pipeline.preprocessing_tasks.chunk_documents manual__2025-10-27T21:18:32.053379+00:00 [scheduled]>[0m
[[34m2025-10-27T17:18:42.159-0400[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG onboarding_data_pipeline has 0/16 running and queued tasks[0m
[[34m2025-10-27T17:18:42.159-0400[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: onboarding_data_pipeline.preprocessing_tasks.chunk_documents manual__2025-10-27T21:18:32.053379+00:00 [scheduled]>[0m
[[34m2025-10-27T17:18:42.160-0400[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task preprocessing_tasks.chunk_documents because previous state change time has not been saved[0m
[[34m2025-10-27T17:18:42.160-0400[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='onboarding_data_pipeline', task_id='preprocessing_tasks.chunk_documents', run_id='manual__2025-10-27T21:18:32.053379+00:00', try_number=1, map_index=-1) to executor with priority 6 and queue default[0m
[[34m2025-10-27T17:18:42.161-0400[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'onboarding_data_pipeline', 'preprocessing_tasks.chunk_documents', 'manual__2025-10-27T21:18:32.053379+00:00', '--local', '--subdir', 'DAGS_FOLDER/main_pipeline_dag.py'][0m
[[34m2025-10-27T17:18:42.161-0400[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'onboarding_data_pipeline', 'preprocessing_tasks.chunk_documents', 'manual__2025-10-27T21:18:32.053379+00:00', '--local', '--subdir', 'DAGS_FOLDER/main_pipeline_dag.py'][0m
[[34m2025-10-27T17:18:42.497-0400[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /Users/mithundineshkumar/Documents/Development/MLOps/Project/MLOPS-Test1/data-pipeline/airflow/dags/main_pipeline_dag.py[0m
[[34m2025-10-27T17:18:42.620-0400[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2025-10-27T17:18:42.621-0400[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2025-10-27T17:18:42.900-0400[0m] {[34mutils.py:[0m161} INFO[0m - NumExpr defaulting to 10 threads.[0m
Changing /Users/mithundineshkumar/Documents/Development/MLOps/Project/MLOPS-Test1/data-pipeline/airflow/logs/dag_id=onboarding_data_pipeline/run_id=manual__2025-10-27T21:18:32.053379+00:00/task_id=preprocessing_tasks.chunk_documents permission to 509
[[34m2025-10-27T17:18:43.044-0400[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: onboarding_data_pipeline.preprocessing_tasks.chunk_documents manual__2025-10-27T21:18:32.053379+00:00 [queued]> on host mithuns-macbook-pro.local[0m
[[34m2025-10-27T17:18:43.258-0400[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='onboarding_data_pipeline', task_id='preprocessing_tasks.chunk_documents', run_id='manual__2025-10-27T21:18:32.053379+00:00', try_number=1, map_index=-1)[0m
[[34m2025-10-27T17:18:43.262-0400[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=onboarding_data_pipeline, task_id=preprocessing_tasks.chunk_documents, run_id=manual__2025-10-27T21:18:32.053379+00:00, map_index=-1, run_start_date=2025-10-27 21:18:43.069219+00:00, run_end_date=2025-10-27 21:18:43.120075+00:00, run_duration=0.050856, state=success, executor_state=success, try_number=1, max_tries=2, job_id=7, pool=default_pool, queue=default, priority_weight=6, operator=PythonOperator, queued_dttm=2025-10-27 21:18:42.159951+00:00, queued_by_job_id=1, pid=64455[0m
[[34m2025-10-27T17:18:43.703-0400[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 2 tasks up for execution:
	<TaskInstance: onboarding_data_pipeline.validation_tasks.validate_schema manual__2025-10-27T21:18:32.053379+00:00 [scheduled]>
	<TaskInstance: onboarding_data_pipeline.validation_tasks.detect_anomalies manual__2025-10-27T21:18:32.053379+00:00 [scheduled]>[0m
[[34m2025-10-27T17:18:43.704-0400[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG onboarding_data_pipeline has 0/16 running and queued tasks[0m
[[34m2025-10-27T17:18:43.704-0400[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG onboarding_data_pipeline has 1/16 running and queued tasks[0m
[[34m2025-10-27T17:18:43.704-0400[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: onboarding_data_pipeline.validation_tasks.validate_schema manual__2025-10-27T21:18:32.053379+00:00 [scheduled]>
	<TaskInstance: onboarding_data_pipeline.validation_tasks.detect_anomalies manual__2025-10-27T21:18:32.053379+00:00 [scheduled]>[0m
[[34m2025-10-27T17:18:43.705-0400[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task validation_tasks.validate_schema because previous state change time has not been saved[0m
[[34m2025-10-27T17:18:43.705-0400[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task validation_tasks.detect_anomalies because previous state change time has not been saved[0m
[[34m2025-10-27T17:18:43.706-0400[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='onboarding_data_pipeline', task_id='validation_tasks.validate_schema', run_id='manual__2025-10-27T21:18:32.053379+00:00', try_number=1, map_index=-1) to executor with priority 4 and queue default[0m
[[34m2025-10-27T17:18:43.706-0400[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'onboarding_data_pipeline', 'validation_tasks.validate_schema', 'manual__2025-10-27T21:18:32.053379+00:00', '--local', '--subdir', 'DAGS_FOLDER/main_pipeline_dag.py'][0m
[[34m2025-10-27T17:18:43.706-0400[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='onboarding_data_pipeline', task_id='validation_tasks.detect_anomalies', run_id='manual__2025-10-27T21:18:32.053379+00:00', try_number=1, map_index=-1) to executor with priority 4 and queue default[0m
[[34m2025-10-27T17:18:43.707-0400[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'onboarding_data_pipeline', 'validation_tasks.detect_anomalies', 'manual__2025-10-27T21:18:32.053379+00:00', '--local', '--subdir', 'DAGS_FOLDER/main_pipeline_dag.py'][0m
[[34m2025-10-27T17:18:43.707-0400[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'onboarding_data_pipeline', 'validation_tasks.validate_schema', 'manual__2025-10-27T21:18:32.053379+00:00', '--local', '--subdir', 'DAGS_FOLDER/main_pipeline_dag.py'][0m
[[34m2025-10-27T17:18:44.036-0400[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /Users/mithundineshkumar/Documents/Development/MLOps/Project/MLOPS-Test1/data-pipeline/airflow/dags/main_pipeline_dag.py[0m
[[34m2025-10-27T17:18:44.161-0400[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2025-10-27T17:18:44.161-0400[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2025-10-27T17:18:44.464-0400[0m] {[34mutils.py:[0m161} INFO[0m - NumExpr defaulting to 10 threads.[0m
Changing /Users/mithundineshkumar/Documents/Development/MLOps/Project/MLOPS-Test1/data-pipeline/airflow/logs/dag_id=onboarding_data_pipeline/run_id=manual__2025-10-27T21:18:32.053379+00:00/task_id=validation_tasks.validate_schema permission to 509
[[34m2025-10-27T17:18:44.614-0400[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: onboarding_data_pipeline.validation_tasks.validate_schema manual__2025-10-27T21:18:32.053379+00:00 [queued]> on host mithuns-macbook-pro.local[0m
[[34m2025-10-27T17:18:44.834-0400[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'onboarding_data_pipeline', 'validation_tasks.detect_anomalies', 'manual__2025-10-27T21:18:32.053379+00:00', '--local', '--subdir', 'DAGS_FOLDER/main_pipeline_dag.py'][0m
[[34m2025-10-27T17:18:45.182-0400[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /Users/mithundineshkumar/Documents/Development/MLOps/Project/MLOPS-Test1/data-pipeline/airflow/dags/main_pipeline_dag.py[0m
[[34m2025-10-27T17:18:45.340-0400[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2025-10-27T17:18:45.340-0400[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2025-10-27T17:18:45.674-0400[0m] {[34mutils.py:[0m161} INFO[0m - NumExpr defaulting to 10 threads.[0m
Changing /Users/mithundineshkumar/Documents/Development/MLOps/Project/MLOPS-Test1/data-pipeline/airflow/logs/dag_id=onboarding_data_pipeline/run_id=manual__2025-10-27T21:18:32.053379+00:00/task_id=validation_tasks.detect_anomalies permission to 509
[[34m2025-10-27T17:18:45.813-0400[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: onboarding_data_pipeline.validation_tasks.detect_anomalies manual__2025-10-27T21:18:32.053379+00:00 [queued]> on host mithuns-macbook-pro.local[0m
[[34m2025-10-27T17:18:46.035-0400[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='onboarding_data_pipeline', task_id='validation_tasks.validate_schema', run_id='manual__2025-10-27T21:18:32.053379+00:00', try_number=1, map_index=-1)[0m
[[34m2025-10-27T17:18:46.036-0400[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='onboarding_data_pipeline', task_id='validation_tasks.detect_anomalies', run_id='manual__2025-10-27T21:18:32.053379+00:00', try_number=1, map_index=-1)[0m
[[34m2025-10-27T17:18:46.039-0400[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=onboarding_data_pipeline, task_id=validation_tasks.detect_anomalies, run_id=manual__2025-10-27T21:18:32.053379+00:00, map_index=-1, run_start_date=2025-10-27 21:18:45.842185+00:00, run_end_date=2025-10-27 21:18:45.891738+00:00, run_duration=0.049553, state=success, executor_state=success, try_number=1, max_tries=2, job_id=9, pool=default_pool, queue=default, priority_weight=4, operator=PythonOperator, queued_dttm=2025-10-27 21:18:43.705142+00:00, queued_by_job_id=1, pid=64477[0m
[[34m2025-10-27T17:18:46.040-0400[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=onboarding_data_pipeline, task_id=validation_tasks.validate_schema, run_id=manual__2025-10-27T21:18:32.053379+00:00, map_index=-1, run_start_date=2025-10-27 21:18:44.640392+00:00, run_end_date=2025-10-27 21:18:44.688311+00:00, run_duration=0.047919, state=success, executor_state=success, try_number=1, max_tries=2, job_id=8, pool=default_pool, queue=default, priority_weight=4, operator=PythonOperator, queued_dttm=2025-10-27 21:18:43.705142+00:00, queued_by_job_id=1, pid=64474[0m
[[34m2025-10-27T17:18:46.804-0400[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: onboarding_data_pipeline.generate_statistics manual__2025-10-27T21:18:32.053379+00:00 [scheduled]>[0m
[[34m2025-10-27T17:18:46.805-0400[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG onboarding_data_pipeline has 0/16 running and queued tasks[0m
[[34m2025-10-27T17:18:46.805-0400[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: onboarding_data_pipeline.generate_statistics manual__2025-10-27T21:18:32.053379+00:00 [scheduled]>[0m
[[34m2025-10-27T17:18:46.806-0400[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task generate_statistics because previous state change time has not been saved[0m
[[34m2025-10-27T17:18:46.806-0400[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='onboarding_data_pipeline', task_id='generate_statistics', run_id='manual__2025-10-27T21:18:32.053379+00:00', try_number=1, map_index=-1) to executor with priority 3 and queue default[0m
[[34m2025-10-27T17:18:46.807-0400[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'onboarding_data_pipeline', 'generate_statistics', 'manual__2025-10-27T21:18:32.053379+00:00', '--local', '--subdir', 'DAGS_FOLDER/main_pipeline_dag.py'][0m
[[34m2025-10-27T17:18:46.808-0400[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'onboarding_data_pipeline', 'generate_statistics', 'manual__2025-10-27T21:18:32.053379+00:00', '--local', '--subdir', 'DAGS_FOLDER/main_pipeline_dag.py'][0m
[[34m2025-10-27T17:18:47.192-0400[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /Users/mithundineshkumar/Documents/Development/MLOps/Project/MLOPS-Test1/data-pipeline/airflow/dags/main_pipeline_dag.py[0m
[[34m2025-10-27T17:18:47.327-0400[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2025-10-27T17:18:47.328-0400[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2025-10-27T17:18:47.674-0400[0m] {[34mutils.py:[0m161} INFO[0m - NumExpr defaulting to 10 threads.[0m
Changing /Users/mithundineshkumar/Documents/Development/MLOps/Project/MLOPS-Test1/data-pipeline/airflow/logs/dag_id=onboarding_data_pipeline/run_id=manual__2025-10-27T21:18:32.053379+00:00/task_id=generate_statistics permission to 509
[[34m2025-10-27T17:18:47.810-0400[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: onboarding_data_pipeline.generate_statistics manual__2025-10-27T21:18:32.053379+00:00 [queued]> on host mithuns-macbook-pro.local[0m
[[34m2025-10-27T17:54:26.675-0400[0m] {[34mscheduler_job_runner.py:[0m248} INFO[0m - Exiting gracefully upon receiving signal 15[0m
[[34m2025-10-27T17:54:26.845-0400[0m] {[34mprocess_utils.py:[0m131} INFO[0m - Sending Signals.SIGTERM to group 73364. PIDs of all processes in the group: [][0m
[[34m2025-10-27T17:54:26.846-0400[0m] {[34mprocess_utils.py:[0m86} INFO[0m - Sending the signal Signals.SIGTERM to group 73364[0m
[[34m2025-10-27T17:54:26.846-0400[0m] {[34mprocess_utils.py:[0m100} INFO[0m - Sending the signal Signals.SIGTERM to process 73364 as process group is missing.[0m
[[34m2025-10-27T17:54:26.851-0400[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'onboarding_data_pipeline', 'generate_statistics', 'manual__2025-10-27T21:18:32.053379+00:00', '--local', '--subdir', 'DAGS_FOLDER/main_pipeline_dag.py'][0m
[[34m2025-10-27T17:54:27.230-0400[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /Users/mithundineshkumar/Documents/Development/MLOps/Project/MLOPS-Test1/data-pipeline/airflow/dags/main_pipeline_dag.py[0m
[[34m2025-10-27T17:54:27.475-0400[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2025-10-27T17:54:27.475-0400[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2025-10-27T17:54:27.821-0400[0m] {[34mutils.py:[0m161} INFO[0m - NumExpr defaulting to 10 threads.[0m
[[34m2025-10-27T17:54:27.991-0400[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: onboarding_data_pipeline.generate_statistics manual__2025-10-27T21:18:32.053379+00:00 [running]> on host mithuns-macbook-pro.local[0m
[[34m2025-10-27T17:54:28.157-0400[0m] {[34mprocess_utils.py:[0m131} INFO[0m - Sending Signals.SIGTERM to group 73364. PIDs of all processes in the group: [][0m
[[34m2025-10-27T17:54:28.158-0400[0m] {[34mprocess_utils.py:[0m86} INFO[0m - Sending the signal Signals.SIGTERM to group 73364[0m
[[34m2025-10-27T17:54:28.158-0400[0m] {[34mprocess_utils.py:[0m100} INFO[0m - Sending the signal Signals.SIGTERM to process 73364 as process group is missing.[0m
[[34m2025-10-27T17:54:28.159-0400[0m] {[34mscheduler_job_runner.py:[0m874} INFO[0m - Exited execute loop[0m
