[
  "The GitLab Legal Commercial team is responsible for all contracting matters at GitLab. This includes, procurement, revenue, channel, technical and alliances. The Commercial Team partners with sales, technical, and business stakeholders to ensure the alignment with GitLab contracting standards, as well as the most efficient timeline to reach execution.",
  "Please note that all links are GitLab-internal only.",
  "Employment law governs the relationship between employers and employees. At GitLab, the Legal Employment team serves as a strategic partner across the organization, providing expert guidance and proactive legal solutions throughout the entire team member lifecycle. What does this partnership entail? We * collaborate strategically with Sales, Go-to-Market, Finance and People teams to develop scalable, compliant employment solutions as GitLab expands globally and responsibly, implementing a comprehensive process for gathering, assessing, and acting on country-specific information. We empower the Talent Acquisition department to achieve its critical goals while ensuring compliance with local laws and regulations during sourcing, recruitment, and hiring processes. We enable the People Operations team to effectively onboard and support team members with location- and role-specific legal support, setting team members up for success from day one. We partner with the Total Rewards team, People Operations team and People Business Partners to develop forward-thinking policies that balance GitLab’s business objectives, team members’ needs, and global legal requirements. We provide strategic counsel to our Team Member Relations team, People Business Partners, and Total Rewards as they navigate complex team member matters, including reasonable accommodation requests, performance management, career progression, and other relationship dynamics. We champion our Diversity, Inclusion, and Belonging initiatives by aligning all employment decisions with our mission statement and applicable laws. We safeguard GitLab’s interests while also providing fair treatment during employment transitions, including offboarding processes and post-employment matters. To work with the Legal Employment team, reach out early in your process, provide complete information about your needs and any time constraints you have. Being specific about what you’re requesting and any deadlines you face will enable the Legal Employment team to best triage requests and support you. To connect with Legal Employment, especially on sensitive matters, you can use the ’ legal-employment@gitlab.com ’ email address, or for non-sensitive queries, you can reach out in the #legal slack channel. Note that GitLab team members with individual employment queries should reach out to Team Member Relations team on ’ teammemberrelations@gitlab.com ’ or to their aligned People Business Partner for support. For cross-functional projects that do not involve individual team members but do require the Legal Employment team’s attention, please (i) open an Issue in the Legal and Compliance Project ; (ii) select the appropriate Issue Template; (iii) apply the label legal-employment :: to-do and (iv) if you know which Legal team member you will be working with, include them as an Assignee. This will update the Legal Employment Issue Board for the Employment team’s benefit, and allow the team to pick up and/or assign appropriately. A table showing coverage for queries by subject matter is available for internal use and can be accessed here . You are welcome to consult any member of the Legal Employment team. It is worth considering the privilege guidelines when communicating with GitLab’s Legal and Corporate Affairs team (aka the LACA team), including the Legal Employment team. Note that, if a communication is privileged, it can be protected from disclosure in litigation or other disputes. If in doubt, there’s an internal, GitLab University micro course on ‘Privileged communication’ , which is relevant to any team member who communicates with the LACA team, seeking legal advice on behalf of GitLab. The training explains the legal protection which may be applied to certain communications between team members and LACA and how best to communicate in order to be protected. It is also important to note that, if you receive any formal documents that appear to be legal notices, demands, subpoenas, court papers, or other potential litigation materials, you should please forward these by email to the legal team immediately (to the ’ legal@gitlab.com ’ email address) without responding yourself. Please add a brief note explaining when and how you received the document. These materials often have strict response deadlines, and your prompt sharing allows the legal team to review and address them properly to protect both you and GitLab.",
  "Corporate Sustainability is a business approach that enhances long term stakeholder value by implementing a strategy that considers every dimension of how a business operates when making social, environmental and economic progress. ESG stands for Environmental, Social and Governance and refers to the three key factors when measuring the sustainability and ethical impact of an investment in a business or company. At GitLab we use Sustainability and ESG interchangeably. Both terms are relevant to our work and both serve different purposes depending on the audience we interact with. The Sustainability Team creates and maintains GitLab’s Corporate Sustainability strategy and programs by driving and integrating responsible business practices and ESG regulatory compliance. The Sustainability team builds and maintains strong internal and external relationships to understand stakeholder expectations - including customer, investor and team members. This engagement allows us to remain customer centric and easy to do business with while meeting shareholder and team member expectations. The Sustainability Team has two primary functions and four corresponding programs: Compliance & Reporting : Manages customer and prospect ESG-related questionnaires, requests for proposals, and sustainability contractual clauses. Leads annual Sustainability report and external assurance process. Monitors global ESG regulation and works cross-functionally to advance GitLab’s culture of compliance with applicable ESG regulations. Responds to investor ESG rating agencies to maintain competitive ESG scoring among peers. Climate Action : Identifies and executes strategic climate programs, including annual measurement of GitLab’s greenhouse gas inventory, emissions reduction target-setting, partners with Procurement to run the sustainable supplier program, identifies emissions reduction opportunities across the business, and purchases high quality carbon credits. Manages team member & customer communications related to GitLab’s commitment to environmental sustainability. GiveLab : GiveLab is GitLab’s team member volunteer program and includes year-round volunteering, GiveLab 30 days of Impact, our annual company-wide volunteer campaign and GiveLab Champions Program. Volunteerism is an effective way to build trust through social connections - this leads to higher individual and team motivation, and greater cross-functional collaboration. GitLab for Non-Profits : GitLab’s in-kind donation program. Manages social impact communications with team members and customers, while developing strategic nonprofit partnerships to advance GitLab’s ESG goals and help enhance our brand reputation by demonstrating our commitment to the nonprofit community. Deeply integrated into our business philosophy, GitLab’s Sustainability strategy is driven by our values of Collaboration, Results for Customers, Efficiency, Diversity, Inclusion and Belonging, Iteration, and Transparency (CREDIT). GitLab’s stakeholders, including customers, investors, team members, and community members play a key role in GitLab’s Sustainability strategy. GitLab conducted its inaugural ESG materiality assessment in 2022. By conducting a materiality assessment with our stakeholders, we identified which ESG topics have the greatest impact on GitLab’s business and where we have the potential to have the greatest impact on the environment, society, and our global communities - those material topics drive the programs, policies and initiatives under our strategy. This page will continue to be updated as we make progress towards developing plans and programs to advance our Sustainability goals. The purpose of the advisory committee is to create cross-functional alignment on ESG objectives and decision making, to go beyond simply compliance and into long-term operational implementation. Members of the Sustainability Advisory Committee Like all functions at GitLab, transparency is a core focus. Every year GitLab publishes an annual sustainability report where we share our approach to managing our key sustainability focus areas, provide updates on programs and policies, achievements to date, metrics and targets, and plans for the future. The Sustainability team works cross-functionally to prepare GitLab for compliance with ESG regulations. The Sustainability team also supports internal teams with customer ESG questionnaires and RFPs. Customers and prospective customers are increasingly asking about GitLab’s Sustainability programs, including questions related to GitLab’s climate commitments and greenhouse gas emissions. Many of our customers are also subject to ESG regulation and we expect customer ESG questions to continue to increase as they look to better align to new regulations. GitLab is committed to doing our part to minimize our environmental footprint, including working to reduce greenhouse gas (GHG) emissions associated with our operations. GitLab’s stakeholders, including customers, investors, regulators, and team members expect the company to operate sustainably and to do our part to reduce our environmental footprint. Many of GitLab’s customers have GHG reduction targets and as a vendor, GitLab’s carbon emissions contribute to our customers’ emissions footprints. To remain easy to transact with, GitLab needs to meet the expectations of our customers by taking action on climate change. GitLab’s Climate Action Program consists of four pillars: Measure & Report: Every year GitLab conducts an annual greenhouse gas (GHG) inventory in alignment with the GHG Protocol, the global best practice carbon accounting standard. We publish the results of the inventory in our annual ESG report and have the data assured by a third-party. In 2026, GitLab is subject to new regulations in the US and the EU that will make these disclosures mandatory. Act: GitLab is taking action to reduce our carbon emissions. As a fully remote software company, the majority of our emissions come from our suppliers. Engaging our suppliers to measure their carbon emissions and set their own reduction targets is a critical component of our reduction pathway, which is why we have set an aspirational supplier engagement target. Please see the Sustainable Procurement Program for more information. The Sustainability team continues to explore other ways to reduce emissions and is doing further analysis on additional reduction pathways. Engage: This includes engaging GitLab team members in climate education and action. In 2024, we launched the GitLab Team Member Sustainability Guide , providing actionable steps team members can take at home to minimize their environmental impact. In FY26, we launched our partnership with Mammoth Climate , a climate literacy and challenges platform, to further engage team members with educational materials, activities, and rewards Accelerate: While GitLab works to reduce our emissions, we are also committed to accelerating climate solutions by purchasing high quality carbon credits to cover a portion of our carbon footprint. We are proud to partner with Rubicon Carbon to purchase a diversified portfolio of high quality carbon credits, financing carbon removal projects that meet Rubicon’s high standards of quality . In FY25, GitLab set an aspirational science-aligned supplier engagement target to reduce our Scope 3 emissions: 70% of our suppliers (by emissions) will have science-aligned climate targets by FY29. Our sustainable procurement program includes the following initiatives: We look forward to sharing updates on this new initiative. The Green DevOps Working Group at GitLab aims to embed sustainability throughout the software development lifecycle to reduce environmental impact while unlocking operational and business value. Its core objectives include lowering carbon emissions and energy use, integrating green practices into business processes, enhancing customer capabilities to meet regulatory and sustainability goals, and positioning GitLab as a leader in sustainable DevOps and green software. Workstreams address areas such as cloud sustainability, CI/CD energy consumption tracking and reduction, community innovation and engagement, sustainable AI and procurement, and reporting. Each workstream is led by the Sustainability team and the relevant functional team member. Customers who are interested in providing insights and feedback on sustainability features are encouraged to contact the Sustainability Team at esg@gitlab.com for more information. At GitLab, all team members do work that supports the company, which supports the enhancement of an open source codebase. This codebase is freely available to everyone to make better software faster and drive progress through what they build. Between 2022 and 2024, team members made over 125,000 commits to the open source part of the GitLab codebase. But, there are also other ways to give back and many team members choose to contribute beyond GitLab. In addition to contributing to GitLab, GitLab offers additional optional pathways for team members to give back while leveraging their unique skills and passions through programs such as GiveLab, GitLab’s Team Member Volunteer Program. GitLab encourages team members to take part in volunteer initiatives such as supporting their local communities, participating in virtual volunteer activities, and organizing volunteer activities as part of team events. Corporate volunteerism has been proven to be an effective strategy for boosting engagement , improving employee retention, and strengthening relationships at work . Volunteering with GiveLab supports team members in fostering connections, building trust among one another and embodying our CREDIT values while positively impacting our communities. Team members may self organize volunteer events at any point throughout the year. To submit a request for a team volunteer activity with a Registered Nonprofit Organization that isn’t on the current GiveLab Nonprofit Directory , please go to the Philanthropic Requests epic and open a new issue using the Volunteer_Support Template . Team members can also request support from the Sustainability Team to organize local or virtual volunteer opportunities on their behalf by going to the Philanthropic Requests epic and opening a new issue using the Volunteer_Support Template . Please write “yes” for the question, “Would you like the Sustainability team’s help organizing the volunteer activity?” All team members and volunteer activities must adhere to the GitLab Philanthropy Policy . Team members must follow GitLab’s paid time off (PTO) policy if volunteering during work hours and use the “public service/volunteer” option in Workday. If volunteering in person, team members may incur some expenses. Team members can expense up to a total of $25 per volunteer event for expenses incurred that meet the allowed for reimbursement criteria. All expenses should be submitted in Navan using the “GiveLab” classification. Please note that GitLab does not allow team members to travel to in-person volunteer events. All in-person volunteering should be local to the team member. As with all company expenses , team members must be thoughtful in spending the company’s money and use best judgment to ensure that all expenses are deemed “ordinary and necessary.” Team members should follow all team member expense responsibilities . Expenses allowed for reimbursement (for in-person volunteer events): Expenses not allowed for reimbursement: As with our unique ways of working, GitLab and its team members have identified and sought out opportunities for impact that speak not only to our values but also to our all-remote nature. To review previous opportunities that team members participated in, visit the historical activities page . GiveLab 30 Days of Impact is GitLab’s annual volunteer campaign created to encourage team members to foster connections, build trust among one another and embody our CREDIT values while positively impacting our communities. We have designed this program with our high-performing team culture and results for customers in mind. GiveLab 30 days of Impact runs annually in Q4 and our goal is to encourage as many team members as possible to volunteer over the course of 30 days. Throughout the month, team members can volunteer as little as one hour of their time to make an impact. We understand that our team members are driven by many different factors, and we welcome that volunteer participation will look different for everyone. Through GiveLab 30 days of Impact we aim to offer many different ways for team members to get involved such as: While GitLab encourages year-round volunteerism through GiveLab , GiveLab 30 days of Impact centralizes our efforts into an annual campaign to have a larger collective impact over a specific timeframe. Corporate volunteerism has been proven to be an effective strategy for boosting engagement , improving employee retention, and strengthening relationships at work . Additionally, this program offers volunteering opportunities around a major holiday season in many parts of the world, a time when many are seeking opportunities to give back. Team member participation is voluntary, should not interfere with work commitments, and time off is required to be in alignment with GitLab’s PTO policy . Travel is not permitted for this program. Team Members may choose to volunteer virtually or through local in-person events. Volunteer events typically last between one and four hours. Managers play an important role in supporting team members in taking time for themselves and their families, while also ensuring accountability to results and coverage for teams and its goals. Our Results for Customers value sits at the top of our values hierarchy, and our PTO policy empowers managers to appropriately manage workloads and deliverables, while also giving team members the time away they need from work. Team Members taking time off to volunteer should communicate time off in advance with their manager. To request volunteer time off, follow the Paid Time Off procedures outlined in our handbook and reach out to People Operations via HelpLab should you have any concerns. A step-by-step guide on how to request paid time off can be found here . Note that volunteer time off should be used towards acceptable volunteer activities and in adherence with our GitLab Philanthropy Policy . Please see our GiveLab reimbursement policy to understand current allowances as they relate to volunteering costs. We have created an internal GiveLab Volunteer Directory that features a list of vetted nonprofit organizations with available volunteer opportunities. Team members can search the document for virtual volunteer opportunities, opportunities to volunteer with GitLab Foundation grantees and search for local opportunities. We encourage all team members to contribute to our GiveLab Volunteer Directory. To recommend a nonprofit organization to add to the Directory, please open a Volunteer Recommendation Issue . The GiveLab Champions are team members who are passionate about giving back to their communities and want to encourage other team members to do the same. GiveLab Champions self-identify to participate in the voluntary Champions group, managed by the Sustainability team. The GiveLab Champions help activate the GiveLab signature program, but also work to organize and promote volunteer opportunities year-round. GiveLab Champions help team members build trust through social connections, build connections within their communities, and help GitLab provide meaningful opportunities for team members to give back. The GiveLab Champions ensure global voices are heard and relevant causes are represented based on where team members live. GiveLab Champions help to make GitLab a better place to work. The time commitment for a GiveLab Champion is estimated to be 3-5 hours per quarter. Participation can vary throughout the year. GitLab launched GitLab for Nonprofits , an in-kind donation program in 2023. Through this program, GitLab supports Registered 501c3 (or jurisdictional equivalent) Nonprofit Organizations in good standing that align with our values by offering free licenses and seats. The program operates on a first come first served basis. Once the annual donation of 5,000 seats is met, the application will remain closed for the year. What are the benefits of the GitLab for Nonprofits program? GitLab is a single platform for project management, collaboration, source control management, git, automation, security, and much more. Because it is easy to use, flexible, and all in one place, it is the best choice for nonprofits to scale their work. The GitLab for Nonprofits Program gives free licenses of GitLab to registered nonprofit organizations. Nonprofits accepted into the program will be provided a free Ultimate license for one year (SaaS or self-managed) for up to 20 seats. Additional seats may be requested although they may not be granted. Who qualifies for the program? GitLab supports Registered 501c3 (or jurisdictional equivalent) Nonprofit Organizations in good standing that align with our Values . A “Registered Nonprofit Organization” is one that has been registered with the local government or authorized agency within its applicable local, state, provincial, federal or national government. For the calendar year 2024, we will limit the in-kind program to 5,000 seats, which was approved by finance and the board in the Philanthropy Policy . Each organization will be eligible for up to 20 seats. This will allow us to assist as many organizations as possible. This will be revisited throughout the year and adjusted as needed. Interested organizations who are new customers may request additional seats although the request may not be granted. To limit churn, current GitLab customers that apply to transition to the Nonprofit Program will not be granted a special request above the 20 seats. What are the terms of the GitLab for Nonprofits program? Upon acceptance, program members are subject to the GitLab Subscription Agreement . The decision to issue a GitLab for Nonprofits license is always at the sole discretion of GitLab. Interested organizations need to visit the GitLab for Nonprofits page and submit the application form . How are applications processed? Nonprofits apply on the GitLab for Nonprofits page. Once the application is submitted, the Nonprofit will receive a message and a link to TechSoup, our verification partner. The Nonprofit will then need to log in or create their TechSoup account. TechSoup provides a rigorous vetting process to ensure the nonprofit is eligible for the GitLab for Nonprofits program and meets all requirements. If a Nonprofit is verified, TechSoup will notify GitLab. GitLab will then undergo its own vetting and approval process. Once all parties have verified and approved the Nonprofit, GitLab will send the instructions directly to the Nonprofit to redeem their license. If a Nonprofit is not verified through TechSoup, TechSoup will provide details on how the Nonprofit can become verified. If a Nonprofit is declined from GitLab, GitLab will notify the Nonprofit via nonprofits@GitLab.com . Please allow up to 15 business days for the application and verification process. Must Nonprofits renew their memberships? Yes. All nonprofits must renew their membership annually, which involves a re-verification process. Nonprofits will submit for renewal the same way they first applied for the program. Where can members receive support? While GitLab for Nonprofits Program benefits do not include product support , program members can receive help with GitLab in a number of ways. In general, we recommend the following: I’m a GitLab Team Member and I have a customer applying for the program. What do I do? Information on how GitLab Inc. supports Registered Nonprofit Organizations can be found in the Philanthropy Policy . Please note that for all Philanthropic Requests, including requests for GitLab to join as a member to an association, program or organization, approval by the Sustainability team and CLO is required as defined by the Oversight Responsibility section of the Policy. If you would like to submit a philanthropic request, please follow the instructions based on your request type. There are two ways that team members can submit a request for monetary support: Request funding from the Sustainability team to support a Registered Nonprofit Organization OR (not currently accepting applications - see instructions below on submitting a non-profit for future consideration) Request utilizing department or TMRG budget to support a Registered Nonprofit Organization If you are requesting funding from the Sustainability team to support a Registered Nonprofit Organization, please note that at this time, we are not accepting applications. If you would like to submit a Nonprofit Organization to be considered for support in the future, please go to the Philanthropic Requests epic and open a new issue using the Monetary_Support Template . You will be notified if there is a future opportunity. If you have a department or TMRG budget that you would like to utilize to support a Registered Nonprofit Organization, please go to the Philanthropic Requests epic and open a new issue using the Monetary_Support Template . Please tag your manager to approve the request if you are submitting on behalf of your department. If you are submitting a request on behalf of a TMRG or DIB, please add the DIB DRI as a reviewer. Please allow a minimum of 10 working days for review. The team member submitting the issue is responsible for obtaining proper approvals and working with Accounts Payable to issue the payment. Please tag the individuals in the approver section of the issue. Once approvals are completed, the team member requesting the donation needs to obtain an invoice from the non-profit that contains the bank payment details and submit this to AP@GitLab.com . For requests related to GitLab Membership of Association, Program or Organization, and includes terms, conditions and/or obligations on GitLab that must be executed, please follow the below process. Open an issue using the Membership Request Issue Template . Complete and attach the necessary information. Note: If you are submitting a request on behalf of a TMRG or DIB, please add the DIB DRI as a reviewer. NOTE: For any request(s) that require payment, please be certain to follow applicable ESG & Procurement processes. At this time, GitLab does not offer a matching gifts program.",
  "The Strategy & Legal Ops team promotes and institutes streamlined processes, efficient tools, and centralized program management to ensure LACA remains agile and able to support every area of GitLab’s business. Check out our issue board to learn more about what we’re working on. GitLab uses Brightflag’s Legal Spend Management platform to process and review legal invoices and accruals. See the Brightflag invoicing process in the internal handbook. Accruals submission reminder notifications are automatically sent through Brightflag to ensure vendors submit their accruals on time. This process is designed to enable LACA team members to submit requests related to attending events/conferences, furthering development, or purchasing tools/software funded by LACA. This process does not apply to equipment , Individual Use Software or other personal reimbursement requests.",
  "The Privacy Team is part of the Legal and Corporate Affairs Team. We provide support and guidance to uphold consistent business processes around the protection of personal data as it relates to GitLab customers, users, Team Members, and other natural persons. We collaborate cross-functionally and serve as advocates to ensure that the data privacy practices of GitLab meet the needs of our cross-functional partners and are continually balanced with an ever-changing global data privacy and protection landscape. Slack channel - #legal is the best place for questions relating to our team that do not require legal advice, deliverables, or any discussion of confidential information. For issues that require action from the Privacy Team, apply the label Privacy::Intake . This will update the Privacy Legal Issue Board and allow the team to triage the issue appropriately. We also use the following labels: For sensitive, private, or confidential requests email legal_internal@gitlab.com . Please do not send emails to this address for engineering, marketing, sales or procurement requests. These should be directed to #legal or an issue should be created in the Legal and Compliance project. Tell people what you are doing with personal data and why you are doing it so that the person can make an informed decision about whether they want to allow it to happen. Do not be creepy about what personal data is collected or how it is used and do not change the way personal data is used without first giving people notice and an opportunity to object, or, where required, obtaining prior consent. Make it easy for people to tell us their privacy preferences and honor those preferences even if they change over time. Build a product or service that has privacy-focused settings turned on by default and let the consumer decide if and when they want to change that. Transparency is a core value and every team member is responsible for the proper collection and use of personal data consistent with our Privacy Statement . Anonymization The process of permanently and irreversibly altering personal data in a way that it is no longer capable of being related back to a specific individual. Consent A freely given, specific, informed and unambiguous indication of an individual’s wishes. Consent is captured by an un-ticked checkbox or other unequivocal statement which signifies agreement to the processing of personal data before or at the time of collection. Data Classification A method of determining types of data associated by risk. See GitLab Security Data Classification Standards for more information. Data Controller A natural or legal person, agency, or other entity which alone, or jointly with others, determines the purpose and means of processing personal data. For example, GitLab is a Data Controller is in the areas of marketing and sales where the personal data of prospects and leads is managed solely at our discretion. GitLab also serves as a Data Controller for all personal data collected from Team Members for employment purposes and any administration of benefits. Data Processor A natural or legal person, agency, or other entity which processes personal data on behalf of a Data Controller. GitLab acts as a Data Processor when we manage personal data native to a Customer’s instance or namespace. GitLab acts as a Processor in these situations because the Customer is the ultimate owner of the data it submits to the service offerings, and our contracts service as Customer’s instructions to GitLab regarding the processing of their data. Data Subject An identified or identifiable natural person. Data Subject Rights Rights granted to individuals in relation to personal data or information processed about them. Because Data Subjct Rights are instrumental to the privacy and protection of data subjects, many of these rights are codified under global privacy legislation, such as the GDPR, CCPA, and LGDP. If a business processes personal data pursuant to certain bases such as consent or legitimate interest, then a data subject may assert one of its fundamental rights and a business is obligated to respond under law. The rights granted vary slightly by country, region, province or state. GitLab treats all users and Team Members the same and will respond to a data subject request from any individual user or team member even if they live in a country, region, or state/province without specific data protection laws. Expand the following section for more information about the data subject rights available. Right of Access A request seeking access to the specific pieces of personal data that have been collected and used by a Data Controller. Right to Correct A request asking for inaccurate or incomplete personal data to be corrected. Right to Delete A request which seeks the erasure of personal data relating to the data subject. Deletion requests must meet certain conditions and businesses are not required to delete any personal data that is processed to meet legal obligations, including that data which may be processed in pursuit or in defense of claims. Right to Portability A request where the data subject wants to transfer their data to another Data Controller; typically seen when the individual changes service providers that share a compatible electronic filing system. Right to Restrict Processing This is a request for the Data Controller to stop processing personal data under certain circumstances. This may also include a request to limit the use and disclosure of Sensitive Personal Data. Right to Object A request to opt-out of all data processing or specific processing of personal data based on consent or legitimate interest. Generally this is a request to opt-out of processing for targeted advertising, which includes the sale or sharing of personal data for profiling or cross-context behavioral advertising. Right Not to be Subject to Fully Automated Decisions This is a request that the data subject not be subjected to a decision based solely on automated processing, including profiling, which would have a significant legal impact. An example might be an algorithm that excludes someone of a certain race from obtaining a credit card. DPIA A Data Protection Impact Assessment is a method to review and document identified privacy compliance risks, as well as evalute higher risks to the rights and freedoms of individuals, including any that pose potential for significant harm. Learn more about GitLab’s process for completing DPIAs here . Personal Data Any data, individually or when combined with other data, that identifies, relates to, describes or is reasonably capable of being associated with or linked to an identifiable natural person (a ‘data subject’), whether directly or indirectly. See also, Sensitive Personal Data. Privacy by Default A concept that should be implemented at the product development stage and uses appropriate measures to ensure that, by default, the only personal data processed is what is truly necessary. In practice, this means a user’s privacy settings prioritize privacy in their default state. Privacy by Design A concept which focuses on intentionally designing a product that incorporates foundational privacy principles and ensures that Controllers and Processors are able to fulfill data protection obligations. This may include appropriate technical and organizational measures such as pseudonymisation and encryption. Pseudonymization The process of altering personal data so that it can no longer be attributed to a specific individual without the use of additional re-identifying information. In order to practice successful Pseudonymization, the re-identifying information should be kept separate from the pseudonymized data. Publicly Available Personal Data Refers to personal data that is publicly available from federal, state, or local government records or made manifestly public by the data subject. Under limited data privacy laws this may also include personal data made public through widely distributed media. Sensitive Personal Data Data that is particularly personal and intimately tied to the core identity of a person. This type of data generally includes racial or ethnic origin, political opinions, religious or philosophical beliefs, trade union membership, genetic data, biometric data, data related to health, data related to sex life or sexual orientation, criminal offenses, and citizenship/immigraion status. In some jurisdictions, Sensitive Personal Data includes government identifiers and financial data. Every vendor that handles personal data is required to go through a Privacy Review prior to being onboarded, which includes completion and approval of the privacy due diligence questionnaires detailed in the Procurement process . Certain vendors who are classified as handling red or orange data under our Data Classification Standards are reviewed annually. Additionally, when new product features are designed, there are times when a formal privacy review is required. This section outlines the process for these reviews. For Third-Party Risk Acceptance, any Moderate/High risk requires VP and/or above approval Anytime a new feature or a change to an existing feature is planned, Product Managers and Engineering Managers should evaluate if the planned development presents a legal risk where personal data is involved. If Personal Data is implicated, utilize the Legal Risk Checklist and Workflow ( internal only ) GitLab Team Members are required to complete annual training which covers general privacy practices worldwide. The goal of annual training is to ensure that Team Members understand what personal data is and how to handle it to ensure that GitLab maintains the trust our customers have placed in us as well as to ensure that GitLab remains compliant with frequently changing legal and regulatory obligations.",
  "The guidance for using open source software has been updated to enable team members to comprehensively determine which open source license types are pre-approved (deemed acceptable) for use, and which require prior review by the Legal & Corporate Affairs team (as their use may be unacceptable). Team members wishing to use open source software should now refer to the comprehensive Blue Oak Council license list, and proceed, as follows: Team members must ensure that we comply with all requirements and restrictions associated with the applicable license (these are typically defined in the body text of the license). If you’re contributing to an open source project on behalf of GitLab, you may be required to enter into a CLA. In accordance with the Authorization Matrix Policy , legal approval is required to you enter into a CLA on behalf of GitLab. If you have the choice between a Corporate and Individual CLAs, opt for the Corporate CLA. Follow these steps to obtain legal approval and enter into a CLA on behalf of GitLab: Contributions to a third-party project on behalf of GitLab should be made using your @gitlab.com email address. Post any questions to the #legal Slack channel. Alternatively, if looking for information on contributing to GitLab see here . GitLab has established guidance to aid with determining authorship of academic papers developed at GitLab. This guidance is accessible to team members only here The purpose behind this initiative is to ensure consistent and fair licensing enforcement for breaches of certain licensing terms, in order to support the continued growth of the open source community. Further information on this initiative is available here . GitLab’s GPL Cooperation Commitment follows: Before filing or continuing to prosecute any legal proceeding or claim (other than a Defensive Action) arising from termination of a Covered License, GitLab commits to extend to the person or entity (“you”) accused of violating the Covered License the following provisions regarding cure and reinstatement, taken from GPL version 3. As used here, the term ’this License’ refers to the specific Covered License being enforced. However, if you cease all violation of this License, then your license from a particular copyright holder is reinstated (a) provisionally, unless and until the copyright holder explicitly and finally terminates your license, and (b) permanently, if the copyright holder fails to notify you of the violation by some reasonable means prior to 60 days after the cessation. Moreover, your license from a particular copyright holder is reinstated permanently if the copyright holder notifies you of the violation by some reasonable means, this is the first time you have received notice of violation of this License (for any work) from that copyright holder, and you cure the violation prior to 30 days after your receipt of the notice. GitLab intends this Commitment to be irrevocable, and binding and enforceable against GitLab and assignees of or successors to GitLab’s copyrights. GitLab may modify this Commitment by publishing a new edition on this page or a successor location. ‘Covered License’ means the GNU General Public License, version 2 (GPLv2), the GNU Lesser General Public License, version 2.1 (LGPLv2.1), or the GNU Library General Public License, version 2 (LGPLv2), all as published by the Free Software Foundation. ‘Defensive Action’ means a legal proceeding or claim that GitLab brings against you in response to a prior proceeding or claim initiated by you or your affiliate. GitLab means GitLab Inc. and its affiliates and subsidiaries.",
  "The Risk Management and Dispute Resolution (RMDR) division of GitLab Legal and Corporate Affairs (LACA) is responsible for informing and guiding GitLab’s risk management strategies as well as managing internal and external investigations, litigation and other dispute resolution. We seek to support resolution across a wide range of topics, including responding to subpoenas and discovery requests, drafting and revising legal documentation, managing investigations and negotiating and drafting agreements. It is our goal to proactively address and resolve these matters in support of GitLab’s business objectives, coordinating with internal business partners across the company whenever appropriate. In support of these goals, we adopt this mantra: if you see something, say something! There are times when GitLab team members must immediately consult with RMDR to ensure that GitLab is managing its legal risks effectively. These include: If in doubt, please involve RMDR earlier rather than later – we would always rather be proactive than reactive. You can reach out to RMDR via rmdr@gitlab.com . Privileged communication is communication, written or oral, that is protected from later disclosure in litigation because it was conveyed to the attorney in confidence by a client for the purpose of seeking legal advice or by an attorney for the purpose of giving legal advice . Privilege can also be asserted over certain confidential documents created by attorneys for the same purpose. The terminology differs depending on the jurisdiction. For example, in the United States, the privilege is generally referred to as “attorney-client privilege” for communications made to or from an attorney for the purpose of providing legal advice or “attorney work product” for communication or documentations created in relation to actual or anticipated litigation. In many of our EMEA and APAC countries, it may be called “client legal privilege,” “legal professional privilege,” “legal advice privilege,” or “litigation privilege.” Additionally, the scope of the privilege differs by country. It is therefore likely that the status of a privileged communication that contains legal advice in respect of foreign law will be determined by reference to the law of the country in which any action is taken. If you have jurisdiction-specific questions about privilege, please contact a LACA team member who sits in that jurisdiction. Practically speaking, this means that communications between our team members and members of LACA are not necessarily privileged just because a member of the LACA team is involved. As a threshold matter, the LACA team consists of both attorneys and non-attorneys. Communications may be privileged if the person wishing to assert the privilege can establish that the communication was made to an attorney of the LACA team and the dominant purpose of the communication was to seek legal advice. Conversely, if the dominant purpose of the communication - even if made to an attorney - is simply to seek business advice, it is very unlikely to be privileged. Additionally, otherwise privileged communication will lose its privilege in most circumstances if it is further or also disclosed to third parties. Thus, team members should always be aware of who has access to the communication and should be very careful about forwarding it. What are some tips for team members to follow to help maintain privileged communication? Below are the steps you can take to help ensure that any communications you have with GitLab’s LACA team can be considered privileged: For more information, please see the internal handbook here . A legal hold is the process GitLab uses to preserve all forms of relevant evidence, whether it be emails, instant messages, physical documents, handwritten or typed notes, voicemails, raw data, backup tapes, and any other type of information that could be relevant to an investigation, pending or imminent litigation or when litigation is reasonably anticipated. Legal holds are imperative in preventing spoliation (destruction, deletion, or alteration) of evidence which can have a severely negative impact on a company’s case, including leading to sanctions. Once GitLab becomes aware of an investigation or potential litigation, a GitLab attorney will provide notice to the impacted team members, instructing them not to delete or destroy any information relating to the subject matter of the investigation or potential litigation. The legal hold applies to paper and electronic documents. During a legal hold, all retention policies must be overridden. Your obligation to follow the procedures outlined in the notice continues until the hold is lifted, even if you depart the Company. If you depart the Company, all Company owned devices and any material you are holding in accordance with any active Legal Hold Notice or active Company investigation should be turned over upon your departure.",
  "Trade control laws, which often consist of sanctions, export controls, and import laws, govern how and under what circumstances technology, software, and technical assistance may be exported. Trade control laws vary from country to country but usually exist to protect national security and further foreign policy and economic interests. Under United States law, exports, re-exports, and transfers, can take many forms, including oral, written, and visual disclosure, physical shipment, and electronic transfer or transmission. An export can also occur when technology, software, or technical assistance is transmitted to U.S. nationals abroad, or to non-U.S. nationals located within the United States. The export of certain software, technology, or technical assistance to certain countries, certain end users, or for certain end uses, may require authorization from the United States government prior to export, re-export, or transfer. GitLab Enterprise Edition, related technology, and services (collectively, “GitLab Software”), are subject to the Export Administration Regulations (“EAR”), administered by the U.S. Department of Commerce, Bureau of Industry and Security (“BIS”), and various sanctions programs administered by the U.S Treasury Department’s Office of Foreign Assets Control (“OFAC”). The GitLab Community Edition is freely available to the public and is not subject to the EAR. GitLab continuosly monitors developments to these regulations to maintain compliance and to leverage any opportunity to broaden access to GitLab in a compliant manner that allows everyone to contribute. The GitLab Software has been classified via CCATS G178430 as a 5D992.c mass market encryption product with eligibility for export to most destinations under 15 CFR 740.17(b)(1) of license exception ENC. GitLab users may not export, re-export, or transfer GitLab Software, without first obtaining authorization from the U.S. government, to (a) any U.S. embargoed country including but not limited to Cuba, Iran, North Korea, Syria, Russia, Belarus, and the Crimea, Donetsk, and Luhansk regions of Ukraine, (b) any party identified on OFAC’s Specially Designated Nationals and Blocked Person list or the Department of Commerce Denied Persons, Entity, or Unverified lists, or (c) for end use involving sensitive nuclear, rocket systems, unmanned aerial vehicles, missiles, chemical or biological weapons, or for any other end use prohibited by 15 CFR 744. GitLab provides this information, which is subject to change without notice, to facilitate GitLab users’ compliance with applicable trade control law. GitLab users remain solely responsible for exporting, re-exporting, and transferring GitLab Software and any user-developed content in accordance with those regulations and should seek legal counsel as necessary.",
  "Insights for the future of software development Behind the scenes of The DevSecOps Platform We're the company behind GitLab, the most comprehensive DevSecOps platform. What started in 2011 as an open source project to help one team of programmers collaborate is now the platform millions of people use to deliver software faster, more efficiently, while strengthening security and compliance. Since the beginning, we've been firm believers in remote work, open source, DevSecOps, and iteration. We get up and log on in the morning (or whenever we choose to start our days) to work alongside the GitLab community to deliver new innovations every month that help teams focus on shipping great code faster, not their toolchain. bring teams together in one application Our mission is to enable everyone to contribute to and co-create the software that powers our world. The GitLab project began with a commit We started releasing a new version of GitLab on the 22nd of every month The first version of GitLab CI is created Joined Y Combinator and published the GitLab Handbook to our website repository Announced our master plan and raised $20 million in B round financing GitLab Inc. became a publicly traded company on the Nasdaq Global Market (NASDAQ: GTLB) We strive to create an all-remote environment where all team members around the world can show up as their full selves, contribute their best, feel their voices are heard and welcomed, and truly prioritize work-life balance. If you're interested in being a part of the team, we invite you to learn more about working at GitLab and apply to any open positions that look like a good fit. TeamOps is GitLab's unique people practice which makes teamwork an objective discipline. It's how GitLab scaled from a startup to a global public company in a decade. Through a free and accessible practitioner certification, other organizations can leverage TeamOps to make better decisions, deliver results, and move our world forward. 50%+ of the Fortune 100 trust GitLab See what your team can do with the intelligent Git is a trademark of Software Freedom Conservancy and our use of 'GitLab' is under license View page source Edit this page Please contribute",
  "It is GitLab’s mission to enable everyone to contribute to and co-create the software that powers our world . There are three ways you can contribute and co-create: To ensure that everyone can contribute with GitLab we allow anyone to create a proposal, at any time, without setup, and with confidence. Let’s analyze that sentence a bit. We actively welcome contributors to enable everyone to contribute to and co-create with GitLab, the application . When everyone can contribute and co-create , users become contributors and we greatly increase the rate of innovation to benefit customers and users. There is also an open dialogue between GitLab and our customers, partners, and the community so that we can also better identify what they need. That way we can not only build a solution for them, but bring that solution to the world. We think that it is logical that our collaboration tools are a collaborative work themselves. More than 3,000 people from the wider community have contributed to GitLab to make that a reality. We do this by having quality code, tests, documentation, popular frameworks, and offering a comprehensive GitLab Development Kit and a dedicated GitLab Design System . We use GitLab at GitLab Inc., we dogfood it and make it a tool we continue to love. We celebrate contributions by recognizing a Most Valuable Person (MVP) every month. We allow everyone to anticipate, propose, discuss, and contribute features by having everything on a public issue tracker. We ship a new version every month so contributions and feedback are visible fast. To contribute to open source software, people must be empowered to learn programming. That is why we sponsor initiatives such as Rails Girls. There are a few significant, but often overlooked, nuances of the enabling everyone to contribute to GitLab, the application mantra: A group discussion reiterating the importance of everyone being able to contribute: To enable everyone to contribute to GitLab, the company we have open business processes. This allows all team members to suggest improvements to our handbook. We hire remotely so our team members can be judged on results, not presence in an office. We engage on social media platforms and in our blog post comments. And we strive to take decisions guided by our values . We welcome all contributors in the www-gitlab-com project so that everyone can contribute to about.gitlab.com . GitLab uses about.gitlab.com to share our expertise with the world and believe we can build even greater levels of trust with contributions from our team and community. We strive to provide a great experience for our existing and new community members by reviewing changes and integrating the contributions into our regularly planned updates. Our Mission is on a 30 year cadence . Our purpose is to help people increase their lifetime earnings through training, access to opportunities, and the DevSecOps platform. Our mission is the way we realize our purpose . By enabling everyone to contribute to and co-create the software that powers our world , we increase access for people to be creators. With more contributors and more creators, we increase both the volume and velocity of innovation. More innovation drives economic progress that benefits consumers, businesses, and the economy as a whole . As a result, innovation both directly and indirectly increases the total volume of available opportunities and average value of each individual opportunity. Access to a broader set of more valuable opportunities ultimately increases people’s lifetime earnings . Our purpose is on the same 30 year cadence as our mission. Our purpose informs our mission, which directly or indirectly informs the rest of the items in our cadence . As a result, progress for the items on our cadence page creates progress for both our mission and our purpose . Our mission is the inspiration for our vision . Our vision is on a 10 year cadence. Our mission guides our path, and our values are the principles we live along this path. We firmly adhere to laws including trade compliance laws – see the GitLab Code of Business Conduct and Ethics , in countries where we do business, and welcome everyone abiding by those legal restrictions to be customers of GitLab. In some circumstances, we may opt to not work with particular organizations, on a case-by-case basis. Some reasons we may choose not to work with certain entities include, but are not limited to: This policy is in alignment with our mission, contributor and employee code-of-conduct and company values. Here are some links that may give you some background at how we arrived at this customer acceptance policy: We acknowledge the concerns to achieving our goals. We document them in our Mitigating Concerns page . See what your team could do with The DevSecOps Platform. Have a question? We're here to help.",
  "GitLab’s six core values are Collaboration , Results for Customers , ⏱️ Efficiency , Diversity, Inclusion & Belonging , Iteration , and ️ Transparency , and together they spell the CREDIT we give each other by assuming good intent. We react to them with values emoji and they are made actionable below. We take inspiration from other companies, and we always go for the boring solutions . Our co-founder, Sid Sijbrandij, has shared the origin of each of the CREDIT values, but just like the rest of our work, we continually adjust our values and strive to make them better. GitLab values are a living document. In many instances, they have been documented, refined, and revised based on lessons learned (and scars earned) in the course of doing business. We used to have more values, but it was difficult to remember them all. In response, we condensed them, created an acronym (CREDIT), and listed operating principles to guide behavior. Everyone is welcome to suggest improvements. Please assign MRs to update these values to our Chief People Officer and if you work at GitLab, also @mention them in the #values Slack channel . Driving Results with CREDIT from GitLab on Vimeo . To achieve results, team members must work together effectively. At GitLab, helping others is a priority, even when it is not immediately related to the goals that you are trying to achieve. Similarly, you can rely on others for help and advice—in fact, you’re expected to do so. Anyone can chime in on any subject, including people who don’t work at GitLab. The person who’s responsible for the work decides how to do it, but they should always take each suggestion seriously and try to respond and explain why it may or may not have been implemented. We value caring for others. Demonstrating we care for people provides an effective framework for challenging directly and delivering feedback. Kindness doesn’t mean holding back on feedback or avoiding disagreements, these are crucial to professional growth and getting results for customers. Kindness means you make a separation between the work and the person, you can criticize someone’s work but still be respectful to the person. Give as much positive feedback as you can, and do it in a public way. There are aspects of GitLab culture, such as intentional transparency, that are unintuitive to outsiders and new team members. Be willing to invest in people and engage in open dialogue. For example, consider making private issues public wherever possible so that we can all learn from the experience. Don’t be afraid of judgement or scrutiny when sharing publicly, we all understand it’s impossible to know everything . Everyone can remind anyone in the company about our values. If there is a disagreement about the interpretations, the discussion can be escalated to more people within the company without repercussions. Share problems you run into, ask for help, be forthcoming with information and speak up . Give negative feedback in the smallest setting possible. One-on-one video calls are preferred. Negative feedback is distinct from negativity and disagreement. If there is no direct feedback involved, strive to discuss disagreement in a public channel , respectfully and transparently . In a GitLab Unfiltered interview on values , GitLab co-founder Sid Sijbrandij offers the following context. We deal with negative all the time at GitLab. If it’s not a problem, then why are we discussing it? We deal with negativity a lot, and that’s also part of our ambition. If you want to get better, you talk about what you can improve. We’re allowed to publicly discuss negative things; we’re not allowed to give negative feedback in a large setting if it could be feasibly administered in a smaller setting. Negative feedback can be given in a group setting if it’s to someone higher in the management chain. This shows that no one is above feedback. We want to solve problems while they are small . If you are unhappy with anything (your duties, your colleague, your boss, your salary, your location, your computer), please voice your concerns rather than keeping them to yourself. If you need to escalate beyond your manager, you could consider speaking to your skip-level , a more senior person, or a people business partner . Recognize the people that helped you publicly, for example in our #thanks chat channel . When publicly thanking, it’s important to recognize the following: Giving feedback is challenging, but it’s important to deliver it effectively. When providing feedback, always make it about the work itself; focus on the business impact and not the person. Make sure to provide at least one clear and recent example. If a person is going through a hard time in their personal life, then take that into account. An example of giving positive feedback is our thanks chat channel . For managers, it’s important to realize that team members react to a negative incident with their managers six times more strongly than they do to a positive one. Keeping that in mind, if an error is so inconsequential that the value gained from providing criticism is low, it might make sense to keep that feedback to yourself. In the situations where negative feedback must be given, focus on the purpose for that feedback: to improve the team member’s performance going forward. Give recognition generously, in the open, and often to generate more engagement from your team. We use a lot of text-based communication , and if you know the person behind the text, it will be easier to prevent conflicts. So we encourage people to get to know each other on a personal level through informal communication , for example, virtual coffee chats , and during GitLab Summit . While it’s wise to seek advice from experts within your function, we encourage GitLab team members to do the same across departments. This enables the company to iterate more quickly, embrace the understanding that everyone can contribute and include more diverse perspectives when possible. If you have to remind someone of the position you have in the company, you’re doing something wrong. People already know our decision-making process . Explain why you’re making the decision, and respect everyone irrespective of their function. This includes using the rank of another person - including the CEO - to sell an idea or decision. We naturally have a double standard when it comes to the actions of others. We blame circumstances for our own mistakes, but individuals for theirs. This double standard is called the Fundamental Attribution Error . In order to mitigate this bias, you should always assume positive intent in your interactions with others, respecting their expertise and giving them grace in the face of what you might perceive as mistakes. When disagreeing , folks sometimes argue against the weakest points of an argument, or an imaginary argument (e.g. “straw man” ). Assume the points are presented in good faith, and instead try to argue against the strongest version of your opponent’s position. We call this arguing against a “steel” position, instead of a “straw” one. This concept is borrowed from argue the “steel man” technique. A “steel” position should be against the absolute most effective version of your opponent’s position — potentially even more compelling than the one they presented. A good “steel” position is one where the other person feels you’ve represented their position well, even if they still disagree with your assumptions or conclusion. There is a lot of good in this article about not wanting jerks on our team, but we believe that jerk is a label for behavior rather than an inherent classification of a person. We avoid classifications. If you made a mistake, apologize as soon as possible. Saying sorry is not a sign of weakness but one of strength. The people that do the most work will likely make the most mistakes. Additionally, when we share our mistakes and bring attention to them, others can learn from us, and the same mistake is less likely to be repeated by someone else. Mistakes can include when you have not been kind to someone. In order to reinforce our values, it is important, and takes more courage, to apologize publicly when you have been unkind publicly (e.g., when you have said something unkind or unprofessional to an individual or group in a Slack channel). Don’t defend a point to win an argument or double-down on a mistake. You are not your work; you don’t have to defend your point. You do have to search for the right answer with help from others. In a GitLab Unfiltered interview , GitLab Head of Remote Darren M. adds context on this operating principle. In many organizations, there’s a subtle, low-level, persistent pressure to continually prove your worth. And I believe that this fuels imposter syndrome and wreaks havoc on mental health . What’s so troubling to me is how often perception is reality. In other words, those who have mastered the art of being perceived as elite reap benefits, though this has nothing to do with actual results. At GitLab, “no ego” means that we foster and support an environment where results matter, and you’re given agency to approach your work in the way that makes sense to you. Instead of judging people for not approaching work in an agreed-upon way, “no ego” encourages people to glean inspiration from watching others approach work in new and different ways. A candidate who has talked to a lot of people inside GitLab said that, compared to other companies, one thing stood out the most: everyone here mentioned wanting to see each other succeed. Keep an eye out for others who may be struggling or stuck. If you see someone who needs help, reach out and assist. This might involve offering to pair program or setting up a sync brainstorming session. The goal is to connect them with someone else who can provide expertise or assistance. We are a team, so we succeed and shine together by supporting each other! Always make suggestions about examples of work, not the person. Say “You didn’t respond to my feedback about the design” instead of “You never listen”. And, when receiving feedback, keep in mind that feedback is the best way to improve, and that others giving you feedback want to see you succeed. Our collaboration value is about helping each other when we have questions, need critique, or need help. No need to brainstorm, wait for consensus, or do with two what you can do yourself . The Bolt Handbook refers to this as the Founder Mentality , where all team members should approach the problem as if they own the company. Investigate mistakes in a way that focuses on the situational aspects of a failure’s mechanism and the decision-making process that led to the failure, rather than cast blame on a person or team. We hold blameless root cause analyses and retrospectives for stakeholders to speak up without fear of punishment or retribution. People joining the company frequently say, “I don’t want to step on anyone’s toes.” At GitLab, we should be more accepting of people taking initiative in trying to improve things. As companies grow, their speed of decision-making goes down since there are more people involved. We should counteract that by having short toes and feeling comfortable letting others contribute to our domain. For example, pointed, respectful feedback to a proposal by GitLab’s CEO led to their own merge request being closed. However, it is not required to respond to comments. We know we must rely on others for the expertise they have that we don’t. It’s OK to admit you don’t know something and to ask for help, even if doing so makes you feel vulnerable. It is never too late to ask a question, and by doing so, you can get the information you need to produce results and to strengthen your own skills as well as GitLab as a whole. After your question is answered, please document the answer so that it can be shared . Don’t display surprise when people say they don’t know something, as it is important that everyone feels comfortable saying “I don’t know” and “I don’t understand.” (As inspired by Recurse .) When collaborating, it is always important to stay above radar and work transparently , but collaboration is not consensus and disagreement is part of collaboration. You don’t need to ask people for their input, and they shouldn’t ask you “Why didn’t you ask me?”. You don’t have to wait for people to provide input, if you did ask them. You don’t need to have everyone agreeing to the same thing - they can disagree, commit, and advocate . Two-way doors decisions can be reversed as part of disagree, commit, and advocate , while one-way door decisions benefit from more input. Recognize these reversible two-way door decisions for when less input is required to iterate faster. We believe in permissionless innovation — you don’t need to involve people, but everyone can contribute. This is core to how we iterate , since we want smaller teams moving quickly rather than large teams achieving consensus slowly. Competencies are the Single Source of Truth (SSoT) framework for things we need team members to learn. We demonstrate collaboration when we take action to help others and include other’s (both internal and external) input (both help and feedback) to achieve the best possible outcome. We exist to help our customers achieve more. Everything we do should be in service of making our customers successful with GitLab. Results for Customers is at the top of our values hierarchy, as our customers achieving results drives overall business performance that enables everything else. The Results for Customers value is displayed through the following operating principles: While we iterate with small changes, we strive for large, ambitious results. We have an ambitious mission and vision , and we aim to be the best in the world across all our functions. Setting ambitious, measurable goals enables us to best deliver customer results. We agree in writing on measurable goals. We have and report against KPIs with guiding targets. All GitLab team members should understand our customers’ needs, issues, and value propositions. We understand how they use GitLab and what they need from a platform in order to meet their goals. Internally facing teams consider the impact of their work as it pertains indirectly to GitLab’s customers. We better understand customers and their needs through: We create together with our customers. There is an open dialogue between GitLab and our customers so that we can better identify what they need. As a result of building a solution for them, we can also bring that solution to the world. Our focus is to increase customer results. At GitLab, one way to drive customer results is through platform enhancements that drive the most value for direct users. This requires being aware of the Concur effect . Arvind Narayanan , a Princeton Professor, described their frustration with Blackboard in a viral Tweet: It has every feature ever dreamed up. But like anything designed by a committee, the interface is incoherent and any task requires at least fifteen clicks (and that’s if you even remember the correct sequence the first time). Software companies can be breathtakingly clueless when there’s a layer of indirection between them and their users. Everyone who’s suffered through Blackboard will have the same reaction to this: try having less functionality! Ryan Falor followed up on Narayanan’s tweet with their definition of the Concur Effect: See the Hacker News discussion for a specific UX example. At GitLab, we want to drive customer results through focusing on platform enhancements that drive the most value for direct users. Customer results are more important than: We care about what you achieve: the code you shipped, the needle you moved, the user you made happy, and the team member you helped. Someone who took the afternoon off shouldn’t feel like they did something wrong, unless it negatively impacted a goal or result they were responsible for. You don’t have to defend how you spend your day if you are performing and delivering against expectations. We trust team members to do the right thing instead of having rigid rules. We trust team members to show up and do their best work. Do not incite competition by proclaiming how many hours you worked yesterday. If you are working too many hours, talk to your manager to discuss solutions. We use our own product in the way our users do to surface improvements that will lead to better customer results . GitLab is a DevSecOps Platform that can be used by people throughout the business. This is how we use it within GitLab. For example, we use our OKR functionality company-wide to inform product enhancements and for team members to have a great understanding of the customer experience. We also dogfood in the following ways: When something breaks, doesn’t work well, or needs improvement, we are more likely to notice it internally and address it before it impacts our larger community. We give people agency to focus on what they think is most beneficial. If a meeting doesn’t seem interesting and someone’s active participation is not critical to the outcome of the meeting, they can always opt to not attend, or during a video call they can work on other things if they want. Staying in the call may still make sense even if you are working on other tasks, so other peers can ping you and get fast answers when needed. This is particularly useful in multi-purpose meetings where you may be involved for just a few minutes. Challenging the status quo can lead to remarkable results - we must never stop. A challenger mindset requires that we continually ask ourselves bold, difficult questions about our business and the problems we solve, while resisting complacency. To succeed we must innovate and delight our customers with the value of the products we build. A challenger mindset requires a relentless pursuit of excellence - we must be tenacious . Each win for our customers builds reputational capital we can use to earn the trust of prospects in a competitive market. While competition is a feature of capitalism, internally as GitLab team members, we must focus our efforts inwardly on achieving our very best results for customers to win market share. You don’t always get results and this will lead to criticism from yourself and/or others. We believe our talents can be developed through hard work, targeted training, learning from others, on-the-job experience, and receiving input from others. It is in our DNA as a company and individuals to look for opportunity, stay humble, and never settle. We try to hire people based on their trajectory, not their pedigree . We also strive to foster a culture of curiosity and continuous learning where team members are provided and proactively seek out opportunities to grow themselves and their careers. We believe that with the right expectations and direction, people can grow to take on new challenges and surpass expectations. Our definition of cross-functional optimization is that you do what is best for the organization as a whole. Don’t optimize for the goals of your team when it negatively impacts the goals of other teams, our users, and/or the company. Those goals are also your problem and your job. For example, you may have set a non-urgent functional milestone that is supposed to land at the end of the quarter. If delivering within the last week requires engagement from the GTM teams , the right decision may be to push your own team’s target by a week to reduce the ask for the GTM team as the GTM focuses on meeting its revenue objectives. In the context of collaboration , if anyone is blocked by you on a question, your approval, or a merge request review, you should prioritize unblocking them, either directly or through helping them find someone else who can. We refer to this as “persistence of purpose”. As talked about in The Influence Blog , tenacity is the ability to display commitment to what you believe in. You keep picking yourself up, dusting yourself off, and quickly get going again having learned a little more. We value the ability to maintain focus and motivation when work is tough and asking for help when needed. We expect team members to complete tasks that they are assigned. You are responsible for executing with attention to detail, connecting the dots across the organization and anticipating and solving problems. As an owner, you are responsible for overcoming challenges, not suppliers or other team members. Take initiative and proactively inform stakeholders when there is something you might not be able to solve. Time gained or lost has compounding effects. Try to get the results as fast as possible, but without compromising our other values and ways we communicate , so the compounding of results can begin and we can focus on the next improvement. It’s important that we keep our focus on action, and don’t fall into the trap of analysis paralysis or sticking to a slow, quiet path without risk. Decisions should be thoughtful, but delivering fast results requires the fearless acceptance of occasionally making mistakes; our bias for action also allows us to course correct quickly. Try to get results as fast as possible, but without compromising our other values and ways of working When a decision is in place, we expect people to commit to executing it. Any past decisions and guidelines are open to questioning as long as you act in accordance with them until they are changed. This is a common principle . Every decision can be changed; our best decision was one that changed an earlier one . In a manager-report relationship, usually the report is the Directly Responsible Individual (DRI). The manager may disagree with the final decision, but they still commit to the decision of the DRI. In a group setting, participants may disagree with a proposal but not articulate their views for one reason or another. Sometimes, many or all individuals may disagree yet choose not to speak up , because no one believes they would get agreement from the group. As a result, everyone loses out on their feedback. Dissent is expression of that disagreement. However, it can be difficult and even socially expensive. Expression of feedback is a way for everyone to grow and learn, and is based on facts rather than opinions . Share your perspective, rather than agreeing simply to avoid conflict or to go along with everyone else. When you want to reopen the conversation on something, show that your argument is informed by previous conversations and assume the decision was made with the best intent . You have to achieve results on every decision while it stands, even when you are trying to have it changed. You should communicate with the DRI who can change the decision instead of someone who can’t. If there is a disagreement and you can’t move forward because of it, agree to escalate and escalate to one or both of your managers. Early escalation, delivered with context of the challenge, enables managers to function as an unblocker. Competencies are the Single Source of Truth (SSoT) framework for things we need team members to learn. We demonstrate results when we do what we promised to each other, customers, users, and investors. At GitLab, efficiency means producing results without wasting materials, time, or energy. We optimize solutions globally for the broader GitLab community over one person or a small group. Focus on efficiency should be global in nature, not just local to a given function. Global efficiency could include efficiency with customers, candidates, and contributors as well. It is easy to prioritize consistency over efficiency because consistency is often more efficient initially and makes managing processes more efficient. We should slow down when optimizing for consistency. Taking a company-wide lens when evaluating changes will help ensure that new processes will improve efficiency for GitLab as a whole and be the best decision for the company as a whole. When we work internally with other team members, we leverage GitLab’s unique working practices and operating principles to achieve top efficiency. We do not expect people outside of GitLab to conform to GitLab’s ways of working, and we will make accommodations to work effectively with them. For example, we may collaborate heavily in-person and not default to async communications. Most companies regress to the mean and slow down over time. While some changes are required as a company grows and matures, not all change is inevitable or should be allowed to passively happen. As GitLab grows, we are conscious of how we operate and how it enables our ability to continue to operate with the agility of a startup . We try to limit ourselves to healthy constraints . We document everything: in the handbook, in meeting notes, in issues. We do that because “ the faintest pencil is better than the sharpest memory .” It is far more efficient to read a document at your convenience than to have to ask and explain. Having something in version control also lets everyone contribute suggestions to improve it. Use the simplest and most boring solution for a problem, and remember that “boring” should not be conflated with “bad” or “technical debt.” The speed of innovation for our organization and product is constrained by the total complexity we have added so far, so every little reduction in complexity helps. Don’t pick an interesting technology just to make your work more fun; using established, popular tech will ensure a more stable and more familiar experience for you and other contributors. Make a conscious effort to recognize the constraints of others within the team. For example, sales is hard because you are dependent on another organization, and development is hard because you have to preserve the ability to quickly improve the product in the future. Team members should first search for their own answers and, if an answer is not readily found or the answer is not clear, ask in public as we all should have a low level of shame . Write down any new information discovered and pay it forward so that those coming after will have better efficiency built on top of practicing collaboration, inclusion, and documenting the results. Team members have more room to grow themselves when they are able to self-service and self-learn. Optimize solutions globally for the broader GitLab community. As an example, it may be best to discard a renewal process that requires thousands of customers to each spend two hours in favor of one that only takes sixty seconds, even when it may make a monthly report less efficient internally! In a decision, ask yourself “For whom does this need to be most efficient?” Quite often, the answer may be your users, contributors, customers, or team members that are dependent upon your decision. Consider the time investment you are asking others to make with meetings and a permission process. Try to avoid meetings, and if one is necessary, try to make attendance optional for as many people as possible. Any meeting should have an agenda linked from the invite, and you should document the outcome. Instead of having people ask permission, trust their judgment and offer a consultation process if they have questions. Every dollar we spend will have to be earned back. Be as frugal with company money as you are with your own. In saying this, we ask team members to weigh the cost of purchases against the value that they will bring to the company. Consider the degree to which a purchase increases your ability to better accomplish your work and achieve business results relative to cost. Lowering overhead reduces the cost to operate the business and lets us shift spend toward other priority areas. We have guidelines around this operating principle to help team members better understand our expensing process and expectations. Amazon states it best with: “Accomplish more with less. Constraints breed resourcefulness, self-sufficiency, and invention. There are no extra points for growing headcount, budget size, or fixed expense.” Give short answers to verbal questions so the other party has the opportunity to ask more or move on. Keep one-to-many written communication short, as mentioned in this HBR study : “A majority say that what they read is frequently ineffective because it’s too long, poorly organized, unclear, filled with jargon, and imprecise.” We want each team member to be a manager of one who doesn’t need daily check-ins to achieve their goals. Team members are given the freedom to own projects and initiatives and are trusted to see them through to a successful end. When team members are managers of one they can have an increased work/life balance, because they are more empowered to make decisions around how they allocate their time throughout each day. When possible, we give people the responsibility to make a decision and hold them accountable for that, instead of imposing rules and approval processes. You should have clear objectives and the freedom to work on them as you see fit. Freedom and responsibility are more efficient than rigidly following a process, or creating interdependencies , because they enable faster decision velocity and higher rates of iteration . When team members have freedom and responsibility over rigidity, they have more room to help others. Not every problem should lead to a new process to prevent them. Additional processes make all actions more inefficient; a mistake only affects one. Once you have accepted the mistake, learn from it. When team members are free to accept mistakes, they can take more calculated risks. We value constant improvement by iterating quickly, month after month. If a task is not the smallest viable and valuable thing , cut the scope. Adoption of features, user requirements, and the competitive landscape change frequently and rapidly. The most successful companies adapt their roadmap and their organization quickly to keep pace. One of the things that makes this challenging is the impact on our team. People may need to change teams, subject matter, or even who manages them. This can rightly feel disruptive. If we coach ourselves to embrace the positive aspects of change, such as increased opportunity and new things to learn, we can move faster as a company and increase our odds of success. It is important to hold management accountable for being deliberate . Competencies are the Single Source of Truth (SSoT) framework for things we need team members to learn. We demonstrate efficiency when we work on the right things, not doing more than needed, and not duplicating work. Diversity, inclusion and belonging are fundamental to the success of GitLab. We aim to make a significant impact in our efforts to foster an environment where everyone can thrive. We are designing a multidimensional approach to ensure that GitLab is a place where people from every background and circumstance feel like they belong and can contribute. We actively chose to build a culture that is inclusive and supports all team members equally in the process of achieving their professional goals. We work to make everyone feel welcome and to increase the participation of underrepresented groups in our community and company. Take initiative to operate asynchronously whenever possible. This shows care and consideration for those who may not be in the same time zone, are traveling outside of their usual time zone, or are structuring their day around pressing commitments at home or in their community. This is demonstrated by communicating recordings of meetings , using GitLab Issues and Merge Requests rather than texts, calls, or Slack messages, and being sensitive to local holidays and vacation statuses. Encourage others to default to documentation rather than pressuring others to be online outside of their working hours. Part of embracing diversity is a willingness to embrace often uncomfortable conversations and situations. This concept is also at the core of inclusion and helping to eliminate the problems that are faced by certain GitLab team members who may not be in the majority. We believe that being willing to embrace discomfort is the path forward to a safe, balanced and inclusive work place for all. Challenge yourself, challenge your own pre-set notions and ideas about different cultures or things you don’t understand. When we are willing to embrace being uncomfortable, we can focus on actually fixing the issues at hand rather than simply “appearing to care”. Microaggressions are much more than merely rude or insensitive comments. They can wear people down by slowly chipping away their sense of belonging/safety/inclusion over time. What is a microaggression? “The everyday slights, indignities, put downs and insults that people of color, women, LGBT populations or those who are marginalized experiences in their day-to-day interactions with people.” - Derald W. Sue At GitLab we believe that everyone is entitled to a safe working space where they can express who they are and participate in conversations without worry of being spoken to in a harmful way, given that we want to encourage everyone to be mindful of what is a microaggression and be mindful of their potential impact. We believe that team members seeking feedback from a diverse group of team members, inside and outside of their group or function, leads to better decisions and a greater sense of team member belonging. For more guidance on how we define Diversity, please refer to GitLab’s definition of Diversity, Inclusion & Belonging . Feedback from a more heterogenous group often leads to better business outcomes as we incorporate diverse perspectives and uncover unconscious bias. An example of this operating principle in action showcases the value of actively seeking diverse perspectives. The term “Brag Document” was used to describe when individuals documented their accomplishments. Documenting accomplishments is critical to team member development. However, team members had the psychological safety to raise the question of whether or not the title of the document made some feel uncomfortable. In an effort to seek a diverse perspective , a survey was conducted in one of the Team Member and Advocacy Resource Group (TMRG) channels. The poll results showed that 100% of those polled preferred a different title and the title was changed. One of the unique elements to an all-remote culture is the ability to visit a person’s home while collaborating. If the tenor of the meeting allows, feel welcome to invite your family members or pets to drop by and greet your colleagues. Be mindful of language and use of profanity to encourage a family-friendly environment. Caregiving, outreach programs, and community service do not conveniently wait for regular business hours to conclude. If there’s a cause or community effort taking place, feel welcome to work with your manager and shift your working hours to be available during a period where you’ll have the greatest impact for good. For colleagues supporting others during these causes, document everything and strive to post recordings so it’s easy for them to catch up. People feel more included when they’re supported. To encourage this, and to support diversified learning across departments, consider GitLab’s Internship for Learning program. We don’t hire based on culture or select candidates because we’d like to have a drink with them. We hire and reward team members based on our shared values as detailed on this page. We want a values fit , not a culture fit. We want cultural diversity instead of cultural conformity. Said differently: “culture add” > “culture fit” or “hire for culture contribution” since our mission is that everyone can contribute . We generally avoid discussing politics or religion in public forums because it is easy to alienate people that have a minority opinion. This doesn’t mean we never discuss these topics. Because we value diversity, inclusion and belonging, and want all team members to feel welcome and contribute equally, we encourage free discussion of operational decisions that can move us toward being a more inclusive company. There is sometimes a grey area where advocating for diversity and political activities may intersect. Team members should use discretion in grey area communications, because a culture of belonging requires us to be respectful of the broad spectrum of views within our work environment. What does this mean in practice? Please feel empowered to share information that highlights diversity, inclusion and belonging issues and how GitLab and GitLab team members can get involved. In line with our Code of Business Conduct and Ethics , avoid posting articles that reference specific political figures or parties. While it is acceptable for individuals to bring up politics and religion in social contexts such as coffee chats and real-life meetups with other coworkers (with the goal to understand and not judge), always be aware of potential sensitivities, exercise your best judgment, and make sure you stay within the boundaries of our Code of Business Conduct and Ethics . We’re a global company where perspectives and local norms may differ from culture to culture. Diversity, inclusion and belonging is about broad inclusion at a worldwide level. If there is a question or concern, please reach out to diversityinclusion@gitlab.com or #diversity_inclusion_and_belonging . Unexpected and unconventional things make life more interesting. Celebrate and encourage quirky gifts, habits, behavior, and points of view. Open source is a great way to interact with interesting people. We try to hire people who think work is a great way to express themselves. Do not make jokes or unfriendly remarks about characteristics of the people who make up GitLab and how they identify . Everyone has the right to feel safe when working for GitLab and/or as a part of the GitLab community. We do not tolerate abuse, harassment , exclusion, discrimination, or retaliation by/of any community members, including our team members. You can always refuse to deal with people who treat you badly and get out of situations that make you feel uncomfortable. We recognize that unconscious bias is something that affects everyone and that the effect it has on us as humans and our company is large. We are responsible for understanding our own implicit biases and helping others understand theirs. We are continuously working on getting better at this topic . We list our Parental Leave publicly so people don’t have to ask during interviews. Use inclusive language. For example, prefer “Hi everybody” or “Hi people” to “Hi guys”, and “they” instead of “he/she”. While there are several good guides from folks like Buffer , APA and The Northern Ireland Civil Services on using inclusive language, we don’t keep an exhaustive list. When new possibly non-inclusive words arise, we prefer to be proactive and look for an alternative. If your goal is to be inclusive, it is more effective to make a small adjustment in the vocabulary when some people have a problem with it, rather than making a decision to not change it because some people don’t think it is a problem. And if you make a mistake (e.g. accidentally using the wrong pronoun or an outdated phrase), acknowledge it, apologize gracefully and move on ; there is no need to dwell on it, and you can work to avoid making that mistake in the future. Please also visit our Gender and Sexual-orientation Identity Definitions and FAQ page if you have questions around pronouns and other topics related to gender / sexual orientation. We attach part of our identity to our names, and if it is mispronounced it can feel less inclusive. If it happens repeatedly, you may be unintentionally sending a message to that person that you are not interested in learning how to pronounce their name correctly. This applies to everyone you are in contact with: team members, customers, candidates for jobs, and anyone else. People whose name is repeatedly mispronounced might feel unimportant or self-conscious, and might not speak up about it. Other negative behaviors include giving a person a nickname without their permission, or actively avoiding using their name in sync calls. It might be challenging to pronounce names from a different language or culture than your own, but with some effort, name pronunciation can be learned by anyone. Some ways to achieve this are: Some people might choose to use a nickname, for example: “Bob” instead of “Robert”. As long as this is their choice this is perfectly acceptable. We should avoid assigning a nickname to a person without their permission. Slack has two features to help with this issue: the phonetic name pronunciation field and the ability to record your own name pronunciation audio clip. We encourage all team members to complete both of these. Update them by editing your profile . This is documented on our page about interviewing . Be consciously inclusive in meetings by giving everyone present an opportunity to talk and present their points of view. This can be especially important in a remote setting. With internal meetings, consider using an agenda document for questions. For example, with GitLab AMAs , every meeting has a numbered list that GitLab team members can add questions to. During the meeting, questions are answered in turn and discussions noted in the same document. Sometimes, these documents can have so much traffic (during the meeting) such that only a limited number of people can edit the document. In these situations, those who have questions should post on zoom chat and those who can edit the document should help copy the question over to the document. In addition, those who can edit the document should also post in zoom chat to see if anyone has any questions that they could help add to the document so that meeting attendees are more empowered to contribute to the conversation. Customers are not used to working in this way. To promote inclusion with customers: ask participants for their goals; make sure during demos that you pause for question; leave time for discussion. Being globally distributed has the benefit that someone can cover for you when you are off work. However, population density is not balanced across timezones. Policies should remain fair to those in less dense regions. For example, the Asia Pacific region covers more timezones but has fewer team members. If we use an algorithm to assign tasks to those in later timezones, all American tasks would fall on the fewer Asia Pacific employees. This can damage belonging and inclusivity and should be avoided. When planning an event, the organizer should cater for location density differences to maximize participation in all regions. As a globally-dispersed company, we have team members from many different backgrounds and cultures. That means it is important for each of us to use great judgment in being respectful and inclusive of our teammates. At the same time, we may sometimes not fully realize we have said or done something to offend someone. It is important that our teammates hold each other accountable and let them know if they have unintentionally or intentionally done something so they can learn and gain additional understanding of perspectives different from our own. It is also important that our teammates don’t feel excluded or minimized by the words we use or the things we do. Thus, we all need to speak up when we see something that isn’t respectful or inclusive. Neurodiversity refers to variations in the human brain regarding learning, attention, sociability, mood, and other mental functions. There are various neurodevelopmental conditions, like autism, ADHD, dyslexia, dyscalculia, dyspraxia, cognitive impairment, schizophrenia, bipolarity, and other styles of neurodivergent functioning. While neurodivergent individuals often bring unique skills and abilities which can be harnessed for a competitive advantage in many fields (for example, cybersecurity ), neurodivergent individuals are often discriminated against. Due to non-inclusive hiring practices, they sometimes have trouble making it through traditional hiring processes. Neurodiversity inclusion best practices benefit everyone, and at GitLab, everyone can contribute. The handbook, values, strategy, and interviewing processes must support the ability for everyone to thrive. At GitLab we embrace Neurodiversity through adopting a variety of different work styles and communication styles, and we lean into transparency , asynchronous as a default working style, and pre-filled meeting agendas. These best practices become even more important when embracing neurodiversity. Providing multiple ways to consume information (written / video / audio) allows everyone to contribute independent of their preferred comprehension style. It is important to ask team members specifically what their preferred communication method is in order to provide them information in a format that is easily consumable for them. Remember, brains work differently and always assume positive intent , even if someone behaves in an unexpected way. While it may be an unexpected behavior to you, it may not be unexpected to the individual exhibiting the behavior. That is the beauty and value of diversity, embracing differences and becoming stronger and better as a result. We also recommend that all team members review the Reasonable Accommodation process. A Reasonable Accommodation for a team member could include noise-cancelling headphones, scheduling smaller group session zoom calls, providing very explicit and precise instructions and due-dates when given tasks, or providing a variety of supportive software tools. The most important thing that managers can do is facilitate an environment in which all team members feel psychologically safe enough to make requests for what they need in order to do their job. Long-lasting relationships are the rocks of life , and come before work. As someone said in our #thanks channel after helping a family member for five days after a hurricane: “THANK YOU to GitLab for providing a culture where “family first” is truly meant”. Use the hashtag: #FamilyAndFriends1st Equity vs. Equality: What’s the Difference? While the terms equity and equality may sound similar, the implementation of one versus the other can lead to dramatically different outcomes for marginalized people. Equality means each individual or group of people is given the same resources or opportunities. Equity recognizes that each person has different circumstances and allocates the exact resources and opportunities needed to reach an equal outcome. Competencies are the Single Source of Truth (SSoT) framework for things we need team members to learn. We demonstrate diversity, inclusion and belongings when we foster an environment where everyone can thrive and ensuring that GitLab is a place where people from every background and circumstance feel like they belong and can contribute. If you would like to improve your skills or expand your knowledge on topics relating to Diversity, Inclusion, & Belonging at GitLab, check out our resources: Merriam-Webster defines iteration as the “the action or a process of iterating or repeating: such as a procedure in which repetition of a sequence of operations yields results successively closer to a desired result.” At GitLab, we iterate to do the smallest valuable thing to get fast feedback and efficiently reach a desired end goal . Feedback can be from internal users (dogfooding), a limited number of external users (through our early access program ), or through feedback from our broader user community. We validate each iteration and adjust, but not at the expense of the user experience that we deliver to our customers. When we iterate at GitLab, we break up the work that we know we need to do into smaller chunks to iterate toward a targeted end state: Iteration does not require us to ship features that are open to all users from day one. Feedback can come from internal users or a limited number of external users (early access program). Moving through the release process is not iteration though. Iteration is also not a replacement for having a plan . We expect you to know where you are going, but you can iterate to get there. An iteration might be additive (adding something) or subtractive (removing something). If you make suggestions that can be excluded from the first iteration, turn them into a separate issue that you link. While you should have a clear vision of the desired outcome and how it addresses a customer pain point or improves the user experience, be efficient in your planning. Unless you identify important cross-functional interdependencies, focus detailed planning on the first step. It might feel you are moving too slowly; however, planning is critical in order to ensure you can move fast when implementing. You’re doing it right if you feel that you have shipped the minimal feature set in the first iteration. This value is the one people most underestimate when they join GitLab. The impact, both on your work process and on how much you achieve, is greater than anticipated. Frequently, the simplest version that provides value turns out to be the best one. Many people who join GitLab say they already practice iteration. But this is the value that is the hardest to understand and adopt. People are trained that if you don’t deliver a perfect or polished thing, there will be a problem. If you do just one piece of something, you have to come back to it. Doing the whole thing seems more efficient, even though it isn’t. If the complete picture is not clear, your work might not be perceived as you want it to be perceived. It seems better to make a comprehensive product. They see other GitLab team members being really effective with iteration but don’t know how to make the transition, and it’s hard to shake the fear that constant iteration can lead to shipping lower-quality work or a worse product. In reality, it is possible to ship a minimally valuable product while continuing to adhere to the documented quality standards. The way to resolve this is to write down only what value you can add with the time you have for this project right now. That might be 5 minutes or 2 hours. Think of what you can complete in that time that would improve the current situation. Iteration can be uncomfortable, even painful. If you’re doing iteration correctly, it should be. Reverting work back to a previous state is positive, not negative. We’re quickly getting feedback and learning from it. Making a small change prevented a bigger revert and made it easier to revert. However, if we take smaller steps and ship smaller, simpler features, we get feedback sooner. Instead of spending time working on the wrong feature or going in the wrong direction, we can ship the smallest product, receive fast feedback, and course correct. People might ask why something was not perfect. In that case, mention that it was an iteration, you spent only “x” amount of time on it, and that the next iteration will contain “y” and be ready on “z”. Iteration enables results and efficiency . In the GitLab Unfiltered video embedded above, GitLab co-founder Sid Sijbrandij shares key operating principles to reinforce iteration in an organization. Iteration involves driving results in pursuit of a long-term vision. While the intermediate goals may change as we iterate, we are unlikely to be successful if we don’t start with a vision of what we are working toward. Shipping that vision in iterations allows us to learn from customers using it and adjust the vision if needed. Iteration for the sake of iteration can lead to inefficiencies and not deliver desired results. Iteration without a plan can lead to inefficiencies and a subpar customer experience. Before iterating we need to plan. A plan should include: Moving through the release process is not iteration. The release process can include: While development stages can be used to indicate release progress, is not itself iteration. If we are not aware of interdependencies beyond our team, and we are not collaborating with others across the organization, we risk deliverables that settle into a “local maximum” of quality, richness, and efficiency. This localization is largely defined by team structure and organizational boundaries. While an iteration can take place within a single team, that team is responsible for identifying inter-dependencies and proactively communicating and aligning with other teams working on related projects. This helps ensure that iterations are not “half-baked” and align with work being done across the entire organization. Don’t wait on the small things. When you have something of value like a potential blog post or a small fix, implement it straight away. Right now, everything is fresh in your head and you have the motivation. Inspiration is perishable. Don’t wait until you have a better version. Don’t wait until you record a better video. Don’t wait for an event (like GitLab Summit ). Inventory that isn’t released is a liability since it has to be managed, becomes outdated, and you miss out on the feedback you would have received had you implemented it straight away. When we don’t wait we signal intent to others that we have a purpose to resolve something. Note : “Don’t wait” should not be used as a justification for not iterating toward the global maximum or at expense of the plan. If there are interdependencies to be considered or the iteration is customer facing, slow down and ensure that we are considering what is best for GitLab and our customers. We always try to set a due date. If needed, we cut scope. If we have something planned for a specific date, we make that date. For example we shipped over 133 monthly releases . But every one of them doesn’t contain all the features we planned. If we planned an announcement for a certain date, we might announce less or indicate what is still uncertain. But we set a due date because having something out there builds trust and gives us better feedback. As discussed in Sid’s interview on iteration , waiting for approval can slow things down. We can prevent this with automation (such as tests of database migration performance) or clean-up after the fact (refactor a Pajamas if something was added that isn’t coherent), but we try to ensure that people don’t need to wait for sign-off. As iteration does not require us to ship to all users on day one, we can clean up after an internal or beta release to mitigate the negative impact to all customers. Iteration does not mean being open to all users from day one. If you do a gradual rollout of your change, prefer: Short iterations reduce our cycle time . Merging frequently also prevents merge conflicts. Small iterations make it easier to work with the wider community. Their work looks more like our work, and our work is also quicker to receive feedback. We encourage MVCs to be as small as possible. Always look to make the quickest change possible to improve the user’s outcome. If you validate that the change adds more value than what is there now, then do it. This may be additive (adding something) or subtractive (removing something). No need to wait for something more robust. More information is in the product handbook , but this applies to everything we do in all functions. Specifically for product MVCs, there is additional responsibility to validate with customers that we’re adding useful functionality without obvious bugs or usability issues. If you need to decide something as a team, make a concrete proposal instead of calling a meeting to get everyone’s input. Having a proposal will be a much more effective use of everyone’s time. Every meeting should be a review of a proposal. We should be brainwriting on our own instead of brainstorming out loud . State the underlying problem so that people have enough context to propose reasonable alternatives. The people that receive the proposal should not feel left out and the person making it should not feel bad if a completely different proposal is implemented. Don’t let your desire to be involved early or to see your solution implemented stand in the way of getting to the best outcome. If you don’t have a proposal, don’t let that stop you from highlighting a problem, but please state that you couldn’t think of a good solution and list any solutions you considered. By making a proposal you also provide better visibility into the work and the context surrounding it. In this GitLab Unfiltered video , GitLab co-founder Sid Sijbrandij converses about iteration in engineering, leveraging proposals to break work into smaller components. At GitLab, we rarely mark any content or proposals as drafts. Everything is always in draft and subject to change. When everything is in draft, contributions from team members as well as the wider community are welcomed. By having everything in draft and assuming others have low context , confusion can be reduced as people have shared access to information. As we continue to expand the number of users we have, they will continue to expect stability and reliability. We must optimize for the long term without sacrificing stability along the way. This means that users may be inconvenienced in the short term, but current and future users will enjoy a better product in the end. Educating users on the longer-term plan helps create a shared understanding of how a small change will incrementally grow into something more. For example, we could share how a dropdown will evolve into a much more nuanced solution in the future. We can take the following steps to articulate our plan: In many organizations, you take a risk when you put forth any work that’s not perfect, work where you haven’t spent endless cycles planning for contingencies or counterpoints. Because of this, you’re incentivized to invest a lot of time and effort into preparing for ‘What if?’ scenarios before any work is presented, even if the release is not customer facing and there is a low level of risk in imperfection. The downside to that is clear when we are dogfooding: If you do eventually put forth the work, but it needed to be course-corrected a long time ago, you’ve squandered time that you could have spent improving it through iteration. Having a low level of shame when dogfooding or working internally requires you to combat a natural inclination to conceal work until it’s perfect, and instead celebrate the small changes. Cultural differences can bring unique challenges and expectations to iteration. For some, expressions like “it doesn’t have to be perfect...” can challenge cultural norms. We encourage you to bring your authentic self and seek shared understanding when iterating. Giving feedback and ensuring psychological safety are necessary for every iterative attempt. We believe great companies sound negative because they focus on what they can improve, not only on what is working well. In every conversation, inside and outside the company, we should ask a question: What do you think we can improve? This doesn’t mean we don’t recognize our successes; for example, see our Say Thanks value. We are positive about the future of the company. We are S hort Te rm C ritical A nd L ong T erm O ptimistic ( STeCALTO , for short). First, optimize for speed and results (and be deliberate about how your change affects other processes/functionality); when it is a success, figure out how to scale it. Great examples are in this article by Paul Graham . Resist the urge to bundle a series of smaller iterations so team members don’t see a project as their last (or best) opportunity to contribute. It’s tempting to create encompassing projects or initiatives that roll many smaller projects up. This incarnation of scope creep drives up cost, encourages fewer risks, and incentivizes perfection (via longer cycle times) over progress. When we resist bundling, we reduce the risk that work will be canceled due to scale or scope. By resisting bundling we also reduce the coordination needed because fewer people or teams may be involved. Most decisions are easy to reverse. In these cases, the Directly Responsible Individual should go ahead and make them without approval. Only when you can’t reverse them should there be a more thorough discussion. By embracing iteration and making two-way door decisions, we are more efficient and achieve more results. Changing something without shipping it is a revision, not iteration. Only when the change is rolled out to users, whether internal users or a limited customer group , can you learn from feedback. When you’re changing a proposal based on different opinions, you’re frequently wasting time; it would be better to roll out a small change quickly and get real world feedback. Never call a revision an iteration because it is almost the opposite. In order to embrace iteration, we should have the attitude that we are trying to achieve as much as possible in a small amount of time; it’s where we land at the end state of an iteration that counts. The benefit of iteration is to get fast feedback from users. Focus on sharing context at the end of the first iteration rather than a hypothetical future state that requires multiple iterations. By embracing iteration we can increase creativity in incremental components. When you are submitting a merge request for a code change, or a process change in the handbook, keep it as small as possible. If you are adding a new page to the handbook, create the new page with a small amount of initial content, get it merged quickly via Handbook Usage guidelines , and then add additional sections iteratively with subsequent merge requests. Similarly, when adding features to GitLab, consider ways to reduce the scope of the feature before creating the merge request to ensure your merge request is as small as possible. Rapid iteration can get in the way of results if it’s not thought out; for example, when adjusting our marketing messaging (where consistency is key), product categories (where we’ve set development plans), organizational structure or product scope alignment (where real human stresses and team stability are involved), sales methodologies (where we’ve trained our teams) and this values page (where we use the values to guide all GitLab team members). In those instances, we add additional review to the approval process; not to prohibit, but to be more deliberate in our iteration. The change process is documented in the GitLab Handbook Usage page and takes place via merge request approvals. Iteration is often counterintuitive and difficult to do. To clarify what an iteration is, it helps to see examples of what is not an iteration. Below are 12 examples of things we’ve seen mistaken as iteration, but don’t meet our definition of iteration. In this GitLab Unfiltered video , GitLab co-founder Sid Sijbrandij elaborates on each of these 12 things that are not iteration. Competencies are the Single Source of Truth (SSoT) framework for things we need team members to learn. We demonstrate iteration when we do the smallest viable and valuable thing, get it out quickly for feedback, and make changes based that feedback. Be open about as many things as possible. By making information public, we can reduce the threshold to contribution and make collaboration easier. Use public issue trackers, projects, and repositories when possible. Transparency is not communication. Just because something exists in the handbook or elsewhere doesn’t mean it can’t be communicated again or in a more robust fashion to the people who need to understand or acknowledge it. On a personal level, be direct when sharing information, and admit when you’ve made a mistake or were wrong. When something goes wrong, it is a great opportunity to say “What’s the kaizen moment here?” and find a better way without hurt feelings. Even as a public company , we know that our value of transparency will be key to our success. This value can be hard to follow at times. You might ask yourself: what should be shared, how much to share, whether or not to speak up but definitely take the time to always opt for maximum transparency by adhering to the operating principles below. Often, company values get diluted as they grow, most likely because they do not write anything down. But we will make sure our values scale with the company. As a public company , we declare everyone in the company as an insider, which allows us to remain transparent internally about our numbers, etc. Everything else that can be transparent will continue to be so. When there are exceptions, material that is not public by default is documented . Everything at GitLab is public by default. The public process does two things: allows others to benefit from the conversation and acts as a filter. Since there is only a limited amount of time, we prioritize conversations that a wider audience can benefit from. One example of transparency at GitLab is the public repository of this website that also contains this company handbook . Others include the GitLab CE and GitLab EE issue trackers, as well as marketing and infrastructure . Transparency creates awareness for GitLab, allows us to recruit people that care about our values, gets us more and faster feedback from people outside the company, and makes it easier to collaborate with them. It is also about sharing great software, documentation, examples, lessons, and processes with the whole community and the world in the spirit of open source, which we believe creates more value than it captures. In line with our value of transparency and being public by default, all GitLab team member profiles should be public. Public profiles also enable broader collaboration and efficiencies between teams. To do so, please make sure that the checkbox under the Private profile option is unchecked in your profile settings . If you do not feel comfortable with your full name or location on your profile, please change it to what feels appropriate to you as these are displayed even on private profiles. Because we are public by default and have the SAFE framework we don’t need to make cases for why things should be transparent. If something is unSAFE and needs to remain not public it can be. We make information public by default because transparency is one of our values . However it is most important to focus on results . Therefore, a category of information is public unless there is a reason for it not to be. If something is not public, there should be a reference in the handbook that states a confidential decision was taken with a link to our Not Public guidelines, unless GitLab Legal and Corporate Affairs believes it carries undue risk. We document what is not public by default on our communication page. If you believe something shouldn’t be public that currently is (or vice versa), then make a merge request to the relevant page(s) suggesting the change so that you can collaborate with others and discuss with the DRI . When content contains information which is not public it is recommended to remove the specific sections which are not public, put them on their own page in the internal handbook, and then link out to that with a “not public/internal only” note. Always share publicly what we can. When information is not public, it may also be treated as limited access, only shared with certain GitLab roles, teams, or team members due to privacy considerations, contractual obligation, or other reasons that the author or DRI can specify. Certain kinds of information default to limited access, including details about team members or customers who did not give permission to share the information. Most companies become non-transparent over time because they don’t accept any mistakes. Instead, we should always err on the side of transparency when there is a choice to be made between caution or inaction, and transparency. If we make a mistake, we now know what the limits of transparency are for the company and we should document this . The only exception to this rule would be in the case when there are legal concerns. Because some information is not public the public information can be lacking some context. We should be cognizant of that. Being direct is about being transparent with each other. We try to channel our inner Ben Horowitz by being both straightforward and kind . Feedback is always about your work and not your person. That doesn’t mean it will be easy to give or receive it. If you state one thing, and then change course and support a different direction, point, or outcome, articulate this. It is OK to have your position changed by new data. Articulating that an earlier stance is not your current stance provides clarity to others and encourages data-driven decision making. Be transparent to the right people (up) at the right time (when still actionable). If you make a mistake, don’t worry; correct it and proactively let the affected party, your team, and the CEO know what happened, how you corrected it, and how—if needed—you changed the process to prevent future mistakes. We practice transparency even when hiding the facts would be easier. For example, many companies do not give you the real reason why they declined your application because it increases the chance of legal action. We want to only reject people for the right reasons and we want to give them the opportunity to grow by getting this feedback. Therefore, we’ll accept the increased risk of holding ourselves to a high standard of making decisions and do the right thing by telling them what we thought. Other examples are being transparent about security incidents and participating in and contributing to Live Broadcasts. Transparency has costs (distraction, mis-interpretation, etc.) but also great benefits (productivity, hiring, retention, brand awareness, etc). We should carefully weigh the tradeoff between costs and benefits, to prevent a knee-jerk reaction to reduce transparency when it has costs. By having most company communications and work artifacts be public to the Internet, we have one single source of truth for all GitLab team members, users, customers, and other community members. We don’t need separate artifacts with different permissions for different people. Our transparency value means more than just making information accessible to all. In order to improve performance it’s important that we not only ensure information is accessible, but also ensure it flows to the correct places and is findable by those who need it. Focusing on information flow will ensure you, for example, utilize multi-modal communication , or that you keep your stakeholders informed of changes by posting links to MRs in Slack. Transparent changes have the reasons for the change laid out clearly along with the change itself. This leads to fewer questions later on because people already have some understanding. A change with no public explanation can lead to a lot of extra rounds of questioning, which is less efficient. This also helps with institutional memory: a year from now when you want to know why a decision was made, or not, the issue or MR that has the decision also shares why the decision was made. This is related to Chesterton’s fence - it’s much easier to suggest removing or changing something if you know why it exists in the first place. If you use generalized terms such as “industry standard” or “best practices,” be sure to give context, as without context they can be seen as potentially vague or opaque. Similarly, merely stating a single value isn’t a great explanation for why we are making a particular decision. Many things could be considered “iteration” or “efficiency” that don’t match our definition of those values. Try to link to an operating principle of the value or provide more context, instead of just saying a single value’s name. Saying why and not just what enables discussion around topics that may impact more than one value; for instance, when weighing the efficiency of boring solutions with the focus on customer results . When decisions align with all of our values, they are easy to discuss and decide. When there are multiple values involved, using our values hierarchy and directly discussing the tradeoffs is easier with more context. Articulating why also helps people understand how something changed when you articulate that you changed your mind . Saying why does not mean justifying a decision against all other suggestions. The DRI is responsible for their decision. The DRI is not responsible for convincing other people, but they should be able to articulate their reasoning for the change. When a GitLab Team Member comes across an ask or material (MR, handbook, etc.) that does not provide a “why” with sufficient context, the Team Member is responsible for getting the why and, if needed, working with the DRI to ensure that it is adequately documented and communicated to give context to other team members. In the absence of a why, team members may speculate the why. This is something that can lead to disruption and inefficiency. Enable everybody involved to come to the same conclusion as you. This not only involves reasoning , but also providing, for example: raw data and not just plots; scripts to automate tasks and not just the work they have done; and documenting steps while analyzing a problem. Do your best to make the line of thinking transparent to others, even if they may disagree . Competencies are the Single Source of Truth (SSoT) framework for things we need team members to learn. We demonstrate transparency when we are open with as many things as possible reducing the threshold to contribution and make collaboration easier. Our values provide guidelines on how to behave and are written to be actionable. They help us describe the type of behavior that we expect from GitLab team members. They help us to know how to behave in the organization and what to expect from others. Values provide a framework for distributed decision making, detailed in GitLab’s TeamOps management philosophy. They allow individuals to determine what to do without asking their manager and they allow teams to make consistent decisions. When teams across the organization reference the same values in their decision making, there is consistency in how decisions are made. This ensures that our culture remains driven by our values. Lastly, values create a conscious culture that is designed to help you prosper and experience exceptional personal growth through work. Our values also help us to prevent the five dysfunctions : Some dysfunctions are not addressed directly by our values; for example, trust is not one of our values. Similar to happiness, trust is something that is an outcome, not something you can strive for directly. We hope that the way we work and our values will instill trust, instead of mandating it from people; trust is earned, not given. Operating principles are behaviors that empower GitLab team members to definitively live out a given value. They clarify what a given core value means and looks like at GitLab . Understanding this distinction is critical to thriving at GitLab, particularly for newer team members who may be familiar with a prior organization’s interpretation of iteration or collaboration (as examples). Values are not just things we do, but things that actively drive good behavior. When we remove them it doesn’t mean we stopped believing in it, just that it wasn’t actively helping to drive behavior. If we don’t prune our operating principles, then we will be like every other company: things that make sense but are not leading to a better culture. Most companies have a list of values. In companies without strong values, folks often use generalizations when they refer to values. For example, “not a value add” or “scored well on values during our interview.” In companies with strong values, folks name the specific, relevant value as it applies to a given topic or situation. Values are only powerful when they are individually understood and applied by team members. For certain business decisions or projects (such as compensation and end-point management ), GitLab team members may have a lot of opinions and interest, and they want to provide their feedback and comments. On the other hand, it might be challenging for the project DRI to digest and respond to all these inputs. What should you do in this scenario? Everyone can contribute at GitLab. We encourage team members to share feedback and leave comments on issues. Leaving feedback and comments shows that team members care about a topic and about GitLab as a company. These perspectives may also uncover potential risks and problems in the project. There shouldn’t be a “Don’t they have their job to do?” type of response. Furthermore, we shouldn’t judge team members who are perceived as being the “squeaky wheel.” At GitLab, we measure impact, not activity . As long as a team member is producing required results, they are empowered to decide how to spend their time. On the other hand, as GitLab grows in size, we need to make decisions and the decisions may not be agreed to by everyone. If a decision or project is sensitive or controversial, and receives large amounts of feedback, it can be challenging for the project DRI to handle. In these cases, it’s best to have time-boxed feedback built into timelines. In a hypothetical example where a DRI needs to decide between red and gold potatoes for a stew, they would create an issue with the following sentiment: We’re deciding between red potatoes and gold potatoes to go into the stew. We have to decide by Tuesday 2020-07-14 so that we can get our order to the grocery store on Wednesday 2020-07-15. We’ll be collecting input and feedback until that point. Jane is the DRI and will make the decision on 2020-07-14 with all the information we have at that point. Here is the framework we’re using for the decision: Once the decision is made, it will be what is going into the stew. This method has shown itself to be effective at soliciting productive feedback that doesn’t derail a timeline while ensuring team members feel heard. Companies are encouraged to copy and implement GitLab’s values. They are Creative Commons and can be copied verbatim. We make our values public for many reasons . There is great power and efficiency in teams who share company values. Concealing values until after someone is hired into an organization is not a wise strategy. Not everyone will see our values and feel aligned with them, and that’s OK. By making values public, it shows respect for the time of job seekers who conduct due diligence on prospective employers. When people who are aligned with GitLab’s values apply for an open vacancy , this allows our hiring teams to more efficiently move candidates through the interview process . Companies may ask you to write a blank check. They’ll say, ‘Come join our organization, and when you’re here, you need to subscribe to our values, our way of working, and our strategy. It’s very essential, and it’s part of our identity!’ But these companies don’t give you the opportunity up front to evaluate it. It doesn’t make any sense to me. If it’s so important that people share your values, have them out there. Occasionally, values can contradict each other. It’s useful to keep in mind this hierarchy to resolve confusion about what to do in a specific circumstance, while remaining consistent with our core values. Think of the hierarchy as a weighting system. Values higher in the hierarchy do not automatically override values lower in the hierarchy. Here are some examples: It’s an attempt to relieve at least some of the tension. It’s not absolute. If you think of values as binary, that’s not going to work. There will always be interpretation, and there’s always magnitude to consider. We made a hierarchy so that it’s clear, in the end, the result matters most. For instance, we’re not going to be transparent for the sake of being transparent. We’re not radical in our transparency. We do it because we think it will lead to better outcomes. Those hierarchies are really important. They won’t preempt every debate, but it helps. Our values are updated frequently and as needed. Everyone is welcome to make a suggestion to improve them. To update: make a merge request and assign it to the CEO. If you’re a team member or in the core team please post a link to the MR in the #values Slack channel . If you’re not part of those groups, please send a direct X/Twitter message to @sytses . Whatever behavior you reward will become your values. We reinforce our values by: Criteria we use for promotions and communicate to the whole company on announcement. What we select for during hiring . What we emphasize during on-boarding . Criteria we use for our annual compensation review . What we refer to when making decisions . The example the E-group sets for the company since a fish rots from the head down . What we expect from all team members, as ambassadors for our values . Keeping them up to date with a stream of commits that add details . Behavior we give each other 360 feedback on. Criteria we use for discretionary bonuses . What we include in our offer letters Criteria we use to manage underperformance . What we do when we let people go . Giving value awards during GitLab Summit . Providing GitLab team members and qualified individuals transparency into all aspects of the company through the CEO Shadow Program to enable them to better engage and collaborate cross-functionally. Linking the takeaways of courses to our values, like we did for the Crucial Conversations training . The default settings of the software we use (for example: Speedy meetings , document sharing , agendas, etc.) Reinforcing our values with features in GitLab, for example the Iterations feature . Applying one of our values virtual backgrounds in video calls. Our GitLab Song Book , the song lyrics often mention GitLab values. Regularly conduct a values exercise at the e-group offsite . The most important moments to reinforce our values are decisions which affect individual team members most: hiring, promotions, and bonuses, which is why every promotion document at GitLab is shared with the entire company and uses the values as its core structure. In negative feedback, we should be specific about what the problem is. For example, saying someone is “ not living the values ” isn’t helpful. Your values are what you hire for, what you praise people for, and what you promote them for. By definition, what you do in those instances are your values. It’s not what you say they are. Values should be explicitly part of our hiring process, our job profiles, and our review process . When we give bonuses and promotions , they are always linked to values. That’s the crucial thing. If you reinforce them there, that’s the most powerful thing you can do. — Sid Sijbrandij, GitLab co-founder Value erosion can occur when indifference and apathy are tolerated. It can also occur when individuals justify undesired behaviors by interpreting values as “me values” rather than “company values.” For example, a team member may speak to the importance of personal efficiency in order to justify not collaborating professionally with peers. This is not what we expect from team members in terms of efficiency and collaboration. If you feel that values are not being lived out in a given scenario, speak up and ask for context in a respectful manner. Navigating value conflicts starts with assuming positive intent from other team members. Offer links to relevant values and/or operating principles when discussing the issue. If there is confusion or disagreement about the interpretation of a value, please surface the discussion in GitLab’s #values Slack channel (for GitLab team members) or @-mentioning @gitlab on X/Twitter (for those who do not work at GitLab). Almost every time we face a hard decision at GitLab, it’s because values are in conflict. It’s not binary logic. It requires conversation, and sometimes there is no obvious answer. We can only achieve resolution by respectfully talking with each other and trusting the DRI to make the ultimate decision. From our values we excluded some behaviors that are obvious; we call them our permission to play behavior: We don’t want people to play politics at GitLab. An example of politics is people discussing a proposal and being overly focused on whose proposal it is. This is a manifestation of the Belief Bias , where we judge an argument’s strength not by how strongly it supports the conclusion but by how strongly we support the conclusion. Proposals should be weighed on their merits and not on who proposed them. Another example is people being promoted based on others liking them or having a lot of alliances. We want people to be promoted based on their results. We value collaboration, but that’s different from being promoted just because people like you. Below are some attributes of political and non-political work environments. GitLab plans to maintain a non-political one. Values make and clarify choices. A well-chosen value has a defensible opposite. Apple, for example, values secrecy over transparency and product perfection over iteration. They are successful building around our counter values — although the result is a very different company. During every GitLab 101 session with new hires we discuss our values. We document the questions and answers to Frequently Asked Questions about the GitLab Culture . New team members should read GitLab’s guide to starting a new remote role , and reference interviews centered on values within the GitLab Unfiltered YouTube channel . Our mission is to enable everyone to contribute to and co-create the software that powers our world . This mission guides our path, and we live our values along that path. We have a page which documents our Mitigating Concerns . Many of our values help to mitigate some of these concerns.",
  "We’re an all-remote company that allows people to work from almost anywhere in the world . We hire great people regardless of where they live, but with GitLab team members across more than 60 countries , it’s important for us to practice clear communication in ways that help us stay connected and work more efficiently. To accomplish this, we use asynchronous communication as a starting point and stay as open and transparent as we can by communicating through public issues, merge requests , and Slack channels . We also place an emphasis on ensuring that conclusions of offline conversations are written down. When we go back and forth three times, we jump on a synchronous video call . We communicate respectfully and professionally at all times. Embracing asynchronous communication and learning to use it effectively requires a mental shift. This can feel unusual or even uncomfortable for those who come from a colocated environment, where in-person meetings and communiques are the norm. Learn more about mastering the use of the written word in an all-remote setting . If you see something that concerns you in Slack, Issues, Merge Requests, Video, Emails or any other forum, we encourage you to respectfully say something directly to the individual in a 1:1 format. If there is an issue to raise regarding someone’s communication or conduct, team members should follow the process for raising communication concerns by sharing their concern with their manager or, if preferred, email Team Member Relations ( teammemberrelations@gitlab.com ) directly. In an all-remote setting , where team members are empowered to live and work where they’re most fulfilled, mastering asynchronous workflows is vital to avoiding dysfunction and enjoying outsized efficiencies and lifestyle flexibility. Asynchronous communication is the art of communicating and moving projects forward without the need for additional stakeholders to be available at the same time your communique is sent. To learn more on when to use asynchronous and synchronous communication, examples of async workflows in practice at GitLab, core async behaviors, and to take an async knowledge assessment, visit GitLab’s guide to embracing asynchronous communication . When working on a problem or issue, communicate directly with the people you need support from rather than working through reporting lines. Direct communication with the people you need to collaborate with is more efficient than working through your manager, their manager, or another intermediary. Escalate to management if you are not getting the support you need. Remember that everyone is a manager of one and they might have to complete their own assignments and inform the reporting lines. (This guidance supplements and overlaps with GitLab’s SAFE Framework , the guidance on the use of the internal handbook , and the additional guidance on this page. We ask our team members to consider the factors below in their communication. ) As GitLab matures, we want to continue to foster discussion while evolving our communication guidelines so that topics that are potentially GitLab sensitive are discussed in appropriate forums. This is particularly relevant as team members heavily leverage async modes of communication including merge requests, issues and epics, and in Slack communication. Words have impact long after they are written, and even when you’re communicating internally, the manner in which you speak with one another should be viewed through an external lens. For additional information, please review our Guidelines for communicating effectively and responsibly through text . At GitLab, we are public by default , but some information is classified as internal or limited access. Please see the confidentiality levels handbook page for details on this. The above examples overlap with the GitLab’s SAFE Framework examples. We recommend you to further review that page for more information and context. We encourage communicating risks to GitLab, its team members, or customers in a synchronous 1:1 setting. The table below outlines an overview of different communication forums at GitLab, and the considerations team members should think through related to potentially GitLab Sensitive topics when determining which forum to leverage. When in doubt, you can reach out to your People Business Partner and/or your leadership team directly. Please see our Project names section . Internal communication is any work related communication at a company. Internal Communication includes team member conversations, wider team discussions, or internal announcements to the company or targeted outreach to select internal audiences. At GitLab, everyone can contribute to the effectiveness of Internal Communications to support aspects of GitLab culture, such as intentional transparency and engaging people in open dialogue. Since we believe that all team members must be Managers of One , most communication is handled by the relevant group, but we know that some communications are more sensitive and contentious than others. In those cases, the DRIs may want to engage the Internal Communications function . We want to avoid unstructured content which includes FAQs (Frequently Asked Questions), especially for internal communication. FAQs tend to take on the voice and concerns of assumed personas. Instead of assuming questions, aim to articulate key facts as statements and use these to structure content under topical headers which aren’t questions. Structured content around GitLab, the product, should be in GitLab Docs and structured content around GitLab, the company, should be in the handbook; we should not use separate documents or locations to share this information. As an example, let’s say your FAQ would have a question like: Q: I’m not seeing widget X, what should I do? A: If you’re not seeing widget X, you can verify if it’s enabled or not by going to User Profile -> Settings and ensure the checkbox is enabled under Enable widget X You can enable widget X by going to User Profile -> Settings and ticking the checkbox next to Enable widget X then clicking on the Save button Content guidelines across the industry support avoiding FAQs: Employ multimodal communication to broadcast important decisions. To reach our distributed organization, announce important decisions in the company announcements Slack channel, email the appropriate team email lists, Slack the appropriate channels, and target 1:1s or other important meetings on the same day, with the same information. When doing this, create and link to a single source of truth : ideally the handbook , otherwise an epic, issue, or Google Doc. The email or Slack message should not be the source of truth. When referring to email that recipients should have received, reference the sender and subject of the email so it’s easy to find. For example, “You should have received an email from Jane Smith with the subject ‘Training Seminar Details’”. If something is behaving strangely on https://gitlab.com , it might be a bug. It could also mean that something was changed intentionally. Please search if the issue has already been reported . If it has not been reported, and you are sure it is a bug, please file an issue . If you are unsure whether the behavior you experience is a bug, you may ask in the Slack channel #is-this-known . If you know which stage of the DevOps lifecycle is affected, it is also okay to ask in #s_{stage}, for example #s_manage . When taking notes in an agenda, in the handbook, or in other documents, keep items numbered so we can refer to Item 3 or 4a. The number is not a signal of the importance or rank of the subject unless explicitly stated to be such. It is just for ease of reference. Linking should not be in one direction. We should go beyond deep-linking to create a richer web of links that can surface content and ensure people consider all pages when making updates. When linking one page to another, try to link back as well. Instead of only linking from Page A to Page B, both link Page A to Page B and link Page B back to Page A. For example, the Live Doc Meeting section of the All Remote Guide links to the Live Docs Meetings page . The Live Docs Meetings page links back to the Live Doc Meeting section of the All Remote Guide. In informal acknowledgement scenarios, such as on Slack or on issue comments, it is common practice to use the following: In order to effectively communicate an important change to hundreds of distributed employees, we occasionally use an ACK process. To prevent overuse, this should only be used by a member of the exec team. Anyone may ask an exec to sponsor one. As a guideline, we’d expect no more than one per quarter to be sent out. Too many ACKs lose power. As we continue to build on inclusion , recognition is a key and transformative tactic. Thanking team members provides an opportunity for them to be recognized for their contributions, influences engagement behavior, and acknowledges to team members their work is seen. By saying thanks, you are contributing to and supporting the value of DIB . Add Values emoji reactions to thank you messages in the #thanks slack channel or feel free to use them in GitLab.com, other slack channels and social media, when you see alignment to our values: GitLab’s values . As a second iteration, we have added these custom emoji to GitLab to enable tanuki values reactions in issues, epics, and MRs within the gitlab-com and gitlab-org groups. As a later iteration, we will begin tracking the number of emoji reactions for each value through the Reacji API and update this page with our findings! Indicate your availability by updating your own calendar using Google’s “out of office” feature and include the dates you plan to be away in your automated response. Note that this feature will automatically decline any meeting invitations during the time frame you select. Informal communication is made up of interactions between co-workers that are unofficial in nature and focus on building social relationships outside of the normal hierarchy of a typical business structure. In other words, it’s what happens when we get to know each other and talk about anything other than work. Informal communication is a vital part of GitLab culture , and we’ve listed 20+ ways to engage . If there is something that you want to discuss, but you do not feel that it is a reasonable option to discuss with either your manager or CEO, then you can reach out to any of the other C-level GitLab team members . When possible, it’s best practice to start a discussion with a Merge Request (MR) instead of an issue. An MR is associated with a specific change that is proposed and transparent for everyone to review and openly discuss. The nature of MRs facilitate discussions around a proposed solution to a problem that is actionable. An MR is actionable, while an issue will take longer to take action on. Some merge requests that involve a big decision or change tend to collect a large amount of feedback. As GitLab grows in size, it is unrealistic for a single person to respond to potentially hundreds of comments. To remain efficient in these MRs and to make it scalable, it is important for the DRI to receive a clear signal of input that is shared on the merge request. Some MRs may be marked as “Manager Mention MRs” by clearly designating them as such at the beginning of the MR description with the following code block: Additionally, add the ~\"Manager Mention MR\" label to the merge request. This will make future analytics on Manager Mention merge requests more easily identifiable. It also enables managers to subscribe to the label to be notified when an MR has elected the Manager Mention MR designation. We tried Manager Mention MR’s for the first time in a recent announcement (2021-03-03) but this did not work well and we are working on making it better. We’re starting with a more thoughtful and transparent process in our communications cadence and approach going forward, including all directs and people managers getting a few days’ notice before important company-wide changes are announced to all team members. This will allow all directs and people managers to feel more enabled and better understand the why behind big changes in order to scale communication to team members. For all managers: It is important to ground yourself in the contents of the changes before the announcement goes live to all team members. If a team member tags you in a Manager Mention MR, it is your role to respond candidly and thoughtfully to their question or comment. If the line of questioning in the Manager Mention MR gets out of your depth, ask the DRI to help answer. If a team member comments without a manager tagged, the comment will be closed with a link to this handbook section or closed without comment. In a situation where a team member leaves a wildly inappropriate comment in the Manager Mention MR, you should feel empowered to delete comment and talk to your team member 1:1. Consider subscribing to the label ~\"Manager Mention MR\" to be notified when MRs transition to this designation. For team members: Check if the MR you are about to comment on has the ~\"Manager Mention MR\" label. Check each time as the label may have been added since you last commented. When leaving a comment in a Manager Mention MR, frame the comment as a question or suggestion to your manager directly, and not anyone else, including the DRI. We do this to scale communication, as it is unsustainable for the DRI to answer every question. MRs should not start out as a Manager Mention MR as we prefer communication to be direct . They should only be designated as such after the number of comments on them grows to a level that is unsustainable for the DRI. An exception to this is compensation changes and other company-wide announcements that can be sensitive/contentious in nature since they have historically generated many comments. When an MR is changed to be Manager Mention , the person making this change should add a comment stating this so that everyone tracking the MR can be informed. Issues are valuable when there isn’t a specific code change that is being proposed, such as: When utilizing issues, it is still important to maintain focus by defining a single specific topic of discussion and the desired outcome that would result in the resolution of the issue. Issues should not be open-ended or go stale due to lack of resolution. For example, a team member may open an issue to track the progress of a blog post with associated to-do items that need to be completed by a certain date (e.g. first draft, peer review, publish). Once the specific items are completed, the issue can successfully be closed. Below are a few things to remember when creating issues: Pro Tip: When creating a Merge Request you can add closes: #[insert issue number here] and when the Merge Request is merged, the issue will automatically close. You can see an example of this here . Note: Automatic issue closing is person with disability on some projects. Our company-wide announcements channel is #company-fyi . It is an announcement only channel, meaning that communications need to be approved before they can be posted. To minimize noise, announcements made in #company-fyi should not be duplicated in #whats-happening-at-gitlab . Be mindful of the attention economy . In order to post or have a message posted in #company-fyi , please reach out to the internal communications team or your function’s executive who can approve the message and post it. Examples of what should not go in #company-fyi (as per new group guidelines): The above should all go in the new #whats-happening-at-GitLab channel (formerly the #company-announcement channel). Due to the volume of posts in the Slack channel, we recommend that you do not use #whats-happening-at-gitlab as a sole location for important announcements as information might get lost or muted. Examples of important items include but are not limited to: Meetings are incredibly expensive since they require synchronous time. The most common meeting problems can all be addressed by following the above guidelines around scheduling meetings. Some of the most common meetings problems are outlined below: If folks are involved in a meeting and have capacity to do so, they should take notes using GitLab’s Live Doc Meetings principles . This is important, because: It may look like a few people are already taking notes, do not see this as a deterrent for helping. Initial note takers may be first to show up and then see it as their responsibility to continue if no one else is stepping in. While meetings recordings are helpful, written notes are more efficient to read and offer greater opportunities for async engagement. Takes notes even when a meeting is being recorded. GitLab Executive Business Administrators sometimes support teams by taking notes. Since note taking takes time away from their other activities and can often be done by other folks, consider the following before engaging an EBA in a meeting solely for note taking purposes. Note taking helps us to work asynchronously. Team members can add thoughts to an agenda in advance of a meeting and understand what was discussed if they cannot attend. It also offers a record of discussion. Consider the following best practice when taking notes in meetings: If at the start of the meeting, it does not look like all team members will contribute to note-taking, identify a set of note-takers who will be responsible for this activity within the meeting Utilize Zoom’s AI Companion to create and/or share a meeting summary. Meeting Summary with Zoom AI Companion uses AI technology, which may include third-party models, and allows meeting hosts to initiate an AI-generated summary of their meeting. When the host enables this feature in a meeting, participants may automatically receive a summary after the meeting ends, if the host chose to share it with them. Note-taking can be a lot for a single person to stay on top of–especially when there is a fast moving conversation with many speakers. Team members should still feel empowered to contribute by helping with notes as needed, even if there is someone assigned. Ask others to scribe answers in real-time to allow the person who asked the question to focus on the answer. Touch up the answer when the conversation has moved on to something less relevant. It can be hard to keep up with the dialog and capture quality notes when there’s fast back and forth conversation. Lead by example and write when you’re not talking, expect others will write when you’re talking. Focus on noting speakers and their key points over capturing all words said. Extensive note-taking should not happen at the expense of correct note-taking. Write down your questions in the agenda before vocalizing. Always ask people to vocalize their questions to provide the most detailed context and for audio-only playbacks. Use discretion in taking notes if sensitive topics are being discussed. For example, do not takes notes on not-public information if the agenda may be available to an audience who should not be privy to this information. If someone requests folks to stop taking notes, stop for the duration of the discussion unless there is verbal confirmation that note-taking should resume. Ask for the confirmation before typing before you resume note-taking. At the end of the meeting, clearly capture key takeaways, next steps, and DRIs . If you have any questions about what may or may not be a sensitive topic, please refer to our SAFE Framework or reach out via the #safe Slack channel Presenting during meetings requires valuable synchronous time. Instead, recorded presentations make content accessible, prevent confusion, and increase participation for team members that prefer consuming content asynchronously . Remember it is not required to have a presentation or have a pre-recorded presentation. In the video below, GitLab co-founder, Sid Sijbrandij, explains why there are no presentations in most meetings. Pre-recorded presentations enable: There are times when presenting during a meeting is needed. This may occur when adding more context to a specific topic on slides. If this is the case, consider the following: Best Practices for Pre-Recorded Presentations While most meetings should not have presentations, there are a few exceptions. Specifically, we may use synchronous touch points in meetings with large numbers of folks. These tend to be meetings used for building team cohesion and alignment. For example, GitLab Assembly or the Functional Leaders Meeting . GitLab has the following meeting framework for determining which meetings should have presentations: Introductions can be helpful during some external meetings, such as executive sales calls. In those meetings, use these guidelines: If you’re scheduling a meeting across multiple regions, consider using the time ranges below to respect common working hours. The suggested times are organized by the regions that you’re trying to accommodate. Each suggested window is shown in the local time zone. For example, if your meeting includes team members in EMEA and AMER, you could consider scheduling from 8:00 AM to 10:00 AM Pacific Time. Note: Time zone offsets change throughout the year due to Daylight Savings Time, Summer Time, and similar time changes, so these suggested times may be less convenient at different times of year. When scheduling meetings with two or more sessions (usually when trying ensure worldwide coverage for all team members), name them after the topic, appended with a session number based on the order they show up in the calendar. Team members will see the meeting invites in their email or calendar in relation to their local time zone and can decide for themselves which session to attend, based on their working hours. You are the manager of your attention, and you decide when you do or don’t pay attention in a meeting. You will always have more work than time in your life. If you get invited to a meeting you don’t think you should go to, you should decline the meeting. It is better to cancel than to show up and not pay attention. On the other hand, not every part of a meeting is relevant, but it can sometimes be helpful to have more people in a call. If you only have one discussion point, if possible, try to reorder the meeting agenda to have your point first and then drop from the call. If you get asked a question when you’re not paying attention, it is an okay use of time to repeat a question every now and then. If training is required for one’s role, team members should plan to give the training full attention–especially if engagement in discussions or breakout rooms is required. If training is ’nice to learn’ or ‘optional’ for team members, multi-tasking can be done at the team members discretion. We don’t use the first 15 minutes of a meeting to read the materials like they do at Amazon . You can use the start of a meeting to review the materials for the meeting if you need to, given you do not have to be paying attention, but that should not delay the start of the meeting for the people that already have questions based on the materials. Meetings start on time at GitLab. Do not use your camera to signal you’re not paying attention; cameras should always be on . There are too few hours in a week, so we expect each team member to manage their attention . If you’re hosting a meeting, don’t tell people to give you their attention or stop multi-tasking. Respect each team member’s agency over their time. Instead of demanding attention, earn participants’ attention by organizing and facilitating meetings so they are compelling to attendees. You should take pride in being the first person to add a question to a meeting agenda, however unlike the First post meme we do want the first post to be more than just “First!”. The meeting DRI will be happy to see there is a question ready before to kick off the meeting. The Meeting DRI should remember to thank the person for asking the first question. Never do a countdown or say something like. “I’ll give it x seconds”, people are very unlikely to ask a question if you do that. Either ask for a question, wait for a question, or end the call. In calls that have remote participants everyone should use their own equipment (camera, headset, screen). When multiple people share equipment the following problems arise for remote participants : The people sharing equipment also have problems because they don’t have their own equipment: The disadvantages for remote people are much greater than for the sharing people and hard to notice for the sharing people. The disadvantages cause previously remote participants to travel to the meeting to be in person for a better experience. The extra travel is inefficient since it is time consuming, expensive, bad for the environment, and unhealthy. Theoretically you can have multiple people in a room with their own equipment but in practice it is much better to be in separate rooms: Ask Me Anything meetings can be a useful opportunity for team members to meet a new leader, learn more about an existing team member, or gain clarity on a recent change. Fireside chats are informal conversations between a host and a guest. The guest is typically a new leader, board member, or guest speaker. They are a useful opportunity to learn specific information about these individuals and their professional careers and personal interests. Fireside chats allow the audience to learn more about the guests in a casual and approachable setting. Format: In advance of the call, the host will prepare questions and share them with the guest. During the call, the host will moderate the conversation with the guest, by verbalizing the prepared questions. There is specific amount of time reserved at the end of the agenda for questions from attendees. A Walk and Talk call is when team members step away from their computers and get outside for a meeting. The difference between a coffee chat and a Walk and Talk call is that a Walk and Talk call be held with people that you interact with frequently at GitLab. It could be social in nature or focused on a specific problem/topic that needs to be discussed. If it’s a problem-solving focused discussion, the outcome should be captured in a merge request. It should not be used if the problem being discussed requires screen sharing or detailed note taking. There are great physical and mental health benefits to a walk and talk call. There are also benefits with increased focus and creativity . A Walk and Talk can also help prevent Zoom fatigue . The team members can use Zoom on their mobile device with the audio only function, or call one another from their preferred mobile device. A walk and talk call should be agreed to in advance to ensure that the local weather is compatible for a walk in both locations and that the walk and talk call fits into both team members’ schedules. We’ve created a Slack channel #walk-and-talk-meetings where, if you’d like, you can share pictures from your walking meetings. After GitLab releases a new version every month , we have a 30-minute call a few days later reflecting on what could have been better: We spend the first part of the retrospective meeting reviewing the action items from the previous month. On the 8th of each month (or the next business day), we have a kickoff meeting for the version that will be released in the following month. The product team and other leads will have already had discussions on what should be prioritized for that release. The purpose of this kickoff is to get everyone on the same page and to invite comments. Both the retrospectives and kickoffs are live streamed to our GitLab Unfiltered YouTube channel and posted to our Unfiltered YouTube channel . As GitLab continues to grow, sharing knowledge across the community becomes even more important. The Deep Dives page describes initiatives we are trying to encourage. This aligns with how we work since everything at GitLab is public by default . GitLab has a specific process to follow in crisis situations to ensure effective communications. Details can be found in the internal handbook . On 2022-01-20, the L&D team hosted Lorraine Lee for a live speaker series on video and presentation techniques in an all-remote workspace. Key points addressed in the training include: As you’re creating external or business content for GitLab, please refer to the GitLab Content Style Guide . For technical content, you can reference this word list . This list offers additional guidance for written communication at GitLab: Many times an explanation can be aided by a visual. Whenever presenting a diagram, we should still allow everyone to contribute. Where possible, take advantage of the handbook’s support for Mermaid . If you are new to using Mermaid or need help troubleshooting errors in your Mermaid code, the Mermaid Live Editor can be a helpful tool. Where taking advantage of Mermaid isn’t possible, link to the original in our Google Drive so that the diagram can be edited by anyone. At GitLab we use ubiquitous language to increase communication efficiency. This is defined in Domain-driven design as: “A language structured around the domain model and used by all team members to connect all the activities of the team with the software.” We use it for activities in GitLab, even ones not implemented in software. By having ubiquitous words to identify concepts we prevent confusion over what is meant, for example we refer to parts of our organization as a function, department, or group depending on exactly what is meant. Make sure that domains don’t overlap, for example organization size and deal size don’t reuse words to prevent overlap. If a term is ambiguous don’t use it, for example our hiring team uses the terms roles and vacancies , but avoid the ambiguous word job . Make sure that projects and working groups have clear and direct names. Prefer “CI Spend Reduction Working Group” to “Project Raven Working Group”. Make sure that people can infer as much as possible from the word, for example our subscription options allow you to know if someone is using self-managed or GitLab.com. Make sure terms don’t overlap without clearly defining how and why, for example see our tier definitions . Keep terms to one or at most two words to prevent people from introducing ambiguity by shortening a term. When using two words make the first word unique because people tend to drop the second word more often. MECEFU is an acronym for Mutually Exclusive Collectively Exhaustive Few words Ubiquitous-language. You pronounce it: MessiFu. Think of the great soccer player Lionel Messi and their kung fu or soccer fu skills. We want to use MECEFU terms to describe a domain to ensure efficient communication. MECEFU terms have 4 characteristics that help with efficiency: An example of a MECEFU term is our sales segmentation : One nit-pick is that the Medium of SMB and Mid of Mid-Market sound very similar. Simple Language is meant to encourage everyone at GitLab to simplify the language we use. We should always use the most clear, straightforward, and meaningful words possible in every conversation. Avoid using “fluff” words, jargon, or “corporate-speak” phrases that don’t add value. When you don’t use Simple Language, you: When you do use Simple Language, you: We’re now launching an optimization of our approach leveraging key learnings from the project’s postmortem. We’re creating a new plan based on what we learned from this project. Simple Language is important both when we’re speaking to other team members and when we’re representing GitLab to people outside the company. Be sure to use Simple Language in written communications as well. Our handbook, website, docs, marketing materials, and candidate or customer emails should be clear, concise, and effective. Corporate marketing maintains guidelines on GitLab’s tone of voice . For example, do not suggest that you’re “working in real-time” when a matter is in disarray. Convey that a lack of organization is hampering a result, and provide feedback and clear steps on resolving. Do not use a cool term such as “tiger team” when the existing term of “working group” is more exact. While cool terms such as these may be useful for persuading colleagues to join you in working towards a solution, the right way isn’t to use flowery language. The last example is when we used ‘Prioritizing for Global Optimization’ for what we renamed to headcount reset. When we renamed it we saw a good reduction in the use of this disruptive practice of moving people around. Using American English as our standard language supports our values such as efficiency, results, and transparency. Careful use of another person’s language can be a celebration of diversity and build an atmosphere of inclusion. The guidance in this section applies to written one-to-one communication, for example, merge request comments between an author and reviewer, not merge request descriptions or commit messages. Also keep the following in mind: Hey @nmalcolm, I left some suggestions for your merge request. Ka mau te wehi! (Te Reo Māori: great work / well done!) ありがとうございます (Japanese: thank you very much) for the review @cynthia! Avoid using Git in the naming of internal and external company related programs (BagGit, GitFit, Gitty, GitIt, etc.). Referencing Git creates an inaccurate perception that GitLab has a narrow focus. While GitLab started as a source control platform, it has become The DevOps Platform . We have a low internal email culture, as we see greater efficiency in other forms of communication (e.g. Slack). If you are emailing, please use the following guidelines: Use a bias for action to quickly move conversations that require collaboration and action out of Slack and into an issue. Only 90 days of Slack activity will be retained, so Slack should specifically NOT be used for: Internal Slack messages between team members are still considered professional communication. Please do not use or add emoji’s to Slack that are of a political, religious or of a sexual nature. You can refer to the Religion and politics at work section of the handbook. When in doubt do not use or add the emoji. If you have any concerns about an emoji that was used, please reach out to the author or if you are not comfortable doing so please reach out to your People Business Partner . There is a lot of information pertaining to Slack, as it is a critical part of GitLab’s communication. See the Slack tools and tips page . Note: We don’t use the term private message , because these direct messages are not inherently private like a phone call or private letter. The messages are potentially accessible by Workspace admins or via Backups. Slack refers to these types of messages as direct messages themselves . When using Slack for work-related purposes, please avoid direct messages. Direct messages discourage collaboration . You might actually be contacting the wrong person, and they cannot easily redirect you to the right person. If the person is unavailable at the moment, it is less efficient because other people cannot jump in and help. Use a public channel and mention the person or group you want to reach. This ensures it is easy for other people to chime in, involve other people if needed, and learn from whatever is discussed. If someone sends you a work-related direct message, it is okay to let them know you’d like to take the conversation to a public channel, linking to this section of the handbook. The process might look something like: If you find yourself getting a lot of direct messages that should go in a public channel, consider changing your Slack status to an attention grabbing emoji and set it to something like: If you must send a work-related direct message, don’t start a conversation with “Hi” or “Hey” as that interrupts their work without communicating anything. If you have a quick question, just ask the question directly, and the person will respond asynchronously. If you truly need to have a synchronous communication, then start by asking for that explicitly, while mentioning the subject. e.g., “I’m having trouble understanding issue #x, can we talk about it quickly?”. Use private channels instead of group direct messages. Group direct messages are very hard to maintain, track, and respond to. They also have a key limitation in that you can’t add people to the conversation. This is a hindrance to collaboration and transparency. Consider whether the conversation can take place in a public channel. If not, please use a private channel instead. This channel may have a short-term purpose. It is acceptable to leave the channel and/or archive it if you are no longer an active participant or the channel is no longer in use. As we grow headcount, we exponentially increase the lines of communication- 3 people have 3 communication lines, 4 have 6, and 41 have 820. As a result, there is a natural tendency for people to prefer private channels of communication. The intentions are good, as people are looking to reduce noise for others, but this can lead to the same problems as described elsewhere on this page, notably: Slack is our primary source of chat communication and is where many personal interactions happen. We want to continue to encourage folks to build personal relationships with one another which will often happen over DMs. Start by understanding what we mean by respecting time. We should err toward putting material into channels over DMs and public channels over private channels even though we understand that this will generate more messages that can be read by more people. Respecting time is not about reducing the overall volume of channel messages that team members receive. It’s about making sure that messages are targeted, expectations for asynchronous responses are clear, and we are communicating with consideration. The following tips provide ways to work respectfully with others given this context, though is not an exhaustive list: If you have a question that you can’t find the answer to in our handbook (or you need help finding something in the handbook) team members across the company are here to help. Go directly to the subject matter experts/source in the designated slack channel to ensure your question is addressed. If your question doesn’t relate to any of the existing topics: The times this feature would be used would be to comply with certain obligations. Corporate Export must be enabled by Slack in accordance with Slack’s policy, which can be found here . Examples of instances where GitLab may need to use this feature may include, but are not limited to, those situations listed in Slack’s documentation . No. The Slack Workspace Owner has the ability to export data from all direct messages and private channel conversations for the maximum retention period set by GitLab, which is currently set for 90-days. All messages that are older than 90-days cannot be exported by the Workspace Owner or any other Team Member at GitLab. While messages are not actively monitored, GitLab reserves the right to monitor its software for the reasons stated in its Employee Privacy Policy , including, but not limited to, the safety and protection of our Team Members, the protection of our intellectual property, and the exercise or defense of legal claims. Please keep GitLab values in mind when communicating directly with other team members. If you have a confidential personal issue that you do not feel comfortable discussing via a business-provided internal communications tool, it is recommended to use a personal form of communication such as a text message or phone call. For additional questions, please address in the issue . To use the “Slack Down!” group chat on Zoom: Once service is restored, go back to Slack. Once service is restored, go back to Zoom. Join the Slack Down! room on Hangouts Chat. Once service is restored, go back to Slack and Zoom. Never use a Google Doc / Presentations for something non-confidential that has to end up on the website or the handbook . Work on these edits via commits to a merge request. Then link to the merge request or diff to present the change to people. This prevents a duplication of effort and/or an out of date handbook. Google Docs can be useful when rapidly iterating /commenting/suggesting on the content, but if the content is meant to be long lived it should be moved to the handbook as an SSOT and deprecated with a link to the handbook page. If the content is short lived, e.g. one-time report that won’t be referred to beyond 2-3 weeks, it can remain in a Google Doc or presentation . Google Docs Pageless format is the preferred format for company documents that won’t be printed. If you set your default to Pageless then this will be applied to all future documents as well. If a document is likely going to be printed (for example, a contract) the older paged style is acceptable. See Good practices and helpful tips for help navigating the pageless format. If you do need a Google Doc, create one with your company Google Workspace (formerly G Suite) account and set the visibility, access controls and searchable flag according to the following guidelines. The recommended defaults when sharing a document for GitLab internal purposes is setting visibility to GitLab , access to Editor and searchable Can find in search results to ensure everyone can contribute! Note: To our knowledge, it is not possible to set the default to Can Edit and you have to change the permissions from View manually. We hope that Google adds this capability in the future. Reference Google’s documentation on Link Sharing to learn more. Pages like this are part of the handbook . The GitLab handbook is the central repository for how we run the company. GitLab Docs - docs.gitlab.com is where you can find documentation on GitLab, the product. repository / repositories are where files are kept under source code management . In most cases, requires MRs to merge. The Handbooks are in a repository, our code is in a repository, etc. README - README.md files are what are shown by default when you browse a repository. Contains useful information to give context on what the project/repository are for. These can also be used for user profiles as personal READMEs. We recommend you set your Google Calendar access permissions to ‘Make available for GitLab - See all event details’. Consider marking the following appointments as ‘Private’: There are several benefits and reasons to sharing your calendar with everyone at GitLab: If you add blocks of time spent on recurring tasks to your Google Calendar to remind yourself to do things (e.g. “Check Google Analytics”), consider marking yourself “Free” for those events so that coworkers know they may schedule a meeting during that time if they can’t find another convenient time. This feature allows you to create a link to an availability schedule that you can send to your customers or coworkers for them to schedule a call according to your availability. This allows you to only show available spots while keeping your other calls private. This also avoids having to go back and forth between you and other person figuring out what day and time works best for both of you. Since this is a native Google Calendar functionality, there is no need to set up integrations with your calendar like other scheduling tools. A member of our Customer Success team created a demo video of how to use this feature . Key practices to consider during any meeting are listed below. GitLab team members are not authorized to speak with the media or analysts on behalf of our company unless authorized by our Marketing department. Unless authorized, do not give the impression that you are speaking on behalf of GitLab in any communication that may become public. This includes posts to online forums, social media sites, blogs, chat rooms, and bulletin boards. This policy also applies to comments to journalists about specific matters that relate to our businesses, as well as letters to the editor and endorsements of products or services. For more, please visit the Corporate Communications handbook section . GitLab as the leader in all remote work creates opportunities for our team members to receive requests from external 3rd parties to participate on panels, blogs or news publication or articles. Recently our team members have been approached by external 3rd parties looking to pay or compensate GitLab team members for their time to discuss GitLab remote practice to help them guide a client. Other third parties may contact GitLab team members to provide subject matter expertise that they may have by virtue of their role at GitLab. As in any request we ask that team members verify who they are speaking with to make sure the source is indeed a valid and legitimate request. Always remember that you represent GitLab and if any question makes you uncomfortable or gives you a pause on whether you should answer then we recommend that you do not answer. A third party’s questions regarding GitLab financials, sales, compliance, executives or specifically where the company is heading should be treated with the most caution. We want and encourage all team members to be remote evangelists and this can be done without giving very specific information about GitLab. If you have any concern about a request please reach out on slack to #external-comms Please see our team member social media policy . See the YouTube page for options and instructions for posting recordings and live streaming to our YouTube channels. If you need to provide the details of GitLab’s contact information you can take the address from the visiting page for reference; or the mailing address of the office in the Netherlands if that is more applicable. If a phone number is required, leave this field empty by default. If that is not possible, then use the general number (+1-415-761-1791), but be aware that this number simply guides to a voice message that refers the caller back to contacting us via email. Competencies are the Single Source of Truth (SSoT) framework for things we need team members to learn. In an all-remote organization effective communication is key to exchanging knowledge, ideas, and information. Effective communication at GitLab: If you would like to improve your skills or expand your knowledge on topics relating to Communication at GitLab, check out our resources: Skills and behavior of applying effective communication as a Team Member : Skills and behavior of applying effective communication as a People Manager :",
  "Please see our company page for more general information about GitLab. At GitLab, our mission is simple: to enable everyone to contribute to and co-create the software that powers our world. Our platform doesn’t just bring people together—it unites teams and organizations, breaking down barriers and redefining what’s possible in software development. Here, you’re not just contributing to a tool, you’re shaping the future of secure, scalable software delivery. Our high-performance culture values collaboration, ownership, and impact, accelerating careers while driving global innovation. This is more than a job, it’s your opportunity to build something that matters. Our size and our mission (enable everyone to contribute to and co-create the software that powers our world.) mean that our team members can — and are expected to — make an impact across the company. Because we all use our product internally, you don’t have to be a developer to learn to collaborate in the GitLab tool. From your very first week, no matter your role, you’ll gain the technical skills needed to access, question, and contribute to projects far beyond your job description. This unique approach works because we’re a team of helpful, passionate people who want to see each other, the company, and the broader GitLab community succeed. We learn from each other, challenge each other, and thank each other. Come prepared to do meaningful work that will help shape the future of the company. While the opportunities to contribute are boundless in a growing organization like GitLab, they may not be clearly defined. You’ll need to think creatively, speak up to see how you can help, and be willing to try something new. At GitLab, our value of iteration has a unique impact on the way we operate and get things done. Working this way means our team members are expected to quickly deliver the minimal valuable change in their work instead of waiting to produce a polished, completed product. While this can be a challenging practice to adopt at first, it’s liberating to be able to make mistakes, get feedback quickly, and course correct to reach a better outcome, faster. As our company and the industry continue to grow, you’ll have the freedom to change and constantly evolve everything from your schedule and your workspace to your job description and your skills. Culture at GitLab is composed of three things.",
  "At its heart is a belief that creating the environment for better decisions and improved execution of them makes for better teams — and ultimately, progress. TeamOps is how GitLab scaled from a startup to a global public company in a decade. Now we’re opening it up to every organization. The four guiding principles of TeamOps are below.",
  "For executives to connect on a weekly basis in order to: Scheduled for 80 minutes on Mondays. These don’t occur during E-Group Offsite weeks and may occasionally be rescheduled for another day due to calendar conflicts or holidays. The EBA to the CEO is responsible for scheduling these meetings. Chief of Staff to the CEO is responsible for managing to the agenda and ensuring that meetings don’t exceed the allocated time. The agenda for each E-Group Weekly has a single section for each meeting. Each agenda item should include a tag . Three of the most often used tags are: The Office of the CEO will order agenda topics. FYIs and TODOs will usually appear at the start of a meeting.",
  "At GitLab, our mission is to enable everyone to contribute to and co-create the software that powers our world. This, together with our values, is at the core of how we manage our business, including our approach to Environmental, Social, and Governance (“ESG”). Our all-remote team drives GitLab’s strategy and business results in more than 60 countries worldwide. In FY24, we sharpened our focus on our people by emphasizing the foundational role of high-performing teams and continuing to leverage our core value of diversity, inclusion, and belonging (DIB) to drive innovation and impact across our business. These efforts included launching new training and development programs and additional resources to encourage and enable team members to contribute and to give our people leaders more of the skills they need to manage efficient and effective teams. Our ESG strategy also encompasses efforts to minimize our environmental impact and better understand and address the potential effects of climate change on our business. In FY24, we continued to enhance our approach to measuring and reporting our greenhouse gas emissions. We conducted GitLab’s first climate risk assessment in alignment with the Task Force on Climate-related Financial Disclosures (TCFD) and launched the first iteration of a supplier engagement program focused on improving the visibility of emissions in our supply chain. We are committed to good corporate governance and high standards of ethics and compliance. In particular, as we continue to harness the power of AI to enhance the GitLab platform, we recognize the importance of working within a robust framework of ethical principles and protecting our customers’ and users’ data. In early fiscal year 2025, we launched the GitLab AI Transparency Center, giving customers direct access to our AI Ethics Principles for Product Development, AI Continuity Plan, and AI features documentation so that they can adopt AI with confidence. Lastly, our strategy and mission are also bolstered by engagement with the wider GitLab community. In the last fiscal year, we continued to expand outreach and drive social impact through community programs such as GitLab for Startups, GitLab for Open Source, and GitLab for Education. We also launched GitLab for Nonprofits, which provides free GitLab licenses to registered nonprofit organizations that align with our values and ESG strategy. Our charitable efforts are further complemented by the GitLab Foundation, which supports people in growing their lifetime earnings through education, training, access to opportunities, and systems changes on a global scale. As ever, GitLab — the platform and the company — is made stronger by the support of our stakeholders, including our team members, customers, shareholders, and the wider community. I am thankful for the contributions so far and look forward to what we can accomplish together in the years ahead. GitLab Co-Founder and Chief Executive Officer GitLab’s mission is to enable everyone to contribute to and co-create the software that powers our world. Our mission is the way we realize our purpose : to help people increase their lifetime earnings through access to opportunities and the DevSecOps platform. Our vision captures what we want to deliver to customers in the next 10 years. Today, GitLab is the most comprehensive AI-powered DevSecOps platform. The DevSecOps Platform shift is part of a larger trend: teams of all types are realizing that breaking down silos has compounding effects on productivity and collaboration. We see it with data and operations teams creating DataOps; we see it with machine learning and ops teams creating MLOps. As more companies (and more teams within a company) rely on our platform, we are positioned to become the AllOps platform — a single application for all R&D. Our efforts are guided by our six core values of Collaboration, Results for Customers, Efficiency, Diversity, Inclusion and Belonging, Iteration, and Transparency. Together, they spell the CREDIT we give each other by assuming good intent. “With GitLab’s second ESG Report, we demonstrate our values of iteration and transparency, sharing how we continue to build on our approach to addressing the issues that matter most to our stakeholders and our business. One key example is the launch of our AI Transparency Center, underscoring how we are leveraging the power of AI within a clear framework of ethics, responsibility, and accountability. As our innovation continues to break down barriers, we recognize the increasing opportunity of integrating our business and sustainability strategies to create value for our customers, our shareholders, our team members, and the broader community.” – Robin Schulman , Chief Legal Officer, Head of Corporate Affairs, and Corporate Secretary GitLab’s Environmental, Social, and Governance (“ESG”) Report includes information on our key ESG focus areas, our programs and policies, achievements to date, metrics and targets that help define our ESG program, and our plans for the future. Data about GitLab’s financial performance is not included in this report and may be found in our financial and SEC filings . Unless otherwise specified, the information included in this report was last updated on July 18, 2024. The content included covers Fiscal Year 2024 (“FY24”) (as of January 31, 2024) and we plan to update the report annually. Throughout the report, there may be mentions of Fiscal Year 2025 (“FY25”), which runs from February 1, 2024, through January 31, 2025. We would also like to note that GitLab maintains a public-facing handbook , pages of which are linked throughout the report. These pages are maintained separately and may reflect a different reporting period than this report. This report has been prepared with reference to the Global Reporting Initiative (“GRI”) Standards released in October 2021 and the Sustainability Accounting Standards Board (“SASB”) Standard for Software & IT Services. We have also provided disclosures based on the framework of the Task Force on Climate-related Financial Disclosures (“TCFD”). Content indexes for these standards are available in the Appendix . GitLab’s Nominating and Corporate Governance Committee on the Board of Directors (“Board”) has reviewed the information in this report. For information on GitLab’s ESG Team, please visit the ESG handbook page . GitLab’s ESG strategy is driven by our values and deeply integrated into the company’s business philosophy. It reflects longstanding practices embedded in our work culture since GitLab’s inception, as well as updated policies and programs designed to meet the evolving needs and expectations of our stakeholders. The ESG Team creates and maintains GitLab’s ESG/sustainability strategy and programs. This includes creating and managing GitLab’s ESG disclosures and public ESG reports, identifying and prioritizing key issues to advance our social and environmental goals, and creating partnerships with nonprofit organizations that support GitLab’s values and mission. Our ESG strategy is informed by an understanding of the ESG topics that matter most to our business and our stakeholders. Completed in December 2022, our ESG materiality assessment included engagement with internal and external stakeholders and a focus on “double materiality,” exploring both topics that have the greatest impact on GitLab’s business, as well as the actual and potential impact of our activities on the environment, society, and our global communities. GitLab senior leadership — including members of the Nominating and Corporate Governance Committee of GitLab’s Board of Directors — were engaged as part of the assessment. The materiality assessment identified the following key ESG topics: We also include in this report other topics relevant to our business and stakeholders, including Community, Social Impact, and Corporate Governance. The United Nations Sustainable Development Goals (“UN SDGs”) provide a shared blueprint for peace and prosperity for people and the planet, now and into the future. Our ESG efforts currently align with five of the 17 UN SDGs, and we seek to drive progress on the goals within our operations. We will continue to revisit and deepen our alignment as we make progress on our ESG strategy. We’re a team of helpful, passionate people who want to see each other, GitLab, and the broader GitLab community succeed. Our mission (to enable everyone to contribute to and co-create the software that powers the world) means that our team members can — and are expected to — make an impact across the company. We empower them by cultivating a high-performance and results-driven culture — one that enables people to do their best work and advance their careers while fully contributing towards advancing GitLab’s business results. We hire, promote, and recognize those team members who best live our values . We also leverage team members’ unique skills and experiences to drive extraordinary outcomes. We believe that when team members seek feedback from a diverse group of peers and leaders, inside and outside of their group or function, they make better decisions and feel more connected. We work to make everyone feel welcome and increase the participation of underrepresented groups in our community and company. Our entire workforce is remote, making it easier for people of diverse backgrounds and abilities to join the team. As GitLab has evolved, we have learned a great deal about what it takes to build and manage a fully remote team, which we share through our publicly available handbook . Our approach to talent management is underscored by our values, particularly of transparency and diversity, inclusion, and belonging. It is also bolstered by the resources we provide to help our team members grow and succeed. Through our website and handbook , we are transparent about available positions , benefits , job descriptions , onboarding and offboarding procedures, facilitating connections with various internal groups, and much more. We also work to build a diverse team to drive extraordinary outcomes by expanding outreach and inclusivity initiatives to increase the number of team members and our Manager+ population who identify as part of an underrepresented group. Our People Group is dedicated to supporting each team member through their career journey at GitLab. They enable GitLab’s strategy through a focus on attracting and retaining diverse talent, creating a high-performance culture with a focus on efficiency and quality results, driving divisional success through talent programs, and cultivating an equitable and inclusive culture. This includes developing and maintaining our talent brand , facilitating ongoing learning and development , and providing resources for team members to connect , share concerns, and seek support . In FY24, we focused on accelerating GitLab’s 3-year strategy through re-emphasizing the foundational role of high-performing teams. At GitLab, high-performing teams: We educate team members about our values through trainings, videos , workshops, forums, certifications, and ambassadors, which are designed to reinforce our values as key drivers of our high-performance, results-driven culture. Managers at GitLab support our mission of enabling everyone to contribute to and co-create the software that powers our world. We need to equip our people leaders with the skills to lead globally dispersed, all-remote teams to support our business growth at scale. Elevate is GitLab’s leadership training program for Managers and Senior Managers, which prepares managers with skills they need to build high-performing teams. In FY24, we certified 73% of our eligible Managers and Senior Managers in Elevate. Although this fell short of our goal of 85%—due to business-critical initiatives, some team members had to delay certification for a year—it demonstrated positive momentum for the program and contributed to the company’s strong performance throughout the year. During FY24, we also developed and launched additional tools to help participants deepen and continue their learning beyond initial certification. Elevate Applied is an ongoing resource that enables Managers and Senior Managers to connect with each other and practice, apply, and integrate concepts taught in Elevate in their day-to-day work at GitLab. In addition, we developed and announced the FY25 launch of Elevate+ , a six-month program designed to enable senior leaders (Directors and Senior Directors) to better understand and practice behaviors anchored in GitLab’s values and leadership competencies, develop new skills to support high-priority business needs, and help establish and strengthen cross-functional connections. We strive to foster a culture of continuous growth so that GitLab remains a great place to work. A positive impact on team member experience leads to a positive impact on results for GitLab and its customers. Our Talent Development team (commonly referred to as “Learning & Development’’) enhances team member performance, expands capabilities, and further develops skills that make GitLab team members the top talent in the industry. Our vision is to be recognized as a top organization for remote learning and development and to create a future where everyone contributes to a culture of curiosity. We accomplish this by focusing on skill-based learning, providing resources to enhance career mobility, and developing learning journeys for teams throughout the organization. To support our aim of facilitating accessible, remote-friendly development for all, we provide asynchronous access to learning opportunities, including: As a global, all-remote company with a team of diverse backgrounds, experiences, and perspectives, GitLab offers a dynamic environment and culture where everyone is encouraged to do their best work. Our talent brand empowers team members to share their thoughts on what it’s like to work here because their voices and stories make GitLab unique and successful. “By fully embracing the values, GitLab has changed the way I work. I have become more productive and satisfied with the work I do, while also having a better work-life balance. It’s motivating to work somewhere so amazing with such great people.” – Cynthia, Strategy and Operations, Office of the CEO We conduct a number of engagement surveys that enable us to capture honest feedback from our team members, understand engagement levels across the organization, and respond to changing needs. By making small iterations and moving quickly, we can get the data needed to evolve engagement programs as needed and align our priorities to the areas team members care about most. Our key survey tools include the annual CultureAmp Engagement survey, Employer Award surveys, and other ad hoc measures like Organizational Health surveys. In FY24, our Annual Engagement Survey reached 88% participation and indicated an overall engagement score of 75% favorable. This is 2% higher than our New Tech peer group, which gives us confidence in knowing we have a highly engaged team member community. Additional highlights included: Subsequently, our global year-end pulse survey demonstrated positive momentum with an overall engagement score of 79% favorable (+4%). In addition, 91% of participants said they’re proud to work at GitLab, while 86% rated GitLab as a great place to work. GitLab received Great Place to Work Certification for the 4th year in a row in May 2024 with 93% of its U.S. based team members saying that they believe GitLab is a great place to work. Great Place to Work recognizes companies for their outstanding workplace culture, benefits, and performance through successful completion of a survey for US-based team members and culture brief application. We are committed to a future where the Diversity, Inclusion and Belonging (“DIB”) value empowers everyone to contribute. As a globally dispersed organization serving customers all over the world, we strive for a team that is representative of our users, an open and transparent work culture where all voices are heard and welcomed, and an environment where everyone can show up as their full selves and contribute to their best ability each day. We believe this not only makes GitLab a great place to work but also supports innovation and promotes better decision-making, helping us continue to deliver results for customers. Our DIB focus also extends to how we show up for people outside of GitLab, through initiatives that promote DIB in our industry and the broader community. To put it simply, we need the contributions of everyone in order to empower our team members, help our users change the world, and transform the industry. We do this through our A.D.A.P.T. strategy: Growing Our Talent & Their Experience Our team members are critical to our success. We provide pathways for them to thrive individually and as a part of the GitLab community. Level Up – Our learning platform is designed to deploy learning content across GitLab internal team members. Career development and mobility – We provide resources and direction to help team members engage in lifelong growth to meet their unique career goals. Team Member Resource Groups (TMRGs) – TMRGs cultivate an inclusive workplace and empower our employees by fostering a supportive environment where everyone can thrive. Our TMRGs are open to all team members, not just those who specifically identify with a particular group. Team Member Advocacy Groups (TMAGs) – TMAGs are groups that share common interests and work together to drive change on behalf of the community through education, action, and enablement across a specific need. From education to mentorship, sponsorship, and advocacy, GitLab educates its leaders, provides resources for its team members, and holds its leaders accountable so that team members feel empowered to utilize the community they are surrounded by every day. Equipping Our People Leaders – Our leadership training ensures that DIB is considered and threaded throughout the team member experience. Community partnerships are a great way for GitLab to engage with the community externally, provide opportunities for career development and networking for our team members, and also add layers of DIB to key areas of our organization. Partnerships are also how we measure our inclusive practices against industry standards and can act as accountability partners in achieving success. External Engagement – We sponsor and support events worldwide that promote inclusion and belonging, and create learning and growth opportunities for our team. By networking with diverse talent at these events, we infuse our team with fresh perspectives and ideas. Reflecting on a year of impactful partnerships, we are inspired to further foster spaces where technology and diverse talent converge, giving everyone a voice. Do Good – Throughout the year, the DIB and ESG teams collaborate to give back to the communities we serve by offering volunteer opportunities to team members, working with TMRGs to donate funds to relevant nonprofits, and providing in-kind donations to DIB related nonprofits through the GitLab for Nonprofits program. When we think about DIB, it’s easy to see it only from a team member perspective. Our goal is to bring inclusivity into as many touch points as possible — from our open source contributors to how we engage with our customers and users. DEI Project Badging Program – In FY24, GitLab partnered with the Linux Foundation through the Community Health Analytics in Open Source Software ( CHAOSS ) project to enable open source projects to signal their focus on building and growing diverse communities. Digital Accessibility – At GitLab, we build understanding, empathy, and allyship around accessibility. We do this through various methods internally, including Inclusive Design Principles , Digital Accessibility Training, webinars, and workshops, to ensure that our Product and Engineering teams are equipped with the skills necessary to make our product more accessible. Inclusive Hiring – GitLab strives to create an equitable application, interview, selection, and offer process for all candidates to ensure we’re building a diverse and inclusive workforce around the globe as the company continues to grow. DIB Working Group – A strategic group of impactful team members who collaborate to drive action and results in support of DIB. This includes promoting inclusive practices in areas such as name pronunciation, pronoun education, and meeting etiquette in an all-remote workplace. We followed through on our commitments and accomplished a lot in 2023, but we still have work to do. We are continuing to build DIB into every part of GitLab – both internally with our team members and externally with our customers, users, partners, and open source community. We have committed to four key objectives in FY25: Deliver our supplier diversity strategy – We have set an aspirational goal to double the number of suppliers categorized as diverse. To achieve this aspirational goal, we are engaging partners, strategically embedding diversity, inclusion and belonging practices into our procurement process, and evaluating our current suppliers’ practices in line with applicable anti-discrimination principles. Increase representation – Extend our representation of team members from our Disabilities, Neurodivergence and LGBTQIA+ communities. We are working with several partners to help us achieve this goal including but not limited to, Unicorns in Tech, DisabilityIN, and Anita B. Org. Visit this Handbook page to learn more about how we select partners and measure success in attracting team members from these communities. Emerging Talent @ GitLab – Our mission is to attract, acquire, and activate talent to unlock their potential, connecting learning agility with employment opportunities, decoupled from academic-based prerequisites. This is anchored on the belief that building a balanced workforce will enable GitLab to scale effectively and inclusively, and that building a strong foundation today will greatly impact our future growth. Ensure inclusive leadership – Inclusive leadership is integrated into leadership development experiences and offered as voluntary resources to promote ongoing education and awareness. These experiences span all front-line and aspiring leaders in addition to VP and E-Group populations. From the beginning, GitLab has been an open source project made possible by contributions from its community. Contributors to GitLab — the platform and the company — make up the GitLab community and are fundamental to GitLab’s strategy and mission. The Developer Relations team supports GitLab’s mission by working with our community to ensure they receive support and recognition for contributing to GitLab. This involves a multifaceted approach that includes creating educational content, organizing events and workshops, developing programs, and providing platforms for knowledge exchange and collaboration. The team focuses not only on promoting GitLab’s features and capabilities but also on actively listening to and incorporating feedback from our community to inform product development and improvements. GitLab’s Strategy Programs aim to put GitLab’s most powerful features in the hands of communities that may not otherwise have the means to access them, helping them to grow and thrive. Members of the Strategy Programs team serve as liaisons, tracking the ways their respective communities prefer using GitLab and translating that knowledge into insights that help GitLab create a better product. While each program is unique in its organization and execution, benefits may include: Our key community programs include the following: For information on GitLab for Nonprofits, see Social Impact below. Finding an open source community to learn, connect, and grow with can be a challenge for many developers. To better align on best practices for building inclusive open source communities and to ensure that our DIB value is reflected in our contributor community, GitLab has partnered with the CHAOSS project to integrate GitLab with their recently released DEI Project Badging program . The DEI Project Badging program enables open source projects to: To learn more about how the program works and how GitLab and CHAOSS worked together to bring it to the wider community, see our blog post . In FY24, our Contributor Success Team sought to enhance community relations and contribute to sustainability by allowing teams to recognize contributors by sponsoring tree planting in lieu of traditional rewards like swag. Through a partnership with Tree-Nation , we have the ability to plant trees on behalf of others, and others can also pay to plant trees in our forest. Similarly, our Marketing team offers attendees at GitLab events the opportunity to plant trees through Rewards.Earth . Launched in 2023, GitLab’s Philanthropy Policy supports GitLab teams that want to engage in charitable giving and activities benefiting registered nonprofit organizations that align with GitLab’s mission, values, and ESG strategy. By enabling teams to contribute to causes aligned with their work, we foster connections between GitLab’s purpose and values and the opportunity to drive impact in the broader community. For example, in FY24: During FY24, we also launched our in-kind donation program, GitLab for Nonprofits , which gives free GitLab licenses to registered nonprofit organizations that align with our values and ESG strategy. Nonprofits accepted into the program are provided a free Ultimate license for one year for up to 20 seats. In the first six months of the program, we granted licenses to more than 100 qualified nonprofits. “GitLab is our canvas for workflow across our nonprofit organization. It allows us to do everything from planning and managing the workloads for our teams all the way to the CI/CD pipelines, which automate efforts ranging from routine tasks to platform deployments in our various environments. Whether it’s documenting processes in place with issue templates or structuring organization-wide initiatives with interdependent issues, epics, and milestones, GitLab gives us the structure we need, with the flexibility to meet our unique needs.\" –Mike Bowie, Chief Technology Officer, Last Mile Through our GiveLab program , we encourage team members to take part in volunteer initiatives that support and ultimately uplift their local communities. In FY24, GitLab provided virtual volunteer opportunities for team members through a partnership with SuitUp , a nonprofit that equips students for life beyond the classroom by partnering with schools/youth organizations and businesses to develop, organize, and implement engaging educational competitions in communities across the U.S. and beyond. “We extend our heartfelt gratitude to GitLab for their invaluable partnership with SuitUp in 2023. GitLab’s commitment to youth education has not only paved a clear pathway for our students’ success, but has opened abundant opportunities for their development in STEM careers and beyond. We are deeply thankful to the GitLab team for their dedication and the transformative impact they have had on the lives and futures of our SuitUp students across the globe.” –Kelsey English Smith, SuitUp’s Interim Chief Executive Officer GitLab is dedicated to supporting charitable organizations with missions that align with our company’s values through the GitLab Foundation. Established in 2022, the GitLab Foundation focuses on supporting people to grow their lifetime earnings through education, training, access to opportunities, and systems change on a global scale. Its vision is a world in which one million more people can afford a better life. The GitLab Foundation is funded by GitLab and its co-founder, Sid Sijbrandij. When GitLab went public in 2021, the Board approved a 1% share donation to capitalize the GitLab Foundation. In FY24, 1,635,545 shares of Class A common stock were registered to be issued to the GitLab Foundation. The GitLab Foundation is an independent nonprofit entity and its operations are autonomous from GitLab. To learn more, please visit the GitLab Foundation . Around the world, many communities are already experiencing climate impacts such as warmer temperatures, more frequent severe weather events, and changes in water availability and other vital ecosystem services. With this in mind, GitLab is committed to doing our part to minimize our environmental footprint, including working to reduce GHG emissions associated with our operations. As a fully remote company, GitLab has no Scope 1 (direct) emissions or Scope 2 emissions from purchased electricity for company-owned facilities, meaning that 100% of our footprint comes from Scope 3 emissions. Our FY24 GHG inventory measures emissions associated with purchased goods and services (which includes cloud services), capital goods, employee commuting (which includes remote work), business travel, and investments (as defined under Category 15 of the GHG Protocol). Our emissions increased in FY24 partially due to improvements in our methodology. For FY24, we estimated emissions using the Comprehensive Environmental Data Archive (CEDA), a multi-regional economic and environmental database, as it better reflects the global nature of supply chains, helping capture differences in emissions profiles that vary significantly from one country to another. We also expanded our measurement to include emissions from investments (as defined under Category 15 of the GHG Protocol). More detailed results of our inventory are available in the Performance Data Table . Our third-party assurance letter of our FY24 GHG inventory is available here . In FY24, we took steps to further understand our climate impacts and advance the development of a formal emissions reduction strategy. This included initiating GitLab’s first climate risk assessment, to help us better understand climate-related risks and opportunities for our business. For more information on our climate risk assessment and scenario analysis, please see our TCFD Index . In FY24, we also launched a supplier engagement program with a focus on climate. As an initial step, we engaged our top 20 suppliers by spend who are not currently disclosing emissions data, asking them to start measuring their emissions and sharing the data publicly. We will be expanding the scope of the engagement to include more suppliers in FY25. In FY25, we will continue working to establish GHG emissions reduction targets. We have also launched the first iteration of a team member sustainability guide with the goal of educating GitLab team members on how to be more sustainable. In January 2024, GitLab purchased and retired carbon removal credits worth 8,580 tonnes of CO2e. The purchase funds a reforestation program called Trees for Global Benefits, a long-running cooperative carbon offsetting program that combines community-led activities to increase carbon sequestration, encourage sustainable land-use practices, and provide farmers with performance-based payments. We also retired additional credits worth 877 tonnes of CO2e that were purchased in FY23. GitLab is strongly committed to good corporate governance practices, which provide an important framework within which our Board and executive leadership can pursue our strategic objectives for the benefit of our shareholders. The Board’s duty is to serve as a prudent fiduciary for shareholders and to oversee the management of the company’s business. To fulfill its responsibilities and discharge its duty, the Board follows the procedures and standards set forth by GitLab’s Corporate Governance Guidelines and other governance documents . Board members’ responsibilities include: The Board has an Audit Committee, a Compensation and Leadership Development Committee, and a Nominating and Corporate Governance Committee. The composition and responsibilities of each committee are described in our Proxy Statement and in the Investor Relations section of our website. Board committees oversee and review areas of risk that are particularly relevant to them based on their responsibilities and charters. GitLab’s management regularly provides reports to support the Board’s oversight obligations. This reporting cadence provides visibility and information regarding the identification, assessment, and management of critical risks and the company’s risk mitigation strategies. The charter of the Nominating and Corporate Governance Committee tasks the committee with assisting the Board in overseeing company programs relating to corporate responsibility and sustainability, including ESG matters. GitLab’s Senior Director, ESG, reports to the Chief Legal Officer (who is also the Head of Corporate Affairs and Corporate Secretary) weekly and to GitLab’s executive leadership as needed. The Nominating and Corporate Governance Committee meets at least twice per year and ESG topics are discussed with this committee as well as with the full Board as needed. At GitLab, we recognize that to maintain trust and deliver the world’s leading DevSecOps platform, security and privacy must remain our top priorities. Our information security and privacy practices reflect our dedication to safeguarding customer data and building security into the core of our product and our company. The GitLab Security Division’s mission is to enable everyone to innovate and succeed on a safe, secure, and trusted DevSecOps platform. As part of this mission, the Security Assurance Department is responsible for monitoring and reporting on GitLab’s compliance with various security frameworks and standards and for providing GitLab customers with a high level of assurance around the security of GitLab Cloud Services, which is comprised of GitLab.com and GitLab Dedicated. To help ensure that user information is protected, we employ a range of administrative, technical, and physical security controls. For more information on our security practices, please see our Technical and Organizational Security Measures for GitLab Cloud Services. We maintain various security certifications covering GitLab Cloud Services, including ISO/IEC 27001:2013, ISO/IEC 27017:2015, and ISO/IEC 27018:2019. In FY24, we expanded our compliance portfolio to include a SOC 2, Type II report for GitLab Dedicated and added TISAX AL-2 certification for GitLab Cloud Services. In January 2024, we enhanced customer access to assurance resources with the launch of the updated GitLab Trust Center . The Trust Center provides a single, unified location for communicating our compliance and assurance credentials, hosting our security and privacy documentation for customer consumption, sharing important notices, and hosting our internal knowledge base where customers can readily access the same answers we provide in questionnaire responses. This self-service approach not only provides customers with increased visibility of critical information but also enables them to accomplish security and risk reviews quickly and efficiently. GitLab’s cybersecurity program was designed in alignment with industry standards and recognized best practices to identify, assess, and manage material risks from cybersecurity threats. Identified risks are assessed for criticality, prioritized for remediation, and reported by GitLab’s security teams to various levels of our management. Our global incident response team iteratively evaluates security events for impact, using both qualitative and quantitative factors. Security incidents that are assessed as potentially material are escalated to designated members of our senior management and Board, as applicable. Our security program also accounts for potential cybersecurity risks associated with third parties with whom we do business. These risks are continually assessed throughout the vendor lifecycle, from onboarding to offboarding. We also engage in continuous monitoring of our cybersecurity risks and perform security assurance activities via independent, external third parties such as consultants, auditors, and assessors during our robust security certification audits, penetration tests, and bug bounty programs. The Audit Committee has oversight responsibility for risks and incidents relating to cybersecurity threats, including compliance with disclosure requirements and related effects on financial and other risks, and it reports any findings and recommendations, as appropriate, to the full Board for consideration. Management is responsible for and regularly discusses identifying, assessing, and managing material cybersecurity risks on an ongoing basis through programs led by the Chief Information Security Officer, the Chief Legal Officer, and the Chief Financial Officer. For more information, see our FY24 Form 10-K and our GitLab Trust Center . GitLab is fully committed to protecting the personal data of its customers, team members, suppliers, and other stakeholders in accordance with global comprehensive data privacy laws. We take the privacy of personal data very seriously and have initiated a variety of methods and controls so that we know what data we collect and hold and that it is protected appropriately. Our Privacy Statement provides details on how we collect, share, use, and protect personal information and on the choices that customers and users have regarding their personal data. Users also have the right to access, correct, restrict, or delete personal data and to port personal data to another company. Although legal and regulatory requirements related to data privacy may vary by jurisdiction, GitLab provides users with the same rights and choices no matter where they live. For more information on our privacy practices, see our Privacy Statement . The Privacy Team (part of GitLab’s Legal and Corporate Affairs Team) provides support and guidance to uphold consistent business processes around the protection of personal data. Privacy Team members collaborate cross-functionally and serve as advocates to ensure that the data privacy practices of GitLab meet the needs of our cross-functional partners and are continually balanced with an ever-changing global data privacy and protection landscape. The GitLab Data Classification Standard defines data categories and provides a matrix of security and privacy controls for the purposes of determining the level of protection to be applied to GitLab data throughout its lifecycle. As part of our commitment to privacy, GitLab ensures that, where appropriate, projects and personal data processing activities are subject to Privacy Reviews and a Data Protection Impact Assessment (DPIA) as key components of a ‘Privacy by Design’ approach. GitLab’s product mission is to consistently create products and experiences that users love and value. We believe this includes our responsibility to design inclusive products that aim to provide access to content and functionality that enables consumption and contribution from everyone. Our product principles guide us in developing products consistent with the approach of other world-class product organizations. We strive to set an example by empowering our wider GitLab community to build and work with the highest levels of security through our DevSecOps platform. This extends to how we are incorporating artificial intelligence (AI) and continuously evolving the platform to enable secure, responsible development. GitLab is dedicated to responsibly building artificial intelligence (AI) into and throughout our comprehensive DevSecOps platform. We offer GitLab Duo, a full suite of AI capabilities across the GitLab platform so that our customers can ship better, more secure software faster. GitLab Duo follows a privacy- and transparency-first approach to help customers confidently adopt AI while keeping their valuable assets protected. As we work to integrate new AI-powered features into the platform, we do not simply adopt the latest advancements for their own sake. Instead, we take a strategic, intentional approach , ensuring that the features we add have a clear purpose, appropriate controls, and safeguards to protect users’ data. This includes a policy not to use customers’ or users’ AI inputs to train any language models without their instruction or prior consent. In early 2024, we launched the GitLab AI Transparency Center to enable our customers to confidently unlock the enormous potential of AI and emphasize our customer-centric approach to responsible AI development and deployment. The AI Transparency Center currently includes GitLab’s AI Ethics Principles for Product Development , AI Continuity Plan , and AI features documentation . The principles and policies in our AI Transparency Center govern how we responsibly select AI models to use based on an extensive model evaluation process. The launch of the center was a cross-functional effort led by the Legal and Corporate Affairs (LACA) team in partnership with the Product, Security, and Marketing teams. Our commitment to responsible product development also encompasses how we support customers in ensuring security and responsibility throughout the software development lifecycle. GitLab’s security and governance capabilities include a wide range of controls, such as automatic scanning of project dependencies for security vulnerabilities, license compliance, and other risks; vulnerability management and security scanning; security and compliance policies; and comprehensive anti-abuse tools. In FY24, we introduced several enhancements, including support for custom roles , expanded security policies , improved security scanner accuracy, and support for group-level dependency lists for centralized dependency management. The incorporation of end-to-end continuous integration and delivery (CI/CD) features in GitLab supports the default integration of best practices, empowering users to focus on developing code with speed, security, and quality in mind. This includes automated code testing capabilities that make software development easier, faster, and less risky for developers. In FY24, we continued to enhance GitLab’s CI/CD offering with new features such as components , preconfigured CI/CD files that automate the process of building, testing, and deploying software applications, and expanded runner machine types to enable faster CI/CD pipelines. We also improved key GitLab features such as variables and secrets , helping users make more informed decisions about their data and application security while also supporting the best practices in design and secure workflows. GitLab is committed to the highest standards of legal and ethical business conduct. It has long operated its business consistent with operating principles and policies that reinforce this commitment. GitLab complies with all laws and regulations that are applicable to its activities and expects all team members to adhere to our ethical standards and legal and regulatory obligations. These expectations are reflected and reinforced by our Code of Business Conduct and Ethics (“Code of Conduct”) and various supporting policies, procedures, and other resources (collectively, “Compliance Standards”). In FY24, we centralized our compliance efforts. The Ethics and Compliance Program , is designed to help GitLab team members maintain its culture of compliance and to promote ethical decision-making by: The Board, through its Audit Committee, is responsible for administering the Code of Conduct, and for addressing material issues and risks concerning Compliance Standards and applicable laws and regulations. The Audit Committee has delegated day-to-day responsibility for administering and interpreting the Code of Conduct to GitLab’s Chief Legal Officer. Each team member is responsible for reading, understanding, and ultimately complying with GitLab’s Compliance Standards. Team members are expected to deal honestly, ethically, and fairly with customers, partners, suppliers, competitors, and other third parties. GitLab expressly prohibits team members from violating applicable laws and regulations, and any team member who does so may face disciplinary action, up to and including dismissal (subject to local law). To memorialize and reinforce GitLab’s expectations, we require each team member to review and acknowledge our Code of Conduct on an annual basis. Team members are educated on GitLab’s policies and procedures and related laws and regulations through continuous training, both at onboarding and throughout the year, and awareness campaigns. Team members are encouraged to ask questions about our policies and seek guidance when necessary through various avenues, including a Slack channel dedicated to ethics and compliance. GitLab strives to foster a work environment in which ethical issues and concerns may be raised and discussed with supervisors or others without fear of retribution . If a team member becomes aware of a suspected or actual violation of law, regulation, or GitLab Compliance Standards, they have a responsibility to promptly report their concern in accordance with GitLab’s Whistleblower Policy and Code of Conduct. GitLab offers team members a variety of ways to report suspected or actual violations. Team members may raise their concerns orally or in writing to their direct supervisor or manager, to GitLab’s Chief Legal Officer, or to the Audit Committee. At any time, team members may also submit reports using EthicsPoint, a GitLab-provided tool that is available 24 hours a day and allows for anonymous reporting about conduct addressed in the Code of Business Conduct and Ethics. Reports to EthicsPoint can be submitted using a toll-free hotline or through the EthicsPoint website . In addition to EthicsPoint, GitLab has engaged Lighthouse Services to provide an anonymous hotline for team members to submit reports involving team member relations. GitLab treats all reports seriously. Notification of reports submitted via EthicsPoint is automatically provided to the Chief Legal Officer and Chairperson of the Audit Committee and those reports are promptly and thoroughly investigated by qualified personnel at the direction of the Chief Legal Officer, as appropriate. Reports involving team member relations are reported to the People Group team. GitLab’s commitment to legal and ethical conduct extends to its suppliers, contractors, resellers, agents, and consultants (collectively, “Partners”). GitLab expects all Partners, their employees, sub-suppliers, and any other party involved in the execution of GitLab work to comply with all applicable laws, regulations, and the standards set forth in GitLab’s Partner Code of Ethics , which includes standards relating to ethical conduct, including human rights and labor, health and safety, bribery and corruption, environmental impacts, and more. For questions regarding GitLab’s ESG report and data, please contact ESG@GitLab.com . This report contains forward-looking statements within the meaning of the federal securities laws. These statements involve assumptions and are subject to known and unknown risks and uncertainties that could cause actual results to differ materially from those discussed or anticipated. For a complete discussion of risk associated with these forward-looking statements in our business, please refer to our SEC filings, including our most recent quarterly report on Form 10-Q and our most recent annual report on Form 10-K. Our forward-looking statements are based upon information currently available to us. We caution you to not place undue reliance on forward-looking statements, and we undertake no duty or obligation to update or revise any forward-looking statement, or to report any future events, or circumstances or to reflect the occurrence of unanticipated events. Additionally, this presentation contains information related to upcoming features and functionality. It is important to note that the information presented is for informational purposes only, so please do not rely on the information for purchasing or planning purposes. Just like with all projects, the items mentioned during the presentation are subject to change or delay, and the development, release, and timing of any products, features or functionality remain at the sole discretion of GitLab.",
  "GitLab is a functionally organized company in which functions are as mutually exclusive as possible to be efficient. Without a COO or central operating department reporting into the CEO, the CEO gets leverage through an Office of the CEO (OCEO for short) led by the Chief of Staff (CoS) to the CEO. While the Chief of Staff to the CEO is not a part of GitLab’s executive team, this senior person reports directly to the CEO and manages a team that supports the CEO and/or cross-functional projects of importance to GitLab. Help the CEO be more effective and lead select cross-functional initiatives of importance to GitLab. The Office to the CEO consists of: When there are gaps, the Office may expand to include additional roles and responsibilities. For example, at times this Office has included our TeamOps initiative, internal communications , and handbook maintenance . As GitLab is a functionally organized company, the goal in most cases is to own an area until another function is prepared to own and support it. The size of the team and which positions are present have varied over time depending on CEO priorities and required resources. For a list of current OCEO team members, please refer to the company team page . Given the cross-functional nature of the company goals and given these projects are important to the CEO, the Strategy and Operators team members are often involved in these initiatives. The Chief of Staff to the CEO and their team may work on projects that fit any combination of the following: This is not an exhaustive list of the types of work the Office of the CEO might do. GitLab is a functional organization , which means the people are organized by function . When a cross-functional project arises between multiple different Departments, Office of the CEO team members may be included to help with the planning and execution. In many cases, a member of the Office of the CEO will be the directly responsible individual (DRI) for the project. Whether it’s a product feature proposal, a new initiative roll-out, or questions from the board, the Office of the CEO is the group that can be trusted to get things done, get them done quickly, and get them done right. Examples of a cross-functional project: Projects come up that are both important and under-resourced. The Strategy and Operations team members within the Office of the CEO should be known for its ability to become 80% effective on any subject quickly. They are generalists at their core and, while they bring special skills to the table, they are meant to be able to address important problems as they come up. A member of the Office of the CEO might help source candidates for a strategic hire, fix grammatical errors in the handbook, and build a financial model all in the same day based on what is important or top of mind for the CEO at a given point. The team helps with work that other teams may not have the bandwidth to do but is important to the organization and/or the CEO. Examples of an under-resourced project: There may be projects with no clear leader for a myriad of reasons, including that we’re still hiring the point person or the lead is on leave. Because of the team’s ability to come up to speed quickly, they may be tasked with something totally out of their domain with the expectation that they bring their leadership experience to the table, will do the work to make good decisions, and will lean on team members who are subject-matter experts. For example, there has historically been no DRI for the handbook or related content sites . As it became clear that this was a GitLab priority, but no function planned to prioritize this within the coming year, the Office of the CEO stepped in to take on handbook ownership until an owner could be identified. Examples of projects with no clear leader: Some projects or initiatives are very broad and cross-functional and make sense to belong to the CEO but would be inefficient uses of the CEO’s time if fully owned by the CEO. The Office of the CEO is the shepherd for these sorts of projects and collaborates with all team members at GitLab to achieve success on such initiatives. The CEO will have other projects that come up that they will task the Office of the CEO with, such as following up on something or carrying on a conversation on their behalf. Examples of tasks or initiatives that are important to the CEO: Many of the tasks are quick asks: handbook MRs, formatting changes, or questions to be answered. Small asks should be handled as quickly as possible. Other asks, such as OKR-related planning or an initiative that requires alignment with multiple stakeholders, require forethought and more appropriate timing. Some amount of time each week needs to be spent moving these sorts of tasks forward. As a rule, everything in the doc is a TODO for the Office of the CEO. When tasks are DONE, they should be labeled as such. The CEO will review and delete the item once it’s been assessed as completed. Since the team has limited capacity to lean into everything that it may want or be asked to, it is thoughtful about team and individual capacity. Members of the Office of the CEO will be expected to manage both directed and leadership (largely self-directed) work. You can find definitions and examples of directed and leadership work on the CoS to the CEO handbook page . When newly initiated, self-directed, leadership work entails a significant time commitment (>20% capacity for more than a week), the work being done should be flagged to the CEO in a 1:1 or team meeting. This work should stay on the 1:1 agenda between the CoS to the CEO and the team member for the duration of the activity. If needed, activities may be reprioritized based on top demands and priorities. More senior roles within the Office of the CEO can expect to have more leadership than directed work, but all roles should have a mix within a fiscal year. Office of the CEO members should monitor the mix of directed and leadership work that they are doing and provide as appropriate in Office of the CEO Meetings, so the CoS to the CEO and CEO are aware of the current balance and can make adjustments, if needed. While there are rare exceptions, members of the Office of the CEO are not forever owners of initiatives. They plug in to help achieve critical milestones or fill gaps. When a member of the Office of the CEO joins an initiative, there should be agreement from key stakeholders around: Office of the CEO members can be reassigned based on the needs of the business and priorities of the CEO. In most cases, a member of the Office of the CEO will remain with a project until the exit criteria for the Office of the CEO is achieved. If a project extends beyond its anticipated timeframe or priorities change, a member may have to leave a project before the agreed upon exit criteria is achieved. When a member of the Office of the CEO exits an ongoing initiative they will: There may be instances in which the Office of the CEO leaving leads to a project scope being reduced or deprioritized. In these instances, the Office of the CEO member will be responsible for engaging key stakeholders around the keep/expand scope or go/no-go decision. It will be clearly documented and key folks will be involved in the decision and aware of the outcome. The executives get together every quarter for the e-group offsite . The CoS to the CEO plays an important role . It’s 3 to 4 days long with a Functional Leaders Meeting within the following days. There are recurring discussion topics as well as a discussion on content chosen by the CEO. In addition, the CoS to the CEO is responsible for preparing the CEO for offsites by: Functional Leaders is a group comprised of all CEO-Skips , select People Business Partners, and a few other folks as nominated by members of E-Group. The CoS to the CEO enables and manages this group. A member of the Office of the CEO is the DRI within GitLab for the success of GitLab’s partnership with JiHu . Coordination with JiHu requires engagement from multiple functions within GitLab. The Office of the CEO ensures that the appropriate folks are engaged at the right times and that GitLab provides appropriate support to this separate entity. For example, this member ensures that there is an aligned process among relevant GitLab functions after a customer requests to transit to JiHu : Due to its cross-functional scope and access to information, the Office of the CEO is uniquely positioned to see what is happening across the company. Members of the Office of the CEO are encouraged to regularly have coffee chats with folks outside of the team and share key insights and feedback from these conversations. We are evaluating different ways to measure our success as a team. These include: The Chief of Staff to the CEO may occasionally have a Chief of Staff Shadow, a GitLab team member who will participate in a specific project or initiative for a fixed time. Depending on the project or initiative, the team member may spend most of their time with an Office of the CEO team member instead. Shadow responsibilities could include: taking notes, providing feedback, and/or supporting the overall initiative success. This role would be in addition to any existing responsibilities at GitLab. Participants would opt in to experience another function within GitLab and contribute to a different part of the business. Since participation would be in addition to an existing workload, managers must sign off before a CoS to the CEO Shadow can participate. Interested team members can share their interest with the Chief of Staff to the CEO in the #ceo-chief-of-staff-team Slack channel. The CoS to the CEO will follow up with you to understand what you are looking to get out of the experience and review projects that may be a good match. If there is not an existing project, you will be kept in mind for future opportunities. Once a project or initiative to Shadow has been identified and the team member decides to participate, the team member should open a merge request to add their name to the below table. The MR should be shared through Slack in the #ceo-chief-of-staff-team channel for review and merge. Optionally, the team member can make use of the internship for learning as a framework to guide the Shadow period. The Office of the CEO maintains a GitLab Group and Google Group to ensure the appropriate level of access is granted or removed as team members join and leave the team. The following mapping of permission levels from GitLab Groups to Google Groups is being used: There is no automatic syncing between these groups so Office of the CEO team members who have the appropriate permission level in each group should add or remove team members as they join and leave the team. The primary project used by the Office of the CEO to track work specific to our team is Office of the CEO . Please file issues here as needed to track work being done.",
  "Per the stated Roles & Responsibilities , changes to this page must be approved or merged by a code owner. All contributions are welcome and encouraged. Everyone at GitLab has a responsibility to prevent and stop harassment. Working remotely means that the majority of our interactions are by video call or written communication, such as email or shared documents. The exceptions to this are team summits, attending conferences together, and local team meetups. No matter the method of communication, it is expected that everyone will contribute to an inclusive and collaborative working environment and respect each other at all times. Team member behavior is not limited to internal interactions with each other. Our Anti-harassment policy applies to GitLab team members interaction with customers, vendors and community members. Should you become aware of or witness any form of harassment or behavior that violates this policy or our company values , please report the incident directly to the Chief People Officer, Team Member Relations , or a People Business Partner immediately for thorough investigation. GitLab is a global organization and our team is distributed all over the world, so we strive to ensure our team is fully aligned with GitLab’s no tolerance harassment policy despite their location. We want everyone to feel confident and comfortable communicating concerns. GitLab respects, appreciates, understands and supports every aspect of diversity. We aim to continuously foster a globally aware team. This policy applies to all team members of GitLab, whether contractor or employee, in all locations. There are local labor laws in every country and in the case of the United States, state laws, that must be followed when handling, reporting and investigating incidents of harassment. The People Business Partner team and legal counsel, if required, in each of those countries will be called upon to ensure compliance and the appropriate legal processes and procedures are followed. Specific country requirements for employees (subject to changes in employment law) are listed in the Country & US State-Specific Requirements section and will be updated regularly. All individual contributors, managers, and leaders will be subject to disciplinary action, up to and including termination, for any act of harassment they commit. The following are considered forms of harassment and will not be tolerated by GitLab: Sexual harassment is considered unwelcome conduct of a sexual nature that is sufficiently persistent or offensive enough to interfere with the receiver’s job performance or create an intimidating, hostile or offensive working environment. Sexual harassment encompasses a wide range of conduct. Examples of misconduct include, but may not be limited to, the following actions: Sexual harassment is unlawful and is considered a form of team member misconduct. Sanctions will be enforced against individuals engaging in sexual harassment and against supervisory and managerial personnel who knowingly allow such behavior to continue. Any retaliation against an individual who complains of sexual harassment or who testifies or assists in any proceeding under the law is unlawful. Having a diverse workforce, made up of team members who bring a wide variety of skills, abilities, experiences and perspectives, is essential to our success. We are committed to the principles of equal opportunity, inclusion, and respect. All employment-related decisions must be based on company needs, job requirements, and individual qualifications. Always take full advantage of what our team members have to offer; listen and be inclusive. We do not tolerate discrimination against anyone, including team members, customers, business partners, or other stakeholders. Any form of discrimination towards an individual is strictly prohibited, including: Report suspected discrimination right away and never retaliate against anyone who raises a good faith belief that unlawful discrimination has occurred. If you believe you have been discriminated against or witnessed discriminatory practices, please contact the Chief People Officer, a People Business Partner , or the Team Member Relations Specialist to initiate an investigation into the behavior. GitLab has also engaged Lighthouse Services to provide anonymous reporting methods, as described in the How to Report Violations section. GitLab does not tolerate violent acts or threats of violence. The company will not tolerate fighting, bullying, coercion, or use of abusive or threatening words directed to, about, or against a co-worker, lead, manager, executive, candidate, client/customer, vendor, contributor, or any other person. No individual employed by GitLab should commit or threaten to commit any violent act or discuss committing such offenses, even in a joking manner. Retaliation of any sort for filing a claim of harassment will not be tolerated. If you believe you have been retaliated against, please contact the Chief People Officer, Team Member Relations or a People Business Partner to initiate an investigation. If someone messes up, people are encouraged to speak up publicly and within the moment, in order to let that person and others know that what happened was not inclusive behavior. This makes for a situation from which all parties can learn, and is one which promotes understanding. Additionally it makes it possible for that person to de-escalate the situation by correcting themselves and apologizing. This does not ensure there will be no consequences. However, it will greatly reduce the chance of escalation and has the potential to help a situation become comfortable and inclusive again. If a threat is made against you or someone else that makes you or another team member question their safety due to an issue, merge request, email or other work related matters do not hesitate to initiate the following process: If managers become aware of misconduct, they must deal with any allegations expeditiously, seriously, confidentially, and fairly, whether or not there has been a written or formal complaint made to People Business partner. Informed managers are expected to: Managers who knowingly allow or tolerate any form of harassment or retaliation, including the failure to immediately report such misconduct to People Business Partner, are in violation of this policy and subject to disciplinary action, including termination. All employees have the responsibility to help create and maintain a work environment free of bullying and harassment and can help by: The Chief People Officer and People Business Partners are responsible for: All individual team members, managers, and leaders will be subject to disciplinary action, up to and including termination, for any act of harassment they commit. Although disciplinary action will be specific to each case, it can generally be classified into 4 levels: First time occurrences of inappropriate behavior. An act out of character. After formal investigation, coworkers still feel comfortable working with the offender. Recurring socially inappropriate behavior. Major infraction, including retaliation, or recurring socially inappropriate behavior after a written admonition. Serious cases, including any criminal offence. Training and guidance on understanding, preventing, and dealing with discrimination and sexual harassment will be provided to both managers and individual team members. The courses and the related enrollment logic can be found here . This training will be provided on a regular cadence, consistent with local requirements, and/or when new legal requirements are introduced. If attempts to resolve the problem in an informal manner prove insufficient or if these attempts were refused or proved to be ineffective, the victim may follow the procedure below: Without prejudice to the provisions that may arise from a judicial process instituted by the victim, one or more of the following sanctions shall be imposed on the person guilty of undesirable conduct: GitLab shall impose, by registered letter and within five working days, the sanctions imposed upon the person who has been guilty of undesirable behavior. In case an employee abuses this complaint procedure, the above sanctions may also apply for the employee. GitLab has engaged with an external health and safety service called Mensura who are responsible for handling any complaints of harassment that are received but can not be resolved informally and internally. Team members in Belgium may contact this service if they wish and make a request for an informal or formal psychosocial intervention. A request for a formal intervention should include the following: The psychosocial intervention advisor will investigate further and provide a report to the employee and People Business Partner with a recommended course of action. Protection from Power Harassment “Power harassment” is defined as any act by a person using his/her authority in the workplace, such as job position or human relationship with a team member, beyond the appropriate scope of business conduct, which causes such team member mental distress or physical pain or degradation of the working environment. The law defines power harassment as “remarks or behaviour by people taking advantage of their superior position that go beyond business necessity, thereby harming the workplace environment. GitLab expressly prohibits any behaviour toward team members that falls within the definition of power harassment. Depending on the individual, a person may feel dissatisfaction when given the instructions, advice, or guidance necessary in the ordinary course of business. In such cases where the actions are conducted within the scope of appropriate business practice, they should not fall under the category of power harassment. Examples of power harassment include, but may not be limited to: Team members who feel that they may have been subjected to power harassment may bring their complaints to their People Business Partner, who will engage the appropriate internal consultation services and respond to claims. GitLab is prohibited from dismissing or treating unfairly any team members who report harassment cases or cooperate in an investigation or consultation process. Prohibition of Sexual Harassment Team members are prohibited from any activities that cause disadvantage or discomfort to other employees or that are damaging to the work environment by way of speech or behaviour of a sexual nature. GitLab shall take all necessary measures in managing employment to prevent sexual harassment at the workplace. Prohibition of Harassment Due to Pregnancy, Childbirth, Child Care Leave, or Family Care Leave Team members are prohibited from any activities that are damaging to the work environment of other employees by way of speech or behaviour related to pregnancy, childbirth, etc., or use of systems or measures related to pregnancy, childbirth, child care, family care, etc. GitLab shall take necessary measures in managing employment to prevent harassment due to pregnancy, childbirth, child care leave, family care leave, etc. at the workplace. Prohibition of Any Other Forms of Harassment In addition to the prohibitions listed above, team members are prohibited from any other forms of harassment at the workplace that are damaging to the work environment of other team members such as by way of speech or behaviours related to sexual orientation or gender identification. The process and procedure for managing complaints of sexual harassment is set out in the GitLab India Private Limited Policy Against Sexual Harassment . Complaint Procedure and Contacts The Company is aware of and recognises the effect that bullying, harassment and sexual harassment can have on its team members. In addition to EthicsPoint and Lighthouse , please review the Team Member Relations handbook page. GitLab also offers a Harassment Complaint Form that any team member may use to document any instance of any type of workplace harassment. Simply copy the form, fill it out with your information and send it to our Team Member Relations Specialist at teammemberrelations@gitlab.com . Every team member located in the state of California will be required to read, review, and understand the following three documents during their onboarding: Every team member located in the state of Connecticut is required to take the 2 hour Anti Harassment Training for Supervisors. This training is to be completed by all individual contributors as well as managers. Team members who reside in Maine have the right to file a complaint through the Maine Human Rights Commission within 300 days of the date of the alleged discrimination. You may contact the Commission by phone at (207) 624-6290, TTY Maine Relay 711, or by visiting the office located at 51 State House Station, August Maine, 04333-0051. It is unlawful under the Maine Human Rights Act for an employer to retaliate against you because you filed a complaint of discrimination or because you aided in an investigation. Team members who reside in Massachusetts have the right to file a complaint to the Massachusetts Commission Against Discrimination (MCAD) : 1 Ashburton Place, Suite 601, Boston, MA 02108, tel: (617) 994-6000, TTY:(617) 994-6196. Nondisclosure or Nondisparagement Agreements Under this policy, a nondisclosure agreement is any agreement by which one or more parties agree not to discuss or disclose information regarding any complaint of work-related harassment, discrimination, or sexual assault. A nondisparagement agreement is any agreement by which one or more parties agree not to discredit or make negative or disparaging written or oral statements about any other party or the company. A no-rehire provision is an agreement that prohibits a team member from seeking reemployment with the company and allows a company to not rehire that individual in the future. The company will not require a team member to enter into any agreement if the purpose or effect of the agreement prevents the employee from disclosing or discussing conduct constituting discrimination, harassment, or sexual assault. A team member claiming to be aggrieved by discrimination, harassment, or sexual assault may, however, voluntarily request to enter into a settlement, separation, or severance agreement which contains a nondisclosure, nondisparagement, or no-rehire provision and will have at least seven days to revoke any such agreement. Nothing in this policy precludes any person from filing a formal grievance in accordance with a collective bargaining agreement [if applicable], the Bureau of Labor and Industries’ Civil Rights Division 800 NE Oregon St., Suite 1045 Portland, OR 97232, tel: (971) 673-0761, TTY Relay 711, or the Equal Employment Opportunity Commission. Note that Oregon state law requires that any legal action taken on alleged discriminatory conduct (specifically that prohibited by ORS 659A.030, 659A.082 or 659A.112) commence no later than five years after the occurrence of the violation. Other applicable laws may have a shorter time limitation on filing. Team members who reside in Rhode Island have the right to file a complaint with the Rhode Island Commission for Human Rights located at 180 Westminster St., 3rd Floor, Providence, RI 02903. Phone: (401) 222-2661, Voice Relay: 7-1-1. If the a team member who resides in Vermont is dissatisfied with this employer’s action, or is otherwise interested in doing so, they may file a complaint by writing or calling the Vermont Attorney General’s Office, Civil Rights Unit, 109 State Street, Montpelier, VT 05609, ago.civilrights@vermont.gov , tel:(888)745-9195 (Toll Free VT) or (802)828-3657 (voice/TDD). Complaints should be filed within 360 days of the adverse action. We are continuously gathering country specific references to review regulation and obtain guidance on the management of harassment or misconduct at work. Here are a few authorities we referred to in the creation of this policy:",
  "At the core of our team’s vision, mission, and strategy is our ability to impact GitLab’s overarching mission : to make it so that everyone can contribute to and co-create the software that powers our world . When everyone can contribute , users become contributors and we greatly increase the rate of innovation. As a Talent Acquisition team, we have an outsized impact on GitLab’s ability to make this mission a reality, by connecting top talent to profound careers from wherever they are in a truly distributed, remote workforce. To create globally inclusive access to opportunities so that everyone can contribute to and co-create the software that powers our world . It is the Talent Acquisition Team’s mission to predictably build distributed, representative teams that enable team members to co-create the future of software. As we set out to achieve this vision, we will continue to rely on core guiding principles to define how we build toward the future. We strive to be as transparent as possible, but these sections are only available for our GitLab team members. Please find pages for potential and active applicants below. Remote.com: remote.com onboarding timeline Processing contracts for candidates hired via Global Expansion could take up to 10 bueisness days. Processing for Italy (Dirigenti) could take up to a month, so we recommend start dates at least 6 weeks after the offer is generated.",
  "Diversity, Inclusion & Belonging is fundamental to the success of GitLab. We include it in every way possible and in all that we do. We strive for a transparent environment where all globally dispersed voices are heard and welcomed. We strive for an environment where people can show up as their full selves each day and can contribute to their best ability. And with over 100,000 organizations utilizing GitLab across the globe, we strive for a team that is representative of our users. Diversity complements our other values , specifically Collaboration, Efficiency and Results. And diversity in our leadership supports innovation , promotes better decision making and improves financial results . At GitLab, Diversity, Inclusion & Belonging (DIB) is fundamental to our success. We are building an environment where everyone can thrive and contribute through three strategic pillars: Pillar 1: Empowering our people Pillar 2: Uplifting our communities Vision : To create a global culture where everyone belongs, can contribute, and thrives—making GitLab the top organization for remote learning, development, and inclusive excellence. This updated structure reflects the comprehensive three-pillar approach shown in your attachment, moving away from the A.D.A.P.T. acronym to a more holistic framework that emphasizes empowerment, community engagement, and inclusive design. The phrase “Diversity, Inclusion & Belonging” (or DIB) refers to the terminology for the initiative to create a diverse workforce and an environment where everyone can be their full selves. Diversity refers to characteristics of the people who make up GitLab and how they identify. Race, gender, age, ethnicity, religion, national origin, disability, sexual orientation are some examples of how the data might be categorized when looking at GitLab’s diversity. Sometimes we can see things that make us diverse and sometimes we can’t. We believe that a company composed of a diverse group of people may lead to diverse opinions and ideas which, if productively engaged with, can build innovation. GitLab uses the term “underrepresented” and it is meant to be a way of recognizing that we need more of what we do not have so that we can be at our best. The context is “at GitLab” or “in a specific department or team at GitLab.” This term is generally used in the context of reporting on how GitLab is working on understanding and improving the sourcing, interviewing, hiring, and retention of those who either want to work or currently work at GitLab. For additional information about how GitLab uses this data to make progress, please see “select underrepresented group” Inclusion is the ability to recognize, respect, and value differences in those around us. It focuses on the action and understanding of what makes us diverse and working towards building a diverse team and creating welcoming workplace. It requires skills such as empathy, openness, listening, etc. This lays the foundation of an inclusive mindset. The foundation of understanding gives way to the actions and being intentional about creating policies and practices that embrace diversity that in the end change the overall company culture to create an environment of inclusion. Inclusion also means being aware of both positive and negative biases and how those biases impact who we hire, work with, and retain. GitLab believes that many perspectives coming together creates a more innovative environment to work in with more satisfied team members, leading to a better product and increased profitability. Belonging is a feeling that your insights and contributions are valued. It goes back to team members feeling they can bring their full selves to work. It’s not enough to simply include people to have a “seat at the table”, but it’s important to amplify everyone’s voices, remove barriers and appreciate each others for their unique backgrounds. Embracing inclusion may increase the sense of belonging . Team members become more engaged and are invested in the work they are doing, because they are able to see themselves in the work being accomplished with the company overall. We believe in empowering team members to get their work done efficiently and collaboratively by establishing clear DRIs for all our work. DRIs do not owe anyone an explanation for their decisions , but DRIs can still acknowledge input by closing an issue and marking it Won't Do or commenting on an issue acknowledging that they have read all the comments. All team members don’t have to agree on the best course of action* we can disagree, commit, and disagree - but everyone can contribute and it is on the DRI to acknowledge those. Some other ways we actively cultivate a sense of Belonging at GitLab include creating and cultivating allies, welcoming family members in the background of a call, and sharing negative feedback in 1-1 settings . A good way to look at Diversity, Inclusion & Belonging is: An underrepresented group describes a subset of a population that holds a smaller percentage within a significant subgroup than the subset holds in the general population. The accepted definition of “underrepresented minorities” from the National Science Foundation and other major research institutions focuses on racial and ethnic groups whose representation in a profession is significantly below their representation in the general population. Populations whose representation in tech roles has been historically low. Tech roles are based on Federal Employer Information Report EEO-1 skill designations. At GitLab, this includes all technical roles across the company, such as Engineering & Product. At GitLab, we consider the following groups to be underrepresented groups : **Due to data and or legal limitations, this is not an exhaustive list of all of our underrepresented groups. Those with disabilities, those that identify as LGBTQIA+, those who choose not to disclose as well as underrepresented ethnicities outside of the US, etc. The DIB Team is actively working on finding data sets outside the US and inclusion metrics for underrepresented groups we cannot report on as team member representation. Of Note: Management refers to Team Members who are People Managers, whereas Leadership denotes Team Members who are in Director-level positions and above. Source: GitLab’s People Analytics Team, WorkDay Inclusive teams are naturally more engaged, collaborative and innovative. We aim to align our values to be reflective of our company wide commitment to fostering a diverse and inclusive environment. The GitLab team is fully distributed across the globe, providing our team the opportunity to connect with each others cultures, celebrations and unique traditions. We collaborate professionally and connect personally! Our unique all-remote team opens our door to everyone. Candidates are not limited by geography and we champion this approach , to the extent that it’s possible, for all companies! By having no offices and allowing each GitLab team member to work and live where they are most comfortable, GitLab offers a uniquely inclusive culture. Learn more about GitLab’s all-remote culture . Please see our identity data . This page provides an overview of our Diversity, Inclusion & Belonging Talent Acquisition initiatives . This page provides an overview of our Diversity, Inclusion & Belonging Engineering initiatives We list our Pregnancy & Maternity Care publicly so people don’t have to ask for them during interviews. In addition GitLab offers an Employee Assistance Program to all team members via Modern Health , a one-stop shop for all tools related to mental well-being and self-improvement. In our GitLab Values we list: ‘Use inclusive language. For example, prefer “Hi everybody” or “Hi people” to “Hi guys”. We have created several TMRGs and welcome interest in creating new ones. Would you like to sign up for an Team Member Resource Group, start an TMRG, or just learn more? See our TMRG Guide . GitLab welcomes military veterans from around the world, as well as military spouses, to learn more about life at GitLab and to apply for vacancies . We recognize the values gained from military experience, and we foster an inclusive atmosphere to thrive in when returning to civilian life. Our all-remote culture provides an ideal work environment for military veterans and spouses. By empowering team members to live and work where they are most comfortable, veterans and spouses can work in a safe, nurturing environment that they choose and design . We encourage military veterans and spouses to read testimonials from GitLab team members to understand the benefits of all-remote when joining the workforce following military service. We are committed to our Military Leave policy. GitLab is actively iterating within Diversity, Inclusion & Belonging and Talent Acquisition to ensure that additional underrepresented groups are pursued, embraced, and positioned for success. GitLab welcomes all types of team members, including any that may choose to identify as ones that currently have or were previously diagnosed as having a disability. In our HRIS (Human Resource Information System) Workday, on the Job tab page, in the Equal Employment Opportunity section, we have a field titled Disability Status that we ask our team members to complete during the onboarding process. The reason we ask is because it is a legal requirement in the United States under Section 503 of the Rehabilitation Act of 1973 , as we are a Federal contractor and are required to ask employees to voluntarily self-identify if they have a disability or have ever had a disability, and to provide equal employment opportunity to qualified people with disabilities. The options of this field are: If you are unsure how to answer, please review our Individual with Disabilities Policy . At GitLab, we are proud to make reasonable accommodations to the known disability of a team member. Please review the reasonable accommodation handbook section if you need a reasonable accommodation due to your disability. Find more information on GitLab Inc’s Individuals with Disabilities policy. The United States Office of Federal Contract Compliance Programs (OFCCP) enforces the affirmative action provisions of the Vietnam Era Veterans’ Readjustment Assistance Act of 1974. This law, sometimes referred to as VEVRAA, requires employers doing business with the United States Federal Government (such as our GitLab Federal entity) to take steps to recruit, hire and promote protected veterans. It also makes it illegal to discriminate against protected veterans when making employment decisions on hiring, firing, pay, benefits, job assignments, promotions, layoffs, training, and other employment-related activities. Under VEVRAA, a veteran who served on active duty in the U.S. military and was discharged or release from service under conditions other than dishonorable may be classified as one or more of the four Protected Veteran categories: In our HRIS (Human Resource Information System) Workday, on the Job tab page, in the Equal Employment Opportunity section, we have a field titled Protected Veteran Status that we ask our US-based team members to complete during the onboarding process. The reason we ask is because it is a legal requirement in the United States for us to request this information. We encourage GitLab team members to self-disclose in our HRIS without any fear of judgment or negative consequences, but it is always optional. All veteran status data is completely confidential, and only requested for mandatory reporting purposes. Above this field, we have a section titled Veteran Status that we ask our US-based team members to review and also complete during the onboarding process, if it applies to them and if they so wish. The reason we ask is because it is a legal requirement in the United States for us to request and document this information. We encourage our US-based GitLab team members to self-disclose their Veteran Status in our HRIS without any fear of judgment or negative consequences, but it is always optional. Again, all veteran status data is completely confidential, and only requested for mandatory reporting purposes. If you are a team member on a GitLab Inc or Federal contract and a person with disability veteran you may request a “reasonable accommodation.” A reasonable accommodation is one that allows you to perform your job, and must be provided by GitLab unless doing so would cause GitLab significant difficulty or expense. A reasonable accommodation does not change essential job functions. GitLab can choose the type of reasonable accommodation that will be made available; however, the accommodation must be effective. More information on how to request a reasonable accommodation is available here . Please review the reasonable accommodation handbook section if you would like an accommodation due to your veteran status. Team members who might use this process include: DIB and L&D team members, managers creating training for their teams, departments creating required compliance training, team members creating training for their peers or community, Developer Relations creating training for the wider GitLab community GitLab team members are distributed across the globe, giving us access to an array of opportunity. We encourage collaboration with global organizations and programs that support underrepresented individuals in the tech industry. GitLab also provides additional support through the GitLab Diversity Sponsorship program . We offer funds to help support the event financially, and if the event is in a city we have a GitLab team member, we get hands-on by offering to coach and/or give a talk whenever possible. We encourage organizers of events that are supported through our GitLab Diversity Sponsorship program to share this sign up link with attendees. Everyone can contribute.",
  "This page contains leadership pointers. The first couple of headers indicate which group they apply to, using the groupings defined on our team structure page . In an all-remote organization, we want each team member to be a manager of one . A manager of one is an attribute associated with our Efficiency value . To be successful at GitLab, team members need to develop their daily priorities to achieve goals. Managers of one set the tone for their work, assign items and determine what needs to get done. No matter what role you serve, self-leadership is an essential skill needed to be successful as a manager of one. In some cases, a individual in the Management group , Director group , S-group , or even E-group may have an “Interim” or “Acting” title. In either case, they will be fulfilling the full responsibilities of the role. If you have any questions, about the future of the role, please ask them or their manager. Individual departments will have their own criteria for who is eligible to occupy these roles, so please check the career development page for your department. Please see the Making Decisions Leadership page. Most companies communicate from top to bottom through a chain of command. This communication flow often empowers managers, but it also introduces inefficiency as team members are not able to connect directly with the people they need to communicate with in order to get their work done. At GitLab, every team member is encouraged to reach out to whoever is the correct person (or people) to quickly unblock issues, solve problems or support in other ways. Do be courteous of your direct manager and copy them on the request. We don’t encourage unnecessary friction in asking team members to escalate through managers and wait for responses to come back. What matters is efficiency in getting to results. Slack the CEO, Slack a VP, or Slack a peer. Do what you need to do to make GitLab successful. Managers should not be bottlenecks or silos for communication. Anyone should feel comfortable reaching out to anyone else with the best information they can to solve a problem. This is a more efficient , transparent , and collaborative way to work. Giving regular feedback is extremely important for both managers and team members. Feedback can take the form of coaching sessions, separate from 1-to-1 meetings . Giving feedback is also about being prepared and, depending on the situation, you should create separate agendas and structure them as follows: Sometimes when performance dips, the best way to tackle it is to try to determine the root cause. This is easier said than done. There is a great tool that CEB (now Gartner) created to help with this called performance issue root cause diagnostic . It may not always be possible or appropriate to determine the root cause, so the underperformance process should be followed. As a leader, the way you respond to negative feedback makes a significant impact on your team. Remember that it can be difficult for people to approach someone in authority with concerns and respond with sensitivity and appreciation. In particular, we recommend that you keep the following in mind: A team member’s README page is intended to help others understand what it might be like to work with them, especially people who haven’t worked with them before. It’s also a well-intentioned effort at building some trust by being intentionally vulnerable, and to share your ideas of a good working relationship, to reduce the anxiety of people who may be on your team, now or in the future. READMEs provide a genuine report on how a person works, reducing bias/assumption and enabling people to work together based on a common framework. As part of GitLab’s transparency value, we encourage each GitLab team member to consider creating their own README. GitLab division README pages are linked below for context. Reading other READMEs is an important way to get ideas on what you can include in yours. Let these serve as a guide and inspiration to you. Alternatively you can create your README dogfooding GitLab’s README profile customization feature. Follow documentation on how to add details to your GitLab profile with a README. Do not forget to add your profile’s link to you division’s holding page. Once your README is created, consider adding a link to it from following places: This provides maximum visibility to others, so that they may read your README in advance of working with you. This allows them to take your working style and communication preferences into account, ideally increasing the overall level of empathy expressed. READMEs are particularly powerful when working with those outside of GitLab, who may be unfamiliar with our values . A README is a beacon of transparency , and helps set the tone for any working relationship. Coaching is about helping others help themselves. It is not about giving advice, instruction, or telling someone what to do. Coaching is about focusing on the future and identifying where the coachee wants to be and what they want to achieve. At GitLab, we’ve defined coaching as a conversation that helps people think for themselves, find their own answers, and commit to action they design. As a coach, your role is to clarify the pathway from the current state to the future. Coaches do this by enabling the coachee to make informed choices based on deeper insight. Please see no-matrix-organization We want to promote organic cross-functional collaboration by giving people stable counterparts for other functions they need to work with. For example, each Strategic Account Executive (SAE) works with one Sales Development Representative (SDR). With our categories every backend team of developers maps to a Product Manager (PM) and a frontend team . Giving people a stable counterpart allows for more social trust and familiarity, which speeds up decision making, prevents communication problems, and reduces the risk of conflicts. This way we can work effectively cross functionally without the downsides of a matrix organization . We want the best combination of a factory and a studio . The studio element means anyone can chime in about anything, from a user to the CEO. You can step outside your work area and contribute. The factory element means everyone has a clearly assigned task and authority. Team members should feel comfortable escalating issues when help is needed to resolve unexpected challenges. Effective escalations are good, because they speed up decision making. When team members escalate an issue, another person is brought in as a decision maker or adviser as other team members disagree or need help with alignment or a serious trade-off is needed. Escalation can offer clarity and a path forward, and can be a sign of seniority for the person initiating the escalation when they know what, how, and when to escalate. As noted in this medium article , explicit esclatation should answer these four questions: Folks who are escalating an issue should avoid surprising folks in the management chain. This means that other relevant team members should be aware that an escalation is occurring. For example, in E-Group, members agree that they will not go to the CEO with an escalation without first notifying other relevant members that this is happening. There may be some exceptions to first notifying managers or peers. For example, a team member feels unsafe in voicing a concern to a manager or their peers and feels that they can’t effectively escalate with standard notification without retribution. While exceptions may be appropriate, they should be rare. After a team member escalates an issue, it is OK if they disagree, commit, and advocate with the decisions made by the person they escalated to. Process has a bad reputation. It has that reputation for things that we try to avoid doing at GitLab. When you have processes that are not needed it turns into a bureaucracy. A good example are approval processes. We should keep approval processes to a minimum, by both giving people the authority to make decisions by themselves and by having a quick lightweight approval process where needed. But process also has good aspects. Having a documented process for how to communicate within the company greatly reduces time spend on on-boarding, increases speed, and prevents mistakes. A counterintuitive effect is that it also makes it easier to change processes. It is really hard to change a process that doesn’t have a name or location and lives in different versions in the heads of people. Changing a written process and distributing the diff is much easier. Managers have an tremendous responsibility around talent acquisition and retention of team members. GitLab leadership and management approach was built using principles covered in the book “ High Output Management .” Please see High Output Management to learn more. Building a team to deliver results is a very important aspect of improving efficiency and iteration . A high-performing team will always deliver results. As a leader at GitLab, your role is to develop a high-performing team to reach the desired level of performance and productivity. There are certain traits that high-performing teams display at GitLab: Watch the replay of our conversation with Jeb Hurley, Co-founder and Managing Partner Brainware Partners where we discussed: Skills and behavior of building high performing teams competency for Managers : The Drexler-Sibbet Team Performance Model is an excellent tool to help build high performing teams at GitLab. The model provides a roadmap for a team and a common language. It is a simplified description of how a team works together that highlights the most important things the team needs to focus on to reach high performance. At GitLab, we can use it as a frame of reference to developing high performing teams. It can help Managers ensure new and existing team members know the mission and direction of the team by the following: 7 Stages to developing high performing teams: Manager Resource: Identifying & Addressing Burnout Building and maintaining high performance includes staying mindful of team well-being and potential burnout. With GitLab’s results-driven culture, the demands of product innovation around AI, the fast-paced and ever-evolving business environment, our organization recognizes the crucial balance between achieving ambitious goals and maintaining the well-being of our team members. Everyone can access this handbook resource designed for managers to identify & address burnout . This has an ongoing impact on team performance. M-teams are management support groups made up of 3 to 6 managers who are in timezones that allow for sync meetings among members. M-teams should set up a regular meeting on a cadence agreed by the members with the agenda being “what’s challenging this week?”. Decide who will facilitate and each person will get a chance to have their challenge discussed in the meeting. When it’s your turn, you talk a little about what you’re struggling with. M-groups agree to a level of confidentiality so that group members are willing to be vulnerable; vulnerability leads to trust and better outcomes for the group. If you’re interested in starting or joining an m-team meeting, reach out to other managers in the #managers Slack channel. Books in this section can be expensed . Notable books from the E-Group Offsite Book Selections may be added to the list below. We sometimes self-organize book clubs to read through these books as a group. When you give leadership training please screen share the handbook instead of creating a presentation . Feel free to reach out to anyone in the People Group for further support on leadership development topics. You can find us on the team page , using the People Group dropdown. The team may also be reached via HelpLab. Learn more on GitLab’s view of being a public company . We have a page which documents our Mitigating Concerns . Many of our values help to mitigate some of these concerns.",
  "Why we exist: GitLab Talent Development (commonly referred to as “Learning & Development”) exists to enhance team member performance, expand capabilities, and further develop skills that make GitLab team members the top talent in the industry. We strive to foster a culture of continous growth so that GitLab remains a great place to work. Positive impact on team member experience leads to a positive impact on results, which leads to a positive impact on GitLab customers. Where are we going: GitLab seeks to be recognized as a top organization for remote learning & development. We aim for a future where everyone contributes to a culture of curiosity. What we do: We empower individual contributors and equip leaders through self-service learning. We accomplish a culture of development by: We are a small team, but we’ve got a big role to play at GitLab! The Learning & Development team has a number of resources to help team members learn new skills. Click the button(s) below to learn more about each one: The Learning and Development team is available to support GitLab teams in the following capacities. Requests will be evaluated as they come in based on team’s capacity for support, business impact, correlation to company OKRs, and number of impacted users. We review and classify requests based on the scalability and possible organizational impact. Learning solutions that can be applied and used company-wide take priority. Our process includes: We’re formalizing a more structured process to engage with the team going forward, but for now please get started by opening an issue the lxp-contributions project and tagging a member of the L&D team to review. If you have any questions, you can always reach out to us in Slack via #learninganddevelopment . For in depth GitLab product training, check out the Get Started with GitLab resources on GitLab University, and take a look at the various certifications available, which are freely accessibly for GitLab Team Members. The Talent Development team do not create learning content for product training. If you have any queries or requests for product training, please get in touch with the digital success team in the #digital-success Slack channel The L&D team frequently uses the following tools for creating and communicating learning opportunities at GitLab. Rise 360 is an excellent tool for creating engaging, interactive elearning content that integrates seamlessly with our new LevelUp learning system. While LevelUp offers many great features, its interactive content authoring options can be somewhat limited, making Rise 360 a valuable complement for developing more dynamic and visually appealing courses. We maintain a small number of Rise 360 licenses that can be easily redistributed to team members based on project needs. If you’re interested in creating a Rise course and would like access to our licenses, please reach out in #learninganddevelopment where our team can support your course development needs and help you get started.RetryClaude can make mistakes. Please double-check responses. If you’re looking to develop your own e-learning content, there are a number of things to think about in order to maximize it’s effectiveness. In July 2024, GitLab published the first iteration of our Learning Content Accessibility Guidelines . We ask that team members who create both internal and external learning content: Adults learn differently in the workplace than in traditional learning environments or how they learned growing up. If you are developing training, consider applying principles related to Adult Learning Theories, those include: GitLab provides access to premium content from leading industry partners to support diverse learning objectives across all teams. Through Masterclass, team members can access world-class instruction from renowned experts across business, leadership, creative, and technical disciplines. For technical teams, Google Cloud Skills Boost offers hands-on labs and courses to build cloud computing expertise and earn Google Cloud certifications. O’Reilly provides an extensive library of technical books, videos, and interactive learning resources covering programming, data science, cybersecurity, and emerging technologies. Additionally, LevelUp houses an extensive, searchable library of content from various providers, making it easy to find relevant courses, tutorials, and resources tailored to specific skills and professional development goals. When planning your learning path, explore these curated content options first—they’re already vetted, high-quality, and immediately accessible to support your growth objectives.",
  "Onboarding is incredibly important at GitLab. We don’t expect you to hit the ground running from day one. We highly recommend taking at least two full weeks for onboarding and only in week three starting with team specific onboarding and training. Please feel free to participate in your team’s work in your first two weeks, but don’t feel like you have to contribute heavily. All onboarding steps are in the onboarding issue template which is owned by the People Operations team. The onboarding process for the new team member is self-driven and self-learning , whilst also remaining as asynchronous as possible settling into the remote life at GitLab. At GitLab we take great pride in dogfooding our own product, that is why all onboarding tasks are completed in a GitLab issue. First of all, what is an issue? You can learn more about what an issue is here . The People Operations Specialist assigned to the team members specific onboarding will open the onboarding issue at least 4 days prior to the hire date. Each onboarding issue has a main section that contains tasks relevant to all GitLab team-members and a due date of 30 days. Below the main section are department and role-specific tasks. Some roles and departments have tasks that link to a supplemental issue template or an additional onboarding page. Reach out to your onboarding buddy or other GitLab team members if you need help understanding or completing any of your tasks. Through onboarding issues, you should gain access to our team member baseline entitlements . On Day 2 of onboarding an Access Request will be generated, if a template has been created for the role. Access requests are owned by the IT team. If you have any access requests related questions, please reach out to #it-help in Slack. In certain instances, the People Operations team may not be able to assist with onboardings due to a national holiday or Family and Friends Day. These specific dates are documented in the People Operations team availability . The People Operations team hosts a pre-onboarding call known as the Ta-New-Ki call (a play on the abstract Tanuki i.e. Japanese raccoon dog you will find in our logo). The purpose of this call, which is hosted in Zoom, is to give soon to onboard team members the chance to meet / socialise and to provide an opportunity to ask any lingering questions ahead of their start date. Internally we refer to this as an AMA (Ask Me Anything) call. Hiring managers and current team members (such as onboarding buddies), are more than welcome to join the call. This call occurs every two weeks on Thursday at three times to account for timezones. For current team members: To add the invite to your calendar, review the GitLab Team Meeting Calendar. For future team members: You will receive an email with the future dates. Please note that this call will take a place a week or two before your actual start date. By default, all new team members are added to the below Slack channels from day 1 to ensure that they are able to ask any questions or for assistance in the correct channels upfront: An issue is created for new team members at least 4 business days prior to their start date. The Manager and a People Operations Team member will be assigned to this issue. Managers and People Operations all have tasks that need to be completed prior to the start date to ensure a smooth and successful onboarding process. For questions or help with any of these tasks feel free to reach out in the issue or by creating a request via HelpLab. The Senior People Operations Specialist completes a monthly audit of all open onboarding issues to ensure that the new team member, manager and People Operations team tasks are completed. More importantly, there are certain tasks which need to be completed in line with our company compliance (security, payroll, etc). If any tasks are still outstanding, the People Operations Specialist will ping the relevant members on the issue requesting action on the items or checking whether the issue can be closed. It remains the responsibility of the People Operations Specialist to close the issue and remain compliant. The employment bot will automatically close any onboarding issues still open after 60 days. First of all, what is an issue? You can learn more about what an issue is here . To ensure a successful completion of the onboarding issue, it is important that all tasks are checked off, whether the task is applicable to the onboarding team member or not. Checking the box indicates one of the following: These templates are used by the People Operations team to onboard new team members. All onboarding related employment templates (country and role based) can be found in the public repository The People team is not involved in the process for hiring, onboarding, or offboarding temporary service providers. You can work with the Procurement team on these onboardings and review the temporary service provider internal handbook page for templates and other helpful information.",
  "The offboarding process is facilitated by the People Operations team who collaborates throughout the process with various other stakeholders such as Team Member Relations, IT Operations and Payroll. If you have any questions around the offboarding process, please be sure to review the Offboarding FAQs handbook page. Note: Departing team members will receive a comprehensive email prior to their final date of employment with information such as the impact on benefits coverage, final pay, arranging for a laptop wipe and stock administration. For system access questions and laptop wipes related to offboarding, send an email to gitlab-laptop-recovery@gitlab.com . For payroll questions or outstanding expense claims, please reach out to either uspayroll@gitlab.com or nonuspayroll@gitlab.com . For any other offboarding questions from active team members please reach out to People Operations via HelpLab or send an email to people_operations@gitlab.com if your access has already been terminated. Note: If a termination date changes, please reach out to People Operations via HelpLab. The team will update Workday. The notice is used as supporting documentation of the change and is saved in the team members Workday record. In addition to submitting their resignation directly in Workday (following the guidelines in the ‘How to Submit a Resignation’ e-Learning or Job Aid ) and complying with any contractual requirements, Team Members resigning in France should also email a copy of their resignation letter to legal-employment@gitlab.com . In addition to submitting their resignation directly in Workday (following the guidelines in the ‘How to Submit a Resignation’ e-Learning or Job Aid ), Team Members resigning in Germany are required to provide a wet-ink signed resignation letter to the GmbH address and should liase with the People Operations team during offboarding to ensure it’s properly provided. Team Members located in Japan who are resigning need to complete this resignation form and send it to payroll via HelpLab by the 10th of the month in order to process this with the local provider/partner for correct taxations. Please download or make a copy of the document in order to complete. Team Members located in Singapore who are resigning and are not Singapore citizens need to complete a Letter of Undertaking document and send it to payroll via HelpLab in order to process this with the local provider/partner. Please download or make a copy of the document in order to complete. Team members the the PEO/EOR on UAE work visa’s who resign, completes their contract, or changes jobs, their visa must be cancelled by the company regardless of whether they leave the UAE. Team member’s with dependents must first either cancel dependent visas (if dependents are leaving the UAE) or place them on hold (if dependents are staying) - this is managed by the team member as the dependent visa sponsor. The PEO/EOR company handles the mandatory work visa cancellation process through MOHRE (for labor card) and GDRFA (for residence visa), which takes 1-3 working days. Team members have 30 days after cancellation to leave the UAE or obtain a new visa. Failure to cancel properly may result in re-entry bans, overstaying fines, and complications with final payroll and health insurance processing. Note: Involuntary terminations are only facilitated by Team Member Relations (TMR) who will initiate the process in Workday. Involuntary offboarding of any team member is never easy. We’ve created guidelines and information to make this process as humane as we can. Beyond the points outlined below, make sure to refer to our guidelines on underperformance , as well as the offboarding issue . The manager and the team member should have walked through the guidelines on underperformance before reaching this point. TMR: Terminate the team member in the system by following the steps documented in the following job aid (Workday Profile by selecting Actions followed by Job Change and finally Terminate Employee ). TMR: You will be prompted to indicate whether the termination is regrettable or non-regrettable and whether the team member would be eligible for re-hire in future or not i.e. with review unless specifically relating to conduct or job abandonment in which case they would not be eligible. TMR: Once you have submitted the termination details, you will be prompted to complete a Questionnaire confirming that the Manager or PBP have communicated termination to the business AND confirming that access has been shut off with IT. People Business Partner: You will receive the termination transaction to review, approve, and then enter a comment to confirm termination is ready for the offboarding process to begin. If PBP does not want the offboarding process to initiate they should NOT approve the termination until they’re ready. People Ops: will review all comment(s) and questionnaire to confirm the offboarding process is ready to begin. If needed, the Peopel Operations team member should reach out to the PBP to confirm the process can be initiated. This will complete the process of the involuntary termination. After the involuntary offboarding call has taken place and the last working day has been determined, team members will have no access to GitLab systems and may not be required to do any work on GitLab’s behalf. If they are on “Garden Leave” they will still be active on payroll through the termination date. When determining the timing of the involuntary offboarding call and termination date it is important to consider any effect this might have on ongoing tasks and responsibilities of the team member. No expenses may be incurred while on garden leave unless necessary to the performance of GitLab duties and approved by the team member’s manager or as subject to local law. As a manager, in collaboration with the Team Member Relations Specialist (TMR) and/or the People Business Partner (PBP), we recommend to avoid scheduling the involuntary offboarding call while a team member is scheduled for any sensitive customer meetings or is on-call . If this is unavoidable, the manager is responsible for ensuring a transition/remediation plan. People Engineering automations will generate the offboarding issue at the end of the team member’s last working day, as per notification from the Team Member Relations Specialist (TMR) and the People Business Partner (PBP). Once the Last Working Day or Garden leave expires the team member will be officially offboarded from GitLab. Prior to the offboarding issue and the overall process for the term listed below. If appropriate (to be determined by conversation with the manager, the Group Executive, and People Ops), use the following offboarding memo , which is provided here as an openly viewable Google Doc, but of course needs to be personalized and tailored to each individual’s situation. As written, it is applicable to US-based employees only. Separation and Release of Claims Agreements do not apply for all offboardings. To review in which cases they do/do not apply, please reference the Severance Eligibility document accessible by Team Member Relations team and PBPs. In the case that a severance agreement is applicable, the steps below should be followed: For team members who will be placed on leave during an investigation please follow the process below: The People Operations member in the relevant rotation will complete a weekly audit of all offboarding issues opened within that specific week and check that all People Operations tasks have been completed and that the label peopleops::done has been added. All offboarding tasks by all Departments need to be completed within 5 days of the offboarding date. For systems that are more critical and time sensitive, these will be completed within the first 24 hours (example 1Password, Slack) by the relevant Departments. Information about application & system deprovisioners can be found on the Tech Stack Applications handbook page . To ensure a successful completion of the offboarding issue, it is important that all tasks are checked off, whether the system/tool is applicable to the offboarding team member or not. Checking the box indicates one of the following: All voluntary exits are eligible to be added to the Slack channel #gitlab-alumni , unless otherwise noted. Involuntary exits are not eligible for the alumni channel, unless otherwise noted by Team Member Relations. The offboarding details provided by the termination transaction in Workday is how eligibility is determined and later shared with IT. The purpose of this channel is to network and socialize with team members. Joining the channel is voluntary and subject to GitLab’s Code of Conduct . GitLab, the company, monitors the channel and can remove people from it at their sole discretion. The GitLab Code of Business Conduct and Ethics is enforced in the channel. Our goals in communicating offboardings are transparency, and to provide an opportunity for team members to say goodbye. We understand that different individuals are comfortable with different levels of communication and that each offboarding situation has different situations and nuances. For this reason and out of respect for individuals, we have a couple of key guiding principles for communicating offboarding: Depending on the team members’ role, timing of communication may vary (e.g. direct team, key stakeholders, etc.), and managers have discretion to determine who should be informed most immediately. The typical order followed for communicating departures is: The “direct team” is typically the team member’s peers within their immediate team (I.E. reporting to the same manager and/or in the same functional group, etc.) and is typically a relatively small group of people. In most cases, team members communicate their departures to their direct team, though messaging should be cross-checked with their manager for consistency. Key stakeholder communication can be done through 1:1 notifications, and/or by posting the update in team-specific channels for general awareness. Key stakeholders are individuals that departing team members work very closely with (typically in their day-to-day work) that will feel the impact of the team member’s departure in their work. Managers of key stakeholders should also be looped into communication to ensure awareness. It is essential that key stakeholders are looped in to offboardings to: Departures should be communicated with stakeholders as soon as possible, with a maximum timeframe of 2 business days after the team member’s departure. Where possible and appropriate, we also encourage managers to work with departing team members to align on transitions plans to transition as much of the departing team members’ work as possible before their departure. In most cases, managers communicate team member departures to key stakeholders. As explained briefly in the offboarding issue , GitLab is not always able to provide full context on why people are leaving when they do. However as mentioned in the procedures, for voluntary offboarding , the team member can work with their manager on messaging to share a company-wide message about their departure. Once the news has been shared with the team member’s team and other stakeholders, and messaging is agreed upon between the departing team members and their manager, the departure message should be shared in the #team-member-updates Slack channel. Managers and team members can optionally leverage this template as a guide on how to communicate a team member’s upcoming departure: There will be situations in which team members prefer to share their offboarding message, and situations in which managers prefer to do so. Either is ok, so long as team member and manager have reviewed the messaging together prior to posting . If someone is let go involuntarily, this generally cannot be shared since it affects the individual’s privacy and job performance is intentionally kept between an individual and their manager . If you are not close to an employee’s offboarding, it may seem unnecessarily swift. Please remember that these decisions are never made without following the above process to come to a positive resolution first - we need to protect the interests of the individual as well as the company, and offboarding is a last resort. According to our values negative feedback is 1-1 between you and your manager and we are limited in what we can share about private employee issues. Please discuss any concerns you have about another employee’s offboarding with your manager or your People Business Partner. Given the expectations and responsibility that come with a VP and above position, when there is an involuntary offboarding for one of these positions, additional context for the personnel change can be provided to the organization. Managers should consider cross-posting the message in #team-member-updates to inform the wider Department of the departing team member. We strive to maintain personal information regarding all team members private, this includes information regarding a team members voluntary or involuntary departure from GitLab. However, a manager with the consent and approval of the departing team member can share more details with the GitLab team regarding the decision to leave GitLab. If the departing team member gives their manager permission to share that information then the manager will share while making the departure announcement on the team call. Regarding involuntary offboarding, certain information can also be shared with the GitLab team regarding the departure. The departing team member may work with their manager to author a goodbye message for voluntary offboarding: In some instances there will be no further clarification on why a team member has departed, if there are concerns you can address those with your manager. Different levels of transparency will exist based on maintaining respectful treatment for all departures. Having team members leave may be a learning opportunity for some, but should not be a point of gossip for anyone. Managers will need to balance the opportunity for learning with the expectation of privacy and consult their People Business Partner should they have questions. Transparency is one of our values. In the case of offboarding, we opt to share the feedback only with peers and direct reports as needed, since we balance transparency with our value of collaboration and constructive guidance shared 1-1. GitLab’s turnover data is only viewable internally. To track all tool deprovisioning, please open an offboarding issue following the offboarding standards . As part of offboarding, any GitLab property valued above 1,000 USD needs to be returned to GitLab. For laptops, please check and refer to the Laptop Buyback Policy which states that team members may , at GitLab’s discretion, have the option to keep or buy back their existing laptops either when it gets refreshed for a new one, or when the team member is offboarding. However, the option to purchase or keep a laptop at no cost may be voided where the team member is involved in cases of investigation, misconduct, termination for cause of any violation of GitLab’s Code of Business Conduct & Ethics , or other legal or security related inquiries. To return your laptop to GitLab, please contact gitlab-laptop-recovery@gitlab.com immediately upon offboarding. This section of the Accounting Department. To remove someone from Navan Expense Log in to Navan Expense and go to “Settings” in the left sidebar. Select the right policy based upon the entity that employs the team member. Select “People” in the left menu. Select the individual’s name and click “Remove”. If the person has a Corporate Credit Card assigned to them, please notify Accounts Payable before un-assigning it. For involuntary offboardings it is optional to do a retrospective on the hiring, onboarding and coaching/communication of the departing team member. As a manager, you can use this template for a retrospective. Please share the filled out template with your manager as well as the People Business Partner for your group. Within the Engineering division this is a required process because it causes hiring managers to reflect on what led to the ultimate decision of parting ways with the team member, and how that might be prevented during future hiring processes. In the United States, Unemployment Insurance provides benefits to GitLab team members who have lost their jobs through no fault of their own. The purpose is to provide temporary financial assistance to employees who meet certain requirements. Unemployment insurance is administered at a state level and in compliance with Federal Law. Each state establishes its own eligibility criteria and regulations surrounding Unemployment Insurance with respect to the amount allocated, duration and eligibility for Unemployment Insurance. Eligibility criteria include meeting the state specified requirements in terms of both earnings and time worked during the base period. In some instances the state may require additional supporting information particularly more extensive wage related details surrounding a claim. Unemployment Insurance is funded through employer contributions and most states will have State Unemployment Taxes that apply however many employers typically pay the State Unemployment Taxes too. If you are a full-time team member and you are contacted by your state’s Unemployment Commission to discuss your request for Unemployment Benefits, you may be a victim of Unemployment Claim Fraud. Before giving out any information to the caller, please confirm that you are speaking with an agency employee. If you confirm with your state’s Unemployment Commission that there is a fraudulent claim, please report it via HelpLab to the People Operations team. Additionally here is a link to the U.S Department of Labor Contact Particulars to report Unemployment Insurance Fraud.",
  "All team members and contractors must protect our Company assets, such as equipment, supplies, cash, and information. Treat Company assets with the same care you would if they were your own. No team member or contractor may commit theft, fraud or embezzlement, or misuse Company property. This table captures all of the links for different policies that are documented around using Company money. This includes information of when to use our expense tool Navan, when to use a virtual credit card and when to get a PO from Zip.",
  "The purpose of the Talent Assessment Program is to identify and retain team members who drive the success of our organization. This is a top priority and strategic process for people managers at GitLab. Our Talent Assessment Program (which is a critical piece of our overall Talent Development Program serves as a mechanism to help mitigate a couple of our biggest concerns: lack of performance management and losing key people . Additional key benefits of the Assessment Program include: We use Workday to complete our Talent Assessment. You can review an overview of Workday Talent here . Please reach out to your People Business Partner if you have any questions. You can also review the following guide which mirrors what is included in the Workday Talent Assessment template to prepare you prior to the tool being launched. There are many talent assessment options, and we have opted to use a Performance/Growth Potential Matrix (commonly known as “9-Box” in the US) and annually review Key Talent. GitLab’s Performance/Growth Potential Matrix is a type of talent assessment that forms part of our Talent Development Program . The FY25 talent assessment should consider performance from the period FY25Q1 - FY25Q4 In addition to assessing Performance/Growth Potential, we annually review Key Talent aligned with the criteria outlined below on this page. The combination of Performance/Growth Potential and Key Talent assessments allow us to identify team members who drive the organization’s success and use our engagement tools to retain them. For team members with a tenure up to three months to the launch of the Talent Assessment Program will be designated as “Too New to Rate”. Also in some cases a “Too New to Rate” designation might be applicable for newly promoted or transferred team members. More about that designation can be found below. This matrix is an individual assessment tool that evaluates both a team members current contribution to the organization and their potential level of contribution. It is commonly used in succession planning as a method of evaluation an organization’s talent pool, identifying potential leaders, and identifying any gaps or risks. It is regularly considered a catalyst for robust dialogue (through a calibration process) and is considered more accurate than one person’s opinion. The performance/growth potential matrix can be a diagnostic tool for career development. The matrix serves as a tool to help managers assess, develop, and coach their team members - ultimately resulting in an ability to more effectively and efficiently lead teams. Performance includes both results and behaviors, and is broken into three areas: Developing, Performing, and Exceeding. Please note that Developing should not be automatically associated with underperformance. As highlighted below, Developing can also be used for new hires or newly promoted team members that are still ramping up in their new roles. Your manager will provide example and detail to ensure the rationale behind the Developing rating is communicated. Based on the Job Family responsibilities, values and remote working competencies , team members in this category are not meeting all of the expectations. There is still more to learn and improve on in the current position to achieve the desired results. This may be due to the following: Based on Job Family responsibilities, values and remote working competencies team members in this category are “on track” and meeting all expectations. They can independently and competently perform all aspects of the Job Family responsibilities in a way that is aligned with our values and competencies. Their performance consistently meets the requirements, standards, or objectives of the job, and they can occasionally exceed expectations. They deliver results in a timely and accurate fashion. This performance may be expected as a result of: Team members that are exceeding consistently surpass the demands of their current position. They demonstrate unique understanding of work beyond the assigned area of responsibility. They contribute to GitLab’s success by adding significant value well beyond Job Family requirements, values , and remote working competencies . This performance may be expected as a result of: Please note that an Exceeding assessment for the performance factor does not guarantee a promotion. While the performance factor is a consideration, there are several considerations that are reviewed when evaluating promotion readiness. Please work with your manager to align expectations. Note that this is an expected distribution, not a forced distribution. The bell curve distribution aligns with what is most commonly used as a benchmark in the industry and serves as a baseline for us to review and calibrate against. There may be exceptions that are justified that can be discussed and reviewed during calibration. Where deviations from the expected distribution occur, managers should use this as a data point to further review and work with their leadership to ensure appropriate growth plans are in place, proper role fit exists, hiring plans are adjusted, that we are promoting at a fair and accurate rate. We expect roughly ~5% of team members to be assessed as TNTR (Too New To Rate) . Performance: Remember that “Developing” for Performance and Growth Potential should not be automatically associated with underperformance, and that many newly promoted team members and new hires fall into this category. While the primary objective of the performance axis of the Performance/Growth Potential Matrix is to calibrate team member performance, this axis of the matrix also directly impacts the Performance Factor , which is a determining factor in the annual compensation review. Please reference the Total Rewards Performance Factor page for more detail on compensation impact. How can managers determine whether their team members are Developing, Performing, or Exceeding? “The Performance Factor” is determined from several components outlined below. It is optional to use The Performance Factor Worksheet for weighing the different Performance Factors. Performance against Job Family responsibilities, requirements, performance indicators, and functional competencies (if applicable) should be weighted at 60%. For management roles, please also review and consider the levels outlined in the Organizational Structure. Performance against these competencies should be weighted at 40%. When completing the talent assessment form, team members and managers should reflect on the applicable competencies and job frameworks when filling out the achievements, strengths, and opportunities section. Given GitLab’s CREDIT values should be lived, with each day representing a new opportunity to refamiliarize oneself with said values and strive to implement them in every professional interaction, there is an opportunity for team members and managers to rate how a team member demonstrates each of the CREDIT values. The following rating scale is used for each value: Use the comment box at the end of the CREDIT values section to provide examples to support the ratings selected. While performance is focused on the past and present, growth potential is focused on the future. Because of the nature of the future-focus associated with growth potential, it is more difficult to measure than performance, and inherently more qualitative than quantitative. A key element in determining growth potential is the manager and leadership observation and experience working with team members. Managers can gauge team member growth potential against the expectations in their current role or their growth potential to take on different roles across GitLab. Growth potential refers to the ability and desire of a team member to successfully assume increasingly more broad or complex responsibilities and learn new skills, as compared to peers and the roles’ responsibilities outlined in their respective Job Family. This could include the growth potential to move up to the next level in their job family, and/or a lateral move. The Growth Potential assessment helps managers determine the best growth trajectory for their team members. The growth potential assessment will be used as input to leverage our engagement tools, offer lateral and upwards career opportunities, and do succession planning. Growth Potential can change over time as team members develop new interests, as new opportunities for growth arise, and as team members continue to broaden their knowledge, skills, and abilities. There are four primary pillars to consider when measuring growth potential: Under each pillar there are a few questions that managers should consider when assessing team member growth potential. Please note that the answer to all of these questions does not have to be yes to determine a team member is “exceeding” growth potential, the questions are here to help guide managers through the thought and evaluation process. After assessing team member growth potential based on the four pillars outlined above, managers can determine whether team member growth potential is currently developing, growing, or exceeding. “Developing” growth potential generally refers to a team member who is not working at full growth potential against the roles and responsibilities outlined in their Job Family. There could be a variety of reasons for this, including: Please note that “developing” growth potential does not equate to “developing” performance, but correlates with the pillars of growth potential outlined in the “measuring growth potential” section below. “Growing” growth potential generally refers to a team member who is growing in their current role and demonstrating interest in advancing (up or laterally) and they exhibit knowledge, skills, and abilities that indicate this. Team members with “growing” growth potential generally: “Exceeding” growth potential generally indicates that a team member will be ready for promotion within the next year (or when an opportunity arises). “Exceeding” growth potential team members: The portion of the Performance/Growth Potential matrix that often entails the most significant time commitment is the live calibration session of team members with leadership. The calibration session is very valuable to ensure consistency across the Job Family and level, raise any questions, and provide cross-departmental and/or cross-divisional feedback on team members to capture the assessment of different managers as opposed to the opinion of the direct manager exclusively. Calibration can be done asynchronously or synchronously, and should be done by level (i.e. Manager calibration for their directs, Director calibration for their directs, etc.) so as not to disclose Talent Assessment evaluations amongst peers. For newly onboarded team members, with a tenure up to 3 months, we have implemented a “Too New to Rate” (TNTR) assessment category. This designation will be applied to individuals with a start date within 3 months of the kick off of Talent Assessment as they have not yet had sufficient time to demonstrate their Performance/Growth Potential according to the defined pillars. The introduction of a “Too New to Rate” category is a solution as feedback was raised that being rated “Developing” in many cases does not reflect the actual Performance/Growth Potential. Therefore a TNTR designation is to accurately reflect the time to get up and running of recently onboarded team members. This category acknowledges that new hires require a reasonable timeframe to familiarize themselves with the organization, their role, and the expectations before they can be accurately assessed using the Performance/Growth Potential Matrix. While newly onboarded team members do not need to complete the Talent Assessment evaluation in Workday, it’s still important for them to have a check-in conversation with their manager. We recommend that this conversation happens after the team member has completed 3 months with GitLab. We’ve created this template for team members and managers to complete. The goal would be for the newly onboarded team member and their manager to come together to have a conversation reflecting on their first 3 months at GitLab, highlighting their strengths and opportunities, then aligning on any support needed for the team member moving forward. Besides team members with a GitLab tenure up to 3 months (New hire TNTR), the “Too New to Rate” designation will also be available for team members that have just transferred or were promoted to a new role. Please see the Promotions or Transfer During Assessment Cycle section for more details and guidance on how to evaluate and consider performance in these scenarios. If any of the above cases arise it is recommended that you discuss with your People Business Partner to ensure the Too New to Rate designation is consistently applied. The calibration session is one of the most important pieces of the Performance/Growth Potential Matrix process, as it provides time for managers, their peers, and their manager to calibrate. Below are a few communication guidelines to ensure efficiency and fairness during the calibration session discussion. It is likely not necessary to discuss each team member in detail, particularly for larger groups. Calibration sessions typically focus on gaps, outliers and areas that might require additional management attention and/or alignment. As a best practice, we should calibrate outliers. “Outliers” are typically considered to Box 1 (“Exceeding Growth Potential” and “Exceeding Performance”) and Box 9 (“Developing Growth Potential” and “Developing Performance”) . People Business Partners and business leaders have discretion to expand this threshold as needed, but live calibration for team members in Box 1 and Box 9 should be the baseline across the board. A few additional reasons to consider calibrating beyond Box 1 and Box 9 are: It is absolutely essential that managers complete the required pre-work to ensure that the live calibration session is as efficient and productive as possible. Pre-work includes: While we want to encourage open and transparent conversation during calibration session, there are certain topics that should be avoided to ensure we respect team member privacy. After the calibration sessions the performance and growth potential outcomes can be used as input for the following: Detailed dates for FY26 are pending and process is subject to modification. In general the timeframe runs from January through March with ACR (compensation statement) release in April and compensation changes and promos effective beginning of May. Note that different departments may have additional due dates built into the high level timeline, so please follow up with your People Business Partner if you have any questions. Below are the current activities for the formal assessment beginning in Q4 (January) *Team members who are on Leave of Absence (LOA) on TBD are excluded from the auto advancement of their talent assessment until they return to work. Note: For full details on our Annual Compensation Review (ACR) timeline and cycle, please refer to our ACR handbook page . GitLab completes talent assessment at least once per year in Q4, with a recommended informal mid-year check-in. The formal assessment ideally takes place prior to the annual compensation review . Our e-group completes Performance/Growth Potential Matrix multiple times a year for their direct reports. Anyone hired on or before January 31st should receive a Performance and Growth Potential assessment, as they are eligible for Annual Compensation Review. However, if the team member being assessed has been with GitLab for a period of 3 months or less or recently transferred roles or was promoted, please see the Too New To Rate description . It is up to all team members’ discretion if they wish to complete their Self-Evaluation in Workday. While they are voluntary, self evaluations are highly encouraged so the team member’s perspective is captured and shared with their manager prior to the manager’s evaluation and Calibration Sessions. Please review the most current timeline to ensure a timely delivery. Team members may use Claude, Insights, and GitLab Wrapped to assist in the preparation of your self evaluation. AI tools like these can be super helpful to assist you in gathering information about your accomplishments, or helping you brainstorm how you might refine your self evaluation. However, meaningful human input and review is always required, and you should not rely solely on Claude to write your self-evaluation. The Performance/Growth Potential Matrix typically takes 4-6 weeks to complete from beginning to end. Steps are as follows: Aligned with the timeline , managers can see final assessments for each of their team members in their respective Workday profiles. The cut-off date to determine whether to assess team members as Too New to Rate based on recent promotion/transfer for the FY25 Talent Assessment cycle is based on their time in role prior to the start of the Talent Assessment cycle, 2025-01-27 . Please note, that a “transfer” constitutes as a change to one or multiple of the following things: job family, department, division, or job grade. A change in job title specialty and manager alone would not constitute as a transfer. It is important to note that because being promoted recognizes high performance both in terms of increased scope/responsibility and monetarily through compensation increase therefore a promotion “resets” the performance factor evaluation . While a recent promotion resets the performance factor evaluation, and a job transfer is internal mobility that may be a lateral job level, there are soft skills and technical skills that are most likely transferable. When assessing team members who have transferred teams or changed roles throughout the year, managers should gather feedback from the previous manager on performance as well and take that into account when evaluating and documenting performance to reflect performance for the entire year. Managers should also provide the team member feedback on how they’re doing in their current role to ensure they’re set up for success, and aware of feedback moving forward in their current position. In most companies the Performance/Growth Potential Matrix is used exclusively as a management tool and results are not typically shared with team members. In the spirit of our transparency value, we want to encourage feedback with team members. Discussion topics that arise during calibration sessions (or at any other point during the assessment process) are confidential. Please do not share with anyone other than each individual team member. After calibration sessions are done and performance and growth potential factors are determined, it’s time to communicate the final results with team members. Please follow the Talent Assessment timeline and communicate performance and growth potential ratings to team members in your performance review conversations within the communication window . The conversation following the talent assessment process is the most important part; it is a great way to drive alignment with your team members and set them up for success moving forward. Below are some of the best practices for communications, but remember, if you are ever in doubt please reach out to your manager or People Business Partner for additional support. The script below is intended to help you structure your Performance and Growth Potential conversation. Personalizing this conversation to reflect your authentic tone will make this a more effective conversation with your team member. “Thank you for taking some time today to discuss your Performance and Growth Potential. I wanted to discuss your achievements and strengths, improvement areas, future development, and the final result of the talent assessment.” “I assessed your performance factor as * [Insert Performance factor] :” “I assessed your performance as [Insert Performance Factor] because:” “I assessed your growth potential factor as * [Insert Growth potential factor] :” “I assessed your growth potential as [Insert Growth Potential Factor] because:” If a team member was identified as key talent , please ensure to communicate to the team member during the talent assessment conversation. I’d like to thank you for your hard work as, and I look forward to continuing to work together! Do you have any questions? [Allow team member to ask questions and respond] As a next step, let’s schedule some time to talk more about what you want to do in the future and create a career development plan together OR let’s talk about how this feedback should be incorporated into your Individual Growth Plan. The most important part of the talent assessment discussion is to determine next steps in partnership with your team member. The team member should be the DRI for the next steps with support from you as their manager. Our Career Development handbook page includes a lot of great information and resources on career development including an Individual Growth Plan guide which should help provide structure and a plan for next steps. If you have any questions or concerns about next steps, please contact your manager and/or your People Business Partner . The Talent Assessment involves all team members and managers. Team Members are assessed according to performance and growth potential . Team Members should be prepared to have a conversation with their manager about their performance and growth potential in Q1 each year. Managers assess their team members’ performance and growth potential . Managers attend calibration meetings in which team members’ ratings are discussed and evaluated to ensure consistency and minimize bias. Once the program closes, managers are responsible for communicating Performance/Growth Potential Factor to team members. Can I use Claude or other AI tools to help me with my self evaluation? Yes. You may use Claude, Insights, and GitLab Wrapped to assist in the preparation of your self evaluation. AI tools like these can be super helpful to assist you in gathering information about your accomplishments, or helping you brainstorm how you might refine your self evaluation. However, meaningful human input and review is always required, and you should not rely solely on Claude to write your self-evaluation. Can I utilize Claude or other AI tools to help with manager evaluations? Claude is the only AI tool you are permitted to use for this purpose, and it may only be used to assist in the writing of manager evaluations. Use of Insights, GitLab Wrapped, or any other AI tool for the assessment of team members or the writing of manager evaluations is not permitted. Claude can be super helpful to assist you in brainstorming how you might refine the feedback in your manager evaluation. However, meaningful human input is required in assessing performance and growth potential of your team members, and you should not rely solely on Claude to write manager evaluations. We also have a robust calibration process in our talent assessment program to ensure we are consistently and fairly assessing performance and growth potential across GitLab. How does this impact the way I work at GitLab? How does this impact the teams I lead? Who can I reach out to in supporting me during Talent Assessments for my team? As a manager, when reviewing my team members, what is the difference between Developing and Underperformance? Will my Growth Potential assessment impact my compensation? Can my performance factor be changed? Does this mean I am automatically getting an X% increase in the upcoming Annual Compensation Review? The Performance/Growth Potential Factor is one of the factors considered in the Annual Compensation review . The recommended increases displayed in the handbook are there to ensure company wide consistency. The factors taken into account for the Annual Compensation Review are: When is the next opportunity for my performance to be reviewed? What can I do to change my performance factor in the future? Can my Growth Potential rating be changed? What can I do to change my Growth Potential rating in the future? My team member is currently on a performance remediation plan; how should I handle this? My team member is on a leave (protected leave or PTO) during the Talent Assessment period. How should I handle communication of their assessment? If they will be returning from leave before the due date to communicate Talent Assessment results , please wait until their return from leave to communicate their assessment. If they are returning from leave after the due date to communicate Talent Assessment results, you may email them to offer to discuss their assessment. If they do not reply or prefer to wait until their return, please respect that choice. My team member was on protected leave for a large portion of the Talent Assessment period. How should I factor this in when assessing them? It’s important to ensure you are assessing the team member’s performance for their contributions while working during the assessment time period. There should be no adverse impact on their overall assessment due to being on leave for a portion of the period. During calibration, please remain vigilant to ensure no private details related to their time away are disclosed. Please reach out to your PBP to discuss the specific details related to your team member. My team member is on leave of absence during the Talent Assessment communication window. What happens to their Talent Assessment? Please do not action the To-Do task “Meet with your Employee” if your team member is on leave of absence (LOA). We will exclude team members on LOA from the auto advancement setting on TBD and you can communicate their talent assessment when they return to work. My team member was assessed as key talent last year, and is not for this assessment period. How should I handle communication with them? There is no guarantee that team members will retain their key talent assessment each assessment period. Make sure to explain the reasons why their key talent assessment has changed, including how we assess Key Talent and the definition outlined in the Handbook . Remember to end the conversation by discussing what your expectations are of key talent going forward. It’s important not to make promises about future assessments, but to focus on what makes a team member critical to the business, and how you, their manager, will support them. My team member was exceeding performance last assessment, and is not for this period. How should I communicate this change? It’s important to take a look at what may have changed since last year. Is your team member in a new role? On a new team? Did they take on bigger stretch assignments? Any of these factors can influence a team member’s performance. Be prepared with specific examples to share which support your performance assessment. The feedback conversation should also focus on what success looks like for the team member in the year to come and how you, as the manager, can support them with their goals. I have just adopted a new team, and the previous manager is no longer at GitLab. How can I assess the performance of my team fairly? You may be able to gather insights from your manager or cross functional stakeholders who have worked closely with the team member in the performance period. You may have access to the recent 360 feedback for each of your team members, as well as their talent assessment from last year to help inform your thinking. If you haven’t already, it’s important to ask the team member to share their own perspective on their accomplishments prior to you taking the role, so you are well prepared for the discussion of your assessment. A team member recently moved to be my direct report, however for the majority of the performance year, they reported to another manager. Are there options to delegate or re-assign the manager evaluation to their previous manager? As a manager, you can delegate the manager evaluation to another manager in your direct management chain. If your team member moved from another manager in another organization, you should collaborate with the previous manager to write the review with their input. The previous manager can leverage the Talent Assessment google template to capture their feedback and provide it back to you so that you can enter it into Workday. As the new manager, you will be responsible for the calibration of team members in your management chain at the time of calibration. You should also ensure your communication of the final talent assessment is done in a joint conversation with the previous manager, you as the new manager and your team member. This would be a great opportunity to ensure a formal transition and knowledge of performance and feedback takes place between previous manager and new manager. Please enter a HelpLab request if you have any questions about your specific scenario. My team member does not agree with their placement in performance and growth potential. How should I approach this? Remember that this conversation will be most effective if you are well prepared. If you haven’t yet, take an opportunity before the performance and growth potential conversation to understand their own assessment of their results. During the performance and growth potential conversation, provide the opportunity for a dialogue with your team member. Remain open and curious about their perspective. Engage in active listening, ensuring that you aren’t planning your counterpoints while they are sharing. The expectation is not that the results will necessarily change but the insights they provide may shape how you both think about their growth and development path. If you feel a conversation might be particularly challenging, you may reach out to the TMR team or your PBP to role play the discussion and ensure you are prepared. My team member is exceeding growth potential expectations, how should I be thinking about their ongoing development? Great! At GitLab team members have the opportunity to own their own development, and deserve a great manager (you) to facilitate it. Ask your team member to articulate where they would like to see their skills and career develop. You may have insights that help them refine those goals based on where you see the business going, as they will be most successful in gaining the experience they’re looking for if there are opportunities to align those goals with business goals. The Career Development handbook page has more ideas to help your team member craft a growth plan, including information on GitLab’s Growth and Development benefit . How should we be communicating Growth Potential to team members? First, ensure the team member understands what we mean when we say we are assessing Growth Potential, including the 4 pillars that you considered when making the assessment. Be prepared with specifics that led to your evaluation. Remember, Growth Potential can change over time as team members are promoted, develop new interests, as new opportunities for growth arise, and as team members continue to broaden their knowledge, skills, and abilities. Avoid making promises, but if your team member sees themselves in a higher position, help them understand behaviors you expect to be able to assess them differently next time. I received unexpected feedback about my team member from a cross-functional stakeholder in my calibration session. What should I do? If you receive unexpected feedback during the calibration, ensure you really understand what happened, and what has been discussed with your team member so far. If someone has tried to help this team member in the past, how did it go? Can your peer provide specifics to help you understand the impact of the behaviors? Talk to your peer about the importance of using this feedback to help your team member grow, and make sure you have agreement on how you plan to deliver this message. During the discussion with your team member, be prepared to gain their perspective on the situation first. Listen actively and consider their point of view. When delivering the message, consider the root cause, focus on impact and alignment with our values, and end with a discussion on future actions that would be more successful. If and when appropriate, check in with your peer on how improvements are going. Be sure to stay close to the situation so you are in a position to deliver more timely feedback if the behavior is ongoing. How do I assess team members in acting or interim roles? For team members who have assumed an Acting or an Interim role , we will assess team members aligned with their permanent positions (i.e. not the Acting or Interim position). As the Talent Assessment impacts compensation, and Acting/Interim periods are not permanent, in the instance that a team member does not end up moving into the Acting/Interim role permanently, we would not want to have their compensation impacted by a temporary position. My team member is performing at a high level in terms of results, but they struggle with behavioral alignment to one or more of our values. How do I take the “how” into account when I am assessing their performance and growth? Review the handbook content on how we measure Performance and how we measure Growth Potential . When a high output/results-oriented team member is struggling with soft skills and/or behavior, this is oftentimes related to improvement areas in terms of alignment to our values competencies , and/or our Growth Potential pillars (particularly, but not limited to, Self-Awareness). Considering our measurement guidelines for Performance and Growth Potential holistically, and be sure that your assessment takes all of our competencies, job family responsibilities, and pillars into account. My team member has the skills to perform, but there are will-based performance issues that are impacting their ability or the team’s ability to succeed. How should I think about this in terms of assessing their performance? First, act early. Do not wait for a formal performance and growth potential assessment conversation to deliver feedback about your concerns. Be specific, and document your conversation. Loop in a TMR specialist for support as these conversations can be challenging. Review the definition of “Developing” performance. Note that one criteria that applies to this category is a team member who is struggling to perform in a way that is aligned to our values and competencies. Think about the specific behaviors or impacts of their work or how they are working that have led you to assess that they are not fully engaged. Despite having the skills, if your team member is not contributing at the level they are expected to be, you should consider that a performance issue in the way you would a skill based performance issue. Keep in mind that not addressing these challenges may be affecting your other team members, especially those who may be picking up the slack. We encourage to review the handbook page on Underperformance . Great managers share feedback with team members year-round in 1:1 meetings and ad-hoc. A dedicated Mid-Year check-in is a useful tool to support a holistic discussion between team member and manager about how things are going as we approach the second half of the fiscal year. During the Mid-Year check in, we assess how things are going from both the team member and managers’ points of view, and share feedback to help inform performance and development plans. Leaning into where things are going well, as well as uncovering blind spots or improvement areas early are the keys to delivering great results. A Mid-Year check in complements weekly, on-going feedback, so we recommend documenting your discussion in your 1:1 doc to support frequent updates on action items and development goals. Goals of the Mid-Year check in are: To participate in the mid-year check in we recommend that team members have at least 3 months of tenure so accomplishments and improvement areas are able to be clearly outlined with examples. Recommended process for our Mid Year check-in will be as follows: Team Member Led: Team member provides a self-assessment on their own performance in their 1:1 document A Google docs template is available to guide the self-assessment process. Team members are encouraged to author a document based on this template, share it with their manager and link it to the top of their 1:1 document. Team members should reflect on their achievements, strengths, and opportunity areas since the last formal Talent Assessment in Q4. It is the team member’s responsibility to provide the self-assessment and bring it to the conversation with their manager to discuss and align on. Linking the document to the 1:1 agenda ensures the self-assessment is available to come back to throughout the rest of the year. The template contains an agenda for the Mid-Year check-in as follows: List your 3 most significant achievements since the last formal Talent Assessment , aligned with the job responsibilities and expectations of the role. This can include significant impact to the department or company, customer-impact, community-impacting, etc. Feedback can also be included in this section if applicable. Strengths & Opportunities in Role In this area the goal is to determine 2-3 strengths and 2-3 opportunity areas in accordance with your role, job family, and job framework . Include examples when necessary to provide clarity or context. Ensure there are actionable takeaways. A place to outline any support needed from your manager for your ongoing success and development at GitLab. Managers will provide additional feedback or response to the self-assessment outlined above. Managers should be sure to reference the Achievements (3) and Roles & Responsibilities (Strengths and Opportunities) outlined by the team member above, and to any additional thoughts or feedback (2-3 points). A space for the team member and manager to document any relevant next steps stemming from this conversation. This could be a full Individual Growth Plan or a couple of key actions or development opportunities for the individual. Manager Feedback: Team members let their manager know when they have provided their self-assessment overview in their 1:1 document Team members should share their Mid-Year Check In document with their manager at least one week prior to the live discussion to ensure managers have time to review the content, and to add in their own feedback in the Manager Feedback section. Team members and managers use a dedicated 1:1 to discuss the Mid Year check in. Team member and manager align on action items stemming from the conversation to best support team member development and alignment to their goals. You are encourage to leverage our Career Development and Mobility ) handbook page for ideas and inspiration. Mid-Year Check-In Timeline (FY2026): Succession planning is an important step in our Talent Assessment process. Completing the Performance/Growth Potential Assessment gives leaders a fresh overview of the top performers and the highest growth team members in their respective organizations. Particularly for individuals in Senior Leadership roles and above (the S-Group and depending on structure the Director-Group , a recommended next step following the Performance/Growth Potential assessment is to do succession planning. Succession planning ensures we are identifying and developing future GitLab leaders to ensure business continuity in the event of an unforeseen change, and providing the current leadership team the opportunity to grow and scale. It may provide an opportunity to diversify our leadership team by developing our internal bench of talent. Note, potential alone does not determine who will be a successor. Strong succession candidates should demonstrate sustained exceeding performance and growth, aspire to have a role at the next level, and be ready for a much larger role in the near term. Questions we should ask ourselves when thinking about succession planning (and the individual(s) identified) are: The succession planning process might vary slightly for different areas of the business depending on size and structure, but at a high level the process should flow as follows: Note that the Emergency Coverage slide in the template above should only be used to if the identified successor could not provide temporary coverage for the current leader in an emergency situation. Key Talent makes up roughly ~10% of the population. Key Talent represents team members who have a significant impact on GitLab’s present and future success. They deliver quality results that are instrumental in moving critical company initiatives forward and do so consistently in alignment with our values. These team members are often recognized as experts in their current role. Team members, at any level, can be considered Key Talent. Team members identified as Key Talent typically have knowledge, skills, and experience that: Team members identified as key talent must be assessed at minimum Performing and Growing (Box 4) in the Talent Assessment process . Although Performance and Growth Potential are a consideration in the Key Talent selection process, they are not the only criteria used to determine the designation and should not be used in isolation. It is important to look holistically at the Key Talent Criteria outlined above when determining who qualifies as Key Talent on your team. “TNTR” (Too New to Rate) team members can be considered key talent but this would be an exception. In order for a “TNTR” team member to be identified as key talent they must meet the criteria above and be ramping successfully in their new role. We exclude team members rated “TNTR” when we calculate our Key Talent distribution across the company. E.g. Team members designated Key Talent during Calibration divided by Talent Assessment eligible team members (excluding TNTR) = Key Talent Distribution % Below are a few examples to help managers think through Key Talent designations as they relate to Performance and Growth Potential: Note: Intentionally hoarding knowledge is in direct conflict with our transparency value and is viewed as a performance issue as we measure performance based on alignment with our values. Single Points of Failure (SPOF) refers to a single person whose absence would significantly impact the ability of an area of the company to function. Team members that are identified as Key Talent can also be SPOF, as the definition of SPOF overlaps with a few of the Key Talent Criteria outlined above, however, a team member who is identified as SPOF is not also Key Talent by default. A few key differences between Key Talent and SPOF include: team member’s performance and growth potential, risk mitigation plans, and support/development strategies. Four scenarios exist in the context of our Talent Assessment: To review the SPOF definition in more detail along with a few examples, please review our Organizational Design handbook page. It is important to have a holistic view of all team members when determining who meets the key talent criteria, which is why we require a certain scope when calibrating key talent and making final recommendations in the organization. There are several factors that can help determine the level at which key talent should be assessed, including things like reporting lines and span of control. As a general rule, if a people manager meets the following requirements, key talent calibrations should start at their level: The process to determine Key Talent is as follows: A very small portion of our business (roughly ~10% of the population) is considered to be key talent. As such, a team members’ value in the organization should not be determined based on whether or not they are identified as key talent. While some team members are identified as key talent, this does not mean the rest of our team is not valued and important to our organization’s success. Aligned with our expected performance distribution , approximately 60-65% of our team are core performers (or “performing” . Core performers are responsible for keeping things consistently moving forward. This group comprises the largest population in companies across the board for a reason and is an essential part of any organization’s success. We want to make sure we use engagement tools to retain Key Talent. A few of our primary engagement tools are: Learning & Development , growth opportunities and compensation. Note that being considered or designated as a key talent one year, does not mean or guarantee that a team member will be considered or designated as a key talent moving forward. Supporting Key Talent at GitLab is a critical element of business scalability and organizational development and growth. Our Key Talent are often individuals that can be looked to for mentorship and guidance by others, but it is also important that we invest in the development of this group and ensure that key knowledge is shared. A couple of key ways we can support our Key Talent is through regular discussions and the development of Individual Growth Plans (IGPs) . Below are suggestions for getting started, resources for managers, a Key Talent Engagement Discussion template , and a recommended cadence. One way to set up a structure and regular cadence for touchpoints with our Key Talent population is through Key Talent Engagement Discussions. We recommended these discussion focus areas are: Overall Engagement, Growth/Challenge, and Support Needed. The main goals for check-ins with this group include: Managers can leverage this template to help facilitate consistency in Key Talent Engagement Discussions. Key Talent Engagement Discussions can be a great starting point to start thinking through Individual Growth Plans, which is the recommended next step. Some ideas for our Key Talent population to consider when thinking through Individual Growth Plans (IGPs) include: We recommend that managers perform Key Talent Engagement Discussions twice per year, roughly every 5-6 months. Discussion timing is at manager discretion, as we want to ensure these discussions are held at the most relevant and impactful time for each team member. Key Talent Engagement Discussions are more geared towards stay interview format, and should not replace ongoing informal check-ins, regular feedback, and growth discussions that happen more regularly in 1:1s. A couple of options for Key Talent Engagement Discussions that managers can consider are: In FY'24 Q2, we will begin optional Key Talent Engagement Discussions across the organization aligned with our upcoming mid-year check-in process. For divisions or departments that choose to opt in, the high level timeline would be as follows (exact dates TBD): As mentioned above, Key Talent Engagement Discussions can be done at any point during the year, it is not necessary that they are coupled with our Mid year check-in process.",
  "The Team Member Relations team is responsible for facilitiating and resolving all people matter related team member issues. They will also assist in creating and informing team members of policies that are fair and consistent for all GitLab team members. As part of the People group the Team Member Relations team works with both the manager and team member in an intermediary function to understand and help resolve workplace issues. The Team Member Relations team can also help coach and advise managers and team members on policies and processes. Team member relation issues can range from simple coaching questions to serious code of conduct violations. We classify these different level of team member cases by tiers. Example of issues/cases by tier level: All Tier 3 cases will be managed together with Legal and the TMR team. Team members who would like to discuss a private matter should send an email requesting support from our Team Member Relations team to teammemberrelations@gitlab.com . Please include the following in your email: The Team Member Relations team will reach out within 24 hours from when the request was sent via email during the business week. GitLab also offers a Harassment Complaint Form that any team member may use to document any instance of any type of workplace harassment. Simply copy the form, fill it out with your information and send it to our Team Member Relations Specialist at teammemberrelations@gitlab.com . Team members and the TMR should both respond to questions or queries via slack or email within 24 hours to continue making progress on the issue and get to a timely resolution. For urgent requests that need immediate attention please ask for urgent assistance by sending a message marked URGENT to teammemberrelations@gitlab.com . If you do not receive support in a timely manner, below is the escalation path: An urgent request would be something that needs immediate attention like team members safety or a code of conduct violation. GitLab believes in preserving the dignity of each team member and expects everyone to treat others with fairness, respect, and transparency. We encourage mutual responsibility for constructive work relationships and communication, information sharing, problem solving and a safe neutral process through which differences can be resolved. Direct and honest communication is strongly encouraged between all team members regardless of title or level and per our values we do not pull rank . Such communication is likely to enhance understanding, avoid misunderstandings and create rapid solutions to concerns. We have built this philosophy to focus on these 4 pillars in line with our values: To support our team members, GitLab has established a Team member relations group to assist team members and their leaders with resolving work related issues. The Team Member Relations team (TMR) respects the privacy of all team members and treats discussions with the fullest degree of confidentiality possible. We have incorporated our Values and the Code of Business Conduct and Ethics into our team member relations practices and philosophy. We believe that by communicating with each other directly and transparently, we can continue to resolve any difficulties that may arise and continue to make GitLab a great place to work. The team member relations function provides all GitLab team members an avenue to express workplace concerns and to resolve conflicts in a safe and unbiased forum. Team member relations provides guidance to managers in their efforts to improve team member performance or to correct unacceptable personal behaviors. Managers are responsible for setting priorities and motivating their team members. They are also responsible for ensuring the care of their team members as well as meeting GitLab’s goals. These two things can be done simultaneously. As managers it is considered one of your primary responsibilities to understand GitLab’s Code of Business Conduct and Ethics , Values and People Group policies and processes. People managers are responsible for upholding compliance on the their teams and considering the best interest of the business. If a manager is made aware of a situation that potentially runs afoul of the Code of Conduct, Values and People Group processes or policy they should immediately reach out to the team member relations specialist . If uncertain about a specific policy or procedure, the manager should reach out to a leader, their aligned PBP or the Team member relation specialist for further clarification. When in doubt, it is always best to ask. A critical responsibility within the role of a GitLab leader is to spend the time required to truly understand the causes of the performance issue and how to address them properly. GitLab managers will determine if the performance concerns are skill-based or behavior (will)-based to determine next steps. Below are the definitions of both skill- and will-based performance concerns. Sometimes skill-based issues appear as a will-based or behavior concern. However, after delving into the cause for the behavior, the leader may find a skill-based gap causing the disruptive behavior. The team member may be embarrassed or concerned about their ineffectiveness or ability to perform their role and may react in a way that is inappropriate. The following are a few recommendations for a manager to address skill-based performance issues: If the team member is still not making progress after the manager has provided additional resources or coaching please review the underperformance page in the handbook for next steps. Will-based issues are described as undesirable behavior that impedes the success of the Team Member, the Team, and/or GitLab as a whole. The leader may hear about the behaviors from others or experience the undesirable behavior directly. It is important that the manager address the concerns right away. While “will” issues can be disruptive, there are different levels of severity which must be considered when determining next steps. The tiers below are examples and are not exhaustive, and should be used for comparision purposes to determine the impact to GitLab. Tier 2 - Misconduct resulting in limited material risk to GitLab For Tier 2 level concerns, the Manager should reach out and discuss with the team member relations specialist immediately. Tier 1 - Gross misconduct or a violation with serious implications to GitLab. Since situations differ, managers should immediately reach out to a team member relations specialist for guidance before taking any action. If unable to contact the team member relations specialist directly, please follow the escalation path listed in the handbook Please review the underperformance page in the handbook for further information regarding managing team member performance.",
  "Why: Our Compensation Principle What: Compensation Calculator Inputs How: Compensation Review Cycle Why: Our Compensation Principle What: Compensation Calculator Inputs How: Compensation Review Cycle Why: Our Benefits Principle What: General Benefits Why: Why we grant Equity to Team Members What: What we grant to Team Members How: Accepting, Understand and Exercising Awards Why: Why we grant Equity to Team Members What: What we grant to Team Members How: Accepting, Understand and Exercising Awards * Compensation at GitLab is currently being updated as part of our Job Architecture Project. All information in related compensation handbook pages is subject to change during or at the conclusion of this transition. Please see the Job Architecture Project section for more information. For feedback or questions relating to compensation, benefits and equity, you can reach out to us via:",
  "This page contains useful tips for working at GitLab and for various tools we use. Don’t forget to update your username in the following places: Team members choose to manage their GitLab activities in different ways. Setup your notifications in a way that works best for you. The GitLab team-member resources project has a wiki for sharing among GitLab team-members . It’s for topics like parenting where people may want to share knowledge, but the handbook is not the best fit. @tipyn ’s home office equipment and macOS setup Link your GitLab email address to an easily recognizable photo of yourself on GitLab, Slack, and Gravatar . It is company policy to use a photo, and not an avatar, a stock photo, or something with sunglasses for any of your GitLab accounts, as we have a lot of GitLab team-members and our brains are comfortable with recognizing people; let’s use them. Note : If you upload your photo to Gravatar associated with your gitlab.com email, then you can simply not set an image in your GitLab and Slack profiles and they will automatically use your Gravatar photo. If you already uploaded individual photos to your GitLab and Slack profiles, simply removing them will cause your avatar to use the Gravatar photo by default. Mermaid is a tool that allows us to create flowcharts, graphs, diagrams, Gantt charts, etc. within GitLab! Check out the examples in the GitLab docs on how to use Mermaid. A few additional resources that can be helpful when working with Mermaid are: Note: When creating Mermaid charts in the GitLab handbook, you need to type three back tick symbols followed by the word mermaid before the chart, and three back tick symbols at the end of the chart. This will enable Markdown to distinguish between .md and Mermaid. Please reference the example Mermaid charts linked above to see how this looks live. If you are working on multiple GitLab instances and want to have a visual differentiation, you can change the default Navigation theme to a different color. Page counts are determined through a simple two-step process: Part of the maintainer training process is to keep track of Merge Request that have been reviewed and writing down an assessment on the review in the maintainer training issue. Manual upkeep of the maintainer training issue can be time consuming. There are tools that others have built to help with this task: Note: When using these tools, avoid adding mentions to maintainers in existing comments. There is a known issue where users are not notified by email when mentioned in an edited comment. It only generates a TODO which a maintainer may not use. Refer to the Handbook Development section to learn more about the architecture, structure and how to edit the handbook locally on your desktop. For how to edit the handbook in your browser, refer to the editing handbook page . Portions of the https://about.gitlab.com site lives in the www-gitlab.com repo. The marketing website is maintained by the digital experience team in their GitLab group . The data/*.yml files live in the www-gitlab-com repository, and are used by numerous sites including the marketing website and the handbook. The documentation for the site itself is in markdown documents under the doc folder in the repo . If you are looking to do local development on the site, doc/development.md is probably the best place to start. Many of the tips shown in this section require ImageMagick, an image manipulation tool. The installation is described in the ImageMagick section . ImageMagick provides the convert CLI command which can be used to resize images, add drop shadows, edit GIFs, etc. On macOS, install ImageMagick with Homebrew: On Linux, use your package manager: We have a dedicated section for that in the handbook. ImageMagick provides the convert CLI command which can be used to resize images. Our blog images do not need resolutions higher than 1920x1080. This saves bandwidth and makes the website load faster. This example converts an existing image to 1920x1080 resolution and replaces it inline: You can also use percentage values for the -resize parameter. The convert CLI command can do more things explained in the documentation . If you need to convert multiple images, combine the convert command with find . Note that this replaces the images inline. Tip: Modern macOS versions provide the Finder right-click menu Quick Actions > Convert Image which automatically converts an image to JPG. Use this method for quick UI conversions. ImageMagick provides the mogrify CLI command which can be used to convert the HEIC image format to other formats like JPG which are accepted on all websites. If you need to convert multiple images, combine the mogrify command with find . Note that this creates new files and requires manual cleanup of .heic|HEIC files, -iname uses a case insensitive match. An example shell alias can be found in @dnsmichi’s dotfiles project . Install ImageMagick and use the convert CLI command to add a drop shadow. The -shadow parameter may need adjustments on the dimension. If 2FA stops working unexpectedly (no new phone or computer) it’s usually because of improperly configured date & time on either device. Make sure that “Automatic Date & Time” is enabled on both devices. If they’re already enabled try toggling them off and on again to force an update. If this doesn’t work, request that IT Ops reset your 2FA setting. Links for finding the settings: For Android there’s no definitive link, since most vendors have different UIs for their settings. But in the Settings-app, look for “Date & Time” and there should be a “Automatic Date & Time” toggle. Netstat is a command line tool which can be useful to print network connections, routing tables, interface statistics, etc. One of the most common uses for netstat during troubleshooting is to display a list of open ports listening for connections. sudo netstat -tulpn | grep -i listen If you find a port already in use, you won’t be able to successfully start up a service or program that utilizes that same port. Options to resolve are: When the GitLab Development Kit cannot start using the ./run command and Unicorn terminates because port 3000 is already in use, you will have to check what process is using it. Running sudo lsof -iTCP:3000 -sTCP:LISTEN -n -P will yield the offender so this process can be killed. It might be wise to alias this command in your .bash_profile or equivalent for your shell. You may also wish to add a function in your .bash_profile (or equivalent file for your shell) like this: and it will kill whatever process is currently using port 3000. By adding this small configuration you will be able to view the git branch that you are using currently. If you are not inside a git repository, it only displays the username and the current directory. Add the following lines in your .bash_profile Doing the following, makes the changes to get reflected in you current terminal: On macOS Catalina, ZSH is the default shell . By installing Oh My ZSH! , the git plugin is automatically loaded and shows the current git branch. Add the following lines in your ~/.zshrc The changes will show on the next terminal or immediately by running. Use command aliases in your shell to speed up your workflow. Take a look at these aliases and others in Sid’s dotfiles project . For example, by adding the following to your .bash_profile or equivalent for your shell, you can just type s to checkout the master branch of this website, pull the latest changes, and open the repository in Sublime Text: After editing, you can just type gca to commit all of your changes, followed by gp to push them to the remote branch. If you are using Oh My ZSH! , you can add custom aliases shown below. You can freely define the file name, only the suffix .zsh is important. An example can be found in Michael Friedrich’s dotfiles project . In order to delete local Git branches which are deleted on the remote server, you’ll need to combine the --prune pull/fetch command with more commands. git branch -vv lists all details of local branches, the followed grep filters for all marked gone in the default remote origin and prints the results with awk . This argument is passed into git branch -d , and executed for all matching results. Note that -d does not delete unmerged branches. -D has more impact but can accidentally delete branches. Run the command without the final deletion command to see the potential affected branches. Note that \\$1 is shell escaped in the alias and needs to be executed as $1 . You can use dotfiles to customize your system, and keep all configuration in a central place. The name dot file is derived from Linux/Unix where all configuration files started with a dot, hiding them from the default list view ls . Team member dotfiles projects: Note: you have to pay for these items yourself. WorkFrom is a crowd-sourced resource of coffee shops and other such places that are remote-work friendly. When using unsecured Wi-Fi, consider a personal VPN. We don’t have a corporate VPN but consider purchasing a personal VPN if you travel for GitLab or use unknown networks often. Remember that if your job has restrictions based upon geolocation (for example supporting customers with specific data restrictions and country-based access), a personal VPN may not be the best choice as often the VPN vendor routes traffic through other countries. If this restriction applies to you, consider tethering. Tethering is when you set up your mobile phone as a hotspot and connect your laptop up to it via Wi-Fi, avoiding the unsecured Wi-Fi network. There is more information here on the subject, and as long as your data plan supports it, you should be good to go. Double check before international travel, as it may be supported but have hidden costs. When connecting to a network with a captive portal , most websites will not load as modern sites use HTTPS, and captive portals interrupt that process. Your device will try and compensate for this, but it can be tough to manage manually. If you have trouble, try connecting to https://captive.apple.com/ first, which is intentionally only HTTP and will load the captive portal. FIDO2 is a cryptographically strong 2FA (2-factor authentication) method. It is hardware-based, and is typically deployed via a USB, NFC, or built into a device such as a MacBook’s Touch ID or iPhone’s Face ID. The standard is open and is maintained by the FIDO Alliance . WebAuthn is a component of FIDO2 that supports verification by web applications using public key cryptography. FIDO2/WebAuthn is the preferred method of 2FA and is highly recommended by GitLab’s Security Department. One of the points of 2FA is that while you are authenticating with a username and password for the first factor, a secondary factor should use a separate - or “out of band (OOB)” - communication channel for the authentication. FIDO2/WebAuthn devices certainly meet that criteria, better than all other methods. There are other 2FA methods outlined below . During the authentication process, you enter in your username and your password. On systems with 2FA enabled and using FIDO2/WebAuthn, the hardware token is queried. By pressing a button or tapping a sensor on the FIDO2 device, the FIDO2 device completes the authentication process in a cryptographically strong way. It is generally considered the most secure form of 2FA. It is also more convenient than manually entering codes generated by a TOTP application. During the registration of the FIDO2/WebAuthn device, a public/private key pair is generated, with the public key being registered with the service you will be authenticating with, and the private key is stored on a secure chip in the device. When authenticating, after you’ve entered in your username and password, the device is queried via an encrypted message that can only be authenticated by the private key, so a button or sensor is pressed on the device to allow the query to be completed, and the user is then granted access to the system. YubiKey: The most popular FIDO2/WebAuthn device is Yubico’s YubiKey. There are a wide variety of sizes and styles of YubiKeys. Yubico (along with Google) helped develop the predecessor to FIDO2 standard (U2F) before it was moved to the FIDO Alliance. Most GitLab team members that have WebAuthn devices have a YubiKey. It should be noted that for a long time Yubico’s source code for its firmware was open source, but some of the newer versions of firmware are closed source. This has caused some concern in the security community, particularly those that prefer to use open source whenever possible. YubiKey has been examined by the security industry at large, third party audits and by the Security Team. YubiKey’s are more than suitable for use within GitLab and work fine with FIDO2/WebAuthn-compatible systems. YubiKey has had a number of security issues which are typically resolved quickly. They have a dedicated page for security advisories. In rare cases, a security issue involving a hardware token arises that requires the hardware token to be replaced as a firmware update will not mitigate the issue. This happened with Yubico in 2017 . Most of the attack models that impact the FIDO2/WebAuthn tokens require physical access to the token itself. That is, the security advisories involve coding issues that can only be exploited via access to the token or the computer that the token is plugged into. This in itself makes the devices more secure. While there are other hardware tokens that are also decent and will function with GitLab, some have limitations. The recommended solution for team members is Yubico’s YubiKeys. What GitLab team members need to keep in mind is that if you travel with your company laptop - either on a business trip, a trip to visit family and friends in another location but you intend to keep working, or just a trip to the local coffee shop - treat the token with the same level of care as a credit card. Do not leave it where it could be stolen. If you are concerned about potentially losing your hardware token, be sure to add Touch ID and potentially a mobile device as an accepted authentication token. Sites following the FIDO2/WebAuthn standard should support multiple tokens. That way if one is lost or stolen, you can still login use another method. Currently YubiKey’s are limited to one per user, in the case of lost or damaged, please reach out in the #it_security_help Slack channel and we’ll assist you with a replacement. Usage of 2FA is mandatory for GitLab team members. In addition to FIDO2/WebAuthn , there are additional protocols used for 2FA. There is “cryptographic push”, TOTP , and SMS-based authentication (text message). Pluses and minuses to each are listed below. This is usually shortened to simply “push technology”. It is called “push” because during the authentication process, after you’ve entered your username and password, the service you are authenticating with automatically “pushes” a secondary authentication via a separate communications channel to a device you possess. This is usually your phone, and it is facilitated via a phone app that is specific to that process. During the registration process a cryptographically-secure key pair is generated, and the app uses that key pair to uniquely identify the Push as coming from the service you are authenticating to. Typically the app then either directly notifies you or you access the app, which will pop up a message asking if you were in fact trying to authenticate. Answer yes and the authentication process is complete. GitLab team members may be familiar with this Push method as it is used by Okta Verify . This method is not quite as secure as a hardware token, as typically those devices stores data securely on the secure chip on the device itself. It is possible that the Push phone app is also storing secret data on the local secure chip on the phone, however the entire process is dependent on the service’s servers being up, and WebAuthn is more self-contained. But note that Push technology is still considered extremely secure, and once configured it is fairly convenient to use. Timed-based One-Time Password is fairly secure. This method involves a rotating value based off of a cryptographic seed that is used to uniquely identify communication between the service and the end user authenticating. The value is (usually) a six digit number that changes every 30 seconds, and during the authentication process after you’ve successfully entered in your username and password, you are asked to enter in the value. GitLab team members should use 1Password to manage TOTP codes. The main problem with TOTP is that during this 2FA process it typically involves the end user entering in all values through a single communications channel (usually a web browser). As this is the case, it is possible that an attacker could send you a fake web page for you to enter in your credentials, including your TOTP value. Granted, the attacker would have to be reading your replies and sending in all of your credential information extremely quickly and get the entire process completed with 30 seconds so the risk is greatly reduced, but it still exists. Both WebAuthn and Push are preferred over TOTP, but as long as you are the one initiating the communication and you are not clicking on a link in email, you should be okay. One of the more popular (and most common) methods of 2FA is SMS-based text messaging. It is similar to TOTP, but instead of using a locally-stored application to calculate the six digit number, the service you are authenticating to sends the six digits to you via an SMS text message. When you set up your account, you provide the service with your cell phone number so they know which phone number to send the six digits to. Due to the number of caveats, SMS is only recommended if there is no other 2FA option available. Here are the main problems with SMS messaging. There may be services that only offer SMS as their 2FA solution, so in those cases it is better than nothing. If this is the case, there are a few things you can do to secure things ever so slightly. Remember the purpose of 2FA - it is a secondary authentication method, invoked after the primary authentication method has succeeded. You should only acknowledge a 2FA request if you personally have just successfully completed the primary authentication. An unsolicited 2FA request means someone has your password, and is in the process of trying to log in as you. This is why 2FA exists, to help protect your user account from attacks involving your password. If you experience any such irregularities, please bring it to the attention of the Security Team. For more information, review the handbook regarding Security Awareness There is a lot of information pertaining to Slack, as it is a critical part of GitLab’s communication. See the Slack handbook page . There is a lot of information pertaining to Zoom, as it is a critical part of GitLab’s communication. See the Zoom handbook page . The Google Calendar invite is the single source of truth for the Zoom link. Avoid linking the Zoom link in Google Document agendas since those may quickly become out of date. If the zoom link changed around the start of the meeting it is OK to have it in there temporarily. Need a new document or new slide deck quickly? Use shortcuts like https://doc.new or https://slide.new . The full list (not even restricted to Google products) is available at https://whats.new/shortcuts/ . See also Live Doc Meetings for more Google Doc tips. Google Analytics (GA) is an essential tool for making data-driven decisions. It receives data from both about.gitlab.com and docs.gitlab.com websites. Read through the Google Analytics Handbook for more information on GA. For example, you can look at the GA data to analyze how visited is a certain page, in a period of your choice. You can also look at the GA referrals data to understand where the users are coming from and where they go when they leave a certain page. Optional: Set your picture in Google so that your picture will show where you are in a Google document (vs showing just your first intial). This will allow others to more easily follow a discussion when meeting attendees move around in a document. Consider adding a phonetic pronunciation of your name and/or a pronunciation recording in your Google profile. Please make use of the Find a Time tab in Google Calendar, especially when scheduling events with teammates in other parts of the world: Find a Time presents a new or existing event’s time for all participants, adjusting for time zones as appropriate. To use Find a Time: For meetings spanning across multiple time zones and with external parties, Time & Date Calculator can help determine the best time to schedule. The GitLab Availability Calendar has been deprecated to allow for GitLab to scale effectively. We have created tools and tips for managing your time off . To help other team members to schedule meetings with you, you can set your normal working hours & location in Google Calendar . The preferred times will show up when someone tries to schedule a meeting with you. To set your working hours, go to Settings -> General -> Working Hours & Location . Working location can be set here or on your calendar directly by clicking on the pill in the All day row. In Settings there’s a helpful Copy to all option as well. Since all GitLab Team Members work remotely Home can be an ambiguous choice as folks may travel and it doesn’t provide any context on timezones. If you’d like, you can choose Somewhere else (in Settings) or Edit pencil -> + -> Another location (from the pill) to set this to something such as City, State, Country (GMT-#) and keep it up to date when traveling or timezones shift, which can help folks with planning and understanding timezones. The GitLab Team Meetings Calendar is available to all team members and can be found in your calendars list after it’s added. To add the GitLab Team Meetings Calendar to your list of calendars: Please reach out to a People Operations Team member if you have any questions. NOTE: Please do NOT remove any meetings from this calendar or any other shared calendars, as it removes the event from everyone’s calendar. You can find the details for the Company Calls, Group Conversations, 101s, and other teams’ meetings here, so you can attend a different teams’ meeting and ask questions, learn about what they’re working on, and get to know the rest of the GitLab Departments and teams. These meetings are open to everyone in GitLab. If you are creating a new team meeting, please add it to the GitLab Team Meetings calendar Please reach out to the People Operations Team via HelpLab with any questions, requests or changes to the GitLab Team Meetings calendar. Add an agenda or relevant content Under Add Guests, add GitLab Team Meetings in addition to anyone else you want invited (if you want the entire company invited please use everyone@). Note: This will appear like adding a room, which is expected If you are familiar with queries in Gmail, add a filter to remove invites responses from your inbox with the following query: *.ics subject:(\"invitation\" OR \"accepted\" OR \"rejected\" OR \"updated\" OR \"canceled event\" OR \"declined\") when where calendar who organizer You can also create a filter to remove ALL invite responses from your inbox with the following search terms: Please click ‘Guests can modify event’ so people can update the time in the calendar instead of having to reach out via other channels. You can configure this to be checked by default under Event Settings . You can change the default notification settings by calendar (add or remove notifications for all-day events, add a second default notification to all events...). This is accessible through Settings -> Settings for my calendars -> pick the calendar -> Event notifications and All-day event notifications . If you change the default notifications, all existing events will inherit that as well (unless their notifications have been customized). (This assumes you are using Google’s new Calendar ). When you have accidentally deleted something from the Team Meetings calendar, you can recover it by: We recommend enabling the ‘Show declined events’ setting if you are unable to attend a meeting but will still contribute to the agenda or attendance list asynchronously. Use this setting if it’s helpful for you to see declined meetings in your calendar view for any other reason. We recommend you set your Google Calendar access permissions to ‘Make available for GitLab - See all event details’. Team member calendars should not have access permissions set to ‘Make available to public’ due to the risk of sensitive data exposure and zoombombing . Consider marking the following appointments as ‘Private’: There are several benefits and reasons to sharing your calendar with everyone at GitLab: When setting up your Google Calendar be sure to set your working hours . If you’d like to share your calendar with e.g. your partner you can use the ‘Share with specific people’ feature and set the permissions to ‘See only free/busy (hide details)’: Enable speedy meetings to automatically provide a buffer at the end of events you schedule. This thoughtfully allows participants with back-to-back events the opportunity to use the restroom or grab a cup of coffee without being late to their next function. Add as many time zone world clocks as you wish by, in Google Calendar, going to Settings -> World Clock in order to see team members’ local times. You can also use sites like TimeAndDate to convert times to/from UTC for example. Check Display secondary time zone and select (GMT+00:00) Coordinated Universal Time (UTC). This enables team members to standardize on a single time zone in communicating when meetings take place. See the Sandbox Cloud page for a listing of cloud resources and how to gain access to them. We would be remiss if we didn’t start this section off with this IMPORTANT message: Your default storage place for information that needs to persist and be available to others in the company should be ON THE WEBSITE/IN THE COMPANY HANDBOOK and not in Google Drive and Google Apps files!! This is from the top. This is how we operate, because Google Docs/Apps can only be found and contributed to by team members, and not by users, customers, advocates, future employees, Google handbook searches, or developers. Having said that, there is content which doesn’t make sense to be created on the website directly (e.g. large collections of data in tables, spreadsheets for calculations, etc) or for which Google Drive storage makes sense. When directing folks to these files in Google Drive please include name of the file in the handbook so that team members can search for it in Google Drive. If you link directly to the URL, people from outside the organization can request access, creating workload and the potential for mistakes. It is important that we not just throw files into random or general places in the shared Google Drives. Doing so makes it harder for others to find and work with the content. Here are some guidelines to organizing the Google Drive content: For starters, when your GitLab Google company account is created you automatically get a Google Drive with unlimited storage allocation in your own “home” directory (called My Drive). You can get to it by: This is great for storing your own working files. As already stated, this should never be the final resting place for shared files that are meant to be used by the rest of the company (or beyond). There are a few Google Drive repositories of GitLab shared files (there might be more, please add if not listed here): How do you use these? You don’t have to remember these URL’s. To add these links to your Google Drive My Drive directory, do the following: To really make your Google Drive easier to access, you can have your Google Drive show up on your Mac Finder as a regular drive. With this it is easier to store and view files such as videos, analyst reports (PDFs), etc. Use these GitLab branded form templates when creating internal or external surveys or forms. Make a copy of the form and only edit the copy; do not edit the template itself. Review what data privacy means at GitLab . Help participants make informed decisions by applying the following guidelines: If you use the archive function, you normally return to your overview. With Auto-advance you can select whether to advance to the next or previous message. “Auto-advance” can be enabled from the Advanced section under Settings. This reveals the Auto-advance settings in the General section under Settings. The default setting of showing the previous (older) message is usually preferred. -Set up an email signature which includes your full name and job title so people can quickly know who you are and what you do. -It is also an option to add your personal pronouns to your email signature. Note: You can copy and paste the template below to use it in your own signature. Alex Doe (they/them) Frontend Engineer | GitLab During onboarding you may have been asked to set up your GitLab Gmail and your email signature using the following example as a guideline . If you are employed by our German entity, you will need to add some company related details to the end of your email signature as set out here ( accessible internally to GitLab only ). Certain company details are required to be added as a signature to the forms of communication listed below, where that communication is going from a team member employed by our German entity GitLab GmbH to outside of the company, and relates to GitLab’s business activities: If you are sending such a communication, externally, please look at the setting of the relevant application and add the company details below to the end of the communication. If you are in doubt, please do add the details. If you are unable to add this information for any reason, please alert legal-employment@gitlab.com . If you are employed by our German entity you may have been asked during onboarding to set up your signature wherever it’s required to be used and to include certain company details, and if you have not been, please do so now, as set out here ( accessible internally to GitLab only ). It might be useful to add a Gmail filter that adds a label to any GitLab notification email in which you are specifically mentioned, as opposed to a notification that you received simply because you were subscribed to the issue or merge request. You can create a Gmail filter that adds a label to any GitLab notification email in which you are assigned as a reviewer: You can create a Gmail filter that adds a label to any GitLab notification email in which an MR has been merged: GitLab issues and merge requests can generate a lot of email notifications depending on your settings and how in-demand your attention is. It can be useful to apply a label to these generated emails and move them out of your immediate inbox. You can learn more about how to use Gmail filters to organize your inbox in Productivity Hack video . To import downloaded filter export go to Gmail => Settings => Filters and Blocked Addresses => Import filters. Keyboard shortcuts only work if you’ve turned them on in Gmail Settings. Here are some shortcuts you can use List your inbox and preview mails in one view with this configuration change: To utilitze Gmail to it’s full potential, consider adopting the Inbox Zero strategy. It’s the same way Google employees use Gmail. There’s also an internal training recording about this. It goes into more details and more “power user” focused, covering keyboard shortcuts, etc. The integration for Google Meet and Google Chat is enabled by default. It can consume too much space in the left menu listing mailbox folders. In order to disable the integration in Google Mail, navigate to Settings at the right top, See all settings , Mail and Chat and select the following: Save the changes and wait for Google Mail to reload. Use this general GitLab Slide Template when creating slide decks for internal and external use. Make a copy of the slide deck and only edit the copy; please do not edit the template itself. To avoid potentially editing the source deck, you can go to the Template gallery and click on the GitLab Slide Template vYear-Month to quickly make a copy and begin a new deck. Ensure that all pages in the slide deck are numbered, with the title page always being number 1. Slide decks are available in Highspot in GitLab’s Official Sales Deck Library . Jamboard is a collaborative whiteboarding platform, accessible at https://jamboard.google.com/ . The results are persisted in Drive and are shareable like any other object.",
  "The GitLab Support Team provides technical support to GitLab.com and Self-Managed GitLab customers. The GitLab Support Team Handbook is the central repository for why and how we work the way we do. Know someone who might be a great fit for our team? Please refer them to the job-family descriptions below. Remember, as members of the support team we are the first to interact with someone when they have a problem or question. As such it is up to us to represent the company and make sure we present ourselves properly. Therefore we are expected to: Our goal is to provide guidance that will lead to the best results for our customers as they use GitLab. In that, we will often point to documentation, product functionality, or open bugs/feature requests. However, there are times when customers will be best served through one of our commercial offerings. Support is part of the Engineering Department and Support Engineers are not commissioned or bonused for upsell for additional services, customer purchases, or lead generation. If you recommend Professional Services or moving to a different tier or offering you may link to this section in your recommendation to give the customer assurance you’re doing so with no mixed motivations. GitLab Support is part of the Engineering division . While most engineering departments are part of the R&D cost center, Support is part of the Cost of Sales (or sometimes Cost of Goods Sold (COGS)) cost center. This unique arrangement is expressed in our Key Performance Indicators , which are largely focused around pursuing customer success and satisfaction while driving efficiency by increasing output while keeping costs within a predefined range. This is also why Support Engineer responsibilities include contributing code and documentation and working alongside Product Managers on our products: By using the knowledge gained from interacting with our customers to make our products or docs better, we solve problems before they become one. This reduces support case load while increasing efficiency for the wider GitLab organization. For example, the Sales department can rely on our docs to answer customer queries instead of leaning on Support or Customer Success for help, freeing up more time to close sales. Part of Support’s role is to amplify the voice of the customer. One way of doing this is inviting other GitLab team members into experiences that will help them understand customer challenges with the product or our own obstacles in helping customers overcome those challenges. Before you start, make sure you get light-agent access in Zendesk so that you can view Support tickets. If you’re looking to get more exposure to customers, there are a few ways to get involved with Support: GitLab team members interested in learning about the GitLab Support team and our responsibilities are encouraged to participate in the Support Shadow Program. The Support Shadow Program is a way that team members outside of Support can spend time shadowing, learning, collaborating, and working together with the GitLab Support team. If you’re not part of the Support team and you’d like to participate in this program, open a Support Shadow Program issue in the support-team-meta project. This issue will be used to organize, plan, and track progress toward this program. GitLab Support uses Calendly to facilitate scheduling Shadow pairing sessions with participants. If you’re part of the Support team and you’d like to volunteer to host Support Shadow Pairing sessions with folks outside of Support, please open a Schedule update request issue requesting that Support Ops add you to the Support Shadow Program Calendly rotation. Support calls are published on the GitLab Support Calendar . There are: A great way to get involved is to join a customer emergency call. You can monitor #support_self-managed for PagerDuty alerts. Alternatively, if you have access to PagerDuty you can be scheduled into a shadow rotation . If you go through the responsibilities for each role in Support you can piece together how the organization works. We wanted to make a simple clear way to think about how the roles work together to solve problems: This simple list helps to give an easy way to set expectations and align problem solving in different roles. We use Key Performance Indicators (KPIs) to keep track of how well each Engineering Department is doing, including the Support team, as a whole. The KPI measurements can be found under the Reporting tab in Zendesk if you have the appropriate access, but progress on meeting these KPIs is also tracked via the aforementioned KPI link. We review these KPIs weekly in the Support Week-in-Review . In service of achieving our KPIs and OKRs, there are three key pillars that we must balance to achieve success: At various times it’s easy to over-optimize on one of the pillars to solve a problem, but considering all three is key to avoiding short-sighted decision making. The Single Source of Truth for information about Support Team Members - everything from email address and personal interests to product skills and group memberships - is the support-team project . The Support Team Home Page is built from the information in that project. Many other Support tools and automations make use of it also. See the Support team entry page of the Support Team wiki for details of the structure of an individual Support team member’s file. Information for and about the different parts of the Support Team can be found in the following sections of the Support Handbook: Below we also have some commonly referenced pages: GitLab Support’s vision is to deliver a consistent, “delightful” experience to our customers. Our team members will collaborate across all timezones to seamlessly deliver the results our customers care about while continuing to strengthen and scale the team. The overall direction for Support in FY25 will continue to build from the foundations laid in FY24. We will continue to focus on KPI achievement and evolve and iterate our approach to support, keeping the customer centered in our outcomes. Following on to the company’s overall strategic objectives, specific areas of focus are: While our publicly visible OKR page and Key Performance Indicators reflect the focus and progress for the current quarter, the following provides more detail on the items included in the themes for the entire FY25. FY24 was a challenging year in many aspects. As the workload and customer expectations grew, we needed to look at how we could improve efficiency and create a differentiated Support experience. FY25 is a year to focus and align on our customer needs and put our customers at the center of our understanding of their situations, perceptions, and expectations. To that end, we will: In FY23 / FY24 we moved towards dividing the team into globally distributed groups of engineers. In FY25 we’ll build on that foundation and extend towards differentiating support offerings to better align with customer requirements. This year, we will: As GitLab grows, Support’s influence within the company as advocates for customers must also grow. We need to continue to strengthen the Values-driven cultural attributes that promote efficient collaboration and results for customers while maintaining GitLab Support as a great place to work. In FY25 we will: FY24 was a year of better understanding the needs of our customers. In FY25 we will focus on delighting them. We will: Unlike typical companies, part of the mandates of our Security, Infrastructure, and Support Departments is to contribute to the development of the GitLab Product. This follows from these concepts, many of which are also behaviors attached to our core values : As such, everyone in the department should be familiar with, and be acting upon, the following statements: Citing our dogfooding operating principle, people sometimes ask why GitLab Support doesn’t use Service Desk . Dogfooding is using a piece of GitLab for its intended purpose . For example, one could use GitLab issues as a newsletter (and we do! See: Support Week in Review ), but creating merge requests to help Issues serve as a newsletter more effectively wouldn’t be dogfooding unless that improvement also helps its core use case. In other words: Dogfooding is using the product in the way that our customers would use it to the end of discovering and solving pain points that they have. Dogfooding supports customer results. At GitLab Support we use Service Desk to process Personal Data Requests , but not for our global support because the customer for Service Desk is primarily small teams soliciting bug reports, feature requests, or general feedback . Through our use of Service Desk in this smaller setting we’ve been able to influence product direction towards adding features like internal notes . We continually evaluate product features for use-cases within Support and provide feedback and feature requests where blockers exist. Support will always prioritize customer results over any other consideration. These were previously populated via a tool we are no longer using. TODO: Replace with current GitLab implementation The GitLab Support Team is part of the wider Engineering function. Be sure to check the communications section in the Engineering handbook for tips on how to keep yourself informed about engineering announcements and initiatives. Here are our most important modes of communication: Where we want to ensure that important messages are passed to the global support team, we will use this messaging template . This ensures that these messages are delivered across our communications channels in a structured and documented manner. We use the following GitLab Groups to notify or add support team members to issues and merge requests on GitLab.com. Our team projects and issue trackers can be found in the Support parent group . Here are some selected projects which are relevant to team communications. We use the Support meta issue tracker for tracking issues and creating issues that may require feedback around support. If you’re interested in working on a project or task related to support feel free to create an issue and link to any external issues or projects so that we can: Issues regarding documentation or features for GitLab, our FOSS project or any of the GitLab components should not go in this issue tracker, but in their appropriate issue tracker. If you have a proposed solution that is actionable, it’s best to start a merge request , tag the team for feedback and link in the Support Week in Review . We follow GitLab’s general guidelines for using Slack for team communications. As only 90 days of activity will be retained, make sure to move important information into the team handbook, product documentation, issue trackers or customer tickets. When naming channels, “spt” is meant for internal channels, meaning those that will be of use to the Support Team mainly. They should be public so others may join if they choose. If a channel has a “support” prefix, it is meant as a public interface where other teams will interact with the Support Team. The Support Daily Slackbot is an automated tool designed to facilitate daily standup Slack threads across various teams and regions at GitLab. It posts customized messages to specific Slack channels based on type and target. For further details of the different variations, please refer to the README file . At GitLab we are to be public by default unless there is a valid reason for it to not be public. While Slack is not public, the spirit of opening up discussions so that everyone can contribute means that private channels should be kept to a minimum. The following private channels are permanent fixtures in support. Usage estimates are approximate based on traffic in Feb 2022. Before starting a new private channel, ask yourself Why can’t everyone contribute here? Appropriate answers might be: Private channels are not appropriate for: Values are only values if you do them when it is hard. See more discussion on how to scale the business while preserving GitLab Values . If you need to be added to one or more of these groups, please open an issue in the access requests project . We use the following team calendars to coordinate events and meetings: Add these calendars to your GitLab Google calendar by clicking on the “+” sign next to “other calendars”, and choose “subscribe to calendar”. Enter the relevant ID mentioned above. If you need access to these calendars, ask a support team member for help. Please use the following formats for your name in Zoom as described in adding your title to your name in Zoom . As a primarily customer facing team, these formats were chosen to help identify you by vendor and role in calls where customers are present. For the sub-department, use the smallest unit you belong to. Again, bias for customer understanding over technical correctness. The Support Team has several meetings each week. These allow us to coordinate and help us all grow together. Each meeting has its own agenda and is led by a different member of the team each week. Discussions are encouraged to be kept in issues or merge requests so the entire team can collaborate, regardless of time zone. Any demos or announcements that need to be shared with the entire team should be shared in the Support Week in Review . All Zoom and agenda links can be found on the relevant calendar entry in the Support Calendar. . Support team members in some regions meet up regularly. Details of these calls are on the Weekly Support Team Call workflow page . The Support management team meets regularly. Details of these calls are on the Support Managers page The leadership team (Staff, Managers, Senior Managers, Directors) meet monthly to align on business objectives and cross-region collaboration. There is SAFE data shared, but a read-out is disseminated for wider team awareness. Some regional Support teams have meetings oriented around company news, Support initiatives, training plans, and connectedness. Senior and Staff Support Engineers are encouraged to host office hours. These office hours are intended to strengthen the team through mentoring. It is up to each Senior/Staff Support Engineer whether they schedule office hours, and how often. Please see the “GitLab Support” Team calendar to view office hours and invite yourself. We encourage hosts to include what they will cover in the calendar event description and optionally a document to track. Some ideas of what one can expect at a Senior/Staff Support Engineers’ office hour: You may wish to host a sync call. To do so, you can create an event on the Support Calendar. . To invite team members to the event, you can use the appropriate Support email alias (internal Handbook, GitLab team members only) Every Friday, we do a week in review, inspired by the greater Engineering organization week in review . You can add topics any time by using the SWIR topic form . Any workflow changes or announcements should be shared in the SWIR and we recommend you check at least once a week to stay up to date on recent changes. Ideally, the information shared here should have a permanent location such as an issue or merge request. We encourage anyone in the team to share. We currently have the following topics, each in its own section in the SWIR: SWIR Issues can also have their own Tags or Labels in the GitLab project. These are used to highlight specific Areas of Focus (L&R, SaaS...). Labels are used on the Issues only, they do not appear in the digest Issue nor in the Google doc. One label, Manager Attention , is used for policy changes or other topics of which Support Managers should specifically be made aware. You can find the Manager Attention label here ] and subscribe to it. You can read about the origins of the auto-generated SWIR in this issue . The Support Team collaborates with many other departments throughout GitLab - Sales, Channel, Product and Legal, to name a few. And we have created two different roles to help those collaborations to be as effective and efficient as possible. The Support Stable Counterpart role is designed to provide a strong connection between a product or non-product team and Support. The purpose is discussing product issues, sharing product knowledge and representing customer needs, as well as sharing knowledge about each team’s work and developing processes and documentation to allow the two teams to work together well. If you are interested in becoming a Support Stable Counterpart, or would like to learn more about the role, read the Support Stable Counterparts page. The Support Team records our institutional knowledge, processes and workflows in multiple places, such as the handbook and project issues templates. When updating such documents, make sure to have visible artifacts of approval on your merge requests before merging even if you have received approval somewhere else. This avoids the impression of changes being made without any oversight or accountability. Artifacts of approval can include: Each Slack channel within Support has a number of Workflows attached to them that are used to provide information to users. The source files for each workflow live in the slack-workflows project. Some workflows are meant to notify the team of new issues created in the relevant project. In these cases, a project webhook passes information to Zapier , which then sends the information to a Slack workflow. Providing information by reacting to a message with a specific emoji. See the Support Onboarding page After getting promoted, make sure to update your title in: Consider updating the title on Slack and on Zoom, following the guidelines in Zoom name format . In GitLab Support, we use Support Pods to organize support engineers as they work. Each Support Pod is a cross-region, single skill group of engineers who are interested in their Support Pod’s specific product area. They are engineer- lead. To join or start a Support Pod you can read more below. See the Support Pods handbook page and the Working with Support Pods workflow page . The Support team uses ‘support-team-meta’ project issues to track ideas and initiatives to improve our processes. The ‘Active Now’ issue board shows what we’re currently working on. It uses three labels: Some principles guide how these labels are used: Each week we look at the board and discuss the issues to keep things moving forward. By keeping a maximum of six issues for each label, we limit work in progress and make sure things are completed before starting new tasks. Adding and managing items on the board: Support managers will regularly review the board to keep items moving forward. The Support Slackbot (archived) has been retired.",
  "As Infrastructure Platforms, our mission is to enable GitLab to deliver a single DevSecOps platform across SaaS and self-managed platforms by building highly available, reliable, performant, and scalable infrastructure solutions while maintaining the lowest total cost of ownership. Deliver the industry leading SaaS solutions, empowering organizations worldwide with the most innovative and efficient DevSecOps platform. If you’re a GitLab team member and are looking to alert the Infrastructure Platforms teams about an availability issue with GitLab.com, please find quick instructions to report an incident here: Reporting an Incident . For all other queries, please see the getting assistance page. Initiatives driven within the Platforms section, often spanning multiple quarters, are represented on the SaaS Platforms section epic (GitLab team member). Unlike typical companies, part of the mandates of our Security, Infrastructure, and Support Departments is to contribute to the development of the GitLab Product. This follows from these concepts, many of which are also behaviors attached to our core values : As such, everyone in the department should be familiar with, and be acting upon, the following statements: (click the boxes for more details) The Infrastructure Platforms department uses GitLab and GitLab features extensively as the main tool for operating many environments , including GitLab.com. We follow the same dogfooding process as part of the Engineering function, while keeping the department mission statement as the primary prioritization driver. The prioritization process is aligned to the Engineering function level prioritization process which defines where the priority of dogfooding lies with regards to other technical decisions the Infrastructure Platforms department makes. When we consider building tools to help us operate GitLab.com, we follow the 5x rule to determine whether to build the tool as a feature in GitLab or outside of GitLab. To track Infrastructure’s contributions back into the GitLab product, we tag those issues with the appropriate Dogfooding label. At GitLab, we have a handbook first policy . It is how we communicate process changes, and how we build up a single source of truth for work that is being delivered every day. The handbook usage page guide lists a number of general tips. Highlighting the ones that can be encountered most frequently in the Infrastructure Platforms department: Classification of the Infrastructure Platforms department projects is described on the infrastructure department projects page . The infrastructure issue tracker is the backlog and a catch-all project for the infrastructure teams and tracks the work our teams are doing–unrelated to an ongoing change or incident. In addition to tracking the backlog, Infrastructure Platforms department projects are captured in our Infrastructure Platforms department Epic as well as in our Quarterly Objectives & Key Results The Infrastructure Library contains documents that outline our thinking about the problems we are solving and represents the current state for any topic, playing a significant role in how we produce technical solutions to meet the challenges we face. Infrastructure maintains a Technical Roadmap for planning projects over the short (1y), medium (2y), and long term (3y). This serves as our strategic compass, helping us balance immediate needs with long-term sustainability. The Technical Roadmap is based on the Product Roadmap , where Product provides the “What” (customer needs) and “Why” (business strategy). Engineers then determine the “How” (technical implementation), while Engineering Managers plan the “When” (scheduling). This comprehensive roadmap emphasizes building high-quality, complete features in a sustainable manner. The Technical Roadmap serves three key purposes: It helps build engineering excellence by addressing critical areas that might not show up in product backlogs, such as technical debt, performance improvements, platform improvements, and system scalability. It enables the department to be proactive rather than reactive. By regularly asking key questions like “Where do we see the biggest instability in our systems?” or “What is generating the most toil?”, we can address issues before they become critical problems. This helps maintain our SLOs and keeps our customers happy. It aligns engineering efforts with business goals, ensuring technical improvements drive GitLab’s success. Each technical roadmap item is prioritized based on business value and strategic alignment. The Infrastructure Roadmap is maintained as a static site. GitLab team-members can review the current technical roadmap, at infra-roadmap.gitlab.com . NOTE : The Infrastructure Roadmap is not publicly available as some of the projects and initiatives may be considered unSAFE . The site presents the roadmap in a visual manner, showing: Changes to the Roadmap are made through merge requests to the infra-roadmap project. The data is stored in YAML format, and changes can be made by editing the YAML. This allows for version control and collaborative discussion through the merge request process. Full instructions for making changes to the Infrastructure Roadmap are available in the project’s README.md . Everyone is encouraged to contribute to the roadmap, whether proposing new initiatives or making smaller changes like updating descriptions or adding links to relevant issues. We have a model that we use to help us support product features. This model provides details on how we collaborate to ship new features to Production. Our main method of communication is Slack. If you need assistance with a production issue or incident, please see the section on getting assistance . The SaaS Platforms group is gradually directing requests for help to the #saas-platforms-help Slack channel. This channel can be used if it is unclear which Infrastructure team the question should be directed to. For more information, refer to the landing page for getting assistance . The #saas-platforms-help channel is monitored by SaaS Platforms Engineering Managers and Staff+ engineers who triage any inbound requests. When triaging this channel, one should locate the team who can best answer this question and instruct the requestor to contact that team using the team’s preferred contact method. When the requestor is connected to the right team, add a green check emoji to the message. Finally, if needed, update the getting assistance page with any changes. Once per week, we hold a Platforms leads call to align on action items related to career development, general direction or answer any ongoing questions that have not been addressed async. The call is cancelled when there are no topics added on the morning of the call. In addition to the Platforms leads call , we have some recurring events and reminders that can be viewed in the SaaS Platforms Leadership Calendar . Please add this to your Calendars to stay up-to-date with the various events. Sr. Director of Infrastructure Marin Jankovski, likes to meet with new team members that join the organization. Marin sets up informal 1:1 coffee chats a few times a month with newer team members to get to know one another and see how they are doing. This process is organized by their EBA who will reach out to team members once they has the availability to meet. As this is a large team, it may take a while to get through everyone. If someone needs to meet with Marin sooner than when the coffee chat is scheduled, you can reach out to their EBA Liki Simonot to set something up. The Engineering Leads for each Stage, along with their Product Managers, hold weekly progress reviews to assess their groups’ progress, share project updates, resolve blockers, and celebrate wins. Additionally, the Director of Product and the Senior Director of Infrastructure Platforms conduct a higher-level leadership review, where they go over summaries from these group-level meetings. The review is private streamed to the GitLab Unfiltered channel because the review covers confidential issues. All recordings are made available in the Platforms Grand Review YouTube Playlist The Infrastructure Platforms Leads Demo is an opportunity for sync discussions between Staff+ IC across the Infrastructure Platforms Group to highlight current ongoing efforts underway in the teams they support. All team members are welcome to join the call, but the emphasis is on Staff+ ICs to present and discuss the work they’re focused on, the problems they’re experiencing, and solutions they’re considering. The call is recorded to the Infrastructure Platforms Leads Demo Unfiltered Playlist . The agenda can be found in Google Docs . While the intention is for the call to be made public on GitLab Unfiltered, the default is for it to be published as private. At the end of the call, a quick vote is held between the attendees and if all agree that the content is #SAFE, it can be published as public. On the landing page for getting assistance , we ask team-members who need assistance to raise Requests for Help using standard templates. These issues are raised in the request for help issue tracker and are automatically assigned to the Engineering Manager of the relevant SaaS Platforms team. The Engineering Manager is expected to: In an effort to enhance the tracking and resolution of requests directed to the Infrastructure team, we are evaluating a bot that converts Slack messages in #infrastructure_lounge channel into GitLab issues. Agents responsible for handling these issues are defined in a JSON file, which serves as a CI/CD variable . Currently, this file contains a static list of all members of the infrastructure department. We use epics and issues to manage our work. Our project management process is shared between all teams in SaaS Plaforms. The Platforms section builds and maintains various tools to help deploy, operate and monitor our SaaS platforms. You can view a list of these tools in the Platforms Tools Index . We use objective and key results to set goals in alignment with OKRs at GitLab . Our OKR process is shared between all teams in Saas Platforms. Our hiring process is shared between all teams in Infrastructure Plaforms. This Infrastructure Platforms Interviewing Guide offers more detail on some of our regular openings, interview process and other useful information related to applying to jobs with us. More information on our current openings can be found on the careers page . All team members are encouraged to schedule time for personal development. The following links may help you get started with Platforms-relevant learning. Please add your own contributions to this list to help others with their personal development.",
  "Our vision is to transparently lead the world to secure outcomes. Our mission is to enable everyone to innovate and succeed on a safe, secure, and trusted DevSecOps platform. This will be achieved through 5 security operating principles: The Security Division provides essential security operational services, is directly engaged in the development and release processes, and offers consultative and advisory services to better enable the business to function while minimising risk. To reflect this, we have structured the Security Division around four key tenets, which drive the structure and the activities of our group. These are : The Product Security Department is primarily focused on Securing the Product. This reflects the Security Division’s current efforts to be involved in the Application development and Release cycle for Security Releases, Infrastructure Security, and our HackerOne bug bounty program. The term “Product” is interpreted broadly and includes the GitLab application itself and all other integrations and code that is developed internally to support the GitLab application for the multi-tenant SaaS. Our responsibility is to ensure all aspects of GitLab that are exposed to customers or that host customer data are held to the highest security standards, and to be proactive and responsive to ensure world-class security in anything GitLab offers. Security Operations Department teams are primarily focused on protecting GitLab the business and GitLab’s platform. This encompasses protecting company property as well as to prevent, detect and respond to risks and events targeting the business and our platform. This department includes the Security Incident Response Team (SIRT) and the Trust and Safety team. These functions have the responsibility of shoring up and maintaining the security posture of GitLab’s platform to ensure enterprise-level security is in place to protect our new and existing customers. Threat Management Department teams are cross-functional. They are responsible for collaborating across the Security Division to identify, communicate, and remediate threats or vulnerabilities that may impact GitLab, our Team Members or our users and the community at large. The Security Assurance Department is comprised of the teams noted above. They target Customer Assurance projects among their responsibilities. This reflects the need for us to provide resources to our customers to assure them of the security and safety of GitLab as an application to use within their organisation and as a enterprise-level SaaS. This also involves providing appropriate support, services and resources to customers so that they trust GitLab as a Secure Company, as a Secure Product, and Secure SaaS GitLab is both a company and a product. The Corporate Security department focuses on implementing and protecting the information technology (IT) related systems that the company uses to conduct business internally, and provides the hardware, software, and tools that our team members and 3rd party service providers (aka contractors) need to be productive and get their job done efficiently. The configurations that we implement for team members internally are designed to protect our customers and their data. We have a 24x5 technical support helpdesk for team members and have engineers that configure and maintain many of our company-wide tech stack applications . We invest heavily in device trust, identity management, and infrastructure governance to provide the highest level of security assurance for the administrators of our product and ensure all appropriate controls are in place when handling customer data. Security Program Management is responsible for complete overview and driving security initiatives across Product, Engineering, and Business Enablement. This includes the tracking, monitoring, and influencing priority of significant security objectives, goals, and plans/roadmaps from all security sub-departments. Security Program Manager Job Family In keeping with our core values and the belief that everyone can contribute , the Security Division is committed to dogfooding and contributing to the development of the GitLab product. For information regarding GitLab’s HackerOne bug bounty program , and creating and scheduling security issues, please see our engaging with security page and our Responsible Disclosure Policy . If an urgent security incident has been identified or you suspect an incident may have occurred, please refer to Engaging the Security Engineer On-Call . Examples include, but are not limited to: GitLab provides a panic@gitlab.com email address for team members to use in situations when Slack is inaccessible and immediate security response is required. This email address is only accessible to GitLab team members and can be reached from their gitlab.com or personal email address as listed in Workday. Using this address provides an excellent way to limit the damage caused by a loss of one of these devices. Additionally if a GitLab team member experiences a personal emergency the People Group also provides an emergency contact email . Many teams follow a convention of having a GitLab group team-name-team with a primary project used for issue tracking underneath team-name or similar. The following group tags can help you get the attention of a specific department, team, or specialty: Primary Security departments : Leadership and program support : Specific teams and specialties : We believe it is important to share regular updates at various levels of the Security Division, and we use Slack as the primary mechanism for providing these updates. Our updates are open to all GitLab team members using the following process: Security Leadership meets twice a month over Zoom to discuss division-wide topics. Individual contributors from across the security organization are invited to present their work, ideas, or projects to this leadership forum. If you’re interested in presenting: Note that these meetings are not on the general Security calendar. Your manager will ensure you receive the meeting invitation for your scheduled time. We encourage all team members to take advantage of this opportunity to share your work and insights with security leadership. For an overview of the communication and response process for a suspected ransomware attack, please see our Responding to Ransomware page. The following best practices will help ensure tokens are handled appropriately at GitLab. For detailed requirements regarding the use of tokens at GitLab, please see our token management standard . We welcome GitLab team members to join meetings that are on our shared Security Calendar . Security crosses many teams in the company, so you will find ~security labeled issues across all GitLab projects, especially: When opening issues, please follow the Creating New Security Issues process for using labels and the confidential flag.",
  "As a company, GitLab is dedicated to open source. Not only do we believe in it, but we use it, and we give back to it. Not just through GitLab, but through contributions to other open source projects. The purpose of this page is to document how a GitLab employee can: As an open source project, we want to stay healthy and be open for growth, but also ready to accommodate a 10x factor of our community. In order to achieve that, we’ve outlined a strategy that is a collaboration between multiple departments. See Creating a new project for the instructions. If you’re contributing to an open source project on behalf of GitLab, you may be required to enter into a CLA. If your GitHub account’s primary email is not your @gitlab.com email, you can add it as an additional address. No need to create a separate account. In the future, we might have a single organization for forks. That will allow us to track various metrics about contributions made by GitLab employees. GitLab allows contribution to open source licensed projects. If the project to you which you wish to contribute does not have a license: The guidance for using open source software, including details of which licenses are pre-approved for use, is now available in the Legal & Corporate Affairs handbook . Avoid using forked code and try to contribute your change upstream. It’s typical for forks to fall far behind the upstream repository and such dependencies become a source of pain: There may be good reasons to create a fork: If you decide to create a fork, make sure you open an issue that: Visit the Legal & Corporate Affairs Product handbook for further information on this.",
  "A list of policies and standards including both internal and external ones is available on the internal handbook . Information on how to add to the list below is available there as well.",
  "Aligned with GitLab’s overarching information security strategy and its three-year plan, the Product Security Department (PSD) within the Security Division is responsible for crafting and directing a comprehensive vision to bolster the cybersecurity posture of the GitLab platform. At GitLab, product security encompasses a broad range of cybersecurity disciplines that enable product and engineering teams to design, develop, deploy, maintain, and refine GitLab’s technologies securely. This goes beyond the conventional confines of security, covering everything from protecting developer workstations to ensuring the integrity of our production environments. Our mission is to set the standard for product security by fostering a culture of rapid innovation and secure product delivery. We are committed to leveraging the GitLab platform, embodying the pinnacle of internal usage (‘dogfooding’) practices. By maintaining close collaboration with product teams and contributing significant security features and capabilities to the GitLab codebase, we aim to enhance our operations and be a vital driver of the broader GitLab vision. Our comprehensive, multi-year product security mission can be found in our internal handbook. Our Product Security Risk Register process details can be consulted on this dedicated page . Success in product security is not confined to PSD or even the Security Division. It requires a concerted effort across the entire GitLab ecosystem. Collaboration is crucial, involving not just our security counterparts but the broader organization. Key disciplines and capabilities, from Security Operations to Site Reliability Engineering, while not directly under PSD’s purview, are vital to our strategy’s success. The Product Security sub-department includes the following teams. Learn more about each by visiting their Handbook pages. The Product Security Department has standardized its labeling system to improve issue tracking, team identification, and cross-team collaboration across all security teams. All issues, epics, and merge requests within the Product Security Department must include the “Division::Security” and “Department::Product Security” labels. Each team uses a standardized naming convention : Multiple team labels can be applied to issues requiring collaboration",
  "Protect company property by identifying, preventing, detecting and responding to risks and security events targeting the business and GitLab.com and its users. We are at the forefront of GitLab’s security. The Security Operations department focuses on the operational aspect of security. Our department consists of experienced breakers, builders, and defenders from all walks of life and geographic locations. We are responsible for improving GitLab’s security capabilities, metrics in the areas of security anomaly/event detection, incident response and abuse mitigation of GitLab.com.",
  "The Security Threat Management sub-department is responsible for identifying and remediating vulnerabilities or threats that may impact GitLab, our Team Members or our Customers and the community at large. The Security Threat Management sub-department’s mission is to support the business and our overall security efforts by ensuring that we are focused on real world threats and vulnerabilities that impact us. We accomplish this by: The Security Threat Management sub-department includes the following teams. Learn more about each by visiting their Handbook pages.",
  "Our mission is to provide a high level of assurance that GitLab (the platform and company) is secure. Our vision is to be a trusted sales enablement partner that is recognized internally and externally for its collaborative and transparent security assurance program, powered by AI and automation. This will be achieved through 10 strategic objectives: There are four teams in the Security Assurance department. The Security Assurance sub department utilizes a variety of tools to carry out day to day activities. The system admin is responsible for the following: All other actions are the responsibility of the assigned DRI. Check out these great security resources built with our customers in mind:",
  "This page contains GitLab’s Accounts Payable department policies, procedures and guidelines. Our goal is to enable payments in a timely manner to both our team members and our external partners. We also provide useful links to other areas of the handbook that are relevant to the Accounts Payable procedures. Welcome to Accounts Payable, commonly referred to as AP. You should be able to find answers to most of your questions in the sections below. If you cannot find what you are looking for, then please connect with us: Vendor Invoices - These invoices are processed in Coupa either PO Backed or Non PO backed. Vendors must be set up or onboarded in Coupa and invoices must contain all required information like Invoice Number, Invoice Date, Payment Terms, PO Number, Invoice Amount, Currency, and Service Dates/Period to proceed with processing. Quotations, Order Forms, Agreements, Contracts are not valid for invoice processing, only “Invoice” is allowed. Payroll Invoices - These are invoices provided by the Payroll Team for processing and will be manually input in Netsuite. Approval signatures are indicated on the invoice. Special Invoices - Mostly these are related to reimbursements (e.g. Speaker Reimbursement, Interview Candidate Reimbursement, Ex-Team Member Reimbursement and BOD Reimbursements). GitLab’s preferred method of payment for suppliers is electronic funds transfer with supplier invoice provided as supporting documentation. In the instances where this option is not available with the supplier, Virtual Card , which is a temporary credit card, can be used as an alternative payment method. Invoicing and Payment Corporate Credit Card Procurement",
  "This page contains GitLab’s accounting and reporting policies, which can be made public. Please find our internal processes in the Accounting and Reporting internal handbook section. Calculated billings is defined as revenue plus the sequential change in total deferred revenue as presented on the balance sheet. We do not believe that calculated billings provides a meaningful indicator of financial performance as billings can be impacted by timing volatility of renewals, co-terming upgrades and multi year prepayment of subscriptions. For order approval and invoicing process please view the Billing Ops page. To amend a customer’s account, choose one of the options below from the subscription page in Zuora. Zuora Subscription Data Management New Accounts vs New Subscriptions There are instances where a new account in Zuora is required rather than just a new subscription in an existing account. This is determined by the sold-to contact person. Within the customer account portal, customers can only see a single Zuora account at a time. If a customer wants to add a subscription and the contact information is the same, then the subscription can be added to the existing account. If a customer wants an additional subscription for a different sold to contact, then a new Zuora account will be created so that every sold to contact can log into the portal and manage their subscriptions. When a customer renews their subscription, a new subscription is typically created. This can create challenges for calculating metrics like dollar retention for a subscription because once subscription has ended and another has started. To address this, a linkage is made between the original subscription and its renewal(s). The field Renewal subscription is used to create the mapping. These are the following constraints on this field: The process to make the linkage is as follows: The active subscription status in Zuora needs to be reviewed in connection to the end date. If the end date is in the future it means that the subscription is still within the term and the customer is able to use the product. An ‘active’ subscription with an end date in the past means that the subscription was not renewed and the customer doesn’t have access to the product since the end date. We currently don’t actively cancel these subscriptions as this is a manual process and the cancellation or lack of it does not have any impact on other processes. Additionally, where subscriptions remain in an active status they can be renewed by the customers on the CustomersDot For step by step processes please view Billing Ops page. How to process a partial refund in Stripe Posting Swag Shop transactions in NetSuite Transactions from the Swag Shop are remitted to the Comerica checking account daily and should be booked in NetSuite at the end of each month. Accounting for customer collections Follow this procedure if the customer paid by credit card. You may recall from the invoicing process that there was still a balance due when saving the invoice. The following steps will record the payment and remove the balance due. Login to Stripe dashboard and click on Payments under Transactions (left hand side). You will see a listing of the latest Stripe transactions listed by amount, Recurly transaction, name, date, and time. There is also an option to filter the report by clicking on XXX at the top left. Click on XXX to export to excel. This will give you a workbook area and also a breakdown of the fees which we will work on later. In NetSuite, click on the “Transactions” tab on the left. Match invoice #s between the Stripe dashboard and NetSuite. If you click on a transaction in the Stripe dashboard, it will take you to a screen that shows more detail, including the invoice # being paid. You can work your way from the bottom up. In NetSuite, click “Receive Payment” on the matched payment and invoice. Post a journal entry to record Stripe Fees. This transaction transfers the payment obligation from the customer to Stripe. The payment obligation from Stripe is removed when Stripe transfers the funds to GitLab’s bank account. Posting a payment from Stripe when a transfer is received from Stripe Posting a payment from a “bank customer” For step by step cash collections process please view Billing Ops page. Account receivable provisions, bad debts and other period close adjustments Time for Invoices to be generated when a deal is closed won in Salesforce < 24 hours The time from when a deal is closed won in Salesforce to when the invoice is generated. Professional services are excluded from this performance indicator. This is tracked over a calendar month. The target is < 24 hours. Procure to pay is the process of requisitioning, purchasing, receiving, paying for and accounting for goods and services. It includes the following sub processes of both the Procurement and Accounts Payable departments: Coupa is a procure-to-pay system that streamlines the purchase request process, initiate workflow with approvals, and enable Purchase Orders. We will be rolling out in a phased approach, with the US and Netherlands entities (GitLab Inc, Federal, IT BV and BV) in Phase I (2021-06-01). The remaining entities will be part of Phase II (2021-12-13). You can learn more about Coupa on our FAQ Page How vendors are added into Coupa: Entering a Bill (invoice) in NetSuite Please note the below steps reference how to manually enter bills into NetSuite. Effective 2019-11-01 all AP invoices were processed through Tipalti. Effective 2021-06-01 (Coupa Phase I) and 2021-12-13 (Coupa Phase II), AP invoices will be processed in Coupa. These systems will automatically record the transaction into NetSuite after the invoice has been approved by the corresponding business partner in the respective system. Coupa is a Procurement and Invoicing Tool. Similarly to purchase requests for goods/services that must be initiated in Coupa, invoices are also created and approved in Coupa. Invoices in Coupa can be created via 4 different channels: A NetSuite error log identifying invoice integration issues will be emailed to the Accounts Payable Team every Tuesday and Friday. The email is sent from Finance Systems Admins and the Subject is Coupa2NS Invoices - Integration Log . Download the attached file and filter by Type = Error and Audit . Review the Details field to find the invoice number/document number that needs to be reviewed/corrected. For information regarding common errors and how to correct them, please review this file ; under the column labeled Script search for Coupa Invoice Integration . If any troubleshooting assistance is needed, please ask in the #coupa_help Slack channel. There is also a View in Coupa that will list invoices not exported to NetSuite. Under Invoices, select View = Not Exported However, there will be no details regarding the integration issue. You will need to review the NetSuite error log (referenced above) for the details. Invoices requiring PO receipts or approval will appear in user’s To Do list within Coupa. Users will also receive email notifications from Coupa (depending on user’s notification setting in Coupa). PO receipts notifications will enable users to “Create Receipt” by clicking on the button and entering the quantity received and receipt date. Please watch the End Users training video (starting at the 30:15 mark) for more information. Approval notifications will enable users to reject or approve invoices by clicking on the appropriate button. Verify invoice copy is attached Verify correct PO is matched Verify coding (Billing) is correct Verify service date is entered and correct Verify tag/class is entered (if applicable) Verify invoice total is correct If there are issues with any of the above items, please tag @Accounts Payable Approval Group in the Comments section with details. If there are issues with any of the above items, please tag @Accounts Payable Approval Group in the Comments section with details. The invoice dispute process in Coupa enables the Accounts Payable team to request corrections on invoices from suppliers. Invoices in “AP Hold”, “Pending Action”, “Pending Receipt”, “On Hold”, “Pending Approval” or “Rejected” statuses can be disputed to the supplier for corrections. Disputing an invoice requires a dispute reason and sends an email notification to the supplier contact on record and any additional listed recipients. Once disputed, an invoice can be Withdrawn from Dispute by Accounts Payable or Voided; or Resolved by the supplier. The invoice rejection process in Coupa allows the Accounts Payable team to make adjustments on invoices before restarting approvals, continuing approvals or disputing the invoice back to the supplier. This is an internal status that suppliers cannot see indicating that an approver or approval group rejected the invoice. Comments are required when rejecting an invoice - please provide as much detail/information as possible. Please note the below steps reference how to manually enter bills into NetSuite. Effective 2019-11-01 all AP invoices should be getting processed through Tipalti and Effective 2021-06-01 we will begin to process in Coupa as well. These 2 systems will automatically record the transaction into NetSuite after the invoice has been approved by the corresponding business partner in the respective system. If adding a new vendor, follow the bullets below before proceeding, otherwise skip to step 3 If you have an expense report that can be billed back to a customer please make sure to check the “billable” flag in Navan along with tagging the customer name under the “customer” field in Navan. Supplier Payment Accounts (SPAs) are required in order to pay suppliers from Coupa Pay. There are four different ways that suppliers will be able to provide their payment information: After the supplier submits their supplier payment account information, it will transfer into Coupa automatically and create a supplier payment account record. For more information regarding how to set up SPAs or Coupa Pay, please check out the lower section of our Coupa FAQ page . A NetSuite error log identifying payment integration issues will be emailed to the Accounts Payable Team every Tuesday and Friday. The email is sent from Finance Systems Admins and the Subject is Coupa2NS Pay Payments Integration Log . Download the attached file and filter by Type = Error and Audit . Review the Details field to find the payment number that needs to be reviewed/corrected. For information regarding common errors and how to correct them, please see this file ; under the column labeled Script search for Coupa Invoice Payment Integration . If any troubleshooting assistance is needed, please ask in the #coupa_help Slack channel. There is also a View in Coupa that will list payments not exported to NetSuite. Under Payments, select View = Not Exported Coupa is available via Okta. To access the platform: If your job function requires you to submit purchase requests in Coupa, follow the below steps: Due to the limited number of licenses available for Coupa, it is recommended that each department identify power users responsible for creating purchase requests on the team's behalf. Please refer to GitLab’s Expense Policy for further details. Entites included: GmbH, BV, PTY LTD, Ireland, IT BV and GK. Further details on the Expense reimbursement process can be found here Average days to action <= 3 business days Number of days from when an team member’s manager approves report to when AP analyst does final approval for payment or responds to team member in Navan if there is a concern. (Approval for payment is not the reimbursement date.) This is calculated on a calendar month basis. The target for this is currently three business days. Time to get a new team member set up in Navan < 3 business days Have new team member set up in Navan within 3 business days from team member start date. When reducing spend, we will not take the easy route of (temporarily) reducing discretionary spending. Discretionary spending includes expenses like travel, conferences, gifts, bonuses, merit pay increases and summits. By reducing in these areas we put ourselves at risk of increasing voluntary turnover among the people we need most . Discretionary spending is always subject to questioning, we are frugal and all spending needs to contribute to our goals. But, we should not make cuts in reaction to the need to reduce spend; that would create a mediocre company with mediocre team members. Instead, we should do the hard work of identifying positions and costs that are not contributing to our goals. Even if this causes a bit more disruption in the short term, it will help us ensure we stay a great place to work for the people who are here. In order to purchase goods and services on behalf of the company, you should first consult the Signature Authorization Matrix to determine the approval requirements. Note that this does not include travel expenses and other incidentals. These expenses should be self-funded then submitted for reimbursement within Navan, or in the case of independent contractors, included in invoices to the company (per the guidelines above). If further approval is not required, then proceed to the Procurement “What are you buying” page for further instructions on the purchasing process at GitLab. Once those procedures are complete, have your vendor send their invoice to Accounts Payable: ap@gitlab.com . Most importantly, the team member making the purchase request is ultimately responsible for final review and approval of the invoices. Final review and approval are critical process controls that help ensure we do not make erroneous payments to vendors. All original invoices and payment receipts must be sent to Accounts Payable. If you would like to track spend for a particular campaign, project and/or event you can do that through expense tag, also known as classes in NetSuite. If you would like to request an expense tag/class to be set up please open this tracker and enter the information required for the General Ledger (GL) team to create the tag. Navan will auto-sync any new “expense tags” on a daily basis, but if the Navan admin wants to manually sync they can do so by following these steps: Import new Classes/Tags In Navan: Please review this page for the current policy/procedures. Please see the campaign expense guidelines in the Marketing handbook . Tracking expenses for company Contributes enables us to analyze our spend and find opportunities to iterate, and in turn, improve subsequent Contributes. To enable tracking we create an expense tag that will allow GitLab team-members to tag Contribute related expenses in Navan. This should be done prior to the announcement of each Contribute. Property, plant and equipment is the long-term asset or noncurrent asset section of the balance sheet. Following are the sub-processes: This policy establishes the minimum cost (capitalization amount) used to determine the capital assets recorded in GitLab’s financial statements. A “Capital Asset” is a unit of property that has an economic useful life extending beyond 12 months and was acquired (or in some cases, produced) for a cost of $5,000 (USD) or more. Capital Assets must be capitalized and depreciated for financial reporting purposes. GitLab establishes $5,000 (USD) as the minimum amount required for capitalization. Any item with a cost below this amount is expensed on the date of purchase. Exceptions are Key Component Assets (i.e. computer laptops). Bulk purchases (“like” items acquired with a single purchase order, which are received within a reasonable period of time of one another (less than 60 days) and that individually have an Acquisition Cost less than the Individual Purchases Capitalization Threshold) have a Capitalization Threshold of $50,000 (USD) All capital assets are recorded at historical cost as of the acquisition date. These assets are depreciated on a straight-line basis, with the number of depreciation periods being determined by asset class. Invoices and purchase receipts for capital assets are retained for a minimum of five (5) years. Items paid for by the company are property of the company. Assets with purchasing value in excess of $5000 USD or Key Component Assets are recorded and tracked through NetSuite Fixed Asset Management (FAM) module, which includes details of individual asset purchased. The Asset Register report provided by NetSuite FAM provides each individual asset purchased with the following information: Once the information is captured in NetSuite FAM a depreciation schedule will populate and NetSuite FAM will post a journal entry each month to record the depreciation of the asset until it is fully depreciated. The asset will remain on GitLab’s balance sheet until the asset is no longer being used and is identified to be disposed. Assets will be disposed of if purchased by an employee upon termination (if approved by IT Ops) or if the item is no longer useful before the useful life. IT Ops will need to identify the asset and inform Accounting to properly dispose of the asset from NetSuite FAM. Record to report process is governed by the following accounting policies: Refer to Legal page for Related Party Transactions policy Our ability to accomplish our Finance and Accounting mission of providing timely, fact-based information to drive business results as a public company depends on the participation of team members during certain critical times of year. Our accounting function provides critical, time based deliverables that directly impact our ability to support business growth and meet our public company obligations. These activities require the entire team’s support. Team Members should prioritize taking time off to refresh and recharge outside of these critical windows. While these exact windows vary by team, generally we need all team members to support the last week of the quarter and first two weeks of the next quarter. Check with your manager if you are unsure what is applicable to your role. When these windows overlap a weekend (particularly the last day of the quarter) or public holiday/ Family and Friends day, we recommend team members work with their manager to reschedule the day off to a day outside of this critical end of quarter time period. If a team member is unavailable to work during this time period, they should provide their manager with at least one months’ notice whenever possible to allow for coverage to be arranged. When appropriate, we will create rotational coverage plans over holidays/weekends, while remaining in line with applicable jurisdictional requirements. The purpose of this policy is to establish the responsibility, authority and guidelines for the investment of operating surplus cash. Surplus cash is defined as those funds exceeding the operating requirements of the Company and not immediately required for working capital or near term financial obligations. This policy shall apply to the Company and all subsidiaries. This investment policy will be reviewed periodically to ensure that it remains consistent with the overall objectives of the Company and with current financial trends. Approved Brokerage Institutions The Company may use the following brokerage institutions: The basic objectives of the Company’s investment program are, in order of priority: United States Government Securities: Money Market Funds must be rated AAA or equivalent by at least one NRSROs. At time of purchase investment restrictions: Investment Products (Rating, Sector Concentration, Issuer Concentration) This policy describes the methodology used to monitor and account for GitLab’s prepaid expenses. A Prepaid Expense arises when a cash disbursement is made for goods and services prior to realizing the associated benefits of the underlying goods and services. These transactions are recorded as assets until the goods and services are realized, at which point an expense is recorded. Our minimum threshold for recording prepaid expenses is $5,000 USD Identification and Recording of Prepaid Expenses Once a purchase request makes it through the company approval workflow , Accounting will take the following steps to ensure prepaid expenses are recorded accurately: The amount involved is equal to or exceeds $5,000 . Prepaid expenses below $5,000 must be recorded as period expense immediately as incurred. The prepayment is for a time period greater than 12 months ; (period of time is excluded for the Deposit of an event) if an amount is equal or greater than $50,000 on a single item in one invoice, it can be capitalized if the prepayment time period spans across fiscal quarters . Any deposits made for events in Marketing, Corporate or other departments of less than $5,000 USD will be recognized as an expense immediately on the day the invoice is received regardless of whether the event has taken place or not. The $5,000 clip level normally applies per invoice or per item. However, situations may exist that would require exercising business judgement on a case by case basis (i.e. any clip level by total amount of purchase per vendor). Also, there are situations when each individual prepaid may not meet the clip level but as a whole, these prepayments are similar in nature and are purchased in a bulk and therefore, the total amount of all the prepaid should be combined and used to decide if the prepayment should be recorded. Any exceptions should be pre-approved by the Corporate Controller or PAO. Amortization is recorded straight line based on a mid-month amortization method as follows: If the first month of service begins on the 1st to the 15th of the month, a full month amortization will be recorded in the current month. If the first month of service begins on the 16th to the last day of the month, amortization will begin on the 1st day of the subsequent month. Mid-Month Amortization Method does not apply to prepaid expenses with a monthly amortization equal to or greater than 50,000 USD or if the amortization if spread only over 1 period. If monthly amortization is equal to or more than 50,000 USD, the first month amortization will be calculated based on actual number of days where services were rendered. Prepaid Not Paid: For any prepaid expenses not processed for payment, an adjustment for “prepaid not paid” is posted to the respective prepaid expense account and AP manual adjustment account (GL Account 2001). A prepaid expense is not treated as an asset if a liability remains in the AP sub-ledger. Prepaid not paid adjustments are performed on a quarterly basis at minimum. Any deposits paid which will be held for more than 12 months such as security deposits or deposits to retain consultants will be booked to Security & Other Deposits (GL Account 1620) Prepaid Bonuses with a Clawback will be recorded to Prepaid Bonus w/Clawback (GL Account 1152) and will be amortized in accordance with the bonus agreement terms, using the mid-month convention. Finally, the balance is reviewed one last time when the Senior Accounting Manager performs a review of the financials prior to closing the period. Team member travel expenses are expensed in the period incurred. Costs related to third party vendors such as hotels, facilities, excursions are recorded to prepaid expenses and recognized as expense at the time of the event. To provide clear guidance concerning the identification and recording of items included in GitLab’s accrued and other liability accounts. The purpose of monthly accrual processes is to allocate expenses to the proper accounting period and match expenses with related revenues. At the close of each month, accrual processes ensure that all expenses related to that month are correctly included in the company’s financial statements. Additionally, this policy establishes standards and guidelines for ensuring that GitLab accounts for monthly accruals in a manner that is compliant with management’s objectives and generally accepted accounting principles (GAAP). This policy applies to GitLab and all subsidiaries. We require that all expenses be recorded where expense exceeds $5K USD or above, in the period the expense was incurred. The accrual process should be completed on a monthly/quarterly basis to ensure liabilities are recorded accurately in their respective periods and/or quarters. In order to meet industry standards for Month-End close deadlines the Finance and Operations teams are responsible to provide on Working Day -1 (ex. Friday July 30th = WD -1, Monday August 2nd = WD 1) the calculations, information, details and backup needed to support the accruals. The following items should be accrued monthly as necessary (note: this list is not all-inclusive): Any other material obligation not mentioned above that is a liability of GitLab Obligations that accrue over time are recorded throughout the accounting period in a methodical and rational manner. Obligations that accrue when an event occurs should be recorded at the time of the event. Factors that are considered in determining the time of recording accrued liabilities include: The Finance team is responsible for having procedures in place to reconcile accounts monthly and for keeping documentation to support accrued liabilities. Payables and accrued liabilities are recorded at face value, plus or minus any applicable adjustments. In most cases, the payable amount can be determined from the vendor bill. If not, then the amount should be verified against any relevant documents before recording the liability. When actual values are not available, the recorded value should be based on best available estimates. Estimates should be based on current market price and experience/history. The Sr. Accounting Manager is responsible for performing an overall review of accrued liabilities, one to three business days after accounts payable closes each month, to help ensure that all expenses are captured accurately. Foreign currency translation describes the method used in converting a foreign entity’s functional currency (as determined and documented in GitLab.com>Finance>Issues>#630) to the reporting entity’s financial statement currency. Prior to translating the foreign entity’s financial statements into the reporting entity’s currency, the foreign entity’s financials must be prepared in accordance with generally accepted accounting principles (GAAP), specifically under Financial Accounting Standards Board (FASB) Statement No.52 . GitLab’s financial statement reporting currency is USD. The functional currency of our non-U.S. subsidiaries is the local currency. Changes in foreign currency translation are recorded in other comprehensive income (loss), which is reported in the consolidated statement of equity and ultimately carried over to the consolidated balance sheet, under equity. Exchange rates used in the currency translation process vary across the three primary financial statement components: Transaction Risk vs Translation Risk Currency transaction risk is due to company transactions denominated in foreign currencies. These transactions must be restated into the entity functional currency equivalents before they can be recorded. Gains(losses) are recognized when a payment is made or interim balance sheet dates. Currency translation risk occurs due to the company owning assets and liabilities denominated in a foreign currency. Cumulative Translation Adjustment A cumulative translation adjustment (CTA) is an entry to the comprehensive income section of a translated balance sheet that summarizes the gains(losses) resulting from exchange rate differences over time. Currency values shift constantly, affecting how a currency is valued against others. The CTA is a line item in the consolidated balance sheet that captures gains(losses) associated with international business activity and exposure to foreign markets. The changes in CTA are recorded in other comprehensive income (loss). CTA’s are required under GAAP since they help distinguish between actual operating gains(losses) and those that arise from the currency translation process. Additional information on our reporting standards surrounding CTA’s can be found in FASB Topic 830, “Foreign Currency Matters.” Recording CTA - Exchange rate gains and losses for individual transactions are captured automatically by our ERP system, NetSuite. However, a CTA entry must be made in order to properly distinguish currency translation gains(losses) from other general gains(losses) in the consolidated financial statements. This entry includes reconciliation of any inter-company activity that generates foreign exchange gains(losses). The CTA is made on a monthly basis as part of our financial statement reporting cycle. This policy establishes GitLab’s guidelines regarding the structure, responsibilities and requirements underlying the chart of accounts (COA). This policy establishes formal responsibilities and accountabilities for how GitLab handles requests for new, modified or closed data elements on the COA. The Controller is responsible for all aspects of financial accounting and reporting, and governs the COA. All requests for new or modified (including closure/deactivation) COA segments, hierarchies, and configuration attributes are subject to approval by the Finance team. All requests for new or modified accounts must be submitted to the Accounting Manager for review and approval through a request using the Finance issue tracker. There are other stakeholders associated with the COA that may influence certain business decisions or financial system configurations. The Controller and Accounting Manager will include selected stakeholders in the related procedures and processes when and if appropriate. Potential stakeholders include, but may not be limited to: The general ledger attributes subject to this policy will be defined by the Controller based upon factors including but not limited to: Once an amendment to the COA has been approved, the Accounting Manager will ensure the necessary changes are implemented by updating and then closing the issue. The COA is maintained in NetSuite. Changes to the COA can only be made by the Controller and/or Accounting Manager. This policy applies to GitLab Inc. (“GitLab” or the “Company”) and all of its subsidiaries. To establish guidelines for assessing, preparing and reviewing balance sheet account reconciliations on a consistent basis in accordance with corporate policies and US Generally Accepted Accounting Principles (“GAAP”). Account reconciliations are prepared and reviewed monthly or quarterly for each active balance sheet account at the natural account level based upon the risk rating assessed (see risk rating assessment below). Account reconciliations will be prepared consolidated in USD or by entity in the respective functional currency. Each month end close the Accounting Manager assigns each balance sheet account or groups of accounts to its respective preparer and reviewer using FloQast (Account Reconciliation Tool). The assignments are set once and will roll over into the next accounting period. The Accounting Manager will make changes to assignments as needed. The preparer and reviewers can not be the same person to ensure segregation of duties. The balances from NetSuite will be auto synced into FloQast each period end so the preparers can prepare their recons based on the NetSuite ending balance for their respective assigned accounts. The preparer(s) will ensure the following: The reviewer(s) will ensure the following: FloQast will auto sign-off the recon on our behalf if the following is met: If the balance changes after review, approval or auto sign-off the recon will be automatically unreconciled by FloQast and the preparer and reviewers will need to follow the above steps again. Once a year in the beginning of Q4, the Controller and/or CFO will review each active balance sheet account and rate it from High, Medium and Low. The risk level of each account is evaluated based both on the quantitative value (to determine materiality) and the qualitative factors listed below: High Risk Accounts will be reconciled by the preparer monthly (for the exception of tax and equity related accounts which will be reconciled quarterly) and will require 1st level review by an accounting manager or above and 2nd level review by the CFO or PAO. Medium Risk Accounts will be reconciled by the preparer monthly and will require 1st level review by an accounting manager or above. Low Risk Accounts will be reconciled by the preparer monthly or quarterly and will require 1st level review by an accounting manager or above. If there is no activity and/or the account balance is zero the reconciliation will be auto certified by BlackLine. Once each reconciliation is reviewed/approved the account reconciliation is locked in BlackLine and no further changes can be made for that period. Once the period is officially closed the Senior Accounting Manager will ensure all recons are in approved, reviewed or in a auto-certified status before moving into the next period.",
  "The IT Enterprise Applications oversees the business technology architecture and systems that power GitLab as a business entity. We ensure that the systems are fit-for-purpose, available, reliable, integrated and secure. We continue to build business aligned technology to run operations, layering integration and automations to support remote work, enhance team member and customer experience, and generate value. Be a trusted business partner providing world-class solutions and value. Provide a stable, secure, compliant and highly integrated solutions ecosystem to empower business with technology and services that enables productivity to fuel growth of GitLab. Company-wide Business Technology - Tech Stack Application EcoSystem We Support Services We Support: Finance Systems Service Desk Partnerships & Collaborations with: Join Our Team - Current Job Openings",
  "GitLab’s Global Travel & Expense Policy is meant to provide guidance to all Team Members to ensure that we act in the best interest of our Company at all times. The purpose of this policy is to: Managers and/or Budget Owners : Following expenses are categorized for reimbursement purposes with specific policy definition attached to it. The limits below are in USD, please use local currency equivalents when considering other currencies. All business related flights need to be booked through Navan, and will be paid by GitLab travel card. For each flight search, Navan determines the price in real time. Also to note that while it is sometimes possible to find a cheaper flight outside of Navan, we get overall better rates in the aggregate with the buying power of the whole company. Even if a flight is cheaper outside of Navan, please book in Navan. Also submit a request under the Expenses -> Travel Related Expenses page of HelpLab with screenshots of Navan’s rate and the other airlines rate so the Expense team can address with Navan. The airfare policy in Navan is set up as a “Dynamic Policy” which calculates a fair market price for every search based on travel dates and destinations. Any purchases that are over 20% of the Dynamic Policy will be flagged for further review by your manager. Also note that anything booked outside of Navan will flag as “Out of Policy”, and will need to be reviewed by your manager and the AP team. Expense Type / Navan Policy Category: Airfare Purchase the liability insurance that is excess of the standard inclusion of State minimum coverage in the rental agreement at the rental agency. GitLab’s insurance policy provides liability insurance for rental cars while conducting company business, but it may be excess over any underlying liability coverage through the driver or credit card company used to purchase the rental. Purchase the liability offered at the rental counter if there are Non-US Team Members renting autos in the US or Canada. While workers’ compensation would protect an injured US Team Member, other passengers may have the right to sue. To ensure that GitLab has protection when a Non-US Team Member invites another person into the car we recommend the purchase of this insurance when offered at the rental counter. B. Physical Damage - Collision Damage Waiver Do Not purchase the Collision Damage Waiver offered at the rental counter. GitLab purchases coverage for damage to rented vehicles. If travel to Mexico is required, purchase the liability insurance for Mexico offered at the rental counter. You should verify that the rental agreement clearly states that the vehicle may be driven into Mexico and liability coverage will apply. Countries other than the US and Canada: In the event of an accident resulting in damage to the rental car, the Non-US rental agency will charge the credit card used to make the reservation with an estimated amount of repair costs if insurance is not purchased. If this happens, GitLab does not purchase Foreign Corporate Hired Auto Physical Damage Coverage to reimburse for damages. Expense Type / Navan Policy Category: Rental cars Expense Type / Navan Policy Category: Lodging Expense Type / Navan Policy Category: Traveling: meals for myself Expense Type / Navan Policy Category: Traveling: Team Events and meals Expense Type / Navan Policy Category: Distance Driven Expense Type / Navan Policy Category: Train travel Expense Type / Navan Policy Category: Public transport, tolls & parking The company will cover all work-related travel expenses. This includes lodging and meals during the part of the trip that is work-related. Depending on the distance of your travel, it can also include one day before and one day after the work related business. For example, if you are attending a 3 day conference in a jet lag-inducing location, the company will cover your lodging and meals those 3 days as well as one day before and one day after. Expense Type / Navan Policy Category: Traveling: internet access Expense Type / Navan Policy Category: Other GL Code NON-COGS: 6080 - Other Travel- Visas, Permits, Covid Tests GL Code COGS: 5080 - Other Travel- Visas, Permits, Covid Tests COGS Expense Type / Navan Policy Category: Books, dues & subscriptions Expense Type / Navan Policy Category: Office Supplies Expense Type / Navan Policy Category: Co-working Space GL Code NON-COGS: 6076 - Coworking Space GL Code COGS: 5076 - Coworking Space COGS Expense Type / Navan Policy Category: G&D (Growth & Development) Expense Type / Navan Policy Category: Work from home Expense Type / Navan Policy Category: Gifts - internal Classification: EmployeeGiftsFlowers Expense Type / Navan Policy Category: Internet access $5,000 needs to go through the procurement process in Zip . Expense Type / Navan Policy Category: Marketing GL Code NON-COGS: Multiple- will be required to choose GL account GL Code COGS: Multiple- will be required to choose GL account Expense Type / Navan Policy Category: Entertaining clients Expense Type / Navan Policy Category: Cell phone Expense Type / Navan Policy Category: Shipping & postage Expense Type / Navan Policy Category: Team members to please use T&E accounts for their own individual expenses relating to any team building events (ie meals, travel, transportation) Classification: FY26 Team Building Budget Expense Type / Navan Policy Category: Other GL Code NON-COGS: 6065 - Training & Development GL Code COGS: 5065 - Training & Development COGS Expense Type / Navan Policy Category: Other GL Code NON-COGS: 6060 - Software Subscriptions GL Code COGS: 5060 - Software Subscriptions COGS If a Team Member is missing a receipt, a Missing Receipt Affidavit will need to be completed and attached to the expense when submitted. Missing Receipt Affidavit located here (please ensure to make a copy). When attaching the receipt affidavit, the Team Member should attach the document as a PDF. The procedure by which reimbursable expenses are processed varies and is dependent on contributor legal status (e.g. independent contractor, Team Member) and subsidiary assignment. Please see Navan Physical Card policy here . These virtual cards within Navan are only utilized by the AP team, and they are used solely for providing new Team Members with a $1,500 USD (or local currency equivalent) stipend for their home office set up. These cards were set up to ensure that Team Members do not spend outside of this amount, as well as ensuring the stipend is only available during their first year of employment. Within the controls in Navan, these cards are only accessible to the AP Admins team, and are not accessible to Team Members. Below is some more information to help you get set up with your Navan account. This policy applies to all GitLab team members registering, or maintaining a registration for, GitLab-related Domain Names, including GitLab-related Domain Names registered or used for sandbox or testing purposes. Domain names are key assets in GitLab’s intellectual property portfolio. Centralizing the registration and maintainance of domain names under the Infrastructure Shared Services group using GitLab’s approved domain name registrar helps us track and protect these valuable assets. Please refer to the Domain Names and DNS Records Handbook Page . “GitLab-related Domain Names” when used in this policy means any domain name: This relates To Laptop Advances only - If more information is required please email the AP team on AP@gitlab.com If a team member whom you manage has left GitLab and comes to you with final expenses that are valid for reimbursement, please verify that they were not already processed in Navan and then contact Accounts Payable. You must submit valid receipts and a completed copy of the Expense Reimbursement template along with your approval. Please note that we may also ask the terminated team member to provide valid banking details in order to process the payment to them. AP will do their best to process and pay the reimbursement to the individual within 1 week.",
  "Maximize GitLab’s long-term valuation by enabling e-group and FP&A to plan, prioritize, and execute effectively FP&A comprises five different sub-teams to support our FP&A goals: Key planning milestones are listed below. For a more detailed timeline and planning checklist please engage your FP&A business partner. The LRO is refreshed on an annual basis, occurring shortly after the fiscal year plan is finalized. After the LRO is refreshed, there may be additional updates throughout the remainder of the year, on a quarterly and ad-hoc basis to determine whether near-term priorities and funding are needed in order to achieve long-term goals and financial targets. As part of the quarterly and ad-hoc updates to LRO, the following are included: The following are inputs in the LRO refresh and subsequent LRO updates: The Corporate Finance team leads the LRO refresh and updates in collaboration with: e-group members to determine key investments, capabilities, and dependencies; GTM Finance team for sales productivity/capacity models, CTB, and bookings attainment; G&A Finance team for total rewards strategy, benefit assumptions; and the R&D Finance team to help inform on allocations, hosting/infrastructure expenses. For GitLab’s top growth drivers and new products, drive and document alignment between contributors to success. Writing a business plan drives clarity of thought and operations. Agree on governance models to manage initiative success and cross-functional accountability. At a minimum, each business plan should cover: GitLab’s FP&A team participates in a rigorous monthly close process. These dates are based on an 8-day accounting close. Corporate FP&A will confirm the close date with the accounting team and update the FP&A Close calendar in Google accordingly. For FY23, the target is to close by WD 5 with full consolidation (including tax entries, eliminations) by WD 10. The Accounting close calendar can be found here . Each month after the financials have been published, GitLab reviews all aspects of the business including Corporate Metrics, Bookings, Revenue, Gross Margins, Expenses. The goal of this meeting is to do a comprehensive review so that finance leadership has a pulse on the business and signs off on the financials. Based on insights from variance analysis, the FP&A team makes actionable recommendations to the CFO and e-group to ensure continued performance to Plan/Forecast. The variance analysis will compare department budgets with actual results and examine any material differences between budgeted and actual costs. Additionally, the actuals for expenses will be compared to the quarterly rolling forecast. The expenses are reviewed at the divisional department level, allowing GitLab to measure progress in meeting its Plan or rolling forecast. The team also evaluates the accuracy of forecasts and will make adjustments to the next rolling forecast. The study of differences between actuals and the Plan or Forecast. During the variance analysis processes the GitLab FP&A team analyzes and isolates any variance in question to the lowest level possible. The team reviews detailed items in order to identify the root cause of the variance. This could include transaction date, cost center, vendor, location, department or additional low level details. The FP&A team takes the following into consideration while evaluating variances in relation to materiality thresholds: The FP&A team delivers an FP&A expense flux review document at each monthly close, documenting and quantifying business drivers for variance. The goal is two-fold: We measure our team performance based on our forecast accuracy, also known as variance percentage. Variance percentage is defined as the difference between actuals and Plan or rolling forecast. We calculate it as follows: Variance analysis should address any inputs or additional requests from the last Variance meeting , as applicable. Generally accepted accounting principles (GAAP) does not provide definitive guidance in distinguishing material information from immaterial information. Therefore, GitLab uses a percentage based approach for defining materiality thresholds and can be found below. The Plan vs Actuals vs Forecast Sisense dashboard provides the data for the threshold analysis via a color coded legend. Our goal is to have revenue and EBIT variance percentage within +/- 2% on a quarterly basis. Key accounts and expenses by division should be within +/- 2% versus Plan or rolling quarterly forecast every quarter. EVPs are held accountable to meet the budget in dollars that they are given. The budgets typically are broken into headcount and program spend. Each finance business partner will run a meeting with their Finance leader and the EVP to review the past month. The information should be presented as timely as possible. Given the accounting close is 8 days, the team is asked to use pre-close numbers for the review to increase the speed of information. During the meeting, the Finance Business Partners will review GitLab results in addition to a detailed overview. Each division can expect to review the following during the monthly meetings: Following the month-end close, the Finance Business Partners will create a variance deck and distribute department income statements to the related budget owners and the e-group members. Each department is then responsible for comparing these reports, which contain actual costs, to the budget. Departments, with guidance from the Finance Business Partners, should analyze their data and if necessary, discuss items of interest and take appropriate action. Any questions regarding the cost data should be discussed with the Finance Business Partner. The close timeline for each quarter follows the timeline above for monthly close and includes additional key dates and processes: As a public company we share financial results publicly after the close of each fiscal quarter or fiscal year. The purpose, timeline, and deliverables can be found on our Investor Relations page here . Financial Planning & Analysis (FP&A) team is the owner of SSOT for GitLab’s Hiring Plan which contributes to our Headcount Forecast. Headcount Forecast is a live forecast in Workday Adaptive (a financial planning software) Hiring Plan is maintained by the Finance Business Partners (FBPs) and shared with Talent Acquisition (TA) team and Business Partners (BPs) Finance owns the Hiring Plan to ensure there is only one Headcount Forecast and Hiring Plan for the business Hiring Plan increases our predictability as a company and streamlines the hiring process Based on historical time to start data, new requisitions released via RLOA in a quarter can be expected to start in the following quarter at the earliest For questions specific to TA’s process and REQ creation, please refer to TA’s section of the handbook Position ID is a unique identifier and is the link between approved headcount and the Hiring Plan FP&A and TA discussed and agreed upon the above process. This SSOT process ensures consistency in Hiring Plan execution Planned Positions include all Net New positions to existing GitLab active employees. Backfill Positions include all positions created due to departure or termination of an existing employee. Both PID and GHPID are created and maintained by FP&A team as unique identifiers in Hiring Plan All PIDs are created by the Corporate FP&A team and are tied to Headcount Forecast (including Hiring Plan) FP&A Position IDs increase visibility and accuracy for headcount forecasting (count and dollars) and simplify & provide transparency on modeled headcount (attrition and backfill). All GHPIDs are system-generated by a FP&A internal Google Sheet FP&A team is the owner and maintainer of SSOT for GitLab’s Hiring Plan The FP&A team and Talent Acquisition Managers collaborate to ensure understanding and implementation of the most up-to-date view of forecasted headcount-related expenses. This interlock enables GitLab to respond quickly and make live decisions through a weekly P&L forecast and the rolling list of asks (RLOA) process . This process also ensures alignment and accuracy of headcount forecasts when FP&A locks its annual plan and monthly rolling forecasts. This process also tracks company metrics against Wall Street expectations related to non-GAAP operating income and non-GAAP earnings per share. Please see the Headcount Metrics and Processes page for definitions and key metrics. The Hiring Plan data is divided into four headcount forecast templates, one for each cost center with Cost of Goods Sold allocated among Sales and R&D. The data and analysis from the four templates roll into the Summary file. The four templates can be found in the Headcount Live Summary Dashboard . Unless otherwise noted in the HC Forecast calendar or communicated via Slack, the interlock process occurs weekly as follows: By noon (PST) Monday , the Corp FP&A team downloads the data from Adaptive and refreshes the headcount forecast templates. Talent Acquisition also refreshes the headcount forecast templates with the latest data from Greenhouse. By end of day (PST) Wednesday , the Talent Acquisition Managers and Recruiters review their respective roles line by line and make adjustments and comments related to expected start dates and other role details provided by the Finance Business Partners. If no indication is made for a specific role in the headcount forecast template, Talent Acquisition signals that the current indication is reasonable and indicative of a 50/50 “most-realistic” forecast. The Talent Acquisition Managers also pay close attention to any recruiting capacity restraints within any given quarter. In order to maximize transparency and understanding of any changes, the Talent Acquisition Managers and Recruiters provide comments (e.g., wrong GHP ID, rejection of an offer, delay due to business decision, etc.) for specific roles. By end of day (PST) Thursday , the Finance Business Partners update Adaptive with their best estimates of the start dates of all planned personnel for the forecast period based on the input from Talent Acquisition in the headcount forecast templates. This reflects a 50/50 “most realistic” view of headcount expenses/timing. By end of day (PST) Friday , the Corp FP&A team sends a P&L snapshot to the CFO incorporating any headcount forecast adjustments. Note: P&L snapshots are not provided during accounting close. Communication related to the interlock takes place in the #fpa-ta_headcount_forecast Slack channel. All parties involved in the interlock are also granted access to the “HC Forecast” Google Calendar. The Finance Business Partners and Talent Acquisition Managers collaborate to provide the most current and 50/50 “most-realistic” view of headcount-related expenses. This is done through the interlock process in the headcount forecast template and is ultimately uploaded into the Planned Personnel sheet in Adaptive. The inputs loaded into Adaptive are then used to provide the CFO a weekly P&L snapshot in order to facilitate in-quarter spending decisions and ensure the company tracks vs. guidance and consensus expectations. The primary mechanism to ensure efficient spend of company assets is the Procure to Pay process, and specifically completion of the vendor and contract approval workflow prior to authorization. The procurement team or your finance business partner can assist with questions related to this process. The second mechanism is the budget vs actual review to determine reasons for variances vs plan. See the section on Variance Meeting with CFO and Variance Analysis . A manual on how to update and maintain Adaptive integration can be found here . This document is maintained by the Corporate FP&A team. Throughout quarter, FP&A maintains alignment with business needs. During quarterly RLOA process, FP&A team finalizes RLOA proposal to share with e-group, and communicates decisions to functional leaders to enable business execution. Below is the timeline Note: WD is defined as Working Days, which are Mondays through Fridays, excluding Federal holidays.",
  "Please refer to the Payroll internal handbook page https://internal.gitlab.com/handbook/finance/payroll/ .",
  "The Procurement team manages GitLab’s Supplier Lifecycle through the process of strategically vetting and selecting vendors, negotiating commercial terms, the purchasing of goods and services, and managing the renewal or vendor offboarding process. Division alignment for spend over >$25k USD / year: Procurement is a cross-functional team that supports GitLab as a public company. We have four key objectives monitored in the following ways: The Procurement team is responsible for ensuring there is a process for suppliers to be managed throughout their lifecycle of doing business with GitLab from initial selection and contracting to recurring reviews and renewals to cancellation. All new spend, changing of vendors for existing services, and market reviews every 3 years for existing contracts should follow the RFP and Vendor Selection process , unless approved by your Procurement Category Manager, to ensure we are choosing the best partner for GitLab with the best commercial terms. This must be done prior to verbally agreeing to terms or contracting with any new or existing vendor. Before sharing details and/or confidential information regarding GitLab business needs, obtain a Mutual Non-Disclosure Agreement from the potential vendor(s). Refer to the Signature Authorization Matrix for signing authority. All vendors must adhere to the GitLab Partner Code of Ethics . It is mandatory all vendors contractually adhere to this if they would like to do business with us. (Note these are typically not required in event related agreements unless the vendor is providing services). Depending on the amount of spend, Procurement will assist or lead in negotiating the pricing and commercial terms of the contract. Depending on the types of data shared with the vendor, Privacy and Security will need to complete a review of the vendor. Depending on nature of vendor’s services and/or whether the vendor has been recommended to us, the Ethics and Compliance team will need to complete an anticorruption/antibribery review and, potentially, recommend additional measures to mitigate that risk. For more information on these requirements and steps, see the Review Steps, Timeline, and Considerations section All work that is done with a vendor must have a completed contract to be compliant and work may not be started until a contract is in place. Contracts include NDAs , Master Service Agreements and Statements of Works. Our legal team assists with this step in the process. Please see the legal review process for more details. Additionally, please note that a small number of team members can sign agreements on behalf of GitLab - please see the Authorization Matrix for more details. In order for vendors to be paid, they need to complete their onboarding in our systems. Please see the New Vendor Onboarding section for more details. Procurement will work with you through the Quarterly Procurement & Business Spotlights to review upcoming renewals and cancellations, new spend projects, and identifying vendor’s segmentation tier: Strategic, Niche, Commodity, or Transactional. Identifying the Segmentation Tier your supplier(s) falls within helps determine the right vendor management approach, e.g. renew the contract, RFP, business reviews on a recurring cadence, continuous improvement plans, etc. If holding a business review with a vendor, the following topics should be discussed: On a quarterly basis, the Procurement team will meet with business stakeholders in each department to review a rolling 12 month list of renewals, focusing on the upcoming 2 quarters. This list is pulled from Zip and Coupa. The list should be reviewed and prioritized with the business owners. The renewal process should start at least 90 days ahead of the renewal date providing ample time to review the terms and decide: The following methods will be used to capture cancellations (which include terminations and/or non-renewals): For each of the above, if a cancellation is desired (which include terminations and/or non-renewals) please follow the “Cancellation Process” outlined below: Upcoming terminations/non-renewals should be discussed with Procurement during the Quarterly Category Spotlight meetings that Procurement holds with category leads/budget owners. As Procurement is made aware of upcoming cancellations during these meetings, Procurement will review on a quarterly basis any known cancellations with Legal to ensure notification requirements are met. Procurement and Legal will then work with the business owner on completing the above process. If a cancellation is determined outside of this quarterly cadence, notify your Procurement Category Manager and complete the above process as soon as possible. Anytime a group of suppliers are being evaluated for services/goods or a purchase is being made on behalf of GitLab that does not qualify as a personal expense or meet the list of exceptions , Procurement must be engaged BEFORE a purchase and/or work can begin. Start the Procurement Process Majority of the Procurement Process lives within our Procurement system called Zip. You can access Zip via your Okta home page , or if you need Zip access, submit an access request here . For more Zip training materials, review the Zip End Users Guide and the Tips for Submitting a Zip Request page . Depending on a few factors of your purchase, there are different ways to engage Procurement and start the Procurement process: If you’re hiring a contingent worker, please read GitLab’s Contingent Worker Policy , which provides comprehensive guidelines on engaging with different types of contingent workers. The policy outlines three main categories: Staff Augmentation Workers (agency-provided temporary resources), Consultancy Services (third-party professional services), and Independent Contractors (used by exception only). You’ll find detailed information about each worker type’s characteristics, duration limits, country hiring guidelines, contractor extension processes, and background screening requirements. This policy is designed to help team members understand when and how to properly engage contingent workers while mitigating classification risks. Note: For subcontractors for the Professional Services team, please use the process for Revenue Partner Payments. Requests can take 5 days to 3+ weeks for processing depending on a number of factors that will impact the time for review of a Purchase Request including, but not limited to: Each review’s targeted approval time is outlined below, but as mentioned above, this is dependent on many factors and the accuracy and completeness of information provided by the requester. If your request meets any of the additional approval criterias outlined below, please plan accordingly and submit your Zip request allowing each cross-functional team enough time to complete their review. Please follow the steps outlined for urgent requests that cannot meet the below timeline and have specific and quantifiable impact to the business. The amount of time for review and reaching execution is based on the details below. Use these SLAs as guidelines, noting that each contract review is unique. If additional terms, requirements, and/or risks are identified, the timeline may be extended. The ability for GitLab to work efficiently through an agreement negotiation relies on the vendor and their counsel responding promptly to GitLab redlines and comments. Delays from the supplier will delay approval. Types of Vendors and Review Times Existing vendors for renewals or upsells: 3-5 days Negotiation, Security, Privacy, and PeopleOps reviews are only required if the purchase request meets certain criteria, as described below. In the event two or more of these activities are required, they will happen in parallel to one another and Legal’s review. If you are unable to plan and have a legitimate reason to escalate a purchase request, follow the process below. Exceptions to the PO Policy are: The procurement team from a compliance and risk perspective has developed a process to handle third party risk to reduce the risk of the following: When any of the following is met: Any time GitLab engages with a third party for the procurement of goods and/or services, which require GitLab to engage in a contract, the GitLab Legal Procurement team will review the terms and conditions. The purpose of this team is to review the contract which GitLab will enter into, and ensure the following: In addition to ensuring terms and conditions, the GitLab Legal Procurement team collaborates frequently with procurement and business stakeholders to ensure any (and all) contracts align with the needs of the team. The GitLab Legal Procurement team addresses the needs of stakeholders ranging from complex technical application and platform services, to creating and drafting event contracts to meet the needs of GitLab events. As GitLab does with its own customers, agreements with third parties include obligations that vendors have to GitLab. These can include, but are not limited to: For any large internal events with a total cost greater than $1M, such as SKO, President’s Club, Commit, etc, the following should be completed before any contracts are executed or any work is conducted. The planning stages for events of this size should be completed at least 18-24 months prior to the actual event. This allows for ample time to get the necessary internal approvals, run any RFPs needed, and book large hotel blocks or buyouts.",
  "The Tax Team is responsible for GitLab’s tax compliance (see here for payroll taxes), tax planning and accounting for business taxes, including income taxes and indirect taxes such as sales taxes and value-added taxes (VAT). Taxation differs between countries, and taxation in one country is not always complementary to taxation in another country, resulting in possible double-taxation on the same GitLab revenue. It is the fundamental goal of the Tax Team to minimize double taxation, to optimize our global effective tax rate, and to timely apply resources to various company needs. Those needs range from routine compliance and tax audits to strategically positioning the company for its next phase of growth. The Tax team is part of the Finance function and headed by the VP Tax. The VP Tax reports to the Chief Financial Officer. The Tax team is structured to address (a) U.S. domestic taxes and global accounting for business taxes and (b) international tax planning and compliance, including US international tax planning. This page has been moved to the internal handbook website due to safety/privacy: https://internal.gitlab.com/handbook/finance/tax/ An Indirect Tax Identification Number (“Indirect Tax ID” or “Tax ID”) is a unique identification number assigned by tax authorities to businesses registered for Value Added Tax (VAT), Goods and Services Tax (GST), or similar indirect taxes. This number identifies your business as a registered taxpayer eligible to charge and collect indirect taxes. GitLab is required to comply with local tax regulations in various jurisdictions. When you provide a valid Indirect Tax ID: Without a valid Indirect Tax ID, GitLab may be required to charge applicable VAT/GST rates based on your location. If your business is not registered for indirect taxes (due to size thresholds or other reasons), you may not have an Indirect Tax ID. In such cases, GitLab will charge the applicable VAT/GST rate according to local regulations. Important Disclaimer: The formats listed below are samples of commonly used indirect tax ID formats and are provided solely as a reference to assist customers. GitLab does not guarantee the accuracy or completeness of this information and assumes no responsibility to update or revise this information. If you’re unsure about your Indirect Tax ID format or have questions about tax registration in your country, we recommend consulting with: For GitLab-specific questions about updating your tax information, contact us at tax@gitlab.com .",
  "This page is intended to capture GitLab Board practices, but we evolve these over time and the page may not capture all changes. The source of truth for Board details is GitLab’s Investor Relations site . The Staff EBA to the CFO is the DRI of scheduling the quarterly Audit Committee meeting in conjunction with the Chairperson, Management DRI and their Sr. EBA All Audit Committee materials being reviewed during the meeting will be uploaded into Boardvantage 5 business days prior to the Committee meeting by the Corporate Paralegal. Committee members will be notified of such by the Management DRI. Audit Committee meetings are attended by: Compensation and Leadership Development Committee Charter The Legal and People Group EBAs are the DRIs of scheduling the quarterly Compensation and Leadership Development Committee meeting in conjunction with the Chairperson, Management DRI and their Sr. EBA All Compensation and Leadership Development Committee materials being reviewed during the meeting will be uploaded into Boardvantage 5 business days prior to the Committee meeting by the Corporate Paralegal. Committee members will be notified of such by the Management DRI. Compensation and Leadership Development Committee meetings are attended by: Nominating and Corporate Governance Committee Charter The Staff EBA to the CEO is the DRI of scheduling the quarterly Nominating & Corporate Governance Committee meeting in conjunction with the Chairperson, Management DRI and their Sr. EBA All Nominating & Corporate Governance Committee materials being reviewed during the meeting will be uploaded into Boardvantage 5 business days prior to the Committee meeting by the Corporate Paralegal. Committee members will be notified of such by the Management DRI. Nominating and Corporate Governance Committee meetings are attended by: Mergers and Acquisitions Committee Charter The Staff EBA to the CEO is the DRI of scheduling the Mergers and Acquisitions Committee meeting in conjunction with the Chairperson, Management DRI and their Sr. EBA All Mergers and Acquisitions Committee materials being reviewed during the meeting will be uploaded into Boardvantage 5 business days prior to the Committee meeting by the Corporate Paralegal. Committee members will be notified of such by the Management DRI. Mergers and Acquisitions Committee meetings are attended by: Board meetings can happen remotely or in-person. We establish the meeting calendar in advance of the coming fiscal year. During this time, we agree on meeting locations. The Staff EBA to the CEO shall ensure that there are separate calendar invites for all attendees within each session, all including exact session start and end times, the appropriate Zoom link, and links to the agenda document. Any supplemental materials are required to be linked in the agenda document for each session. This is the outline of a past agenda, but we adapt agenda based on what is important to discuss: This section is updated after every Board Meeting by the Staff EBA to the CFO for the next Board Meeting. GitLab uses Nasdaq’s Boardvantage (NBV) portal for the electronic dissemination and storage of materials relating to GitLab’s Board Meetings. For a more detailed overview of navigating the NBV portal, please click here to view the Internal Reference Guide . We usually have a Board Dinner on the evening before a Board Meeting. The EBA to the CEO is responsible for coordinating this dinner. Board Members, Local E-Group members, and the CoS to the CEO are all optional attendees. Bios for attendees of the Board Dinner should be sent by the EBA to the CEO 3 business days in advance of the Board Dinner. Each quarter, E-Group can choose to invite an additional team member as a Key Talent Guest. If this is desired in a specific quarter, at least a month before the dinner, the CoS to the CEO will gather nominations and ensure that E-Group has identified an invitee. Criteria for consideration includes: Once the Key Talent Guest has been identified, it is the relevant E-Group team member’s responsibility to ensure that managers of this person are aware of the invite. The EBA to the CEO will then confirm that the guest is able to join. If not, the invite will go to the runner up. Once a Key Talent Guest is confirmed, the EBA to the CEO should enter their name in the Key Talent Board Dinner Guests . GitLab will cover this person’s travel expenses. A person’s location will not be considered in the selection criteria though the person’s travel must be allowed under GitLab’s travel policies and guidelines . As part of joining the dinner, the Key Talent Guest should come prepared to discuss: Though no formal presentation is required, the Key Talent Guest should advise on how they want to discuss these topics with the Board and E-Group and coordinate with the EBA to the CEO. The EBA to the CEO will ensure that the Key Talent Guest has dedicated time at the dinner. Past and future Key Talent Dinner Guests are listed below. Board member onboarding from an internal processes checklist perspective: To be completed prior to onboarding: Board member onboarding from a GitLab orientation perspective: New Director will be provided with: Links to relevant Handbook pages, including: Access to the Legal Board Drive Summary of Director and Officer Liability Insurance Executed Indemnification Agreement Update Board distribution list with Board Member and support staff Send calendar invites to Board member and support staff for: Update Board Member contact information list and circulate to Board and support staff Schedule AMA with New Board member Schedule Functional Reviews within the first month: Corporate Strategy (CEO) - 50 mins Finance (CFO) - 80 minutes suggested Legal, Compliance, Regulatory, Governance (CLO) - 50 minutes suggested People Group (CPO) - 50 minutes suggested Sales (CRO) - 80 minutes suggested Marketing (CMO) - 50 minutes suggested Product Overview (EVP, Product, EVP, Engineering, CEO) - 80 minutes suggested Investor Relations (Sr. Director of IR) - Optional 50 minutes suggested",
  "Please refer to the Internal Audit page in the internal handbook.",
  "Principles - Processes - Categories - GitLab the Product - Being a PM - Leadership We believe that a single application for the DevOps lifecycle based on convention over configuration offers a superior user experience. The advantage can be quoted from the Wikipedia page for convention over configuration: “decrease the number of decisions that developers need to make, gaining simplicity, and not necessarily losing flexibility”. In GitLab you only have to specify unconventional aspects of your workflow. The happy path is frictionless from planning to monitoring . We’re doubling down on our product for concurrent DevOps which brings the entire lifecycle into one application and lets everyone contribute. We are leaning into what our customers have told us they love: our single application strategy, our pace of iteration, and our deep focus on users. Consider opportunities to take advantage of this unique attribute in early iterations. Integrating features with different parts of the application can increase the adoption of early iterations. Other advantages: Although not every feature needs to be integrated with other parts of the application, you should consider if there are unique or powerful benefits for integrating the feature more deeply in the second or third iteration. GitLab.com runs GitLab Enterprise Edition. To keep our code easy to maintain and to make sure everyone reaps the benefits of all our efforts, we will not separate GitLab.com codebase from the Enterprise Edition codebase. To avoid complexity, GitLab.com tiers and GitLab self-managed tiers are named the same. GitLab.com subscriptions work on a namespace basis, which can mean: This means that group-level features are only available on the group namespace. Public projects get Ultimate for free. Public groups do not get Ultimate for free. Because: Admittedly, this is complex and can be confusing for product managers when implementing features. Ideas to simplify this are welcome (but note that making personal namespaces equal to groups is not one of them, as that introduces other issues). For more guidance on feature tiers and pricing, visit tiering guidance for features See the terminology of deprecations . By definition, a removal is a breaking change. With a few exceptions , if the answer is yes to any of the following questions, the change is to be considered a breaking change and should be avoided other than for critical business risk. For special definitions of what constitutes a breaking change for our APIs, see: For Experiment or Beta features, please see Support for experiment, beta, and generally available features . Introducing a breaking change in a minor release is against policy because it can disrupt our customers, however there are some rare exceptions: In all cases, the PM or EM must follow the Request a Breaking Change process . Deprecating and removing a feature needs to follow a specific process because it is important that we minimize disruption for our users. As you move through the process, use the language deprecated or removed to specify the current state of the feature that is going to be or has been removed. Please follow the process outlined in the docs . We deploy changes to GitLab.com many times a day. Because they are part of a continuous delivery process, these changes, including breaking changes, are not as predictable for customers. Starting from GitLab 17.0, we introduced fixed windows during which breaking changes are rolled out to GitLab.com. The fixed windows are set as the Monday, Tuesday and Wednesday of the three weeks preceding the major release date, typically following the X.11 release date. A detailed example of what this looks like can be found in our 17.0 introduction issue . Breaking changes should only be enabled during the breaking change windows. This means that where breaking changes are behind feature flags, the changes (feature flag) should only be switched during one of these windows to ensure customers workflows are not impacted outside of these communicated periods. Product Managers will be responsible for making sure that as part of the deprecations and removals process, where applicable the deprecation/removal is aligned with a publicly communicated Breaking Change Window. In June of 2023, we changed the process so that all deprecations and removals are displayed on the Deprecations page . The announcements are grouped by the milestone they will be removed in. The deprecation announcement date is listed below each individual item. Our CI syntax keeps evolving. We cannot support all keywords indefinitely, so deprecating and removing keywords is inevitable. GitLab does not have a versioning system for CI/CD configuration. Therefore, it is critical to over-communicate our deprecation purposes to our users and take the necessary precautions to reduce the impact on their projects. Deprecating a keyword is risky because it will break all pipelines using it, and in some cases, users are not aware of the keyword they use in their pipeline. The deprecation process described below is similar to the deprecating and removing features process, with additional steps to reduce the risks which involved with removing a CI/CD keyword. Deprecation notice - Syntax removal introduces a breaking change, as outlined in our deprecation process, we must notify the community and customers, which means including a deprecation notice in the monthly release post. Track keyword usage - Tracking keyword usage should begin as early as possible. It is a mandatory step that helps estimate the user impact, timing, and needed effort. The more users use the keyword, the more time it takes to remove it (It took more than four years to move from remove to deprecation for ’type’ keyword). In-app warning - Provide our users with an in-app notification that we plan to remove a keyword they use in their pipeline. Our customers will get a notification in each run of the pipeline that uses the deprecated keyword. The warning will be printed: This step is optional if the keyword usage is relatively low (Recommend minimal reach of ~5% impacted users). Keyword removal - The keyword will be removed from our code and schema and should happen in a major version. Once removed, using the keyword will result in a lint error. Naming new features or renaming existing features is notoriously hard and sensitive to many opinions. The bar for renaming existing features is extremely high, especially for long-time features with a lot of usage. Some valid but not exclusive reasons are: When renaming features follow the process to rename a category starting with an MR to change the name in data/features.yml . Request review by the Product Director and Engineering Director of your Section for approval. Optionally, add a comment including your technical writer, engineering manager, product design manager, and product designer for transparency. When renaming a feature other items to consider are updates to documentation, blogs, direction pages and competitive information. What’s New is a feature that is part of GitLab.com and Self-managed GitLab that is used to communicate highlights from each release. After each major release, a yaml file is published that contains 3-10 highlights from the release along with links to the relevant documentation to get started using them. A small notification dot appears above the “?” icon, and when users click on “What’s new” in the menu, a drawer containing the updates slides into view. The goal of What’s New is to make it easy for users to be aware of the most important changes in GitLab so we can help them stay up-to-date on all of our changes and feature updates. Use this section as guidance for using existing features and developing new ones. To keep the permissions system clear and consistent we should improve our roles to match common flows instead of introducing more and more permission configurations on each resource, if at all possible. For big instances with many users, having one role for creating projects, doing code review and managing teams may be insufficient. So, in the long term, we want our permission system to explicitly cover the next roles: All the above can be achieved by iteratively improving existing roles. You can now find our security paradigm on the Secure Strategy page. Also see our Secure Team engineering handbook . Traditionally, applications only reveal valuable information about usage and performance to administrators. However, most GitLab instances only have a handful of admins and they might not sign in very often. This means interesting data is rarely seen, even though it can help to motivate teams to learn from other teams, identify issues or simply make people in the organisation aware of adoption. To this end, performance data and usage statistics should be available to all users by default. It’s equally important that this can be optionally restricted to admins-only, as laws in some countries require this, such as Germany. GitLab is developed in English, but supports the contribution of other languages. GitLab will always default to English. We will not infer the language / location / nationality of the user and change the language based on that. You can’t safely infer user preferences from their system settings either. Technical users are used to this, usually writing software in English, even though their language in the office is different. Fast applications are better applications. Everything from the core user experience, to building integrations and using the API is better if every query is quick, and every page loads fast. When you’re building new features, performance has to be top of mind. We must strive to make every single page fast. That means it’s not acceptable for new pages to add to performance debt. When they ship, they should be fast. You must account for all cases, from someone with a single object, to thousands of objects. Read the handbook page relating to performance of GitLab.com , and note the Speed Index target shown there (read it thoroughly if you need a detailed overview of performance). Then: You must prioritize improvements according to their impact (per the availability & performance priority labels ). Pages that are visited often should be prioritized over pages that rarely have any visitors. However, if page load time approaches 4 seconds or more, they are considered no longer usable and should be fixed at the earliest opportunity. In addition, to meet the ethical criteria of GNU , all our JavaScript code on GitLab.com has to be free as in freedom. Read more about this on GNU’s website . The ability to monitor, visualize and improve upon cycle time (or: time to value) is fundamental to GitLab’s product. A shorter cycle time will allow you to: When we’re adding new capabilities to GitLab, we tend to focus on things that will reduce the cycle time for our customers. This is why we choose convention over configuration and why we focus on automating the entire software development lifecycle. All friction of setting up a new project and building the pipeline of tools you need to ship any kind of software should disappear when using GitLab. We understand that not everyone will use GitLab for everything all the time, especially when first adopting GitLab. We want you to use more of GitLab because you love that part of GitLab. GitLab plays well with others, even when you use only one part of GitLab it should be a great experience. GitLab ships with built-in integrations to many popular applications. We aspire to have the world’s best integrations for Slack, JIRA, and Jenkins. Many other applications integrate with GitLab , and we are open to adding new integrations to our technology partners page . New integrations with GitLab can vary in richness and complexity; from a simple webhook, and all the way to a Project Service . GitLab welcomes and supports new integrations to be created to extend collaborations with other products. GitLab plays well with others by providing APIs for nearly anything you can do within GitLab. GitLab can be a provider of authentication for external applications. There is some natural tension between GitLab being a single-application for the entire DevOps lifecycle, and our support for better user experience via integration with existing DevOps tools. We’ll prioritize first our efforts to improve the single-application experience, second to enable a rich ecosystem of partners, and third to improve integration with the broader ecosystem to other tools. GitLab is open-source so this should not prohibit contributors adding integrations for anything that they are missing - as long as it fits with GitLab product vision. If you don’t have time to contribute and are a customer we’ll gladly work with you to design the API addition or integration you need.",
  "This document describes what Product Management does, where, when, and how to engage with the product management team. At GitLab, the PMs lead their specialization. That is, the Create PM decides what the Create group works on, for which release, and makes sure this furthers our goals. This includes bugs, features, and architectural changes. The PM can’t be expected to parse every single bug or issue that comes by, so they have to rely heavily on the input of the various stakeholders. To be able to achieve this, both the PM and the stakeholders have to actively work together. It’s a two-way street. In general terms, if you require something to happen with the product or if you need engineering staff for a particular change, you approach a PM. Preferably through creating an issue (the GitLab way), and mentioning them there. In the same vein, PMs are required to ask for feedback from the stakeholder on particular changes. If a change will affect GitLab.com and its maintenance, a PM should proactively reach out to infrastructure engineers to help with the scope, design, and decisions regarding this change. It is then up to the PM to weigh all these inputs and decide on a prioritization. It is to be expected that they are the best equipped to make this prioritization, while also keeping in mind all goals of GitLab. Please see the Product Categories to know which product manager handles which category. Generally speaking, all product feedback should be provided via issues. For detailed overview of how to create an issue, please read this section on the process . If you have any product-related questions, comments, input, or otherwise, the Product Manager is the primary person you should talk to, if creating an issue does not suffice . Creating an issue includes, but is not limited to, features, bugs, and other changes that need to be prioritized, changed, discussed, or need more attention. Product Managers will reach out to stakeholders when making or communicating any decision. The pressure of balancing priorities while ensuring we build excellent software is on the product managers and they need all the input they can get to achieve this. Paid features fall under their respective PMs, not under one PM in particular. For instance, Service Desk falls under the Plan PM . All feedback must follow the GitLab Community Code of Conduct . Failure to do so will result in the issues or comment being deleted. If a customer has a feature request that doesn’t already exist, refer to the process to create an issue in the gitlab-org issue tracker and choose the Feature Proposal template, following the instructions and providing as much information as possible. Once you’ve created the issue, make sure to add the appropriate labels for the product stage and/or group (e.g ~\"devops::plan\" ) if known and add a comment tagging the appropriate Product Manager. If an issue already exists, make a comment on the issue with their information and use case. Whenever you’re sharing feedback on an issue (e.g. “Customer X wants this”), please make sure to do the following: If a customer expresses interest by simply mentioning an issue number or e.g. “an integration with X”, that is not sufficient information. Before creating or commenting on an issue, make sure to ask them: The Product Manager is responsible for figuring all of this out, but being one step ahead of them will speed things up. If a customer is a member of our Product Customer Advisory Board , the CSM should add two labels: ~CAB Takeaway and ~CAB Takeaway Qx FY20xx to the issue. It is highly recommended to use the feedback template below to make this easier. You can copy/paste this to make sure you don’t miss anything or create a comment template for reusability: The ~customer priority::* labels are inputs for the prioritization model powering the customer issue prioritization framework dashboard: These dashboards represent the relative importance of a given issue to the specific customer. 1 is the lowest priority and 10 is the highest. These can be updated at any point in time and will be reflected in the model within 24 hours. You can find more context about priority labels on the customer issues prioritization framework handbook page . A customer with more than 1000 users mentioned they are interested in this feature to be able to do their sprint planning more effectively. The problem they are trying to solve is that with the current implementation, they can’t X and need to do so because Y. They are using software X to do this today, but would be able to move to GitLab if we would do this. @productmanager this issue doesn’t have a milestone right now, are we planning to address this in the near term? Customers that are assigned a Customer Success Manager typically have a collaboration project on GitLab.com, which is used to share information, document customer details, and track issues in a place that both the GitLab team and the customer’s team can access. Generally, CSMs maintain a main issue and/or enable the CS-Tool - TAM issue tracker , which lists out all feature requests the customer is interested in with links to the public GitLab issue. When a customer expresses interest in a feature, the CSM should capture that in the public GitLab issue, as well as add it as an entry in the main feature tracking issue of the customer’s collaboration project . The feature tracking issue should be maintained regularly by updating priority (elaborated on below) and milestones as the single source of truth on customer product needs. It can also be used for reviewing metrics of previously delivered feature requests. If there is a lot of discussion with the customer about a specific feature request, create an issue on the customer collaboration project about it and list that issue as a related issue on the main GitLab issue. This is another signal of the main product issue of customer interest and also allows discussion with the customer and internal GitLab team members about their needs and concerns. If you have followed the process of creating/commenting on issues and have not gotten traction, confirm that all of the necessary information is included in the issue . Follow up with the Product Manager again in the issue and in the product stage Slack channel (linking to the issue) to get additional attention and team member involvement. If a customer has identified an issue that is high priority for them, such as a work-stoppage bug or a feature required for the customer to meet a deadline, follow the expected steps for logging and tracking customer feature requests above by adding the customer’s interest in a GitLab issue and including it in the collaboration project issue. In addition, reach out to the Product Manager who is responsible for the corresponding group and discuss it with them directly. A general idea of high priority is that the customer needs a particular feature as soon as possible. Critical Priority Requests are extremely rare, but, when they occur, they are agreed upon by both Product and Engineering, with the CSM facilitating the request. If a customer is unable to continue using GitLab without a specific feature, the CSM should begin the triaging the account , follow the process to indicate customer interest in the issue, then set up regular check-ins with the Product and Engineering teams to assess the status of the feature, expectations, and potential secondary plans. For the product & engineering process, please refer to the details of a critical customer merge request . The following part of this UX design article sums it up well: Listening to the right customers at the right time is a great first start, but you also need to make sure you are interpreting their feedback/requests correctly. The reason for that is generally customers ask for something to be better, not different — they interpret their problems through existing solutions . A customer is unlikely to tell you what new product to create (that’s your company’s job!) but they will tell you what problem that product needs to solve. To do this you need to get to the underlying why behind the feature request — what is the basic problem to be solved, and then think about how to solve that problem in a fundamentally better (e.g., 10x faster, easier, cheaper) way. This concept is best described by the (most likely misattributed) quote by the founder of the Ford Motor Company, Henry Ford: “If I had asked people what they wanted, they would have said faster horses.” When the customer asks for a faster horse, you should then ask why. You would invariably hear things like: I’d like to shorten my commute from home to work I’d like to be able to sell my widgets to more cities I’d like to win the Kentucky Derby Now that you understand the basic problem to be solved (of which there are many solutions including a faster horse), its your job to think of a fundamentally better way to solve it — e.g., what does a 10x faster horse look like? And one potential solution to that is obviously a car. If you hear a feature request from a customer, and they do not have a CSM assigned to their account, you should follow the normal procedure: create an issue and label it correctly. Let’s say the customer requests an enhancement to Issues. You know by reading above that you’ll have to label this with Discussion and you can mention or reach out to the Plan PM to expedite this if warranted. A salesperson for an organization asking for a paid-tier feature request shall work with the product manager to arrange conversations to further explore the feature request and desired outcome. The process will be: In the event that a paid customer is willing to pay for us to develop a specific feature, we should still respond as above. It’s great that they’re willing to pay for it: that means they really need it. However, we will not make a custom version of GitLab; even gitlab.com is running on GitLab Ultimate, and we move faster that way by minimizing technical complexity to determine features to follow after, it’s a trade-off to make. This doesn’t mean that “no” is always going to stay “no.” We keep an open mind to improvements. For example, they need support configuring a self-managed runner with a SaaS license. If you need support with a specific customer and your Customer Success Manager is unable to configure what is being requested or you are being asked to provide very specific guidelines for use of GitLab, we suggest creating an issue using the Product Support Request , and following the steps suggested in the issue. To remain focused on customer results and efficiency , we recommend setting a due date on the issue in 5 business days and assigning the issue to the Product Leader of the section for triage. Same as before, make sure an issue is made and make your case with the PM that this is becoming a problem and needs to be fixed. The PM will make sure that this is fixed or resolved in some other way. Everything in GitLab should be fast and creating files falls under the repository, so you create an issue and make the PM aware of it by mentioning it. The PM in turn will investigate whether this is a general problem or one specific to GitLab.com, in collaboration with infrastructure and others, and schedule any necessary changes for an upcoming release. The Product team maintains a Tableau dashboard to aggregate issues and customer interest in those issues. Information is automatically gathered from GitLab issues by scanning for Salesforce customer account links. The same dashboard can be used by Sales and CS. The Tableau page automation will detect when Salesforce links are added and use the customer’s Salesforce data, such as Total Account Value and seat licenses, to add them to the page. This also maintains a customer’s privacy on public issues, since Salesforce links are only accessible to GitLab employees with proper credentials. If your customer would like to report a bug, refer to the example of how to express the customer’s interest in an issue and use the Bug template, following the instructions and the same steps as above. You can refer to the following steps if a medium priority bug has become stale on the Product Management triage board . Another potentially helpful view is the triage report label . Follow the same steps as for logging and tracking bugs as with feature requests, but be sure that steps to reproduce and workarounds are included whenever possible. It is essential for the Product team and the CSM team to have a close working relationship, so that the business has a pulse on customer interests, feedback, and sentiment. If you are on the Product team and seeking feedback from customers, you should consult with the Customer Success Management organization, as CSMs have direct access and regular communication with customers across all regions, tiers, use cases, and industries. To request a meeting with a customer, open an issue in the CSM project and use the Product Engagement issue template, filling out the appropriate fields. If you have a specific customer in mind that you’d like feedback from, please share the customer name in the issue and tag the CSM assigned if you know who it is (you can find this information in Salesforce if you have access; otherwise, someone else will check for you). The CSM team gets notified via Slack whenever a new issue is opened, and they will respond in the issue with specific customers when they are available. If you don’t receive a response within a week (allowing the CSM to review with their customers), feel free to ping the @timtams group in the issue. One of the responsibilities of CSMs is collaborating with the Product team to help prioritize features by indicating demand from customers and relaying customers’ use cases and experiences. Following the process described below will ensure that customer interest in features is shared with Product properly, so that Product can take appropriate action. Through following the documented process, we are able to increase our efficiences, decrease back-and-forth communication, build a better product, and get answers and resolutions to customers faster. It can be very helpful for both Product and customers to be on a call together to discuss feedback, roadmaps, etc. Take the following steps to ensure an efficient and productive meeting: In advance to the product call, have a conversation with the customer about expectations and their background. Ask the following questions, as applicable: It is best practice to ask for a PM to join your call through the slack channel for their group or category . Direct messages are problematic because it is hard to loop in other PMs if the right person for the call is not included in the DM, and it limits visibility into topics that may be discussed. If you are unsure about the right group to reach out to, you can ask in the general #product channel. Before reaching out to the PM, fill out the PM Customer Meeting Briefing Document and send it to the PM along with your request. The PM will review the doc and let you know if they are able to attend the meeting. PMs may decline meeting requests if they do not receive the context doc or if it is only partially complete. We recommend sending this context along with your request at least 3 business days before the meeting. The PM will review the document and provide feedback to ask for further clarity if needed. In addition, once the call is scheduled, make sure you have a detailed agenda set at least 24 hours before the call that is shared with both the customer and the Product team. CSMs regularly hold Executive Business Reviews with their customers and often request involvement from Product Managers. When a CSM is looking for Product involvement, they will reach out to the PM (in their group Slack channel) with the date, time, and desired topic. The CSM will involve the PM in preparation for the EBR, working with them to ensure expectations of content, timing, and desired outcomes are fully established. EBRs are usually quite long (60-90 minutes) and the product presentation is only a small portion (15-20 minutes) of the overall EBR, so the PM is welcome to only join when they will be speaking. Of course, if they want to join and/or participate in the full call, that is definitely welcome, as customers often share product feedback and requests throughout an EBR. Throughout the customer lifecycle, customer-facing teams (Support, Account Executives, CSMs, Solutions Architects, Professional Services, Renewals Managers, etc.) may need the assistance of a Product Manager. This can include a detailed discussion of our direction, and how to address specific use cases, or gaps in functionality, within an organization. To ensure these requests can be quickly triaged, easily scheduled, and later tracked, there is a standardized request process based on issues. For time-sensitive and high-impact requests, please paste a link to the issue in the #product Slack channel, and @mention the recommended PM’s in the template. When a support request is opened, labels will automatically be assigned to categorize the request. Three fields are particularly important when triaging requests: All Product Managers should ensure they are set up to receive label notifications for their respective stages: GitLab is designed and developed in a unique way. Consistent with our value of Efficiency the product is developed with directly responsible individuals from Product, UX, Quality and Development working together. At GitLab, we develop our product for self-managed as well as SaaS-hosted customers. We realize that while we have DRIs there are many stakeholders who must have input, including Engineering, Quality, UX, Product, Security, and Infrastructure. For example, the Security team often has the deeper context of what it takes to run a secure SaaS system. Similarly, the Infrastructure team has insights into what we should build into the product to reduce toil and enable efficient, reliable, performant, and scalable systems. We call this the Product Group model. It is an extension of the classic quad concept at the leadership level and is currently comprised of Development, Quality, User Experience, Infrastructure, Product, and Security. The Product Group can be used to facilitate a global optimization , including product-wide technical debt . There are many counterparts that PMs work with. Here are some best practices for working across the organization. In some cases, Product Managers may have items that incur expenses toward the budget. These can be related to external vendors for research, contractors for development staffing, and infrastructure. The CProdO is the DRI for the product budget and all changes or requests for budget spend must be approved through them. To request a forward-looking new budget item, open an issue in the Product project using the Product Budget Request Template and assign it to the CProdO and manager. Budgets are planned annually and quarterly, so approval may not be immediately given because it depends on the timing of budget planning. The CProdO will bring the budget request to the next budget planning session with Finance. To request approval for an increase in the expected spend for a pre-existing item, open an issue in the Product project using the Product Budget Request Template assign to the CProdO and tag your manager. The CProdO will review, approve or decline the budget change. The CProdO will then notify the Finance Business Partner of changes for forecast updates. Content marketers and Product Managers can partner together when using a Blog to communicate product changes and engaging the market with thoughtful changes. See the blog post handbook page for guidelines on when and how to start engaging Content Marketing for creating a blog post for a feature. Product marketers and managers should be joined at the hip. Just as a feature without documentation should not be considered shipped, benefits of GitLab that we’re not actively talking about might as well not exist. Product marketers rely on product managers to be guided to what is important and high impact. In general, you should: A core element to use cases is effective objection handling against major competitors in the market. In order to effectively support this effort, a partnership between Product Marketing and Product Management to maintain adding competitor walkthroughs and competitive content to the existing use cases is critical. To date we have competitive sections on the following uses cases: In FY22-Q2, we are taking on an effort to continually update these Tier 1 and Tier 2 use cases and are assigning DRIs, follow along this effort in gitlab&1446 When working on the release of larger features or new capabilities, it is important the product manager consider various aspects of the go to market plan and inform or partner with the appropriate stable counterparts for strategic and logistical considerations. As a PM you’re responsible for making sure changes you’ve shipped are well represented throughout GitLab’s documentation and marketing materials. This means that on release, features.yml is updated, documentation is merged and deployed, and any existing content is updated where necessary. It’s not acceptable to do this after the release. GitLab is very complex, and features and functions are easily missed, even those that provide significant value to customers (e.g. the many ways you can authenticate with GitLab). You can recruit the help of the marketing and technical writing team if needed, but it’s highly recommended to do small updates yourself. This takes less time and overhead than communicating what needs to be done to someone else. It’s important to keep features.yml updated because there are a number of different pages (internal-facing and external-facing) that read from that file. These include: The standard for working as a team at GitLab is the Product Development Workflow . Product Managers and Product Designers should work together as strategic counterparts to better understand the problem and discover user needs. The Product Designer and the Product Manager will pair to understand the target audience, their challenges when using a particular feature and then designing a solution that helps users solve them. It’s important to remember that User Experience (UX) does not only relate to visual features or interface design. UX is the intangible design of a strategy that brings us to a solution, so it also refers to the experience of writing code, working with .yml files, designing APIs, working with a CLI, etc. All of those functionalities are meant to be read and used by people. Involving a Product Designer into their planning and development can be highly beneficial. A guide to consider is: anytime a person is interacting with something, there is an opportunity for that interaction to be designed. As the GitLab product matures, we know we must make important workflows easier to use through feedback-loop mechanisms as is captured in the “Improve” section of the Product Development Flow . We can use the Category Maturity Scorecards and UX scorecards as mechanisms to provide insights into how might be able to improve these user workflows. For areas with minimal maturity, or low/internal-only adoption, iteration and quickly adapting the product is the priority. In cases where the product experience desired would take longer to implement than required for the current maturity stage, it is advised the Product Manager work with the Product Designer and/or Engineering Manager to scope an iteration plan to ensure the experience is delivered incrementally over time to provide value quickly with quality. For areas with more adoption, or beyond viable maturity, we recommend using the below escalation path if there is a disagreement on the approach to solve for the product direction and experience for users/customers. As a team, there may be cases where a proposal exceeds the expected time to market to achieve the optimal customer experience. As this impacts potential business results, product managers are the DRIs of the decision. As DRIs, it is important to consider the input from other team members and to know when to trust in their experience and judgment. It is advised to use an opportunity canvas lite . The PM is expected to compile the canvas lite with inputs from the Product Designer and/or Engineering Manager. The PM then makes a decision after weighing input from the product designer and engineering, as appropriate. The PM should then share the decision, articulating the costs of waiting, and shipping earlier with less polish, as well as why no smaller iteration exists as part of this decision. In the event that a decision is made to build something that is less polished, has a lesser user experience, or otherwise doesn’t live up to our standards of where we want this UI to end up the team should generate follow-up Deferred UX issues to be addressed in the next upcoming milestone(s). If a quad member remains concerned and in strong disagreement with the decision made by the PM DRI, the quad member should exercise our disagree, commit, and disagree value , by initiating an escalation to bring in management layers above into the decision. Results are the most important aspect to consider for the business and our users. If there is a perceived risk to potentially harm the business financially, reduce customer satisfaction or value, or lead to legal trouble, teammates are empowered to seek an alternative perspective for the product decision. Within the Product Division, we recommend escalating first to the management layer immediately above where the disagreement is happening for input and further escalating to PLT and ultimately the Chief Product Officer. Product Designer assignments are listed in the team.yml file. Unfortunately, we are currently unable to assign a dedicated Product Designer for every group. Instead, Product Designers are assigned to the areas of highest business priority and will continue to provide focused support to those priorities throughout the year. Due to the limited capacity, we are also not able to do UX reviews on MRs for groups without a designer. If there isn’t a designer listed for a group, then that team is expected to be self-sufficient in taking responsibility for the design needs of their product area. Product Design does not have the capacity to review complex proposed design solutions or provide design solutions for unsupported groups. If you have questions or need support you can do so in the following ways: Here are some practices for how PMs work with groups outside of GitLab. Product managers are the DRI for their group’s product direction which must include delivering on our greater company strategy of dual flywheels. Community contributions are a critical part of the product direction. To support contributions product managers may consider the following guidelines:",
  "Principles - Processes - Categories - GitLab the Product - Being a PM - Leadership These are core principles that we believe will deliver world-class products through customer-centric innovation. Our goal is to build a practice that fosters these principles with the customers’ voice at the core. Everything we do is for our customers, and we are only successful when they succeed in delivering secure software faster to their customers and internal users. From development teams to marketing organizations, everyone needs to collaborate on digital content. Content should be open to suggestions by a wide number of potential contributors. Open contribution can be achieved by using a mergeable file format and distributed version control. The mission of GitLab is to allow everyone to collaborate on all digital content so people can cooperate effectively and achieve better results, faster. Ideas flow through many stages before they are realized. An idea originates in a chat discussion, an issue is created, it is planned in a sprint, coded in an IDE, committed to version control, tested by CI, code reviewed, deployed, monitored, and documented. Stitching together all these stages of the DevOps lifecycle can be done in many different ways. You can have a marketplace of proprietary apps from different suppliers or use a suite of products developed in isolation. As a single-application for the entire DevOps lifecycle, GitLab strives to enable you to bring ideas to production rapidly. We will do so AND avoid demonstrating our capabilities with toy app demos and simple trivial examples. That’s because we understand that building a prototype is easy, but building a production line is hard . Minimal Valuable Change (MVC) is the GitLab path to delivering the smallest measurable improvement for our users, customers, and the wider community. Our approach requires four pillars: When considering how to scope a feature for a release, remember that it is not ok to ship an “incomplete” feature to customers (see the definition of done ). Consider the use of Pajamas components for UI in your MVCs. When introducing a new component or pattern not found within Pajamas, it is the responsibility of that team to follow our component lifecycle guidelines to determine whether it should be added and, if so, contribute the addition/update back to Pajamas. MVC means reducing the scope so we can ship quickly. It doesn’t mean shipping something that hurts the usability of GitLab. First impressions are important. A feature that does not offer enough value or hinders the user experience may have a negative effect that discourages users from trying that feature again in the future. If there are obvious gaps in your MVC or you can anticipate follow-up requests, consider whether your feature is complete enough to be released to users. If you are unsure whether your feature is complete enough to be an MVC (or if you know your feature is not complete enough to be an MVC and you want to gather additional feedback), you can use approaches such as dogfooding, beta programs , feature flags, and/or user research to help build confidence in your decision. In terms of talking about your feature, it’s ok to add a release post item that announces your incomplete feature (making clear that it is an early iteration, and points to the direction for the feature) and follow up in a later release post with a new item when you’ve completed more of the functionality. As long as you call it cookie dough, not a cookie, it manages user expectations. There are scenarios when an MVC approach is not advised. These include: An MVC approach is a byproduct of our spirit of iteration. That means we break problems down as small as possible , and focus on reduced cycle time . Thinking iteratively is not always intuitive, and breaking certain topics or projects down can be challenging. Here’s a helpful video from our CEO with guidance on how to think more iteratively. Here is a great video that illustrates how to build MVCs using Iteration. It shows Lego climbing obstacles. The first design fails. The second one can climb a book and so on. It also illustrates how modularity and good interfaces help with iteration as things get complex. To ensure that our efforts consistently deliver value to our users, each iteration must adhere to the following guidelines: Success of an initiative is not measured by the deployment of a change or the completion of an iteration. True success is determined by whether the iteration achieved its predefined objectives, as evidenced by tangible business and product metrics. We celebrate achievements when we can clearly see that an iteration or launch has resulted in tangible value for our users. Humans tend to favor solutions that add features than solutions that remove them, even when removing features is more efficient - great PMs recognize this bias and utilize subtractive thinking to create great user experiences. Customers will tell us when we’re missing something they need, but they’re unlikely to explicitly tell us when we’re overwhelming them with unwanted features. However, we do have evidence that this challenge is already a consideration for us, as reflected consistently in our System Usability Scale verbatims . Here is an episode on the Hidden Brain podcast that explores this bias further. Our customers choose SaaS because it reduces their operating costs, helps them adopt the latest capabilities without performing upgrades, and provides them peace of mind of high availability. This principle implies the following: This principle does not mean SaaS only. For more information on parity between SaaS and self-managed, see our parity principle . An MVC approach allows for maximum feedback while iterating. To help gather that feedback, Product Managers are encouraged to create feedback issues ( example ) to consolidate suggestions and experiences from users. Consider mentioning the feedback issue in any release post items and related implementation issue(s) for awareness. It’s important that team members know they are encouraged by E-group to continue moving fast as GitLab grows and gets bigger as a company. This involves moving quickly even in the face of risk and complexity. In alignment with our transparency value , we’d like to celebrate examples of failures or mistakes that were made while moving fast, where we ultimately learned from it and moved on. The following failures provided by the product team are celebrated as opportunities to gain insight, share learnings and move on with additional knowledge: If you have a failure that can serve as a learning opportunity, please make an MR to this page Just because something is not invented here doesn’t mean it doesn’t have a perfect home within our solution. GitLab is an Open Core product and is part of the broader ecosystem of Open Source tools in the market. Every day there are new innovative open source tools out there that solve real-world customer problems; we should not be afraid of embedding these tools into our own products in order to solve those same problems for our customers too. Leveraging existing technology allows us to get to market much more quickly, to contribute to Open Source (and help strengthen Open Source as a whole), and allows us to focus our own people on making GitLab itself better. Building professional relationships with these tool creators also is a positive for GitLab since they may have important user perspectives around your categories. We have achieved many successes following this approach: There are also many more examples throughout the company where this has been successful. As a product manager you should be monitoring the world of Open Source as it relates to your area to see where new innovative tools are being developed, and not be afraid of integrating those. One thing to keep in mind, integrating could be anything from a blog post describing how the tool works together with GitLab all the way up to bundling it inside of our own installation, and this can evolve iteratively. We understand that a natural inclination when using application development tools is to create an array of buttons to press and knobs to turn. We believe, however, that adding options to an application does not necessarily improve the user’s experience of that application. The best way to serve our users is to create an application that reduces complexity while still offering the features they need. We admire other “convention over configuration” tools—like Ruby on Rails (the doctrine of which perfectly describes the value of integrated systems ), Ember , and Heroku —and we strive to offer the same advantages for continuous delivery of software. Furthermore, Ruby on Rails has significantly and positively influenced the Ruby community, uplifting the tool and making it more powerful and useful than ever before. We want GitLab to be to Kubernetes what Rails is to Ruby. You should prefer well-considered choices based on current best practices. Avoid unnecessary configuration. Avoid configuration to support fragile workflows. When considering adding new configuration, we follow the following principles: Sometimes fast deployments are needed to fix a service or application outage that can cost a business money and reputation, we understand time is of the essence in these situations. That’s why we believe giving the team control over this is important in crucial moments of the development lifecycle. Controls that prevent changes from reaching Production are okay as safeguards, but they should be able to be quickly removed or person with disability if necessary. When controls are changed in this way, logs or records should be created to support post-mortem analysis and allow for understanding why the control needed to be removed or disabled. We want to provide the same capabilities to end users regardless of the method they choose to use GitLab (GitLab SaaS, Dedicated or Self-managed). All GitLab SaaS environments leverage the same installation method available to self-managed users, with a different licensing structure. By designing and implementing features for self-managed, we achieve maximum parity between the various installations. In line with our SaaS-first principle, some features may be released on SaaS to gain operational experience and apply learnings prior to recommending and supporting customers using it. Features would be present on self-managed codebase but are person with disability until General Availability. For functionality where implementation may be especially challenging without cloud services, for example with AI, self-managed functionality may be dependent on an underlying SaaS service. This allows us to provide end users the same capabilities regardless of deployment type, and not overly constrain our feature set or impose significant operational complexities on each deployment. Product managers need to be aware that this may impact the adoption of these features, as not all customers may be willing or able to leverage underlying SaaS services, such as air-gapped deployments. Exception to this product principle requires CEO approval. Work with VP, Product Management to add your request to the Product Scale agenda describing the situation and request for exception to gain CEO approval. Our simplicity and SaaS/Self-Managed Parity principles require that we adhere to our established knowledge architecture. Our established architecture is Organization , Group and Project . Note: This implies that we will take pains to avoid instance-level features as we expect to move all capabilities from instance to organization over time. After making a tier decision on a new feature, we should strive to maximize the number of users who can use it. As part of this objective, we should avoid building instance-level features when possible. Building at the instance level (in the admin area ) leads to a separation between GitLab.com and self-managed and limits your audience to self-managed customers only: Historically (and even net-new proposed features) we’ve often started with an “instance-wide” mindset which then means we need to iterate and adjust features to work at a group-level. This often delays functionality for our SaaS customers and makes GitLab.COM feel like a second-class citizen. There are factors that may justify an instance-level feature, like engineering efficiency and high infrastructure cost , but we should always have a clear view on how we might bring the feature to GitLab.com and clearly document why we started with the instance-level in the issue. Product Managers at GitLab are frequently confronted with the choice of whether to add new configurations or not. These can frequently be times where an outside perspective is important. That’s why we’ve created the option to request a New Config Review . Here’s an example of how to consider whether to add new configuration. Let’s say you are proposing we add a checkbox or two radio boxes in a feature dialog box. Think carefully about what users really want. Most of the time, you’ll find you really only need one solution, so remove the other option. When two possible choices really are necessary, the best or most common one should be the default, and the other one should be available. If the non-default choices are significantly less common, then consider taking them out of the main workflow for making decisions, by putting them behind an Advanced configuration tab, for example. Avoiding configurations is not always possible. When we have no choice, the secondary priority is to configure something in the GitLab interface. A configuration should only appear in a file ( gitlab.rb or gitlab.yml ) as a last resort. When you have to add a new configuration, make sure that the features and services are on by default. Only add a configuration line to either of these configuration files if the feature or service cannot be fully person with disability from the admin UI. If the decision to add a configuration follows the principles above , add it to the repository-specific CI configuration options and be sure to default it to the option that results in the best user experience. We are much more liberal with additions to CI configurations than Instance configurations. Features should be owned by one group, including the respective DRIs of that group. Make sure that the documentation metadata and the features.yml for your team are kept up to date to make it easier for other teams to find the correct owner. This principle is important because unowned product features are unsupervised, and continue to accrue technical debt over time. This increases the risk of performance and maintenance issues, which tend to only get resolved once the situation has become critical. In addition, by having clear DRIs for our entire surface area, teams are able to advocate for investment and/or removal of features. If you encounter a feature that does not seem to be owned or documented, work with the team that originally introduced the functionality to decide on ownership. If the feature is large and needs to be broken down, document which elements are owned by which team. If you cannot decide who should own the feature, escalate the decision to the lowest common reporting line between the involved teams. If there are features that no group wants to own, or features that a group no longer wants to own, that feature should be considered for deprecation and removal. A highly usable interface with cohesive workflows and comprehensive documentation is a must to stay ahead of our best-in-class competitors. Work closely with the individuals in UX to achieve our user experience goals. The UX team has a high level of expertise in Product Design, Technical Writing, and UX Research. They can help decipher or decide how to simplify or avoid complexity. While our Product Designers review user interface changes in merge requests , they are not limited to just the UI. Anything that impacts the user journey is relevant to them. Keep in mind these general user experience principles. Additionally, you can familiarize yourself with the UX team mission and GitLab’s Pajamas design system principles . Many crazy, over-ambitious ideas sound like they are impossible just because no one else is doing them. Since we have amazing engineers and a culture of shipping minimal valuable changes, we are able to accomplish many more “impossible” things than other organizations. That’s why we’re shipping merge conflict resolution, why we shipped built-in CI before anyone else, why we built a better static pages solution, and why we’re able to compete. Here at GitLab, we are an ambitious company and this means we aim for big things with every release. The reality of taking chances and planning aspirationally means that we won’t always be able to deliver everything that we wanted to try in every release, and similar to our OKRs , we believe this is a good thing. We don’t want to shy away from challenging ourselves and always want to keep a sense of urgency, and aiming for more helps us do that. Also see the importance of velocity We arrived at our preference for ambitious planning after measuring our velocity and finding that our velocity was unchanged whether we scheduled ambitiously or scheduled for providing slack. Discovering new features can enhance the experience and unlock significant value for users. And the more users see and try our features, the faster we can get feedback to improve them. However, excessive feature discovery efforts can become irritating for users. This erodes trust and reduces engagement with other UI elements in the future. Even worse, they might leave GitLab due to this deteriorating experience. Context plays a significant role in how users engage with new functionality. By presenting features in a way that resonates with a user’s current situation and needs you increase the liklihood of them using this new functionality. Work with your product designer to improve the discoverability of your features. The Pajamas Design System has best practices and examples to support feature discoverability . We can also design new patterns. The Growth team can also help you with this, as they think about things like onboarding new users and promoting feature use within the app while supporting, not annoying, the user. As the GitLab userbase and team members who work on GitLab continue to grow we need to support both our users and team members by helping to connect users who may be interested in speaking with a member of the sales team to that particular person. We can call this a Product Qualified Lead or a PQL. Our goal is to develop a world-class PQL system whereby we monitor product usage to understand and constantly iterate on what constitutes a usage-based PQL and provide a unified intelligent interface in the product where users can submit a hand-raise, start a trial or upgrade touchlessly. By monitoring product usage, usage PQL volume, SAO rate, and ASP we will be able to work in partnership with marketing and sales to ensure we’re sending high quality leads to the sales team. In the product experience, we will develop an intelligent module for feature discovery moments whereby we help recommend what we believe should be the preferred option for the user whether it’s a hand-raise, trial or touchless upgrade by updating the default CTA based on their usage of the product along with demographic and firmographic data. This experience will be present on both SaaS and self-managed instances for air-gapped instances the CTAs will provide the user with external URLs to visit to complete the associated step. This experience should be able to be deployed by any stage to further their paid adoption rate. Users can only experience GitLab’s value when they actively use the product features. Therefore the Product team’s mission isn’t only shipping features and building products, but also driving usage and delivering value. There are two frameworks we use to think about driving GitLab’s product usage: we use the AARRR framework to think about how to drive a single feature’s usage, and use the Customer Adoption Journey to think about cross-adoption of product features. These two frameworks are also interconnected with each other. AARRR stands for Acquisition , Activation , Retention , Revenue , and Referral which is often referred to as “Pirate Metrics” . These five words represent the customer journey and the various means a product manager may apply Product Performance Indicators to drive a desired behavior in the funnel. While the AARRR framework is commonly used to drive overall active users, it is also a great way for PMs to think about how to drive feature usage. Add AARRR funnels for your stage or group’s Product Performance Indicators directly with mermaid markdown. It’s easy if you use this live editor . Product managers can use these various states to prioritize features that drive a desired action. This could mean focusing on the Activation metric to drive awareness and generate more top of funnel leads. As an example, in the Release stage the Release Management group tracks actions on the Release Page in GitLab. Users that view a Release Page have been acquired and those that create a release on a Release Page are activated users. The Product Manager can choose to target features that drive users to view the Release Page more, resulting in a greater interest in the number of users that become activated and create their own Releases. GitLab is a complete DevOps platform. Our customers get the most value out of the GitLab product when they use multiple features together. Below is the most common path our customers follow to adopt GitLab’s product stages. As PMs, in addition to driving usage of individual features, we should also proactively think about how to design product and user experiences to help users adopt more stages and features, therefore benefiting more from using GitLab. Note: There are numerous potential variants to this adoption journey, but it’s important to keep this representation simple and consistent. Please check with David DeSanto first before making any changes to the adoption journey image. Shipping only MVCs can result in a large set of loosely connected pieces that don’t necessarily combine into a single, great user experience. An obvious solution to this would be to plan out the future in detail, creating a long-term detailed plan. However, this is unwanted as it can restrict your flexibility and ability to respond to changing needs or feedback. Flow One offers an alternative. You draw out a workflow consisting of MVCs (that can be shipped individually). The workflow should only cover a specific, narrow use-case, and nothing more. Flow One should cover the first iteration of a particular workflow. After this, individual MVCs can be introduced to expand the use-cases or loosen the assumptions (e.g. from a feature that can only be used if you’re using feature branches, to one that works for other git strategies). Using data to learn from our users is important. Our users are spread across GitLab.com and self-managed instances, so we have to focus our efforts on learning and providing benefit to both when we decide to collect more data, or build and use additional analytics tools. If we do this, we can help make the rest of the company successful as well. This means that we should: Per GitLab Stewardship , we will not introduce artificial limits in Core. Artificial means arbitrarily setting a small number (such as: 1) as a limit on a given GitLab object category, that would incur no additional effort or cost had we chosen a larger number. The additional effort includes product, design, and engineering effort to create the feature in the first place, and to maintain it over time. For example, GitLab Core has the issue board feature in every project. In GitLab EE, each project supports multiple boards . This does not mean that Core has an artificial limit of one board per project, because there is additional effort to manage multiple boards such as supporting the navigation interface, and all the associated engineering work. This principle does not apply to our SaaS offering as limits are occasionally introduced to limit our hosting costs and protect other users from potential abuse. As an example we have shared runner minute quotas and implement rate limiting . We’re discussing enforced workflows in this issue . Enforced workflows should be avoided in GitLab. For example, there are three issue states ( Open , In Progress (as of 10.2), and Closed ), and any issue should be allowed to transition from one state to any other state without workflow restrictions. (Roles and permissions is a separate concern.) A comment on Hacker News perfectly details what can go wrong when enforcing workflows: “The down side for the true end-users, those who actually use the software day-to-day, is that most business processes are awful. If your experience is the hellish existence that I see strolled about on threads where JIRA comes up ...: But that comment also specifies the advantage: “JIRA’s most powerful feature is that it affords for mapping businesses processes onto software. This is incredibly compelling to enterprise customers. Software that enforces workflows, procedures and requirements can be an incredible lever and JIRA’s price point makes build vs buy decisions an absolute no-brainer.” We should ensure that GitLab makes it easy to help with enterprise workflows: When considering a customer need for enforcement or limitations: As an example, customers requested instance-wide enforcement through required CI jobs. Doing this would have been a mistake. Instead: Small primitives are building blocks in GitLab. They are an abstraction not at the technical level, but truly at the product level. Small primitives can be combined, built-upon further, and otherwise leveraged to create new functionality in GitLab. For example, the label lists in issue boards use the smaller primitive of labels . They are especially powerful because they usually take less effort and provide higher leverage than you would get from a more “complete” but standalone feature. Think of how simple Unix command line utilities can be chained together to do really complicated things, much easier (and certainly more flexibly) than you could have done with a dedicated tool. When iterating on GitLab, strongly consider using small primitives instead of creating new abstractions, especially when considering MVC features that will provide the foundations for further improvements. To do this you can start with easy to apply concepts that meet the needs of intermediate to advanced users; from here document the usage clearly and be sure to think about discoverability. The UX can very often be refactored or enhanced later when there’s a demonstrated need for refinement, onboarding for less sophisticated users, or other new abstractions needed that were identified through real-world usage. In GitLab the product, it is sometimes the case that optional software or infrastructure is required to enable new capabilities. Some examples include: The following are best practices we consider when building such components. As we learned with GitLab CI/CD, the ability for developers to quickly attach needed Runners to enable their own use of GitLab CI/CD allowed for rapid adoption of GitLab CI/CD within an organization. When considering the workflow to enable additional capabilities, start with enabling developers first. A guiding principle should be low-friction developer enablement, which will positively impact adoption. As we learned from the certificate-based Kubernetes Integration, building starting experiences that support demoing a getting-started process doesn’t necessarily translate into real usage. For example, the certificate-based integration lacked strong security primitives and the ability to manage the integration on an ongoing basis. As a result, we should build capabilities that target real-world production use first, even as part of MVCs. GitLab’s vision is to be the best single application for every part of the DevOps toolchain. However, some customers use tools other than our included features, and we respect those decisions. With this in mind, it’s sometimes valuable to integrate with 3rd-party services and products to help bridge the gaps in their toolchain. While a single application is the best approach, multiple applications that work well together is better than ones that don’t. With this in mind, below are some product guidelines to consider: Note - We intend to provide a place where everyone can contribute, such as code snippets , project templates and CI components in the CI/CD Catalog . Marketplaces are traditionally transaction based, where users are purchasing solutions. GitLab CI/CD components on the other hand, will offer an in product experience for consuming components for YAML configuration from a library of GitLab maintained components . Closed source software vendors commonly depend on plugins and commercial marketplaces because: Because GitLab is an open core product , third parties can add functionality directly to GitLab. Adding directly to the GitLab codebase (as opposed to building a plugin) may mean more work for them and will limit the ways in which they can charge for that functionality. However, for users of GitLab , this has significant advantages: And for developers of GitLab including the third parties, this has significant advantages as well: Overall, we believe that this approach creates the best possible experience for both the users of and the contributors to GitLab, and to that end we encourage people to contribute functionality to GitLab directly . If adding code directly to GitLab isn’t an option, we encourage third-parties to integrate through our APIs . Note: GitLab does support plugins that respond to system hooks , which tie directly to application events and are primarily used for administrative purposes such as auditing, logging, and other administrative tasks. GitLab is a DevOps Platform, not a collection of DevOps point solutions; the naming of GitLab products and features should reflect this. Adopting descriptive names brings other advantages: As an example , CI/CD Components Library is a descriptive name while CI/CD ATOM is a distinctive name. Exceptions to this principle are considered in limited circumstances - if a product or feature is a differentiator in the market, adopting a descriptive name risks it getting lost in the sea of same . Here, a distinctive name may be justified. To discuss an exception, reach out to #marketing in Slack. When naming a GitLab extension, plugin, app, or integration for a third-party product or service (a “tool”), either include the third-party service name with a preposition like for , or don’t include it at all. A preposition is important as it indicates that the third party isn’t officially affiliated with the tool. For example, our integration with Jira Cloud is called GitLab.com for Jira Cloud . Provided we use the tool’s full name in marketing materials and technical documentation, as exceptions to this general principle we can: (1) refer to the tool solely by the third-party service name in places where we list GitLab’s tools in product; and (2) refer to the tool solely as “GitLab” in the third-party product’s app directory. For example, we refer to the GitLab for Slack app as “Slack” in our lists of GitLab integrations, and as “GitLab” in the Slack App Directory . For more guidance on the feature naming process, see naming features . While our big, hairy, audacious goal spans all development processes, personas, and use-cases, there are primary targets in each one of these venues. When considering prioritization we should first aim to provide complete maturity for developers building cloud native applications in a modern way prior to moving to other development methodologies, personas, and application types. When developing features to compete with existing competitors, make sure to solve problems for modern development teams first, and then see what’s missing for legacy teams. e.g. For project management, make great project management capabilities for teams doing conversational development, lean, or even agile development before doing Scaled Agile Framework (SAFe) or waterfall. It’s important that modern first does not mean non-modern never. It means that we should first learn how teams are using the feature in a modern way, and then see what’s missing. The modern way provides the path forward, and then we can add customizability or the path to modern for teams who are not quite there yet. Our strategy includes going after a lot of new personas, going from developers to operations, security, product managers, designers, etc. But when developing features in these new areas, it’s important to remember to start with the developer. If we can make security great for developers and then great for security professionals, we’ll be much more successful. Development teams deploy to tons of different platforms, from bare metal to cloud VMs to cloud-native Kubernetes clusters. We build features for cloud-native first, and then support the rest. This allows us to focus on where development is going, and deliver solutions that every company aspires to use eventually, even if they’re not ready to today. By focusing on next-generation development flows, personas, and use cases - we build features and experiences where our initial users are in the relatively small population of early adopters. While we might build experiences to support them today, we presume there will always be a much larger population of future users of these experiences. Therefore, we optimize GitLab to support the larger number of current and future adopters of next-generation principles - those who are beginning to operate in the workflow (modern), team setup (developer first), or application architectures (cloud native) we support. We focus our investment in the most modern workflows that will best support those current adopters. This will come at the cost of sustained investment in initial workflows for early adopters. When doing so we will ensure we clearly communicate with our users what the preferred path is. For example - We first provided the ability to attach Kubernetes Clusters via the Certificate method. After realizing this wasn’t optimal for production use cases we added the GitLab Agent for Kubernetes method of cluster attachment. As soon as we were certain that the certificate method was no longer the preferred method we communicated via our docs and in the product that the Agent was the preferred path for current adopters. This should not mean an immediate deprecation , but a clear signal that the legacy method will become deprecated once the alternative approach is able to substitute it. We provide customer support to paying customers on all features that are in the tier of their paid license as well as any lower tiers. This means a feature in Core, should get customer support in all paid tiers. For example, when there is a feature that is available in the lowest paid tier, but only has customer support in higher tiers, the feature should be removed from that lowest paid tier. In accordance with our stewardship policy we can never remove features from Core into paid tiers, but we can build additional features around it which are available to paying customers only.",
  "Principles - Processes - Categories - GitLab the Product - Being a PM - Leadership As a Product Organization, we work to create a flexible yet concise product development framework for developing products that customers love and value. The Product Principles section is where you can learn about our strategy and philosophy regarding product development, here we discuss the processes we use tactically. Introducing changes requires a number of steps, with some overlap, that should be completed in order. GitLab follows a dual-track product development flow spanning product, engineering, UX, and quality. We use GitLab to power product development flow . When changes are released, we follow the release post process to communicate externally about new capabilities. This process should be both up front and on an on-going basis when building features. Documenting a Section, Stage, Group and Category direction is critical to communicating where we are heading and why to all of our stakeholders. We document our direction in direction pages. Read more about related processes under Planning and Direction . In November 2024 we adjusted our communicated touch points based on feedback from our FY25-Q2 Engagement Survey , AMAs, skip levels, and 1:1s. The two key areas we are prioritizing: When communicating change or a request for action to the entire product function, utilize the following levels and corresponding activities. Before shipping a new or updated feature, you are responsible for championing it, both internally and externally. When something is released, the following teams need to be aware of it as they will all need to do something about it: You can promote your work in several ways: When referencing issues in written communication using just the issue number #123456 and a link is not low-context communication . Instead use the title of the issue and the link or the issue number and description of the problem that issue will solve: In order to support findability and to clearly articulate when we change our minds especially when it comes to product direction, category changes, shifts in investment themes, or priorities for engineering, Product Managers must evangelize these changes in multi-modal communication channels to ensure our users and customers aware. Some internal methods for communication include: External channels for consideration linking direction pages to: When conducting End-of-Line testing that may impact internal workflows or customer-facing demonstrations, product teams should coordinate internal announcements to avoid surprises for field team members who demo to customers. This communication is a product-led responsibility, with product managers coordinating with their engineering counterparts to ensure appropriate stakeholder notification. As a PM, it is important to remember a bias towards action (and other value actions like sense of urgency , make a proposal , boring solutions , write things down , don’t wait , and make two way doors decisions which enables PMs to drive an async discussion to being action oriented. Every time you write a comment or create an issue ask yourself: Will this allow us to take an action and move us forward? As PMs we need to constantly write about the features and upgrades we ship: in a blog post, internally to promote something, and in emails sent to customers. There are some guidelines that one should take into account when writing about features, the most important being a clear communication of the problem we’re solving for users. When writing about a feature, make sure to cover these messaging guidelines which help produce clear internal and external messaging. Please also keep in mind that we should avoid using acronyms that others my not recognize, such as “MVC” for Minimal Valuable Change. For more guidance you can visit our writing style guidelines . Let’s highlight the messaging guidelines mentioned above with a concrete example, Preventing Secrets in your repositories, that we shipped in 8.12 . It’s a bad idea to commit secrets (such as keys and certificates) to your repositories: they’ll be cloned to the machines of anyone that has access to the repository. If just a single one is insecure, the information will be compromised. Unfortunately, it can happen quite easily. You write git commit -am 'quickfix' && git push and suddenly you’ve committed files that were meant to stay local! GitLab now has a new push rule that will prevent commits with secrets from entering the repository. Just check the checkbox in the repository settings, under push rules and GitLab will prevent common unsafe files such as .pem and .key from being committed. Here are some additional examples of well written release blog posts for inspiration: In addition to the written medium, video is an important medium that caters to the different goals you are trying to accomplish and learning styles of your audience. Depending on the type of video you are recording, there are some guidelines to keep in mind. As our documentation guidelines actively encourage linking video content, please consider following the Documentation Style Guide section on language , and working with your technical writing team to include links to your speed runs, walk-throughs and demos at relevant locations in the product documentation. Animated GIFs are an awesome way of showing of features that need a little more than just an image, either for marketing purposes or explaining a feature in more detail. Checkout our guide to Making Gifs ! Speed runs are informal videos meant to focus on a single workflow and the experience for performing that workflow. It should not require much planning and is typically short in duration (less than 5 min.). This video type is meant to inform and not necessarily to influence buyers. Demos are scripted recordings meant to influence buyers. Generally has higher production value and typically involves both a slide-style presentation and/or live screen-sharing. Duration varies depending on the topics being covered. Product walk-throughs are informal videos meant primarily for an internal audience as a recorded, visual form of product critique. Walk-throughs typically focus on the user experience across categories and workflows within a Product Manager’s product scope . There are particular benefits to walk-throughs which span product hierarchy boundaries (multi-category, multi-stage, multi-section) as they help highlight disjointed experiences across our single-application. Walk-throughs are typically longer in length as they cover more ground and often involve some “live” troubleshooting and are best performed with no planning. Use the Product walk-through issue template when creating a walk-through. After the feature freeze, it’s expected of each product manager to test their own features and perform quality assurance to the best of their ability and follow up where necessary. Product managers can use the staging environment once the release managers have deployed a release candidate (RC) to staging. Release managers should post in the #product channel in Slack that a new release candidate is available. Product managers can also use other environments as needed, such as GitLab provisioned on Kubernetes with GKE. Before a new feature is shipped, the PM should test it out to make sure it solves the original problem effectively. This is not about quality assurance (QA), as developers are responsible for the quality of their code. This is about feature assurance (FA). FA is necessary because sometimes there are misunderstandings between the original issue proposal and the final implementation. Sometimes features don’t actually solve the intended problem, even though it seemed like it would, and sometimes solutions just don’t feel as useful as intended when actually implemented. If you can test out the feature during development, pulling down branches locally (or with a review app!), that’s great. But sometimes it’s not feasible to test a feature until it’s bundled into a release candidate and deployed to GitLab.com. If so, make sure to test out features as soon as possible so any new issues can be addressed before final release. Also, take the FA cycle into account when scheduling new milestone work. If you are looking to test code that has not been merged to GitLab.com or is not yet part of an RC, you can pull the branch down locally and test it using the GitLab Development Kit (GDK). Quality Engineering Managers (QEM) are the DRIs for prioritizing bugs. These include security issues which are prioritized in conjunction with the security team. Product Managers must work with their QEM to set Milestones for issues marked with the bug::vulnerability type label to guarantee they are shipped by their due date, as defined in the Security Team process . While Product Managers are the DRIs for milestone planning , they must respect the prioritization order for bugs and maintenance issues as determined by their QEM and EM, respectively. As such they should deeply understand the implications and risks of security-related issues and balance those when prioritizing a milestone work. Addressing a serious security issue by its due date may require temporarily adjusting the desired work type ratio for one or more milestones. Priority labels and Due Date designations for security issues should never be modified by Product Managers as they are directly managed by the Security Team and used to track metrics and progress. When thinking about new features, we must not only think about the functional requirements of a feature (defining what the feature will do), but also to think about foundational requirements (defining how the feature works). At the highest level, foundational requirements define items such as performance, scalability, compatibility, maintainability and usability characteristics of a feature. It is important to have foundational requirements in place up front, as this is much easier than trying to add them later and change expectations, or break existing workflows. Our definition of done contains specific areas of consideration that are required for the acceptance of new contributions. For an in depth review of foundational requirements (often referred to as non-functional requirements), see this resource . To deliver features, we must have both functional and foundational requirements defined. To enhance availability and performance of GitLab, configurable limits should be put in place for features which utilize storage, or scale in a manner which could impact performance. For example, we limit the number of webhooks per project , and we allow admins to set rate limits on raw endpoints . These limits ensure more consistent performance, reduce the likelihood of outages, and offer admins tools to limit abuse or enforce specific standards. While these limits can be configurable, sensible default limits should be defined for our GitLab SaaS and GitLab dedicated offerings. There is a guide about developing application limits in the GitLab Docs. See Rate Limiting::Managing Limits . As we continue to scale our product, we need to consider the amount of data being stored for new features. Data storage is not an infinite resource, so we should think carefully about what data needs persistent storage to provide the desired user experience. We also need to consider the cost implications around data storage. Everything we store impacts our bottom line, and we should therefore be careful to ensure we are only storing necessary data for well thought out time-frames. We are working on defining a sustainable data retention policy , and will iterate on this section as more general guidelines are developed. Data storage comes in three main forms for GitLab – object storage, database storage, and Git repository storage. While we have dedicated teams devoted to ensuring we can scale these storages appropriately, it is in our best interest to only store what is required for a feature to perform as intended. Additionally, there are situations where storage should be subject to data retention policies. When evaluating feature data storage, the following data storage topics should be considered. A good example where we’ve successfully evaluated data storage is our CI/CD Artifacts. We’ve set some sane default values for both maximum artifact size and for default artifacts expiration , while making these both configurable for administrative users. See this page for details on working across stages at GitLab. Stages, groups, and categories serve as a common framework for organizing and communicating the scope of GitLab. If you follow the principles and workflow above, you won’t be writing long, detailed specs for a part of the product for next year. So how should you be spending your time? Invest the majority of your time (say 70%) in deeply understanding the problem. Then spend 10% of your time writing the spec for the first iteration only and handling comments, and use the remaining 20% to work on promoting it. A problem you understand well should always have a (seemingly) simple or obvious solution. Reduce it to its simplest form (see above) and only ship that. See the Cross-Functional Prioritization page for more information. *indicates forced prioritization items with SLAs/SLOs Any of the items with a “*” are considered issues driven by the attached SLO or SLA and are expected to be delivered within our stated policy. There are two items that fall into Forced Prioritization: Any issues outside of these labels are to be prioritized using cross-functional prioritization . Auto-scheduling issues based on automation or triage policies are not forced prioritization. These issues can be renegotiated for milestone delivery and reassigned by the DRI. While we have moved to the cross-functional prioritization process to empower teams to determine the optimal balance of all types of issues, we will keep Engineering Allocations as a way to allow teams to quickly shift to a critical priority, designating the EM as the DRI to drive the effort. Engineering is the DRI for mid/long term team efficiency, performance, security (incident response and anti-abuse capabilities), availability, and scalability. The expertise to proactively identify and iterate on these is squarely in the Engineering team. Whereas Product can support in performance issues as identified from customers. In some ways these efforts can be viewed as risk-mitigation or revenue protection. They also have the characteristic of being larger than one group at the stage level. Development would like to conduct an experiment to focus on initiatives that should help the organization scale appropriately in the long term. We are treating these as a percent investment of time associated with a stage or category. The percent of investment time can be viewed as a prioritization budget outside normal Product/Development assignments. Engineering Allocation is also used in short-term situations in conjunction and in support of maintaining acceptable Error Budgets for GitLab.com and our GitLab-hosted first theme. Unless it is listed in this table, the Engineering Allocation for a stage/group is 0% and we are following normal prioritization . Refer to this page for Engineering Allocation charting efforts. Some stage/groups may be allocated at a high percentage or 100%, typically indicating a situation where all available effort is to be focused on Reliability related (top 5 priorities from prioritization table ) work. During an Engineering Allocation, the EM is responsible for recognizing the problem, creating a satisfactory goal with clear success criteria, developing a plan, executing on a plan and reporting status. It is recommended that the EM collaborate with PMs in all phases of this effort as we want PMs to feel ownership for these challenges. This could include considering adding more/less allocation, setting the goals to be more aspirational, reviewing metrics/results, etc. We welcome strong partnerships in this area because we are one team even when allocations are need to resolving issues critical to our business. During periods of Engineering Allocation, the PM remains the interface between the group and the fields teams & customers. This is important because: Each allocation has a direction page maintained by the Engineering Manager. The Engineering Manager will provide regular updates to the direction page. Steps to add a direction page are: To see an example for an Engineering Allocation Direction page, see Continuous Integration Scaling . Once the Engineering Allocation is complete, delete the direction page. One of the most frequent questions we get as part of this experiment is “How does a problem get put on the Engineering Allocation list?”. The short answer is someone makes a suggestion and we add it. Much like everyone can contribute, we would like the feedback loop for improvement and long terms goals to be robust. So everyone should feel the empowerment to suggest an item at any time. To help with getting items that on the list for consideration, we will be performing a survey periodically. The survey will consist of the following questions: We will keep the list of questions short to solicit the most input. The survey will go out to members of the Development, Quality, Security. After we get the results, we will consider items for potential adding as an Engineering Allocation. Once the item’s success criteria are achieved, the Engineering Manager should consult with counterparts to review whether the improvements are sustainable. Where appropriate, we should consider adding monitoring and alerting to any areas of concern that will allow us to make proactive prioritizations in future should the need arise. The Engineering Manager should close all related epics/issues, reset the allocation in the above table to the floor level, and inform the Product Manager when the allocated capacity will be available to return their focus to product prioritizations. When reseting a groups Engineering Allocation in the table above, the goal should be set as floor % , the goal should be empower every SWEs from raising reliability and security issues , percentage of headcount allocated should be 10% , and N/A in place of a link to the Epic. All engineering allocation closures should be reviewed and approved by the VP of Development . A Feature Change Lock (FCL) is a process to improve the reliability and availability of GitLab.com. We will enact an FCL anytime there is an S1 or public-facing (status page) S2 incident on GitLab.com (including the License App, CustomersDot, and Versions) determined to be caused by an engineering department change. The team involved should be determined by the author, their line manager, and that manager’s other direct reports. If the incident meets the above criteria, then the manager of the team is responsible for: If the team believes there does not need to be an FCL, approval must be obtained from either the VP of Infrastructure or VP of Development. Direct reports involved in an active borrow should be included if they were involved in the authorship or review of the change. The purpose is to foster a sense of ownership and accountability amongst our teams, but this should not challenge our no-blame culture. Rough guidance on timeline is provided here to set expectations and urgency for an FCL. We want to balance moving urgently with doing thoughtful important work to improve reliability. Note that as times shift we can adjust accordingly. The DRI of an FCL should pull in the timeline where possible. The following bulleted list provides a suggested timeline starting from incident to completion of the FCL. “Business day x” in this case refers to the x business day after the incident. During the FCL, the team(s) exclusive focus is around reliability work , and any feature type of work in-flight has to be paused or re-assigned. Maintainer duties can still be done during this period and should keep other teams moving forward. Explicitly higher priority work such as security and data loss prevention should continue as well. The team(s) must: After the Incident Review is completed, the team(s) focus is on preventing similar problems from recurring and improving detection. This should include, but is not limited to: Examples of this work include, but are not limited to: Any work for the specific team kicked off during this period must be completed, even if it takes longer than the duration of the FCL. Any work directly related to the incident should be kicked off and completed even if the FCL is over. Work paused due to the FCL should be the priority to resume after the FCL is over. Items created for other teams or on a global level don’t affect the end of the FCL. A stable counterpart from Infrastructure will be available to review and consult on the work plan for Development Department FCLs. Infrastructure FCLs will be evaluated by an Infrastructure Director. Please also note the corresponding Engineering handbook section about the relative importance and prioritization of availability, security, and feature velocity. To ensure we’re providing an appropriate focus on security, data loss, and availability, PMs should consider: To help PMs plan, stage group stable counterparts can participate in prioritization sessions. They serve mainly as an internal sensing mechanism for PMs to make more informed prioritization decisions for different planning horizons. Usually, teams focus on the product releases horizon, but can also focus on the FY themes or strategy horizons. This group exercise also boosts team morale, improves communication and empathy, and broadens individual’s perspectives. Besides, it can be a more informal and joyful way of connecting the team and discussing work. The output of these sessions is a priority matrix that shows the relative priority of a set of items based on two weighted criteria. Generally, the criteria are importance and feasibility , each one visualized as an axis of the matrix. You can change the criteria depending on the planning horizon or goals. To better understand how the sessions work, see an example mural and session recording . Always consider asynchronous sessions first, in an effort to be more inclusive and respectful of others time. That said, if possible, synchronous sessions can be ideal, as they allow limiting the time spent and make great use of the activities’ momentum for a more efficient discussion and voting. Use our Mural template for prioritization sessions , built for product releases but adaptable for other planning horizons or criteria. Adapt this process as needed, and consider changing it to an asynchronous mode of communication. For example, participants can review the items async, add questions as comments in Mural , and vote using dot voting or in voting sessions held on different days for each criterion. RICE is a useful framework for prioritization that can help you stack rank your issues. The RICE framework is a great tool for prioritizing many issues that seem to be of equal value at first glance. In order to drive clarity and alignment in the prioritization of work across the entire DevOps platform, and to help prioritize items that may compete for resources from different teams, we have set a standard for the RICE factors so all prioritization decisions based on RICE are using the same metric. Reach How many customers will benefit in the first quarter after launch? Data sources to estimate this might include qualitative customer interviews, customer requests through Support/CS/Sales , upvotes on issues, surveys, etc. Higher reach means a higher RICE score: Impact How much will this impact customers and GitLab? Impact could take the form of increased revenue, decreased risk, and/or decreased cost (for both customers and GitLab). This makes it possible to compare revenue generating opportunities vs. non-revenue generating opportunities. Potential for future impact should also be taken into account as well as the impact to the GitLab brand (for example unlocking free-to-paid conversion opportunities). Higher impact means a higher RICE score: Confidence How well do we understand the customer problem? How well do we understand the solution and implementation details? Higher confidence means a higher RICE score. Effort How many person months do we estimate this will take to build? Lower effort means a higher RICE score. These four factors can then be used to calculate a RICE score via the formula: (Reach x Impact x Confidence) / Effort = RICE Here is an example RICE calculation you can use to help prioritize work in your area. Feel free to embed this at the Epic level to provide context for why you did or did not prioritize. Other important considerations: We schedule a prioritized issue by assigning it a milestone; for more on this see Planning a Future Release. Conducting a RICE prioritization exercise with your cross-functional counterparts is a powerful way to make the process more inclusive and improve the quality of your rankings. Consider making this an async-first process to accommodate team members across different timezones. For an example of how to do this async-first, see this issue that the Geo team used to collaborate on a RICE prioritization exercise. This blank async RICE template is also available for you to copy for your own async prioritization exercise. For prioritizing most issues, we should utilize the RICE framework noted above , which will capture an aggregate of customer demand. You can also augment RICE scores with the Customer Issues Prioritization Framework Dashboards : Customer Requested Issues (Product) for product managers Customer Requested Issues (CSM) for Sales, CS and CSM These dashboards provide several inputs for calculating RICE and aggregate all customer requested issues and epics into a single dashboard. These dashboards are not meant as a replacement or sole input for Top ARR Drivers for Sales/CS . Further requirements such as the integration of themes need to be implemented before this framework can be used to fully inform or replace tools such as the Top ARR tracker . In some cases however, we may become aware of a feature which is particularly important to deliver on by a certain date. Examples of this could include an issue necessary to embark on a new GitLab rollout, a feature needed by a partner to launch an integration, or a method to import data from a service which is being discontinued. In these instances, the responsible PM can apply the customer or customer+ label along with a due date and initial milestone . This set of labels can serve to indicate externally that the issue is particularly important, as well as a reminder for internal teams of its importance. It is important to note that the customer and/or customer+ label does not constitute a promise for the issue to be delivered in any given milestone or time frame. GitLab is open-source, encouraging and promoting a large ecosystem of contributors is critical to our success. When making prioritization decisions, it’s important to heavily weight activities which will encourage a stronger community of contributors. Some of those activities are: Product managers are not responsible for prioritizing contributions outside of their group. These contributions should be reviewed and merged swiftly allowing everyone to contribute, including non-product teams at GitLab. The SaaS-First product investment theme will put us in a better position to support our customer base who is expected to accelerate adoption of SaaS products in the coming years. Features will also end up more secure, resilient, performant, and scalable for our self-managed customers if initially built to the expectations of SaaS. Therefore, it is important for PMs to understand and prioritize needs related to the SaaS business. When prioritizing SaaS related issues, we follow the same guidelines above . Within those guidelines there are a few areas that are especially important for PMs to focus on to ensure the success of our SaaS users. Downtime of GitLab.com has a material impact on our customers. From a 2014 report Gartner estimates that downtime costs companies on average “$5,600 per minute, which extrapolates to well over $300K per hour.” Furthermore, SaaS downtime can severely disrupt the productivity of GitLab Inc since we rely heavily on GitLab.com to run our business. Finally, downtime can also lead to customer churn and damage to our reputation. Thus, it is crucial as a company we collectively work towards consistently maintaining our 99.95% SLA on GitLab.com . There are a few things that PMs can do in partnership with their engineering team to help ensure overall Availability for GitLab.com. The infradev process is used to triage issues requiring priority attention in support of SaaS availability and reliability. As part of the broader effort to responsibly manage tech debt across the company, PMs should partner with their EMs to identify and incorporate infradev labeled issues of all severities . Note, issues labeled with a severity must be mitigated and resolved within specific time frames to meet the SLO. As EMs are the DRIs for prioritizing infradev work, PMs should familiarize themselves with the infradev process and Board . Other resources PMs can consult to identify and prioritize Infradev issues include: While not required, PMs are encouraged to listen in on Incident Management calls for incidents related to their product areas to 1) build empathy with the SRE team by gaining insight into how they handle incidents 2) gain a better sense of the impact of the incident to their customer base, and 3) identify improvements to their product areas, whether technical or feature-related, that could have prevented the incident. PMs are not expected to be in the decision-making path on actions taken to resolve the incident. They are there to listen and learn rather than attempting to decide/influence the course of resolution. After incidents involving their product area, PMs are also encouraged to engage in the Incident Review , including attendance at the Sync Incident Review call if their incident is scheduled. PMs can periodically review incidents via the Production Incident Board Enterprise customers interested in adopting SaaS may have common hard requirements to be able to use the product. For example, large enterprises may need certain security related features, such as Audit Logs, available before their security team will agree to the use of GitLab.com. This can also be about more than just features; it may include how and where we apply features so they can administrate their GitLab instance at enterprise-scale. For instance, permission management and shared configurations are best implemented top-down first instead of Project-up to meet the requirements of large organizations who may have 100s or 1000s of projects and only a small handful of people to perform these system-wide administrative tasks. In order to encourage more Enterprise adoption of GitLab.com, prioritize these common “hard-blockers” to adoption over “nice to have” features. PMs can use customer interviews to hone in on which issues are hard blockers to adopting SaaS vs more “nice to have” features that can be delivered later. To track hard adoption blockers, use the ~“GitLab.com Enterprise Readiness” label within the GitLab-Org and GitLab-com groups. There are a few special considerations when it comes to delivering features for SaaS. In order to achieve parity between SaaS and Self-managed installations PMs should prioritize efforts to eliminate existing feature gaps that exist across the two installations . Additionally, new features should ship for SaaS and self-managed at the same time. Features should be implemented at the group level first, before being implemented at the instance level, so that they will work across both self-managed and SaaS . Finally, in order for new features to be adequately monitored, they should include appropriate logging and observability , which makes troubleshooting much easier. As a product manager, you will be assigned as the stable counterpart to a single group . At GitLab we abide by unique, and extremely beneficial guidelines when interacting with our groups. These include: As an all-remote company, our crispness when it comes to responsibilities throughout the Product Delivery process was born out of necessity, but it pays untold dividends. Some of the benefits include: As described above, prioritization is a multi-faceted problem. In order to translate the priorities of any given group into action by our engineering teams, we need to be able to translate this multi-faceted problem into a flat list of priorities for at least the next release cycle. Product Managers are responsible for taking all these prioritization considerations and creating a clear, sequenced list of next priorities. This list should be represented as an issue board so that each team has a clear interface for making decisions about work. From this list, Product Designers, Engineering Managers and Product Managers can work together to determine what items will be selected for work in the immediate future. This does not mean that items will be addressed in strict order - Product Designers, EMs and PMs need to be cognizant of dependencies, available skill sets, and the rock/pebbles/sand problem of time management to make the best decisions about selecting work. Together with your Engineering Manager, you will have an important role in ensuring that the Build Plans defined for issues are created with iteration in mind. Iteration is highly valuable for the following reasons: As a company we emphasize velocity over predictability . As a product manager this means you focus on prioritizing, not scheduling issues. Your engineering stable counterparts are responsible for velocity and delivery. However, there are instances when there is desire for predictability, including: As the DRI for milestone prioritization , it is the Product Manager’s job to prioritize for predictability when it is needed. You should do so by ensuring you prioritize a deliverable, and its dependencies, so that it can reasonably be expected to be delivered by any committed dates. If there is time pressure to hit a date, the PM should also explore de-scoping the issue to meet the deadline, rather than pressuring engineering to move abnormally fast or cut corners. These information sources may be useful to help you prioritize. Individual product managers must consider, and advocate for global optimizations within the teams they are assigned to. If your assigned team requires expertise (remember everyone can contribute) outside the team you should make all reasonable efforts to proceed forward without the hard dependency while advocating within the product management team for increased prioritization of your now soft dependencies. Execution of a Global prioritization can take many forms. This is worked with both Product and Engineering Leadership engaged. Either party can activate a proposal in this area. The options available and when to use them are the following: We have found the following methods less successful in ensuring completion of work that warrants global prioritization: As a PM, you must plan for the near term milestones (more detailed) as well as for the long term strategy (more broad), and everything in between. While monthly milestone planning is done in GitLab, longer horizon planning (1-3 years) is done in direction pages. This will enable you to efficiently communicate both internally and externally how the team is planning to deliver on the product vision . Documenting a Section, Stage, Group and Category direction is critical to communicating where we are heading and why to all of our stakeholders. This is especially important to the members of your Product Group. Establishing a direction for stakeholders (including team members) to participate in, and contribute to ensures there is a concrete connection to “Why” we are iterating and how it furthers GitLab’s mission . Here are some of those connections: As a Product Manager you can highlight these connections in: Communicating this connection requires a multi-channel approach. We should strive to share and communication about the connection to our Direction warrants consistent reinforcement. Section leaders are responsible for maintaining Direction pages that lay out the strategy and plan for their respective section and stages. The direction pages should include topics outlined in this template . A category strategy is required which should outline various information about the category including overall strategy, status, what’s next, and the competitive landscape. The category strategy should be documented in a handbook page, which allows for version control of the category strategy as well as the ability to embed video assets. One of the most important pieces of information to include in the category strategy is a tangible next step or MVC and a clear description of focus and out-of-focus/maintenance areas. Your category strategies should contain short paragraphs with lots of references to specific epics and issues. Referencing topics, instead of features is encouraged as it’s more stable over time. We use this category strategy template as the outline for creating the handbook pages. If additional headings are needed you are empowered to create and populate them in your category strategy. You must keep these categories in sync with categories.yml and for new categories. Category direction should be reviewed on a regular basis (at least monthly) by the responsible product manager. To indicate the last time a category direction page was reviewed, please ensure pages include Content Last Reviewed: yyyy-mm-dd at the top of the category content. Update this date with every review, even if other content on the direction page has not changed. You should link to your category strategy from your stage strategy page. For categories that have already shipped, and that have a marketing product page, categories.yml should link to the product page. Inside of the categories.yml file there are dates assigned for either achieved or anticipated maturity achievement. These should be kept inline with communicated dates for achievement and updated as required. If the category has developed a UX Roadmap we recommend the product designer to create a merge request to incorporate UX Roadmap themes into the category direction page roadmap. Assign the MR to the PM for review and merge. In some cases there may be direction pages that span multiple stages or sections. A direction page that summarizes the collective vision as well as all the contributors of that direction is critical to maintain transparency and adequate assignment of ownership. There are several examples of these types of direction pages today: The steps for creating and managing a cross-section or stage direction are: Once the direction page has been added, there needs to be an assigned DRI for maintaining monthly updates for the page. It is the DRIs responsibility to ensure the shared direction page is regularly reviewed and is up to date. This requires cross-section / cross-stage collaboration from the DRI. You should use the ~direction label together with category and section labels to mark epics and issues that fall into the given direction. Product Direction items (i.e., with the label) should be direction-level items that move the strategy forward meaningfully. This is up to the PM to set the bar for, but there should be a clear step forward with real user value. It’s important to note here that your plan is not simply a list of new features and innovation. Those are included for sure, but so are issues related to all of your sensing mechanisms . A category upgrade from minimal to viable or delivery of a top customer issue (for example) can contribute to your plan just as much as a brilliant new innovative feature can. It’s up to PMs to balance this through a coherent longer-term strategy. Conversely, in a broad sense anything could move the plan forward in a general way. Finally, issues are the substance of your plan. Ensure you are applying the label to both revelant epics and its issues. As product managers, a core job is to set the correct expectations. We do this typically through discussing our direction and assigning issues to milestones. When you need to communicate specific dates, it’s recommended doing it with limited visibility internally or directly to the customers. When you need to communicate specific dates use calendar year (CY) dates . Fiscal year (FY) does not translate well outside the company. Accordingly, the direction pages are expected to refer to specific issues only for the next 3-4 months. Everything beyond that should discuss the topic, not specific issues. Creating a thoughtful direction for your section, stage, or category is a useful thought exercise that can help focus efforts, aid in prioritization, and get large groups of people on the same page. But beware of simply executing your long term plan. Our industry is incredibly dynamic, and we learn new things every day that can and should cause us to re-think our long term plans. We should ship what brings value to our customers, not what is easy to ship. Stay focused on creating value each and every milestone, and be quick to adjust your longer term direction as you learn more. For each category, we recommend tracking the improvements required to advance to the next level of maturity . You are welcome to track maturity plans either with ~maturity::... labels or maturity issues. Maturity plans are highly encouraged - but not required - for non-marketing categories. Product groups may choose to have OKRs . To use them effectively, you should have plans for the next three months in terms of driving specific product metrics through discovery and delivery actions. You should discuss the product metrics with your manager, your design and engineering counterparts and the actions to reach the results with your design and engineering counterparts. For each milestone, the planning quads come together to scope and plan work for the group for the upcoming milestone. Planning begins asynchronously with the creation of the planning issue. The planning issue is the SSOT for communication and all resources that are needed to plan a successful milestone. There are many ways to achieve to plan a milestone that should be curated based on the needs of the team. Below are a few examples of planning issues from groups acorss R&D to aid you in creating one that works best for your team. As you adapt your own issue, it is recommended you apply the label planning issue to aid in tracking and to incorporate our Product Principles into the process. Refer to the Product Development Timeline for details on how Product works with UX and Engineering to schedule and work on issues in upcoming releases. There are two non-exclusionary ways to plan and communicate work for future releases As a Product Manager you can maintain prioritization of your groups issues using a fully prioritized issue board where the ordering of the issues reflects their priority. Product Managers can assign milestones to issues to indicate when an issue is likely to be scheduled and worked on. Still, whether an issue can be delivered within a milestone is the decision of the engineering team. As we consider more distant milestones, the certainty of the scope of their assigned issues and their implementation timelines is increasingly vague. In particular, issues may be moved to another project, disassembled, or merged with other issues over time as they bounce between different milestones. The milestone of an issue can be changed at any moment. The current assigned milestone reflects the current planning, so if the plan changes, the milestone should be updated as soon as possible to reflect the changed plan. We make sure to do this ahead of starting work on a release. Capacity is discussed between the PMs and the engineering managers. There are helper labels to signals these plans like ~next::1-3 releases and its variants. In addition, we have two special milestones: Backlog and Awaiting further demand . Product Managers assign these issues to milestones that they have reviewed and make sense, but do not fit within the upcoming release milestones due to either a lack of comparative urgency or because we have not yet seen enough user demand to prioritize the item yet. The best way to demonstrate urgency on either of these items is to vote on them and, if possible, add comments explaining your use case and why this is important to you. Recommendation for when to change ‘Awaiting further demand’: Always focus on the overall value of the feature. Do you have a good understanding of the user problem? Do you have a good understanding of the impacted user base? Was the proposed solution validated? Issues with the ‘Awaiting further demand’ label often mean poorly understood requests that require more information from our users and the market. Often public feedback only comes from a small percentage of people using or evaluating a feature or product. You should always consider reaching out directly to our users to learn more about their use cases. Recommendation when changing a previously planned issue to Backlog : When moving a previously planned issue to Backlog , especially one planned for within the next release or two, consider the message that this may be sending to parties that were interested in this feature. In some cases, they may have been depending or planning upon the issue to be delivered around the assigned milestone, and with the change to Backlog that is now unlikely to occur. In these instances, it is best to concisely explain the rationale behind the change in a comment, so the community can understand and potentially respond with additional justification or context. It is also encouraged to move the issue to the Backlog as soon as it is clear that it will not be scheduled in the near future. This will help with understanding the change, as it will not seem like a last minute change. Communicating clearly changing priorities might encourage the community to contribute the issue to GitLab. Again, the milestone of an issue can be changed at any moment, including for both of these special milestones. From time to time, there may be circumstances that change the ability for a team to ship the features/issues they committed to at the beginning of the iteration. These steps also apply when an issue is broken into multiple issues. When this happens, as a PM you must coordinate with your EM counterpart that the impacted issues and their milestones are updated to reflect the new reality (for example, remove deliverable tag, update milestone , etc.). Additionally, notify your manager of the shift. Our design system provides the means to work autonomously, without always needing UX insight, feedback and design. When problems can be solved using an already documented paradigm, you don’t need to wait for UX approval to bring an issue to a reasonable state within a first iteration. If lingering questions remain, subsequent iterations can address any shortcomings the feature might have. Always consider that with a dedicated product designer, it’s much faster and cheaper to iterate on a design than to re-implement it. At the same time, not everything needs a design, and the design system is here to support your engineers and you in those cases. Iteration is a core value of GitLab, and product management has a central role to play in it. Iteration should be apparent as we deliver new features in MVCs, but it has implications for discovery too. As solution validation can move much faster than delivery, we should aim to validate features before building them. At this point, the feature validated is likely way bigger than an MVC if we would build it. We should pay special attention as product managers to still aim at iterative delivery after a bigger feature-set got validated, as delivered features provide the final validation. For example, once a direction is validated, we can start the delivery by documentation. As product managers we should aim to iterate as part of solution validation, and while delivering already validated solutions too. Here are several strategies for breaking features down into tiny changes that can be developed and released iteratively. This process will also help you critically evaluate if every facet of the design is actually necessary. As part of design and discovery, you likely created a minimal user journey that contains sequential steps a user is going to take to “use” the feature you are building. Each of these should be separated. You can further by asking yourself these questions: View, Create, Update, Remove and Delete are actions users take while interacting with software. These actions naturally provide lines along which you can split functionality into smaller features. By doing this, you prioritize the most important actions first. For example, users will likely need to be able to visually consume information before they can create, update, remove, or delete. Often, the criteria for features are built on is implicit. It can help to use a test-driven development mindset where you write the tests and the outcomes you need from the software before building the software. Writing these tests can uncover the different criteria you need the development team to meet when building the new feature. Once you’ve outlined these tests, you may be able to use them to continue to break down the feature into smaller parts for each test. Here are a few examples: Software often fails and can fail in different ways depending upon how it is architected. It is always best to provide the user with as much information as possible as to why something did not behave as expected. Creating and building different states to handle all possible errors and exceptions can easily be broken down into individual issues. Start by creating a generic error state to display when anything goes wrong, and then add on to handle different cases one by one. Remember to always make error messages useful , and add additional error messages as you identify new error states. Breaking down a design into pieces that can be released iteratively is going to depend on what you are building. Here are a few helpful questions to guide that process: Continuously improving the software we write is important. If we don’t proactively work through technical debt and Deferred UX as we progress, we will end up spending more time and moving slower in the long run. However, it is important to strike the right balance between technical debt, deferred UX, and iteratively developing features. Here are some questions to consider: For large projects, consider separating the announcement from the actual feature launch. By doing so, it can create more freedom to iterate during the customer rollout. For example, you could announce in advance to give customers ample notice, and then roll it out to new customers first, then to existing Free customers, then to existing paid customers. Or you could do the opposite, and roll it out to customers first, before announcing broadly, to ensure the user experience is great before making a marketing splash. When considering dates for a product announcement or launch that may impact our Field team, consider the blockout restrictions recognized by the Field team to ensure there won’t be any major disruption to the business near quarter end. Sometimes the objective is to cut over from one experience, or one system, to another. When doing so, consider having four transition phases rather than a hard cut over. The phases are: 1) experienced experience. 2) Run the experienced experience and new experience side-by-side, with the experienced experience the default, and the new experience is gradually rolled out to a subset of users. 3) Run them side-by-side, with the new experience the default for the majority, but the experienced experience is still available as a fallback in case of problems. 4) Deprecate the experienced experience and offer only the new experience. This strategy enables teams to have more flexibility and demonstrate more iteration in the rollout, with reduced risk. When something is important, it is natural to want to launch it all at once to get to the end game faster. However, big bang style launches tend to need everything perfect before they can happen, which takes longer. With iteration you get feedback about all the things that aren’t a problem and are done enough. It’s better to launch in small increments, with a tight feedback loop, so that the majority of users have a great experience. This tends to speed up the overall timeline, rather than slow it down. A Design Sprint , is a 5-day process used to answer critical business questions through design, prototyping and testing ideas with customers. This method allows us to reduce cycle time when coming up with a solution. As an all-remote company we run Remote Design Sprints (RDS) . Check out our guidelines for running an RDS to determine if it’s the right approach for the problem at hand. If you’re faced with a very large or complex problem, and it’s not clear how to most efficiently iterate towards the desired outcome, consider working with your engineers to build an experimental spike solution . This process is also sometimes referred to as a “technical evaluation.” When conducting a spike, the goal is write as little code within the shortest possible time frame to provide the level of information necessary the team needs to determine how to best proceed. At the end of the spike, code is usually discarded as the original goal was to learn, not build production-ready solutions. This process is particularly useful for major refactors and creating architecture blueprints . If the spike involves changes that will require approvals from other teams, consider engaging with relevant stakeholders and domain experts early to validate your approach. When launching a feature that could be controversial or in which you want to get the audience’s feedback, it is recommended to create a feedback issue. Here are some examples of feedback issues: Feedback issues are intended to collect feedback from the wider community and users. In some cases, internal user will be posting on behalf of users and customers. As a result we need to consider the following: Consider the following to improve iteration: Engaging directly with the community of users is an important part of a PM’s job. We encourage participation and active response alongside GitLab’s Developer Relations team . A general list of conferences the company is participating in can be found on our corporate marketing project. There are a few notable conferences that we would typically always send PMs to: If you’re interested in attending, check out the issue in the corporate marketing site and volunteer there, or reach out to your manager if you don’t see it listed yet. A stakeholder, or stable counterpart, is someone that is outside of your direct team who meets one or more of the following: Examples of stakeholders include Leadership, Sales, Marketing, Customer Support, and Customer Success. You may have stakeholders in any area of GitLab depending on your focus area and the specific issue. Stakeholders are also present outside of GitLab, for example, when a feature is being developed for a specific customer or set of customers. If you’re not sure who the stakeholder is to collaborate with or keep informed, visit product sections, stages, groups, and categories . Stakeholder collaboration and feedback is a critical competitive advantage here at GitLab. To ensure this is possible, and facilitate collaboration, you should maintain an updated single source of truth (SSOT) of your stage direction, category strategies, and plan, at all times. This equips anyone who wants to contribute to your stage’s product direction with the latest information in order to effectively collaborate. Some sections and teams use the scheduled Direction Update issue template to remind themselves of this task. Actively and regularly reach out to stakeholders. Encourage them to view and collaborate on these artifacts via these (non-exhaustive) opportunities: Here is some guidance for new PMs to ensure your stage direction, category strategies and plan are up-to-date and visible to critical stakeholders: It’s important to get direct feedback from our customers on things we’ve built, are building, or should be building. Some opportunities to do that will arise during sales support meetings . As a PM you should also have dedicated customer discovery meetings or continuous interviews with customers and prospects to better understand their pain points. As a PM you should facilitate opportunities for your engineering group to hear directly from customers too. Try to schedule customer meetings at times that are friendly to your group, invite them, and send them the recording and notes. If you’re looking for other ways to engage with customers here is a video on finding, preparing for, and navigating Customer Calls as a Product Manager at GitLab . Before the meeting , ensure the Sales lead on the account has provided you with sufficient background documentation to ensure a customer doesn’t have to repeat information they’ve already provided to GitLab. During the meeting , spend most of your time listening and obtaining information. It’s not your job to sell GitLab, but it should be obvious when it’s the time to give more information about our products. For message consistency purposes, utilize the Value Drivers framework when posing questions and soliciting information. Customer discovery meetings aren’t UX Research. Target them to broad-based needs and plan tradeoff discussions, not specific feature review. There are two primary techniques for targeting those topics: Follow the below guidance to prepare and conduct Customer Discovery Meetings: You can find some additional guidance on conducting Customer Discovery Meetings from these resources: PMs should also feel free to collect and evaluate customer feedback independently. Looking at existing research can yield helpful themes as well as potential customers to contact. You can use the following techniques to source customers directly: GitLab Solution Architects know our customers the best, especially from a technical perspective. GitLab Issues customers will often comments on issues, especially when the problem described by the issue is a problem they are experiencing firsthand. The best strategy is to capture their feedback directly on the issue, however, there are times when this is not possible or simply doesn’t happen. You can find alternative contact info by clicking on the user’s handle to see their GitLab user page; this page often includes contact information such as Twitter or LinkedIn. Another option is to directly mention users in issues to engage async. In popular issues you can just leave a general comment that you’re looking for people to interview and many will often volunteer. Customer Issues Prioritization Dashboards: The customer issues prioritization framework aggregates customer data with the issues and epics that they have requested. When viewing the dashboard , double click on the issue or epic of interest within the “priority score by noteable” table then scroll down to “QA Table - User request weighting by customer” to see the specific customers that are interested in the issue or epic. GitLab.com Broadcast Messages Broadcast Messaging is a great tool for acquiring customer feedback from within the product. You can leverage this workflow to use broadcast messaging. GitLab Sales and Customer Success You can ask for help in Slack customer success channel or join the Field Sales Team Call and the All CS Team Call to present a specific request via the Zoom call. Customer Success Managers (CSM) If a customer has a dedicated CSM, they may also have a regular meeting with a CSM. These meetings are a great opportunity to spend 15 minutes getting high-level feedback on an idea or problem. In Salesforce, CSMs are listed in the Customer Success section in the customer’s account information. CSMs are also very familiar with the feature requests submitted by their customers and can help identify customers that may be interested in the feature you are working on. Zendesk is a great tool to find users who are actively making use of a feature and either came across a question or an issue. Users who’ve had recent challenges using the product really appreciate PMs taking the time to learn from their experience. This establishes that we are willing to listen to users, even if they are not having a great experience. This is also a great opportunity to discuss the roadmap and provide context so that users understand what we are going to improve. The best way to request a chat is through the support ticket; however, you can also click on the user that initiated the interaction and their contact information will display on the left hand side panel. If you don’t have a Zendesk account, see how to request a light agent Zendesk account . You can use Zendesk’s trigger feature to receive email alerts when specific keywords relevant to your product area are mentioned in a support ticket. Additionally, it is possible to create a simple dashboard that lists all the currently active support tickets that match the trigger. Reach out in #support_escalations to receive some help in setting this up. Social Media can also be effective. If your personal account has a reasonable number of connections/followers, you can post your desire to connect with users on a specific question directly. When posting, remember to include the subject you want to discuss as well as how people can reach out. You can also reach out to the #social-media channel to have your tweet retweeted by the @gitlab account. If you want to reach a wider audience, consider asking a community advocate to re-post using the official GitLab account for the relevant platform. You can reach advocates on the #community-advocates Slack channel. You can also reach out to authors of articles related to tech your team is working on, via various publications such as Medium . A clear and brief email via the publication website or LinkedIn is a good way to engage. You’re able to request a LinkedIn Recruiter license . This Unfiltered video and slide deck provide an overview on how to use LinkedIn Recruiter to source participants for your study. If you’ve tried these tactics and are still having challenges getting the customer feedback you need, connect with your manager for support and then consider leveraging the UX Research team . Additionally, you can connect with Product Operations directly or by attending Product Operations Office Hours for troubleshooting support. Non-users are often more important than GitLab users. They can provide the necessary critical view to come up with ideas that might turn them into GitLab users in the end. The best non-users are the ones who don’t even plan on switching to GitLab. You can reach these people at local meetups, conferences or online groups like, Hacker News. In every such case, you should not try to interview the user on spot, instead organize a separate meeting where nobody will be distracted, and both of you can arrive prepared. One specific, recurring opportunity to get direct feedback from highly engaged customers is the GitLab DevOps Customer Advisory Board . You may be asked by the CAB to present your stage at these meetings. Here are some guidelines when doing so: You may be asked by the CAB to present your stage or a specific product offering at these meetings. Here are some guidelines for presenting: Product Focused Highlights: All presentation materials should be focused on products we plan to launch or evaluating products we have available to customers. Emphasize Dialogue over Monologue: Structure your presentation to encourage meaningful two-ways discussions. Prepare Targeted Questions: Develop 2-3 specific, through provoking questions to engage members in conversation. These questions should be focused on presentation, strategic decisions GitLab is currently grappling in your stage that you would like to gather customer feedback on, or a question related directly to customer workflows. Connect to Previous Feedback: Reference previous feedback you have received from advisory meetings in the past. This will help illustrate to CAB members the value of their time and that GitLab takes their recommendations into consideration. Prompt Follow Through: Document key insights and actions items during your session. Be Prepared: Be sure to prepare for the meeting ahead of time independently. Please review GitLab Product Customer Advisory Board Page for more details. When someone requests a particular feature, it is the duty of the PM to investigate and understand the need for this change. This means you focus on what is the problem that the proposed solution tries to solve. Doing this often allows you to find that: Do not take a feature request and just implement it. It is your job to find the underlying use case and address that in an elegant way that is orthogonal to existing functionality. This prevents us from building an overly complex application. Take this into consideration even when getting feedback or requests from colleagues. As a PM you are ultimately responsible for the quality of the solutions you ship, make sure they’re the (first iteration of the) best possible solution. When someone posts information in the #competition channel that warrants creating an issue and/or a change in features.yml , follow this procedure: You may want to interview a specific account because they are exhibiting atypical usage patterns or behaviors. In this case, request Support to contact GitLab.com user(s) on your behalf . If it is the weekend, and the contact request is urgent as a result of an action that might affect a users’ usage of GitLab, page the CMOC One of the primary artifacts of the validation track is the Opportunity Canvas. The Opportunity Canvas introduces a lean product management philosophy to the validation track by quickly iterating on level of confidence, hypotheses, and lessons learned as the document evolves. At completion, it serves as a concise set of knowledge which can be transferred to the relevant issues and epics to aid in understanding user pain, business value, the constraints to a particular problem statement and rationale for prioritization. Just as valuable as a validated Opportunity Canvas is an invalidated one. The tool is also useful for quickly invalidating ideas. A quickly invalidated problem is often more valuable than a slowly validated one. Please note that an opportunity canvas is not required for product functionality or problems that already have well-defined jobs to be done (JTBD) . For situations where we already have a strong understanding of the problem and its solution, it is appropriate to skip the opportunity canvas and proceed directly to solution validation. It might be worth using the opportunity canvas template for existing features in the product to test assumptions and current thinking, although not required. Reviewing opportunity canvases with leadership provides you with an opportunity to get early feedback and alignment on your ideas. To schedule a review: Opportunity Canvases are a great assessment for ill-defined or poorly understood problems our customers are experiencing that may result in net new features. As noted previously, opportunity canvases are helpful for existing features, except they are tailored for new feature development which is where the Product-Opportunity-Opportunity-Canvas-Lite issue template delivers. This template offers a lightweight approach to quickly identify the customer problem, business case, and feature plan in a convenient issue. The steps to use the template are outlined in the Instructions section and for clarity, one would create this issue template for an existing feature they are interested in expanding. For example, this template would be great to use if you are evaluating the opportunity to add a third or fourth iteration to an MVC. This issue should leverage already available resources and be used to collate details to then surface to leadership for review. Once you fill out the template, you will assign to the parties identified in the issue and you can always post in the #product channel for visibility. Part of being a product manager at GitLab is maintaining engagement with analysts, culminating in various analyst reports that are applicable to your stage. In order to ensure that this is successful and our products are rated correctly in the analyst scorecards, we follow a few guidelines: It’s important to be closely connected with your product marketing partner, since they own the overall engagement. That said, product has a key role to play and should be in the driver’s seat for putting your stage’s best foot forward in the responses/discussions. Product managers should take advantage of the internal customers that their stage may have, and use them to better understand what they are really using, what they need and what they think is important to have in order to replace other products and use GitLab for all their flows. We want to meet with our internal customers on a regular basis, setting up recurring calls (e.g., every two weeks) and to invite them to share their feedback. This is a mutual collaboration, so we also want to keep them up to date with the new features that we release, and help them to adopt all our own features. Each quarter, we reach out to User Satisfaction (USAT) survey responders who opted-in to speak with us. This is a fantastic opportunity to build bridges with end users and for Product Managers and Product Designers to get direct feedback for their specific product area. If a user has taken the time to share a verbatim with us and offered to have a conversation, they deserve to be followed up with - especially if that user is dissatisfied with GitLab. When we speak to users directly during this workflow, we must be mindful of Product Legal guidance and the SAFE framework , just as we would be with any other documentation or communication within Product. Note: GitLab Customer Success Managers can also follow the process above, so please be mindful to coordinate with them if they reach out or if they’ve already signed up to speak with a user. Users should never be contacted by more than one GitLab team member. Users should never be contacted more than twice if they do not respond to an outreach email. Hello, My name is X and I’m the Product Manager/Designer for X at GitLab. Thank you for giving us the opportunity to follow up on your response to our recent survey. I would be very interested in speaking further about some of the points you raised in your survey response. Would you be willing to do a 30 minute Zoom call to give us some more detailed feedback on your experience using GitLab? You’d be able to schedule the call at a time convenient to you. Schedule a time for the call using this link: https://calendly.com/yourname/30min Thank you for your feedback and let me know if you have any questions. Copy for three extra questions in Calendly invite : To make sure we correctly represent what you say in any followup issues or discussions, we would like to record this conversation. Please indicate if you give permission to record this conversation. Yes, you may record our conversation. No, you MAY NOT record our conversation. At GitLab, we value transparency. We would love to share the recording of conversation publicly on GitLab. Please indicate whether you give your permission for the recording to be shared on GitLab. Yes, you may share the recording publicly on GitLab. No, you MAY NOT share the recording publicly on GitLab. I agree that by participating in this, and any future, research activities with GitLab, GitLab B.V. will retain all intellectual property rights in any suggestions, ideas, enhancement requests, feedback, or other recommendations I provide which are hereby assigned to GitLab B.V. Note: It’s important to tag your USAT related issues to help tracking/reporting such as the improvement slides in Product Key Reviews. Every Product Manager is responsible for the user experience and cost profile of their product area regardless of how the application is hosted (self-managed or gitlab.com). If a feature is unsustainable from a cost standpoint, that can erode the margins of our SaaS business while driving up the total cost of ownership for self-managed customers. If a feature is slow, it can impact the satisfaction of our users and potentially others on the platform. There are a few questions a Product Manager should ask when thinking about their features: These items do not all need to be implemented in an MVC, though potential costs and application limits should be considered for deployment on GitLab.com. Product Managers should also regularly assess the performance and cost of features and experiences that they are incrementally improving. While the MVC of the feature may be efficient, a few iterations may increase the cost profile. There are a few different tools PM’s can utilize to understand the operational costs of their features. Some of these are maintained by Infrastructure, based on the operational data of GitLab.com. Others tools, like service ping, can be utilized to better understand the costs of our self-managed users. Ultimately, each product group is responsible for ensuring they have the data needed to understand and optimize costs. When performing the role of Life Support PM only the following are expected: Some discouraged responsibilities: As a Product Manager you may need to make a decision on whether GitLab should engineer a solution to a particular problem, or use off the shelf software to address the need. First, consider whether our users share a similar need and if it’s part of GitLab’s scope. If so, strongly consider building as a feature in GitLab : If the need is specific to GitLab, and will not be built into the product, consider a few guidelines: If after evaluating these considerations buying a commercial solution is the best path forward: When considering open source software in build vs. “buy” decisions we utilize the following general criteria to decide whether to integrate a piece of software: Please see Analytics Instrumentation Guide Goal: Increase product instrumentation across our offerings to deliver greater product insights. There is a need to retroactively evaluate what features have been instrumented and need instrumentation from past feature launches. Post launch implementation will allow us to gather insights and allow better visibility into feature usage + adoption that may not currently be captured. In order to better understand the perceived performance of GitLab, there is a synthetic page load performance testing framework available based on sitespeed.io . A Grafana dashboard is available for each stage, tracking the Largest Contentful Paint and first/last visual change times. These metrics together provide high-level insight into the experience our users have when interacting with these pages. The Grafana dashboards are managed using grafonnet , making it easy to add additional pages and charts. Testing a new set of pages requires just 2 steps: Assign both MR’s to a maintainer. After they are merged, the stage’s Grafana dashboard will be automatically updated. A video walkthrough is available as well.",
  "GitLab’s product mission is to consistently create products and experiences that users love and value. To deliver on this mission, it’s important to have a clearly defined and repeatable flow for turning an idea into something that offers customer value. Note that it’s also important to allow open source contributions at any point in the process from the wider GitLab community - these won’t necessarily follow this process. This page is an evolving description of how we expect our cross-functional development teams to work, and reflects the current workflow being leveraged. All required actions or outcomes in this page are denoted as follows: Denotes a required aspect of the product development flow. Feature development is expected to pass through all phases to achieve specified outcomes, while the rest of the workflow should be considered as a set of best practices, tools, and recommendations. We realize there are unique cases in which certain product improvements may not need to flow through all the phases. We trust product managers to use their best judgement with alignment from their design and engineering team. The goal of this page is to support teams in their workflows by highlighting the necessary outcomes to target in each phase as well as sharing strategies/tactics, activities , teams can employ to achieve these outcomes. Additionally, this page aims to clarify the minimal set of required actions, such as labels, needed across all phases to keep the product system efficient in terms of tracking, searching and cross-functional collaboration. To maintain clarity and avoid confusion, we do not list optional actions on this page but teams may choose to employ additional actions, such as labels for planning, even if they are not mentioned on this page. As teams leverage the product development flow, they may find that certain strategies/tactics are serving their teams well toward success. Therefore, we welcome MRs to this page, so we can create a robust playbook of options to build valuable features for customers. All team members are encouraged to follow the change process for this page to share their best practices. No. Although the phases described on this page appear to be independent and linear, they are not. They’re presented in this way for simplicity and ease of navigation. At GitLab, we do not promote working in a linear manner. Phases in the product development lifecycle may overlap or occur in parallel. We aim to achieve key outcomes in each phase in order to de-risk subsequent phases. However, the product development flow doesn’t dictate the order we go through the phases, or the time spent in each. When teams have a high confidence in their direction, they should feel empowered to skip or shorten phases that won’t contribute to improved confidence. An engineering team conducts a technical review while other team members are performing Validation Phase activities. The team can then move to the Build phase rapidly with high confidence that their improvement is good for customers and technically feasible. A bug is reported by a GitLab customer. The Product Manager tests the bug and confirms its existence (Problem Validation). The team is extremely confident in the solution, so Design and Solution Validation are not needed. The bug is moved immediately to Build. Through the page, whenever a DRI is mentioned, the person referred to may be different depending on the phase, and there may be more than one. For who the DRI is, refer to the Product Development Roles and Responsibilities page . We use workflow labels to efficiently communicate an issue’s state. Using these labels enables collaboration across teams and communicates an issue’s current state. The workflow labels are prefixed with workflow:: , such as workflow::ready for development . The following diagram shows how a new issue moves to each workflow label, though states can be skipped when appropriate. The rest of this document describes each workflow step in detail. Issue descriptions shall always be maintained as the single source of truth. It’s not efficient for contributors to need to read every comment in an issue to understand the current state. Guidance on what and how to prioritize is covered in: For situations when the customer problem isn’t well understood , Product Managers (PMs) and the User Experience Department (UXers) should work together to validate new opportunities before moving to the Build track. The Validation track is an independent track from the always moving Build track. PMs and UXers should work together to get at least two months ahead, so that the Build track always has well-validated product opportunities ready to start. Milestone work should be prioritized with the understanding that some milestones may include more validation efforts than others. Validation cycles may not be necessary for things like bug fixes, well understood iterative improvements, minor design fixes, and technical debt. The types of activities and depth of research required in the validation track will depend on how well we understand the customer problem and solution. When: When our confidence about the proposed problem or solution isn’t high. For example, if we aren’t reasonably sure that the problem is important to a significant number of users, or that the solution is easy to understand and use. Who: Product Manager, Product Designer, UX Researcher, Product Design Manager, Engineering Manager Understand the user problem we are trying to solve. Identify business goals and key metrics to determine success. Generate hypotheses and research/experiment/user-test. Define MVC and potential future iterations. Minimize risks to value, usability, feasibility, and business viability with qualitative and quantitative analysis. Outcome: We have confidence that a proposed solution will positively impact one or more Product KPIs . There may be reason for exceptions, so the team would need to be clear in that case, and be able to justify that it’s still important without mapping back to our KPIs. If we don’t have confidence in the MVC or what success looks like, we should continue validation cycles before we move to the Build track. Label: workflow::validation backlog The growth of a world class product is built from a well maintained backlog. Product Managers are responsible for refining a group’s backlog to ensure validation opportunities are scoped and prioritized in line with category direction, stage, and/or section level strategy. The backlog is also the single source of truth for stakeholders to understand and engage with your group. An issue position in the backlog, along with the description, discussion, and metadata on those issues are key pieces of data necessary to keep stakeholders up to date. Label: workflow::problem validation To ensure the right solutions are delivered, the team must start their work with a validated problem . This can take many forms . If the problem is documented and well-understood, it may be possible to quickly move through this phase by documenting the known data about the user problem. A documented problem can be categorized as a pre-existing experience from feedback directly from users or an issue that has user engagement confirming that the problem is experienced by multiple users. A well-understood problem can be one that has a series of documented qualitative research from customer interviews, triangulating different sensing mechanisms confirming the problem, or using known data. Some examples of known data include Customer Request Issues or pre-existing Actionable Insights from prior research. To document that a problem is well-understood, link the known data and any customer calls to the relevant issues and epics. If the problem is nuanced or not yet well understood, then it will likely take longer to validate with users properly. This phase’s primary outcome is a clear understanding of the problem, along with a simple and clear way to communicate the problem to various stakeholders. Although optional, it is recommended to use an Opportunity Canvas as a tool that helps individuals better understand a problem, and communicate it to various stakeholders. An Opportunity Canvas can also be used to recommend creation of a new category including asking for new resourcing. After understanding and validating the problem, we can begin or continue to ideate potential solutions through a diverge/converge process. However, if the outcome from the problem validation phase confidently suggests an incremental modification to the existing solution, the aforementioned diverge/converge process could be skipped. This phase involves ideating potential solutions and exploring different approaches (diverge) before converging on a single solution. Solutions are evaluated by determining if they meet customer and business goals, are technically feasible, and align with legal compliance considerations. The team is encouraged to engage with stakeholders to determine potential flaws, missed use cases, potential security risks, and if the solution has the intended customer impact. THe DRI is responsible for reviewing the Legal Risk Checklist (accessible to team members only) and determining whether any sections need to be completed. After the team converges on the proposed solution or identifies a small set of options to validate, the issue moves into the Solution Validation phase. To start the Design phase, apply the workflow::design label to an existing issue or, if needed, creates a new issue with this label. Label: workflow::solution validation After identifying one or more potential solutions that meet business requirements and are technically feasible, the DRI must ensure that we have confidence that the proposed solution will meet the user’s needs and expectations. This confidence can be obtained from work performed during the design phase and supplemented with additional research (including user interviews, usability testing, or solution validation). If necessary, this phase will launch a Solution Validation issue within the GitLab UX Research project which will walk the team through research to validate their proposed solution(s). In addition, any non-functional requirements for the feature need to be considered and documented. These include such things as evaluating whether or not application limits need to be introduced, or any considerations around data storage should be evaluated. Defining these non-functional requirements up front ensures we are considering scalability and the long-term success of the feature. Sensible default values should be identified at this stage which align with the long-term vision for the feature. To start the Solution Validation phase, the DRI applies the workflow::solution validation label to an existing issue. The build track is where we plan, develop, and deliver value to our users by building MVCs , fixing defects, patching security vulnerabilities, enhancing user experience, and improving performance. This track is also a time when we get insight into whether we are creating the right thing for our users. The team works closely together to implement MVCs. Decisions are made quickly if challenges arise. We instrument usage and track product performance , so after MVCs are delivered to customers, feedback is captured quickly for learnings to refine the next iteration . For an example of how to leverage GitLab’s various features to create a focused and collaborative board to flow through the Build track, check out this video . When: As we build MVCs according to our product development timeline Who: Product Manager, Product Designer, Development team, Software Engineer in Test Release to a subset or full set of customers as appropriate. Assess UX, functional, technical performance, and customer impact. Collect data to measure MVC against success metrics to inform the next iteration. Iterate until success metrics are achieved and the product experience is optimal. Outcome: Deliver performant MVCs that improve one or more of our Product KPIs and/or Engineering KPIs . If it fails to do so, honor our Efficiency value (that includes a low level of shame), abandon it, and restart the validation cycle to identify the right solution. This phase prepares features so they are ready to be built by engineering. Bugs, technical debt, and other similar changes that are not features may enter the process in this phase (or may benefit from entering in earlier phases based on the cost of doing the work requiring the full problem to be validated to ensure it makes sense to do the work). Following Validation Phase 4 the feature should already be broken down into the quickest change possible to improve the user’s outcome and be ready for a more detailed review by engineering. During this phase, the DRI will surface issues they intend to prioritize for a milestone by applying the workflow::planning breakdown label. At this point, the appropriate DRI will assign an engineer to further break down and apply weights to that work. Tradeoff decisions can be made and feature issues evolve from validation solutions to clear MVCs that can be delivered in a single milestone. Be sure to document all decisions on issues. During this phase, the DRI must revisit the Legal Risk Checklist (accessible to team members only) to make sure none of their previous determinations during Validation phase 3: Design need revision. By reviewing and weighing work in the beginning of the Build Track, the DRI is able to make better prioritization tradeoffs and engineering teams can ensure they’ve scoped the right amount of work for the milestone. If an issue enters the workflow::planning breakdown state it doesn’t necessarily mean it will be prioritized in the next milestone, the DRI may make a tradeoff decision depending on capacity, and urgency. Once work has passed the workflow::planning breakdown step, the workflow::ready for development , type:: labels along with an upcoming milestone are applied to the issue. If an issue has been broken down, but not yet ready to pull into a milestone you may optionally apply the workflow::scheduling label; however, in this state any issue that has the workflow::ready for development label without a milestone has an implied status of “waiting to be scheduled”. The DRI will apply Deliverable label to issues with a milestone and marked workflow::ready for development signaling acceptance of the issue for that milestone. This process occurs at the beginning of milestone planning . During this phase, it’s important to keep Application Security Engineers informed to ensure that they have visibility into planning schedule. This provides them with sufficient time for planning dynamic testing so they can keep the product manager and development team informed of any time/resource requirements. The develop and test phase is where we build the features, address bugs or technical debt and test the solutions before launching them. The DRI is directly responsible for overall prioritization of a milestone, including bugs and maintenance work; however, the Engineering team is responsible for the implementation using the engineering workflow . Engineering owns the definition of done and issues are not moved into the next phase until those requirements are met. Keep in mind that many team members are likely to contribute to a single issue and collaboration is key. This phase begins after work has been broken down, and prioritized in Phase 1. Work is completed in priority order as set at the beginning of the milestone. The DRI will assign an issue to an engineer who is responsible for building the feature or addressing a bug or maintenance issue. An engineer can also self-serve and pick up the next priority order issue from the workflow::ready for development queue on their team’s board. That engineer will update its workflow:: label to indicate where it’s position in the development process . When an issue is in workflow::in review , the Application Security Engineer would help validate the risk mitigations through the non-blocking application security review process . Documentation for the work will be developed by the engineer and the Technical Writer (see Documentation with code as a workflow ). The Technical Writer should review the documentation as part of the development process. Items discovered during a documentation review should not block issues moving into the next phase. This may drive the creation of follow-on improvement MRs for the documentation, after release. After the feature code has been merged, the issue should be moved to workflow::verification . When an issue is in workflow::verification , the responsible engineer will manually test the feature in either the Staging or Production environment. Note: Work deemed out-of-scope or incomplete by engineering is taken back into the plan phase for refinement and rescheduling for completion. When the change becomes available in production and any needed verification is complete, the issue is closed and the workflow::complete label is added so stakeholders know work on it has been completed. Afterward, the DRI coordinates the release post and dogfooding process when they apply. After launch, the DRI should pay close attention to product usage data. This starts by ensuring your AMAU is instrumented and reporting as you expect. From there consider how the feature has impacted GMAU and SMAU . At this point you should also solicit customer feedback to guide follow-on iterative improvements, until success metrics are achieved/exceeded and a decision can be made that the product experience is sufficient. To create a combined and ongoing quantitative and qualitative feedback loop, consideration of the outcomes and potential activities below are recommended. Teams should release features as generally available from the start unless there are strong reasons to release them as experimental, beta, or limited availability first. Product development teams should refrain from making changes that they believe could create significant risks or friction for GitLab users or the platform, such as: In addition to the experiment details for users, experiments: All experimental features that meet the review criteria must initiate Production Readiness Review and complete the experiment section in the readiness template . In addition to the beta details for users, beta features: All beta features that meet the review criteria must complete all sections up to and including the beta section in the readiness template by following the Production Readiness Review process . Publicly available features must: In addition to the publicly available criteria above, GA features: Our mission is “everyone can contribute” , and that is only possible if people outside the company can try a feature. We get higher quality (more diverse) feedback if people from different organizations try something, so give users the ability to opt in to experimental features when there is enough value. Where possible, release an experimental feature externally instead of only testing internally or waiting for the feature to be in a beta state. We’ve learned that keeping features internal-only for extended periods of time slows us down unnecessarily. Experimental features are only shown when people/organizations opt in to experiments, so we are allowed to make mistakes here and literally experiment. To ensure the phases before general availability are as short as possible each phase of experiment, beta, and limited availability should include exit criteria. This encourages rapid iteration and reduces cycle time . GitLab Product Managers must take the following into account when deciding what exit criteria to apply to their experimental and beta features: For the exit criteria of AI features , in addition to the above, see the UX maturity requirements . All merge requests to this page require informing Product and Engineering Leadership (see page “maintainers” or codeowners). To make updates such as grammatical fixes and typos, you can have any approver review and merge.",
  "This document explains the workflow for anyone working with issues in GitLab Inc. For the workflow that applies to the wider community see the contributing guide . Products at GitLab are built using the GitLab Flow . We have specific rules around code review . In line with our values of short toes , making two-way-door decisions and bias for action , anyone can propose to revert a merge request. When deciding whether an MR should be reverted, the following should be true: Reverting merge requests that add non-functional changes and don’t remove any existing capabilities should be avoided in order to prevent designing by committee. The intent of a revert is never to place blame on the original author. Additionally, it is helpful to inform the original author so they can participate as a DRI on any necessary follow up actions. The pipeline::expedited label, and master:broken or master:foss-broken label must be set on merge requests that fix master to skip some non-essential jobs in order to speed up the MR pipelines. If you notice that pipelines for the master branch of GitLab or GitLab FOSS are failing, returning the build to a passing state takes priority over everything else development related, since everything we do while tests are broken may: A broken master is an event where a pipeline in master is failing. The cost to fix test failures increases exponentially as time passes due to merged results pipelines used. Auto-deploys, as well as monthly releases and security releases, depend on gitlab-org/gitlab master being green for tagging and merging of backports . Our aim should be to keep master free from failures, not to fix master only after it breaks. Any question or suggestion is welcome in the #g_development_analytics channel who owns the broken master automation proceess. There are two phases for fixing a broken master incident which have a target SLO to clarify the urgency. The resolution phase is dependent on the completion of the triage phase. Note: Recurring incidents are negatively impacting master pipeline stability and development velocity. Any untriaged, recurring incident will be automatically escalated to #dev-escalation following this timeline: If an incident becomes a blocker for MRs and deployments before being auto-escalated, the team member being impacted should refer to the broken master escalation steps to request help from the current engineer on-call as early as needed. Additional details about the phases are listed below. Recurring broken master incidents are automatically escalated to #dev-escalation unless it is triaged within 4 hours. If a broken master is blocking your team before auto-escalation (such as creating a security release) then you should: Note: Stable branch failures follow the same process as described here, but incidents are tracked in gitlab-org/release/tasks . See stable branches documentation for details. Master broken incidents must be manually escalated to #dev-escalation on weekends and holidays if necessary. Without a manual escalation, the service level objective can extend to the next working day; that is, triage DRI is expected to triage the incident on the next working day. Regardless of when the label was applied, we always consider an incident to be in an escalated state as long as it has the ~“escalation::escalated” label, until the incident is resolved. If a failed test can be traced to a group through its feature_category metadata, the broken master incident associated with that test will be automatically labeled with this group as the triage DRI through this line of code . In addition, Slack notifications will be posted to the group’s Slack channel to notify them about ongoing incidents. The triage DRI is responsible for monitoring, identifying, and communicating the incident. A notification will be sent to the attributed group’s Slack channel and #master-broken . Pipeline failures are sent to the triage DRI’s group channel, if one is identified, and will be reviewed by its group members. The failures will also be sent to #master-broken for extra communication. If an incident is announced in a DRI group’s Slack channel, the channel member should acknowledge it and assume the triage DRI responsibilities. If the incident is a duplicate of an existing incident, use the following quick actions to close the duplicate incident: If the incident is not a duplicate, and needs some investigation: Review non-resolved broken master incidents for the same failure. If the broken master is related to a test failure, search the spec file in the issue search to see if there’s a known failure::flaky-test issue. If this incident is due to non-flaky reasons , communicate in #development , #backend , and #frontend using the Slack Workflow. If you identified that master fails for a flaky reason , and it cannot be reliably reproduced (i.e. running the failing spec locally or retrying the failing job): Quarantine the failing test to restore pipeline stability within 30 minutes if the flakiness is continuously causing master pipeline incidents. Alternatively, if the failure does not seem disruptive, and you have a fix that you are confident with, submit the fix MR with the ~“master:broken” label to ensure your pipeline is expedited. If a flaky test issue already exists, add a comment in it with a link to the failed broken master incident and/or failed job. We have automation in place to create test failure issues automatically. The issue is named after the spec path, which can be a search keyword. If a flaky test issue doesn’t exist, create an issue from the New issue button in top-right of the failing job page (that will automatically add a link to the job in the issue), and apply the Broken Master - Flaky description template. Add the appropriate labels to the main incident: Add the stacktrace of the error to the incident (if it is not already posted by gitlab-bot), as well as Capybara screenshots if available in the job artifacts. Identify the merge request that introduced the failures. There are a few possible approaches to try: If you identified a merge request, assign the incident to its author if they are available at the moment. If they are not available, assign to the maintainer that approved/merged the MR. If none are available, mention the team Engineering Manager and seek assistance in the #development Slack channel. If no merge request was identified, ask for assistance in the #development Slack channel. Please set the appropriate ~master-broken:* label from the list below: If the triage DRI believes that there’s an easy resolution by either: The triage DRI can create a merge request, assign to any available maintainer, and ping the resolution DRI with a @username FYI message. Additionally, a message can be posted in #backend_maintainers or #frontend_maintainers to get a maintainer take a look at the fix ASAP. If the failures occur only in test-on-gdk jobs, it’s possible to stop those jobs from being added to new pipelines while the cause is being fixed. See the runbook for details. For an initial assessment of what might have contributed to the failure, we can try the experimental AI-assisted root cause analysis feature following this documentation . To confirm flakiness, you can use the @gitlab-bot retry_job <job_id> or the @gitlab-bot retry_pipeline <pipeline_id> command to retry the failed job(s), even if you are not a project maintainer. The merge request author of the change that broke master is the resolution DRI. In the event the merge request author is not available, the team of the merge request author will assume the resolution DRI responsibilities. If a DRI has not acknowledged or signaled working on a fix, any developer can take assume the resolution DRI responsibilities by assigning themselves to the incident. Once the resolution DRI announces that master is fixed: Merge requests can not be merged to master until the incident status is changed to Resolved . This is because we need to try hard to avoid introducing new failures, since it’s easy to lose confidence if it stays red for a long time. In the rare case where a merge request is urgent and must be merged immediately , team members can follow the process below to have a merge request merged during a broken master . Merging while master is broken can only be done for: First, ensure the latest pipeline has completed less than 2 hours ago (although it is likely to have failed due to gitlab-org/gitlab using merged results pipelines ). Next, make a request on Slack: A maintainer who sees a request to merge during a broken master must follow this process. Note, if any part of the process below disqualifies a merge request from being merged during a broken master then the maintainer must inform the requestor as to why in the merge request (and optionally in the Slack thread of the request). Next, ensure that all the following conditions are met: Next, add a comment to the merge request mentioning that the merge request will be merged during a broken master , and link to the broken master incident. For example: Next, merge the merge request: #master-broken-mirrors was created to remove duplicative notifications from the #master-broken channel which provides a space for Release Managers and the Developer Experience teams to monitor failures for the following projects: The #master-broken-mirrors channel is to be used to identify unique failures for those projects and flaky failures are not expected to be retried/reacted to in the same way as #master-broken . We run JiHu validation pipelines in some of the merge requests, and it can be broken at times. When this happens, check What to do when the validation pipeline failed for more details. To guarantee the readiness of any GitLab release, it is fundamental that failures on stable branches are addressed with priority, similar to master branch failures. It is the merge request author’s responsibility to backport the following to the maintained stable branches : Follow the engineering runbook to backport changes to stable branches. Security issues are managed and prioritized by the security team. If you are assigned to work on a security issue in a milestone, you need to follow the Security Release process . If you find a security issue in GitLab, create a confidential issue mentioning the relevant security and engineering managers, and post about it in #security . If you accidentally push security commits to gitlab-org/gitlab , we recommend that you: For more information on how the entire process works for security releases, see the documentation on security releases . A ~regression implies that a previously verified working functionality no longer works. Regressions are a subset of bugs. The ~regression label is used to imply that the defect caused the functionality to regress. The label tells us that something worked before and it needs extra attention from Engineering and Product Managers to schedule/reschedule. The regression label does not apply to bugs for new features for which functionality was never verified as working . These, by definition, are not regressions. A regression should always have the ~regression:xx.x label on it to designate when it was introduced. If it’s unclear when it was introduced, the latest released version should be added. Regressions should be considered high priority issues that should be solved as soon as possible, especially if they have severe impact on users. When identified in time, for example in a SaaS deployment, fixing them within the same milestone avoids their being included with that release. For better efficiency, it’s common for a regression to be fixed in an MR without an issue being created, either through reversion of the original MR or a code change. Regardless of whether there is an issue or not, the MR should also have the ~regression and ~regression:xx.x labels. This allows for trends to be accurately measured. Start working on an issue you’re assigned to. If you’re not assigned to any issue, find the issue with the highest priority and relevant label you can work on, and assign it to yourself. You can use this query, which sorts by priority for the started milestones , and filter by the label for your team. If you need to schedule something or prioritize it, apply the appropriate labels (see Scheduling issues ). If you are working on an issue that touches on areas outside of your expertise, be sure to mention someone in the other group(s) as soon as you start working on it. This allows others to give you early feedback, which should save you time in the long run. If you are working on an issue that requires access to specific features, systems, or groups, open an access request to obtain access on staging and production for testing your changes after they are merged. When you start working on an issue: You are responsible for the issues assigned to you. This means it has to ship with the milestone it’s associated with. If you are not able to do this, you have to communicate it early to your manager and other stakeholders (e.g. the product manager, other engineers working on dependent issues). In teams, the team is responsible for this (see Working in Teams ). If you are uncertain, err on the side of overcommunication. It’s always better to communicate doubts than to wait. You (and your team, if applicable) are responsible for: Once a release candidate has been deployed to the staging environment, please verify that your changes work as intended. We have seen issues where bugs did not appear in development but showed in production (e.g. due to CE-EE merge issues). Be sure to read general guidelines about issues and merge requests . Team members use labels to track issues throughout development. This gives visibility to other developers, product managers, and designers, so that they can adjust their plans during a monthly iteration. An issue should follow these stages: Workflow labels are described in our Development Documentation and Product Development Flow . For larger issues or issues that contain many different moving parts, you’ll be likely working in a team. This team will typically consist of a backend engineer , a frontend engineer , a Product Designer and a product manager . In the spirit of collaboration and efficiency , members of teams should feel free to discuss issues directly with one another while being respectful of others’ time . Avoid adding configuration values in the application settings or in gitlab.yml . Only add configuration if it is absolutely necessary. If you find yourself adding parameters to tune specific features, stop and consider how this can be avoided. Are the values really necessary? Could constants be used that work across the board? Could values be determined automatically? See Convention over Configuration for more discussion. Start working on things with the highest priority in the current milestone. The priority of items are defined under labels in the repository, but you are able to sort by priority. After sorting by priority, choose something that you’re able to tackle and falls under your responsibility. That means that if you’re a frontend developer, you work on something with the label frontend . To filter very precisely, you could filter all issues for: Use this link to quickly set the above parameters . You’ll still need to filter by the label for your own team. If you’re in doubt about what to work on, ask your lead. They will be able to tell you. It’s every developers’ responsibilities to triage and review code contributed by the rest of the community, and work with them to get it ready for production. Merge requests from the rest of the community should be labeled with the Community contribution label. When evaluating a merge request from the community, please ensure that a relevant PM is aware of the pending MR by mentioning them. This should be to be part of your daily routine. For instance, every morning you could triage new merge requests from the rest of the community that are not yet labeled Community contribution and either review them or ask a relevant person to review it. Make sure to follow our Code Review Guidelines . GitLab.com is a very large instance of GitLab Enterprise Edition. It runs release candidates for new releases, and sees a lot of issues because of the amount of traffic it gets. There are several internal tools available for developers at GitLab to get data about what’s happening in the production system: There is extensive monitoring publicly available for GitLab.com. For more on this and related tools, see the monitoring handbook . GitLab Inc has to be selective in working on particular issues. We have a limited capacity to work on new things. Therefore, we have to schedule issues carefully. Product Managers are responsible for scheduling all issues in their respective product areas , including features, bugs, and tech debt. Product managers alone determine the prioritization , but others are encouraged to influence the PMs decisions. The UX Lead and Engineering Leads are responsible for allocating people making sure things are done on time. Product Managers are not responsible for these activities, they are not project managers. Direction issues are the big, prioritized new features for each release. They are limited to a small number per release so that we have plenty of capacity to work on other important issues, bug fixes, etc. If you want to schedule an issue with the Seeking community contributions label, please remove the label first. Any scheduled issue should have a team label assigned, and at least one type label. To request scheduling an issue, ask the responsible product manager We have many more requests for great features than we have capacity to work on. There is a good chance we’ll not be able to work on something. Make sure the appropriate labels (such as customer ) are applied so every issue is given the priority it deserves. Teams (Product, UX, Development, Quality) continually work on issues according to their respective workflows. There is no specified process whereby a particular person should be working on a set of issues in a given time period. However, there are specific deadlines that should inform team workflows and prioritization. With the monthly release date being the third Thursday of the release month, the code cut-off is the Friday prior. The next milestone begins the Saturday after code cut-off. All other important dates for a milestone are relative to the release date: Refer to release post content reviews for additional deadlines. Note that deployments to GitLab.com are more frequent than monthly major/minor releases. See auto deploy transition guidance for details. At the beginning of each release, we have a kickoff meeting, publicly livestreamed to YouTube. In the call, the Product Development team (PMs, Product Designers, and Engineers) communicate with the rest of the organization which issues are in scope for the upcoming release. The call is structured by product area with each PM leading their part of the call. The Product Kickoff page is updated each month, which follows the content on the livestream. Engineering Managers are responsible for capacity planning and scheduling for their respective teams with guidance from their counterpart Product Managers. To ensure hygiene across Engineering, we run scheduled pipelines to move unfinished work (open issues and merge requests) with the expired milestone to the next milestone, and label ~\"missed:x.y\" for the expired milestone. Additionally, label ~\"missed-deliverable\" whenever ~\"Deliverable\" is presented. This is currently implemented as part of our automated triage operations . Additionally, issues with the ~Deliverable label which have a milestone beyond current +1, will have the ~Deliverable label removed. We keep the milestone open for 3 months after it’s expired, based on the release and maintenance policy . The milestone cleanup is currently applied to the following groups and projects : Milestones closure is in the remit of the Delivery team . At any point in time a release might need to be created for an active milestone,and once that is no longer the case, the Delivery team closes the milestone. The milestone cleanup will happen on the milestone due date. These actions will be applied to open issues: Milestones are closed when the Delivery team no longer needs to create a backport release for a specific milestone. When working in GitLab (and in particular, the GitLab.org group), use group labels and group milestones as much as you can. It is easier to plan issues and merge requests at the group level, and exposes ideas across projects more naturally. If you have a project label, you can promote it to a group milestone. This will merge all project labels with the same name into the one group label. The same is true for promoting group milestones. We definitely don’t want our technical debt to grow faster than our code base. To prevent this from happening we should consider not only the impact of the technical debt but also consider the impacts spreading like a contagion. How big and how fast is this problem going to be over time? Is it likely a bad piece of code will be copy-pasted for a future feature? In the end, the amount of resources available is always less than amount of technical debt to address. As we innovate our platform, we will have situations where a strategic decision will be made to incur technical debt in order to preserve a higher feature velocity to meet market and customer demand. This accrual of technical debt presents risks due to the long-term implications for the usability, security, reliability, scalability, accessibility, and/or availability of our product and platform. Therefore technical debt may be accrued but must be: If it is to be deferred by more than 6 months, it should be considered for the product and engineering roadmap. Technical debt issues you wish to be closed must not affect the “*abilities” and have a justification included on why it cannot be remediated within the next 18 months (meaning, there is an action plan on the Engineering roadmap). To help with prioritization and decision-making process here, we recommend thinking about contagion as an interest rate of the technical debt. There is a great comment from the internet about it: You wouldn’t pay off your $50k student loan before first paying off your $5k credit card and it’s because of the high interest rate. The best debt to pay off first is one that has the highest loan payment to recurring payment reduction ratio, i.e. the one that reduces your overall debt payments the most, and that is usually the loan with the highest interest rate. Technical debt is prioritized like other technical decisions in product groups by product management . For technical debt which might span, or fall in gaps between groups they should be brought up for a globally optimized prioritization in retrospectives or directly with the appropriate member of the Product Leadership team . Additional avenues for addressing technical debt outside of product groups are Strategic Priority Codes and working groups . Sometimes there is an intentional decision to deviate from the agreed-upon MVC , which sacrifices the user experience. When this occurs, the Product Designer creates a follow-up issue and labels it Deferred UX to address the UX gap in subsequent releases. For the same reasons as technical debt, we don’t want Deferred UX to grow faster than our code base. These issues are prioritized like other technical decisions in product groups by product management . As with technical debt , Deferred UX should be brought up for globally optimized prioritization in retrospectives or directly with the appropriate member of the Product Leadership team . UI polish issues are visual improvements to the existing user interface, touching mainly aesthetic aspects of the UI that are guided by Pajamas foundations. UI polish issues generally capture improvements related to color, typography, iconography, and spacing. We apply the UI polish label to these issues. UI polish issues don’t introduce functionality or behavior changes to a feature. Open merge requests sometimes become idle (not updated by a human in more than a month). Once a month, engineering managers will receive an Merge requests requiring attention triage issue that includes all (non-WIP/Draft) MRs for their group and use it to determine if any action should be taken (such as nudging the author/reviewer/maintainer). This assists in getting merge requests merged in a reasonable amount of time which we track with the Open MR Review Time (OMRT) and Open MR Age (OMA) performance indicators. Open merge requests may also have other properties that indicate that the engineering manager should research them and potentially take action to improve efficiency. One key property is the number of threads, which, when high, may indicate a need to update the plan for the MR or that a synchronous discussion should be considered. Another property is the number of pipelines, which, when high, may indicate a need to revisit the plan for the MR. These metrics are not yet included in an automatically created a triage issue. Security is our top priority. Our Security Team is raising the bar on security every day to protect users’ data and make GitLab a safe place for everyone to contribute. There are many lines of code, and Security Teams need to scale. That means shifting security left in the Software Development LifeCycle (SDLC) . Each team has an Application Security Stable Counterpart who can help you, and you can find more secure development help in the #sec-appsec Slack channel. Being able to start the security review process earlier in the software development lifecycle means we will catch vulnerabilities earlier, and mitigate identified vulnerabilities before the code is merged. You should know when and how to proactively seek an Application Security Review . You should also be familiar with our Secure Coding Guidelines . We are fixing the obvious security issues before every merge, and therefore, scaling the security review process. Our workflow includes a check and validation by the reviewers of every merge request, thereby enabling developers to act on identified vulnerabilities before merging. As part of that process, developers are also encouraged to reach out to the Security Team to discuss the issue at that stage, rather than later on, when mitigating vulnerabilities becomes more expensive. After all, security is everyone’s job. See also our Security Paradigm . From time to time, there are occasions that engineering team must act quickly in response to urgent issues. This section describes how the engineering team handles certain kinds of such issues. Not everything is urgent. See below for a non-exclusive list of things that are in-scope and not in-scope. As always, use your experience and judgment, and communicate with others. A bi-weekly performance refinement session is held by the Development and QE teams jointly to raise awareness and foster wider collaboration about high-impact performance issues. A high impact issue has a direct measurable impact on GitLab.com service levels or error budgets . The Performance Refinement issue board is reviewed in this refinement exercise. The infradev process is established to identify issues requiring priority attention in support of SaaS availability and reliability. These escalations are intended to primarily be asyncronous as timely triage and attention is required. In addition to primary management through the Issues, any gaps, concerns, or critical triage is handled in the SaaS Availability weekly standup . The infradev issue board is the primary focus of this process. (To be completed primarily by Development Engineering Management) Issues are nominated to the board through the inclusion of the label infradev and will appear on the infradev board . Issues with ~infradev ~severity::1 ~priority::1 ~production request labels applied require immediate resolution. ~infradev issues requiring a ~“breaking change” should not exist. If a current ~infradev issue requires a breaking change then it should split into two issues. The first issue should be the immediate ~infradev work that can be done under current SLOs. The second issue should be ~“breaking change” work that needs to be completed at the next major release in accordance with deprecation guidance . Agreement from development DRI as well as the infrastructure DRI should be documented on the issue. Infradev issues are also shown in the monthly Error Budget Report . Triage of infradev Issues is desired to occur asynchronously. These points below with endure that your infradev issues gain maximum traction.",
  "This page is intended to help Product Managers at GitLab understand what data is available to them and how they can use it to understand how their product is used. This page primarily covers two topics: how to consume data, and what data is available. The user-facing end of GitLab’s data stack consists of our BI Tool, Tableau, which is connected to our Snowflake data warehouse. The Tableau handbook page of the Data team handbook has general information about Tableau aimed for a wider GitLab audience. Here are some useful links that we recommend for you to bookmark: In order to gain access to Tableau, you will need to follow the instructions here and open an access request. Published data Tableau sources are great ways to allow Tableau users to build charts without writing any SQL or modeling. The Data team has created several Published Data Sources in Tableau that have the official “Certified” badge. For example, Mart Ping Instance can be used to look at Service Ping ping-level details. The Data team uses a tool called dbt for our data transformation layer. A nice feature of dbt is dbt docs, which automatically creates documentation for all of the models in our schema. Our dbt docs instance can be found here . You will need to locate the file you wish to update or create in the gitlab-data analytics project . Please be sure to read and follow the SQL style guide when creating the changes. If you wish to update only the descriptions or information about tables you will be looking for a schema.yml file. If you wish to actually change the structure of tables it will be a *.sql file. Next, create a branch and then submit an MR using the dbt Model Changes template to the gitlab-data analytics project . When creating your branch and MR please follow the Data team workflow and use the appropriate Data team labels . If you ever get stuck or have a question, please ask for help in the #data slack channel . It is recommended to also cross-post questions in your #g_, #s_, or #product channels because many PMs have data related expertise and can provide you quick assistance for common product data questions. If needed, you may create an issue in the Product Data Insights project and assign it to a product data analyst . You can read more about working with the PDI team here . As a GitLab PM, you’re responsible for defining and tracking metrics for your team’s features. This guide will walk you through the process, tools, and resources available to help you succeed. If your analytics needs for your new or recently modified feature are met by these dashboards, you can skip creating a Product Data Insights (PDI) Issue: Plan Your Analytics Requirements Option A: Use the CLI Generator to generate requirements for your Instrumentation Issue Option B: Use Usage Data Instrumentation Issue Template to outline metric requirements When instrumenting features routed through the AI Gateway, follow these guidelines: Represent new features routed through the AI Gateway as unit primitives Set up tracking for the new unit primitive By following this process and understanding the roles involved, PMs can effectively instrument and track metrics for their features, enabling data-driven decision-making and product improvement. We have three primary data sources for product usage data: Each data source comes with its own caveats, capabilities, and limitations. The first question we on the Data or PDI teams ask product managers is usually “are you interested in knowing this for Self-Managed or GitLab.com?” Our approach to answering your question and the data source(s) available differ greatly between the two. Although our Self-Managed offering has many more active customers, our GitLab.com offering has much more granular data available to analyze. Service Ping is a custom tool that GitLab built to collect weekly aggregated information from our customers across various deployment options: Here is an example of a query that provides ping-level details, filters out GitLab.com, and limits to the last ping of the month: Here is an example of a query that provides metric-level reporting by month and deployment type: Because GitLab.com is a GitLab instance hosted by GitLab, we have access to the instance’s postgres database and can load parts of it into our Snowflake data warehouse. This means we can get a very detailed look into how our product is used on GitLab.com. Our ELT process works by explicitly stating which columns and tables we want to import into the data warehouse. This means we might be missing a column or whole table that you want to have in the data warehouse for analysis. When this is the case, please create a Data issue letting us know what you want us to import using the New Data Source template . Before doing so, please confirm that the table/column is truly part of the production schema . Here is an example of a query that will generate GitLab.com UMAU by day: Here is an example of a query that will generate paid GitLab.com GMAU by month: Snowplow Analytics is an open-source enterprise event-level analytics platform that enables data collection from multiple platforms for advanced data analysis. Analytics Instrumentation has built Internal Event tracking , which will guide you on how to instrument Snowplow events. To get started, use the Quick Start Guide to Internal Event Tracking . Once your Snowplow events have been instrumented, as part of the validation process, the newly instrumented event should be tested to ensure it is working properly. While you as the PM probably won’t be doing the validation yourself every time, it is nice to know how it works. You can learn more about testing Snowplow events in the Internal Event documentation here . The data you have instrumented is most useful if it can be visualized in a chart. Refer to the Tableau section of the handbook for information on creating charts. The Snowplow models are quite large and can be slow to query. To make your query faster, use a date ( behavior_at ) filter in your WHERE statement. Here is an example of a query that will look at the top 100 Snowplow events from the last several days: Here is an example of a query that will look at top 100 pages viewed in the last several days (note how the URLs are pseudonymized): Analytics Instrumentation is part of the product org and is completely separate from the Data and Product Data Insights teams. However, these teams collaborate closely as the Customer Product Adoption pod.",
  "GitLab’s pricing strategy is set by the CEO. Everyone can contribute, and the best way to communicate is on the #pricing Slack channel. Contributions are part of the day-to-day jobs of the following people: Most of GitLab functionality is and will be available for free in our Free tier. Our paid tiers include features that are more relevant for managers, directors, and executives . We promise all major features in our scope are available in Free too. Instead of charging for specific parts of our scope (CI, Monitoring, etc.) we charge for smaller features that you are more likely to need if you use GitLab with a lot of users. There are a couple of reasons for this: Because we have a great free product we can’t have one price. Setting it high would make the difference from the free version too high. Setting it low would make it hard to run a sustainable business. There is no middle ground that would work out with one price. That is why we have Premium and Ultimate tiers . The price difference between them is half an order of magnitude (5x). We will charge per user, per application, or per instance. We do include free minutes with our subscriptions and trials to make it easier for users to get started. As we look towards more deployment-related functionality on SaaS it’s tempting to offer compute and charge a percent on top of, for example, Google Cloud Platform (GCP). We don’t want to charge an ambiguous margin on top of another provider since this limits user choice and is not transparent. So we will always let you BYOK (bring your own Kubernetes) and never lock you into our infrastructure to charge you an opaque premium on those costs. As a commercial organization, we always want to grow the number of paying customers, therefore we are focused on increasing the free to paid conversion rate. However, GitLab offers a free product, and our free users bring tremendous value to the company beyond just the likelihood of converting to a paid customer one day. Our pricing philosophy is aligned with our GitLab Values . We take a value-based pricing approach at GitLab. The core of the value-based pricing is to understand the value proposition that our innovations provide to our current and future customers for that offering. We aim to price our offerings to achieve continued growth and financial success for both the customer and GitLab. To understand the value that we deliver to our customers, we focus on the benefits that our customers can receive to achieve their business goals. The value to our customers can include: Defining and quantifying the value being delivered does not happen in isolation. We regularly engage with customers to understand how our solutions help them. These engagements are important as they will provide insights on how our innovations are valued by the customers and how it impacts our customers’ willingness to pay. Willingness to pay is a crucial element when we decide on how to price and packaging our offerings. We understand that customers have choices when they decide to make a purchase. Hence, understanding the competitive landscape of our products is very critical. With respect to competition, we focus on the key differences between our value proposition and the competition’s value proposition. Our pricing aims to reflect the differential value that we provide. We aim to design the pricing structure that works best for our customers compared to our competitors inclusive of pricing metric. To following our pricing philosophy, we aim to support long-lasting customer relationships. When we develop our pricing, the target is that both our customers and GitLab become successful together. For long term success, where we can run a sustainable business and meet our company’s financial goals, we do also consider cost to serve and margins as factors when pricing is determined. It is important to note that while costs and margins are factors in pricing, value provided to our customers is the foundational element. Pricing affects Product, Marketing, and Sales. Therefore, general pricing decisions are made by the CEO. Product makes most decisions on a day-to-day basis about what feature should go in what plan based on the paid tiers . The following table describes how large cross-functional pricing decisions should be made. This decision making method differs from our normal RADCIE method due to the extremely cross-functional nature and business risk of large pricing decisions. To ensure alignment across stakeholders, a monthly Pricing Steering Committee is held to align on upcoming monetization decisions, discuss historical insights/trends, and prioritize areas of opportunity. We have three pricing tiers. How we make decisions on a day-to-day basis is specified on our stewardship page . When considering buyers as part of product tiering decisions we use the following guidance: When the tiering of a feature is being evaluated, the stewardship promise will override this guidance. Understanding the distinction of our buyer-based model can be difficult. In it we focus not on the user of the feature, but on the buyer and in what cases a feature would be useful to that buyer even in cases where the buyer is not the user. When making these decisions we ask questions like: Below we list product categories and the current and proposed features which reside in a given tier to highlight how the buyer-based model works in practice. While our tiers are based on who leads the purchasing decision there are occasions where driving increased usage takes precedence over the natural buyer-based tiering, always in favor of moving features to lower tiers. Here’s why: As GitLab develops new categories that are likely to be in paid tiers, it is still reasonable (and in many cases advisable) to get the early MVC versions to land in the free or lower paid-tier to spur adoption, encourage contributions and gain feedback from the wider user base. Over time, the team can then add more features on top of the MVC functionality that will be placed in the paid-tiers. However, Product Managers should not move features down in violation of the buyer-based model unless there is a concrete high confidence plan to follow up with meaningful additions to the base feature that can be added to and monetized with the right paid tier When considering tiering, if the feature is geared to be used (not purchased) by individuals and the answer to the question of who cares most about this feature? is today - very few people we should consider open-sourcing it to get more usage and contributors. Regardless of the reasoning behind the down-tiering of a feature, the process should still be followed. If a feature in the free tier (typically useful to ICs, but also to others) will open up the possibility to circumvent or abuse our tiering policy, we should exercise extreme caution and err on the side of maintaining the feature in a paid tier. An example of this is repo pull mirroring that is useful for ICs and can also be great for adoption (i.e. trying out GitLab.). However, it may also lead to larger instances in Free tier mirroring to smaller instances in paid tiers to make use of paid-tiered features. This goes against our pricing terms & conditions and therefore we chose to keep the feature in a paid tier instead of Free tier. When building integrations to partners it is possible to make exceptions to our buyer-based model when our tiers don’t align well with those of the partner, but only in favor of lower tiers. As an example - a partner who provides a team collaboration tool with a free tier might desire any GitLab integration to be present in our free tier despite it clearly being appropriate for single team usage. The go-to-market benefits of this partnership can outweigh the divergence from our buyer-based model. We’ve found that bundling our features within tiers into themes resulted in improved conversion performance on our pricing page . While it is how we present the value of our tiers, this themes-based representation serves as an additive filter for, and is not a replace of, our Buyer Based Tiering decision. Themes are then bundled into customer value drivers . The following are our current set of capabilities: 10,000 compute minutes per month 50,000 compute minutes per month Because themes are filters for our Buyer Based tiers, there can occasionally be instances where a feature’s tier and its natural theme don’t match. In that case we should either: Note - it is OK for the features of a theme to be in the listed tier and any lower tier. Our higher tier is still the best option for our customers: There are multiple reasons why our lower tiers have more relative value: Arguments in favor of raising the price of the lower tier (which we won’t necessarily do) are: Please note that all the above is not a plea to add more or fewer features to lower tiers, we should just follow our Buyer Based Open Core model . We use the same names for SaaS and Self-Managed tiers because: There is a big price difference between the different tiers (0$, $29, custom per user per month, a price difference of infinite, 5x). For GitLab Inc., the majority of revenue comes from large enterprises buying the top two tiers. Most companies in a similar situation would focus only on the highest tiers. But we want to make a our hybrid model work for the following reasons: A 5x higher price doesn’t mean there is 5x more value, just like the Premium tier doesn’t provide infinitely more value than the gratis Free tier. When deciding between tiers, organizations should look at the ratio between how much extra value they get divided by how much extra they pay. If this ratio is comfortably above 1, it makes sense to move to a higher tier. The value is in making people more effective, saving time on integrating tools, driving faster time to value, and retiring other tools. This should more than pay for the increased price of a tier. An analogy would be Apple’s iPhone: it is twice as expensive as an average Android phone, and while it doesn’t deliver twice as much value, the extra value is worth the extra cost. As Stripe documented : hybrid is hard, because “The most common result of attempting both models simultaneously is that only one of the models receives any traction, and (because these models weave themselves into all operations of the company) it typically strangles the other.” This hybrid models is how we bridge the chasm between self-service and enterprise. We tried selling one feature at a time, but this was not feasible. An improved version of that would be selling 7 main features, instead of 3 plans. Examples of main features would be: High Availability, Security, Service Desk, etc. We currently think the disadvantages outweigh the advantages. We believe having a single plan for one customer works because the advantages outweigh the disadvantages. For the GitLab sales team and for GitLab customers, we suggest handling the objection by focusing on the value and business outcomes As our customers adopt GitLab as the single application for their entire DevOps lifecycle, we are hearing more and more non-developer use cases using GitLab for Agile project management. The main functionality that some non-developer roles need aligns fairly well with our reporter permission access, including Managing Labels, Assigning Issues, and Creating and Editing Epics, etc. Those users don’t really need to use much other developer functionality. Currently, we do not offer a different list price for users who only need the reporter permission because: We understand that our customers have non-developer use cases. Please contact our sales team or connect with us via this issue to discuss your specific use cases. Arguments to charge more for SaaS: Arguments to at least make them equal: Not sure what is normal in the market. Adobe did a good job, but they moved from perpetual licensing to subscriptions, where it is hard to compare the two prices . This is the title of a great article of which we’ll apply the 8 points to GitLab below: Annual, up-front pricing is currently our only offering. All internal systems should show only annual pricing to keep comparisons simple. We do show monthly pricing externally. Arguments supporting annual up-front pricing: Arguments supporting also offering monthly pricing: Almost all SaaS products show monthly pricing on their pricing pages. GitLab shows monthly pricing on our website and notes clearly that it is billed annually. This is also a standard practice followed by companies like Salesforce . We previously have tried showing annual pricing on the website, but repeatedly heard from customers that they were confused by it. They often thought our product was priced significantly higher than it actually was because they expected the pricing to be displayed in monthly units and read the annual price as though it were per month. Most companies evolve in the following way: An example is Microsoft Office, where it is costly to buy components of Office365 separately, although higher tiers include more products. At GitLab, we decided to skip the intermediate steps and immediately only offer a suite that includes all our products. Having our complete scope included in our open source version is even part of our stewardship promises . Selling only a suite has risks, after the => is how we mitigate those at GitLab: Companies evolve to selling only a suite for the following reasons, after the => is how this applies to GitLab: We’re going even further than selling a suite by integrating everything in a single application. We do that because of the advantages mentioned on our handbook page about us being single application . A secondary effect is that the user doesn’t have to make a buying, or even an adoption, decision. Charging one price that incorporate all our stages is the perfect bundle . There are two factors that determine how much value GitLab creates for an organization, in order of importance: When an organization is larger, the benefits of GitLab are larger because: Since GitLab is an open core project, we’ll always create much more value then we (are able to) capture. Based on the value created, the straightforward way to capture value would be to: These straightforward ways are not possible for the following reasons: So we’re left with charging for features. We can’t charge for each feature separately, since that is unwieldy for the customer. So we charge for tiers that contain a bundle of features. We select features in the (more expensive) paid tiers that: Adding features to a (more expensive) paid tier is not the only thing stopping users from adopting them, but it is a very important factor. To simplify the above, we base our feature groupings on champion position (see below). We make feature tiering decisions based on: “Who cares most about the feature” . Our three tiers are differentiated based on the buyer persona or IC who buys GitLab, from individual contributor, to manager/director, to executive. Every person in the company is on the same tier , even if they don’t use all the features. The feature is put in the plan based on what champion is most likely to care about it. Buyers make sense, since a higher-cost plan needs a higher-placed buyer. More detail about this in Sid’s presentations about Buyer-Based-Open-Core at the Open Source Leadership Summit in 2019 and at Heavybit . Also, see fork and commoditize (internal) in our list of concerns. A customer asked why merge request approvals were not included in free: Thanks for asking. We think that managers are more likely to care about merge requests approvals than individual contributors. That doesn’t mean that individual contributors don’t care about them. And it doesn’t mean that in all cases managers care more, just in most cases. For more information please see our Buyer Based Open Core model: #buyer-based-open-core . We should not hesitate to open source features. When we put a feature into too high of a tier we should move it quickly because not having it in all tiers limits who can use the feature and this hurts: In most cases the following guidelines apply: Below is a video with the CEO discussing the philosophy of this with a Product Manager: When in doubt, we will default to moving features to a lower tier, moving it down quickly if data shows that this is more appropriate. Not all tier changes will involve moving things to lower-priced plans and paid features may move into the higher tier, but open source features should never move to paid tiers . Occasionally, a feature may unintentionally become available at a lower-tier due to a bug. This can include the free (open source) tier. When we fix the bug and return the feature to the correct tier, this is not the same as moving an open source feature to a paid tier. The definitive source for a given feature’s tier is our documentation . We should focus on building new features that buyers want and making sure that the initial assessment of new features is never too low. While we reserve the ability to move features up in tier, proving the necessity has a much higher bar since we are constraining the existing reach of a feature rather than expanding it. Please also note that the CEO is in charge of pricing and tiers; this is delegated to product for the day-to-day work. While other parts of the GitLab organization are consulted, the CEO is the directly responsible individual . To propose a change that impacts pricing or changes a feature’s tier (e.g. moving a feature from Premium to Free), please follow the process outlined in the Feature Tier or Pricing Change issue template . What is interesting is that GitLab creates more value as you adopt more of it. This shouldn’t be confused with DevOps maturity. DevOps maturity is how advanced your practices are and how fast your DevOps lifecycle is, shown in cycle analytics . With the best practices embedded in GitLab, you will mature faster than without it. GitLab enables a 200% faster DevOps lifecycle. But DevOps maturity is mostly about organizational change. GitLab the product is just an enabler of it. Even if an organization uses everything of GitLab (high DevOps score), they can still have a slow process (slow lifecycle). We know there is a correlation between a higher DevOps score and a faster lifecycle; but especially in organizations new to DevOps, it is a trend, not an absolute. Linking our tiers to maturity would mean we don’t ask any money from the large organizations that currently have a slow lifecycle but that are making it faster by adopting all of GitLab. These large organizations with a slow lifecycle benefit the most from GitLab, since they can adopt it completely, because they are not held back by an existing toolchain. As suggested by a user on X (X login required). This is a good suggestion to consider given the GitLab approach to adapting to local markets throughout the business. GitLab will assess viability of the idea in our overall pricing strategy planning. It is difficult to do and our assessment of this will consider:",
  "GitLab’s key goal in pursuing acquisitions is to accelerate our roadmap and offer better tools to customers more quickly. We are seeking to introduce new product categories and mature early categories faster than by building everything internally. We want to acquire strong teams that have already built great tools and products relevant for GitLab customers and have them integrate their tech into GitLab. Additional benefits to GitLab: We are looking for companies interested in helping deliver on our vision to create a single application for the entire DevOps lifecycle. We value strong teams and technology and are looking for a combination of both that will help us accelerate our roadmap. We’re looking for acquisitions which can either present a strategic value-add or create potential for significant revenue upside. Below is a set of general, yet not strict, characteristics of companies that are a potential fit for our acquisition process: Acquisitions receive a compensation offer specific to that engagement, which will be evaluated as part of our acquisition process as we learn more about your company and your technology. If you are interested in starting acquisition discussions with GitLab, please send an email to Corporate Development to connect and start the process. Learn more about our internal acquisition process and how you can prepare for an efficient process. For additional information contact Corporate Development .",
  "We’re the GitLab User Experience (UX) department. We comprise four areas to support designing and building the GitLab product. Our goal is to make our product easy to use, supportive of contributions from the wider GitLab community, and built for a diverse global community. We want GitLab to be the easiest and most delightful product in its class. We work closely with the community, and our stable counterparts Product Managers (PM), Frontend engineers (FE), Backend engineers (BE), Quality engineers, and the Brand team. We follow GitLab’s shared process referred to as the Product Development Flow and R&D Interlock . Every Product Designer is aligned with a PM and is responsible for the same customer benefits the PM oversees. Technical Writers each support multiple stage groups. UX Researchers support multiple groups within a section. In the spirit of having stable counterparts, we plan headcount as follows: To request a new product/service, open an issue using the UX procurement proposal issue template . Your manager will work with UX Leadership to identify spending needs and partner with FP&A to secure budget before following the vendor lifecycle . GitLab uses labels to categorize, prioritize, and track work. The following is a breakdown of the labels most directly related to the UX workflow. An overview of all the label types and uses can be found in the contributing doc . UX label : Indicates that UX work is required on this issue. These issues can be new features, ideas for improvement or anything else where UX should contribute their expertise. Inclusion label : A change to GitLab that promotes inclusion as it relates to our diversity value. Inclusive design label : Considering, exploring, and evaluating the different ways someone would access, interact with, or contribute to content that results in a more accessible experience. Accessibility and scoped accessibility labels are used to identify issues with accessibility impact. The scoped labels should be added after an accessibility audit has validated the impact and used in combination with priority and severity labels to triage an issue. learnability label : Issues that address learnability problems by helping users quickly become familiar with GitLab features. Scoped workflow labels from the Product Development Flow should be used to indicate where an issue is in the development lifecycle. Issues can move between workflow labels as many times as necessary, and not all labels will be applicable to every issue. Issues that require UX would use one of these labels as defined in the Product Development Flow: Pajamas component lifecycle labels are scoped labels used for creating and updating Pajamas components. Label usage guidelines can be found in the Pajamas component lifecycle documentation . UX problem validation label : Indicates that the issue requires UX work to validate that the problem is relevant to users. We use this label in addition to the Product Development Flow scoped labels, so that we can track validation efforts over time in our UX Performance Indicators . UX solution validation label : Indicates that the issue requires tasks to validate that the proposed solution is technically feasible and meets user needs. We use this label in addition to the Product Development Flow scoped labels, so that we can track validation efforts over time in our UX Performance Indicators . UI polish label : Indicates the issue covers only visual improvement(s) to the existing user interface. Deferred UX label : Deferred UX results from the intentional decision to deviate from the UX vision or MVC, which sacrifices the user experience. Deferred UX labeled issues are to be included in subsequent releases. Use this label to indicate that the UX released does not meet: This label is applied to any follow-up issues that address a UX gap. It does not apply to the issue or merge request that created the Deferred UX. For example, if the agreed MVC design solution is not fully realized due to release pressures or implementation oversight, that’s considered Deferred UX. If the design is implemented correctly but unforeseen UX issues are identified, it is not considered Deferred UX. If in doubt about when to apply this label, use the following rule: If you can say “This UX problem did not originate from an issue or merge request,” then it’s just UX, not Deferred UX. In case your team makes the decision ship an MVC that contains Deferred UX, it is recommended to create an issue to track it as soon as the change has been released. Learn more about Deferred UX as a UX Department Performance Indicator . Seeking community contributions System Usability Scale (SUS) labels : Indicates that the issue is related to usability problems surfaced in one of our SUS research efforts. More specifically, issues related to SUS that are prioritized can be labeled with the corresponding Fiscal Year and Quarter. For example: SUS::FY22 Q2 - Incomplete . Learn more about SUS score as a UX Department Performance Indicator Regression label : Indicates a bug introduced in the latest release that broke correct behavior (see the contribution guidelines for more info). UX scorecard label : Indicates the primary issue or epic for the UX Scorecard . We use this label to help us easily find current work and track efforts over time. UX scorecard-rec label : Indicates this issue is a recommendation that was a result of a UX scorecard review. It’s OK if the issue was created prior to the scorecard being done; it can still be pulled into the set of recommendations. CM scorecard label : Indicates the primary issue or epic for the CM Scorecard . It is used to easily find current work and track efforts. cm-scorecard-rec label : Indicates this issue is a recommendation that was a result of a CM Scorecard. Actionable Insights document learnings from research that need to be acted on. Type labels : Used to track feature, maintenance, and bug issues and MRs. UX Leadership are active participants in influencing the prioritization of all three work types. See also who are the DRIs for prioritization . Theme labels can be created to group issues that solve a similar user experience problem but don’t have a category. This can be especially useful for a user experience that spans the product. These issues still require a UX label. UX: Feature Discovery Improvement : Indicates issue may improve feature discoverability. UX: Onboarding Improvement : Indicates issue is a potential onboarding improvement. The UX Calendar ( internal only ) is the SSOT for our team meetings. You can find the details for UX calls, UX Forum, and other team meetings here. These meetings are open to everyone in GitLab. Anyone in the UX department can add events to the Google Calendar. Managers and above can make changes and manage sharing, while ICs can make changes to events. Please reach out in the #ux_leadership Slack channel with any questions or requests. The UX All Hands meeting takes place every six weeks, with two sessions to accommodate both EMEA/AMER and APAC/AMER-friendly time zones. The purpose is to share company updates, stay connected, and receive feedback. The entire UX department is invited, though anyone at GitLab is welcome to attend and contribute to the All Hands agenda (internal). As with all general meetings at GitLab, attendance is optional though encouraged, and will be recorded. To understand the specific challenges faced by the UX Department, we hold an async UX retrospective after every milestone. This retro is carried out through a new Issue created for the recent release in the ux-retrospectives project. The goal is to evaluate what went well, what didn’t go well, and how we can improve. The UX Forum is a recurring meeting for UX team members to share and discuss their work. This includes past, current, or future work, and covers Product Design, UX Research, and Technical Writing. All meetings are recorded and made available on Unfiltered for Product, UX, Engineering, and Leadership to watch at their convenience. In UX, we utilize performance factor worksheets ( internal only ) as a way to facilitate talent assessment and growth conversations between manager and their direct reports. These worksheets are available in Google Sheets format and the spreadsheets include tabs for a mid-year and year-end review, as well as a tab to list Achievement, Strengths, and Opportunities throughout the year. It is strongly encouraged for each team member to have their own worksheet created at the start of the fiscal year so that it can be used as a tool throughout the entire year. Performance factor worksheets can be utilized to help efficiently complete the year-end company talent assessment program within Workday .",
  "The GitLab Legal Commercial team is responsible for all contracting matters at GitLab. This includes, procurement, revenue, channel, technical and alliances. The Commercial Team partners with sales, technical, and business stakeholders to ensure the alignment with GitLab contracting standards, as well as the most efficient timeline to reach execution.",
  "Please note that all links are GitLab-internal only.",
  "Employment law governs the relationship between employers and employees. At GitLab, the Legal Employment team serves as a strategic partner across the organization, providing expert guidance and proactive legal solutions throughout the entire team member lifecycle. What does this partnership entail? We * collaborate strategically with Sales, Go-to-Market, Finance and People teams to develop scalable, compliant employment solutions as GitLab expands globally and responsibly, implementing a comprehensive process for gathering, assessing, and acting on country-specific information. We empower the Talent Acquisition department to achieve its critical goals while ensuring compliance with local laws and regulations during sourcing, recruitment, and hiring processes. We enable the People Operations team to effectively onboard and support team members with location- and role-specific legal support, setting team members up for success from day one. We partner with the Total Rewards team, People Operations team and People Business Partners to develop forward-thinking policies that balance GitLab’s business objectives, team members’ needs, and global legal requirements. We provide strategic counsel to our Team Member Relations team, People Business Partners, and Total Rewards as they navigate complex team member matters, including reasonable accommodation requests, performance management, career progression, and other relationship dynamics. We champion our Diversity, Inclusion, and Belonging initiatives by aligning all employment decisions with our mission statement and applicable laws. We safeguard GitLab’s interests while also providing fair treatment during employment transitions, including offboarding processes and post-employment matters. To work with the Legal Employment team, reach out early in your process, provide complete information about your needs and any time constraints you have. Being specific about what you’re requesting and any deadlines you face will enable the Legal Employment team to best triage requests and support you. To connect with Legal Employment, especially on sensitive matters, you can use the ’ legal-employment@gitlab.com ’ email address, or for non-sensitive queries, you can reach out in the #legal slack channel. Note that GitLab team members with individual employment queries should reach out to Team Member Relations team on ’ teammemberrelations@gitlab.com ’ or to their aligned People Business Partner for support. For cross-functional projects that do not involve individual team members but do require the Legal Employment team’s attention, please (i) open an Issue in the Legal and Compliance Project ; (ii) select the appropriate Issue Template; (iii) apply the label legal-employment :: to-do and (iv) if you know which Legal team member you will be working with, include them as an Assignee. This will update the Legal Employment Issue Board for the Employment team’s benefit, and allow the team to pick up and/or assign appropriately. A table showing coverage for queries by subject matter is available for internal use and can be accessed here . You are welcome to consult any member of the Legal Employment team. It is worth considering the privilege guidelines when communicating with GitLab’s Legal and Corporate Affairs team (aka the LACA team), including the Legal Employment team. Note that, if a communication is privileged, it can be protected from disclosure in litigation or other disputes. If in doubt, there’s an internal, GitLab University micro course on ‘Privileged communication’ , which is relevant to any team member who communicates with the LACA team, seeking legal advice on behalf of GitLab. The training explains the legal protection which may be applied to certain communications between team members and LACA and how best to communicate in order to be protected. It is also important to note that, if you receive any formal documents that appear to be legal notices, demands, subpoenas, court papers, or other potential litigation materials, you should please forward these by email to the legal team immediately (to the ’ legal@gitlab.com ’ email address) without responding yourself. Please add a brief note explaining when and how you received the document. These materials often have strict response deadlines, and your prompt sharing allows the legal team to review and address them properly to protect both you and GitLab.",
  "Corporate Sustainability is a business approach that enhances long term stakeholder value by implementing a strategy that considers every dimension of how a business operates when making social, environmental and economic progress. ESG stands for Environmental, Social and Governance and refers to the three key factors when measuring the sustainability and ethical impact of an investment in a business or company. At GitLab we use Sustainability and ESG interchangeably. Both terms are relevant to our work and both serve different purposes depending on the audience we interact with. The Sustainability Team creates and maintains GitLab’s Corporate Sustainability strategy and programs by driving and integrating responsible business practices and ESG regulatory compliance. The Sustainability team builds and maintains strong internal and external relationships to understand stakeholder expectations - including customer, investor and team members. This engagement allows us to remain customer centric and easy to do business with while meeting shareholder and team member expectations. The Sustainability Team has two primary functions and four corresponding programs: Compliance & Reporting : Manages customer and prospect ESG-related questionnaires, requests for proposals, and sustainability contractual clauses. Leads annual Sustainability report and external assurance process. Monitors global ESG regulation and works cross-functionally to advance GitLab’s culture of compliance with applicable ESG regulations. Responds to investor ESG rating agencies to maintain competitive ESG scoring among peers. Climate Action : Identifies and executes strategic climate programs, including annual measurement of GitLab’s greenhouse gas inventory, emissions reduction target-setting, partners with Procurement to run the sustainable supplier program, identifies emissions reduction opportunities across the business, and purchases high quality carbon credits. Manages team member & customer communications related to GitLab’s commitment to environmental sustainability. GiveLab : GiveLab is GitLab’s team member volunteer program and includes year-round volunteering, GiveLab 30 days of Impact, our annual company-wide volunteer campaign and GiveLab Champions Program. Volunteerism is an effective way to build trust through social connections - this leads to higher individual and team motivation, and greater cross-functional collaboration. GitLab for Non-Profits : GitLab’s in-kind donation program. Manages social impact communications with team members and customers, while developing strategic nonprofit partnerships to advance GitLab’s ESG goals and help enhance our brand reputation by demonstrating our commitment to the nonprofit community. Deeply integrated into our business philosophy, GitLab’s Sustainability strategy is driven by our values of Collaboration, Results for Customers, Efficiency, Diversity, Inclusion and Belonging, Iteration, and Transparency (CREDIT). GitLab’s stakeholders, including customers, investors, team members, and community members play a key role in GitLab’s Sustainability strategy. GitLab conducted its inaugural ESG materiality assessment in 2022. By conducting a materiality assessment with our stakeholders, we identified which ESG topics have the greatest impact on GitLab’s business and where we have the potential to have the greatest impact on the environment, society, and our global communities - those material topics drive the programs, policies and initiatives under our strategy. This page will continue to be updated as we make progress towards developing plans and programs to advance our Sustainability goals. The purpose of the advisory committee is to create cross-functional alignment on ESG objectives and decision making, to go beyond simply compliance and into long-term operational implementation. Members of the Sustainability Advisory Committee Like all functions at GitLab, transparency is a core focus. Every year GitLab publishes an annual sustainability report where we share our approach to managing our key sustainability focus areas, provide updates on programs and policies, achievements to date, metrics and targets, and plans for the future. The Sustainability team works cross-functionally to prepare GitLab for compliance with ESG regulations. The Sustainability team also supports internal teams with customer ESG questionnaires and RFPs. Customers and prospective customers are increasingly asking about GitLab’s Sustainability programs, including questions related to GitLab’s climate commitments and greenhouse gas emissions. Many of our customers are also subject to ESG regulation and we expect customer ESG questions to continue to increase as they look to better align to new regulations. GitLab is committed to doing our part to minimize our environmental footprint, including working to reduce greenhouse gas (GHG) emissions associated with our operations. GitLab’s stakeholders, including customers, investors, regulators, and team members expect the company to operate sustainably and to do our part to reduce our environmental footprint. Many of GitLab’s customers have GHG reduction targets and as a vendor, GitLab’s carbon emissions contribute to our customers’ emissions footprints. To remain easy to transact with, GitLab needs to meet the expectations of our customers by taking action on climate change. GitLab’s Climate Action Program consists of four pillars: Measure & Report: Every year GitLab conducts an annual greenhouse gas (GHG) inventory in alignment with the GHG Protocol, the global best practice carbon accounting standard. We publish the results of the inventory in our annual ESG report and have the data assured by a third-party. In 2026, GitLab is subject to new regulations in the US and the EU that will make these disclosures mandatory. Act: GitLab is taking action to reduce our carbon emissions. As a fully remote software company, the majority of our emissions come from our suppliers. Engaging our suppliers to measure their carbon emissions and set their own reduction targets is a critical component of our reduction pathway, which is why we have set an aspirational supplier engagement target. Please see the Sustainable Procurement Program for more information. The Sustainability team continues to explore other ways to reduce emissions and is doing further analysis on additional reduction pathways. Engage: This includes engaging GitLab team members in climate education and action. In 2024, we launched the GitLab Team Member Sustainability Guide , providing actionable steps team members can take at home to minimize their environmental impact. In FY26, we launched our partnership with Mammoth Climate , a climate literacy and challenges platform, to further engage team members with educational materials, activities, and rewards Accelerate: While GitLab works to reduce our emissions, we are also committed to accelerating climate solutions by purchasing high quality carbon credits to cover a portion of our carbon footprint. We are proud to partner with Rubicon Carbon to purchase a diversified portfolio of high quality carbon credits, financing carbon removal projects that meet Rubicon’s high standards of quality . In FY25, GitLab set an aspirational science-aligned supplier engagement target to reduce our Scope 3 emissions: 70% of our suppliers (by emissions) will have science-aligned climate targets by FY29. Our sustainable procurement program includes the following initiatives: We look forward to sharing updates on this new initiative. The Green DevOps Working Group at GitLab aims to embed sustainability throughout the software development lifecycle to reduce environmental impact while unlocking operational and business value. Its core objectives include lowering carbon emissions and energy use, integrating green practices into business processes, enhancing customer capabilities to meet regulatory and sustainability goals, and positioning GitLab as a leader in sustainable DevOps and green software. Workstreams address areas such as cloud sustainability, CI/CD energy consumption tracking and reduction, community innovation and engagement, sustainable AI and procurement, and reporting. Each workstream is led by the Sustainability team and the relevant functional team member. Customers who are interested in providing insights and feedback on sustainability features are encouraged to contact the Sustainability Team at esg@gitlab.com for more information. At GitLab, all team members do work that supports the company, which supports the enhancement of an open source codebase. This codebase is freely available to everyone to make better software faster and drive progress through what they build. Between 2022 and 2024, team members made over 125,000 commits to the open source part of the GitLab codebase. But, there are also other ways to give back and many team members choose to contribute beyond GitLab. In addition to contributing to GitLab, GitLab offers additional optional pathways for team members to give back while leveraging their unique skills and passions through programs such as GiveLab, GitLab’s Team Member Volunteer Program. GitLab encourages team members to take part in volunteer initiatives such as supporting their local communities, participating in virtual volunteer activities, and organizing volunteer activities as part of team events. Corporate volunteerism has been proven to be an effective strategy for boosting engagement , improving employee retention, and strengthening relationships at work . Volunteering with GiveLab supports team members in fostering connections, building trust among one another and embodying our CREDIT values while positively impacting our communities. Team members may self organize volunteer events at any point throughout the year. To submit a request for a team volunteer activity with a Registered Nonprofit Organization that isn’t on the current GiveLab Nonprofit Directory , please go to the Philanthropic Requests epic and open a new issue using the Volunteer_Support Template . Team members can also request support from the Sustainability Team to organize local or virtual volunteer opportunities on their behalf by going to the Philanthropic Requests epic and opening a new issue using the Volunteer_Support Template . Please write “yes” for the question, “Would you like the Sustainability team’s help organizing the volunteer activity?” All team members and volunteer activities must adhere to the GitLab Philanthropy Policy . Team members must follow GitLab’s paid time off (PTO) policy if volunteering during work hours and use the “public service/volunteer” option in Workday. If volunteering in person, team members may incur some expenses. Team members can expense up to a total of $25 per volunteer event for expenses incurred that meet the allowed for reimbursement criteria. All expenses should be submitted in Navan using the “GiveLab” classification. Please note that GitLab does not allow team members to travel to in-person volunteer events. All in-person volunteering should be local to the team member. As with all company expenses , team members must be thoughtful in spending the company’s money and use best judgment to ensure that all expenses are deemed “ordinary and necessary.” Team members should follow all team member expense responsibilities . Expenses allowed for reimbursement (for in-person volunteer events): Expenses not allowed for reimbursement: As with our unique ways of working, GitLab and its team members have identified and sought out opportunities for impact that speak not only to our values but also to our all-remote nature. To review previous opportunities that team members participated in, visit the historical activities page . GiveLab 30 Days of Impact is GitLab’s annual volunteer campaign created to encourage team members to foster connections, build trust among one another and embody our CREDIT values while positively impacting our communities. We have designed this program with our high-performing team culture and results for customers in mind. GiveLab 30 days of Impact runs annually in Q4 and our goal is to encourage as many team members as possible to volunteer over the course of 30 days. The information on GiveLab 30 Days of Impact 2025 can be found on the internal Loop page . Throughout the month, team members can volunteer as little as one hour of their time to make an impact. We understand that our team members are driven by many different factors, and we welcome that volunteer participation will look different for everyone. Through GiveLab 30 days of Impact we aim to offer many different ways for team members to get involved such as: While GitLab encourages year-round volunteerism through GiveLab , GiveLab 30 days of Impact centralizes our efforts into an annual campaign to have a larger collective impact over a specific timeframe. Corporate volunteerism has been proven to be an effective strategy for boosting engagement , improving employee retention, and strengthening relationships at work . Additionally, this program offers volunteering opportunities around a major holiday season in many parts of the world, a time when many are seeking opportunities to give back. Team member participation is voluntary, should not interfere with work commitments, and time off is required to be in alignment with GitLab’s PTO policy . Travel is not permitted for this program. Team Members may choose to volunteer virtually or through local in-person events. Volunteer events typically last between one and four hours. Managers play an important role in supporting team members in taking time for themselves and their families, while also ensuring accountability to results and coverage for teams and its goals. Our Results for Customers value sits at the top of our values hierarchy, and our PTO policy empowers managers to appropriately manage workloads and deliverables, while also giving team members the time away they need from work. Team Members taking time off to volunteer should communicate time off in advance with their manager. To request volunteer time off, follow the Paid Time Off procedures outlined in our handbook and reach out to People Operations via HelpLab should you have any concerns. A step-by-step guide on how to request paid time off can be found here . Note that volunteer time off should be used towards acceptable volunteer activities and in adherence with our GitLab Philanthropy Policy . Please see our GiveLab reimbursement policy to understand current allowances as they relate to volunteering costs. We have created an internal GiveLab Volunteer Directory that features a list of vetted nonprofit organizations with available volunteer opportunities. Team members can search the document for virtual volunteer opportunities, opportunities to volunteer with GitLab Foundation grantees and search for local opportunities. We encourage all team members to contribute to our GiveLab Volunteer Directory. To recommend a nonprofit organization to add to the Directory, please open a Volunteer Recommendation Issue . The GiveLab Champions are team members who are passionate about giving back to their communities and want to encourage other team members to do the same. GiveLab Champions self-identify to participate in the voluntary Champions group, managed by the Sustainability team. The GiveLab Champions help activate the GiveLab signature program, but also work to organize and promote volunteer opportunities year-round. GiveLab Champions help team members build trust through social connections, build connections within their communities, and help GitLab provide meaningful opportunities for team members to give back. The GiveLab Champions ensure global voices are heard and relevant causes are represented based on where team members live. GiveLab Champions help to make GitLab a better place to work. The time commitment for a GiveLab Champion is estimated to be 3-5 hours per quarter. Participation can vary throughout the year. GitLab launched GitLab for Nonprofits , an in-kind donation program in 2023. Through this program, GitLab supports Registered 501c3 (or jurisdictional equivalent) Nonprofit Organizations in good standing that align with our values by offering free licenses and seats. The program operates on a first come first served basis. Once the annual donation of 5,000 seats is met, the application will remain closed for the year. What are the benefits of the GitLab for Nonprofits program? GitLab is a single platform for project management, collaboration, source control management, git, automation, security, and much more. Because it is easy to use, flexible, and all in one place, it is the best choice for nonprofits to scale their work. The GitLab for Nonprofits Program gives free licenses of GitLab to registered nonprofit organizations. Nonprofits accepted into the program will be provided a free Ultimate license for one year (SaaS or self-managed) for up to 20 seats. Additional seats may be requested although they may not be granted. Who qualifies for the program? GitLab supports Registered 501c3 (or jurisdictional equivalent) Nonprofit Organizations in good standing that align with our Values . A “Registered Nonprofit Organization” is one that has been registered with the local government or authorized agency within its applicable local, state, provincial, federal or national government. For the calendar year 2024, we will limit the in-kind program to 5,000 seats, which was approved by finance and the board in the Philanthropy Policy . Each organization will be eligible for up to 20 seats. This will allow us to assist as many organizations as possible. This will be revisited throughout the year and adjusted as needed. Interested organizations who are new customers may request additional seats although the request may not be granted. To limit churn, current GitLab customers that apply to transition to the Nonprofit Program will not be granted a special request above the 20 seats. What are the terms of the GitLab for Nonprofits program? Upon acceptance, program members are subject to the GitLab Subscription Agreement . The decision to issue a GitLab for Nonprofits license is always at the sole discretion of GitLab. Interested organizations need to visit the GitLab for Nonprofits page and submit the application form . How are applications processed? Nonprofits apply on the GitLab for Nonprofits page. Once the application is submitted, the Nonprofit will receive a message and a link to TechSoup, our verification partner. The Nonprofit will then need to log in or create their TechSoup account. TechSoup provides a rigorous vetting process to ensure the nonprofit is eligible for the GitLab for Nonprofits program and meets all requirements. If a Nonprofit is verified, TechSoup will notify GitLab. GitLab will then undergo its own vetting and approval process. Once all parties have verified and approved the Nonprofit, GitLab will send the instructions directly to the Nonprofit to redeem their license. If a Nonprofit is not verified through TechSoup, TechSoup will provide details on how the Nonprofit can become verified. If a Nonprofit is declined from GitLab, GitLab will notify the Nonprofit via nonprofits@GitLab.com . Please allow up to 15 business days for the application and verification process. Must Nonprofits renew their memberships? Yes. All nonprofits must renew their membership annually, which involves a re-verification process. Nonprofits will submit for renewal the same way they first applied for the program. Where can members receive support? While GitLab for Nonprofits Program benefits do not include product support , program members can receive help with GitLab in a number of ways. In general, we recommend the following: I’m a GitLab Team Member and I have a customer applying for the program. What do I do? Information on how GitLab Inc. supports Registered Nonprofit Organizations can be found in the Philanthropy Policy . Please note that for all Philanthropic Requests, including requests for GitLab to join as a member to an association, program or organization, approval by the Sustainability team and CLO is required as defined by the Oversight Responsibility section of the Policy. If you would like to submit a philanthropic request, please follow the instructions based on your request type. There are two ways that team members can submit a request for monetary support: Request funding from the Sustainability team to support a Registered Nonprofit Organization OR (not currently accepting applications - see instructions below on submitting a non-profit for future consideration) Request utilizing department or TMRG budget to support a Registered Nonprofit Organization If you are requesting funding from the Sustainability team to support a Registered Nonprofit Organization, please note that at this time, we are not accepting applications. If you would like to submit a Nonprofit Organization to be considered for support in the future, please go to the Philanthropic Requests epic and open a new issue using the Monetary_Support Template . You will be notified if there is a future opportunity. If you have a department or TMRG budget that you would like to utilize to support a Registered Nonprofit Organization, please go to the Philanthropic Requests epic and open a new issue using the Monetary_Support Template . Please tag your manager to approve the request if you are submitting on behalf of your department. If you are submitting a request on behalf of a TMRG or DIB, please add the DIB DRI as a reviewer. Please allow a minimum of 10 working days for review. The team member submitting the issue is responsible for obtaining proper approvals and working with Accounts Payable to issue the payment. Please tag the individuals in the approver section of the issue. Once approvals are completed, the team member requesting the donation needs to obtain an invoice from the non-profit that contains the bank payment details and submit this to AP@GitLab.com . For requests related to GitLab Membership of Association, Program or Organization, and includes terms, conditions and/or obligations on GitLab that must be executed, please follow the below process. Open an issue using the Membership Request Issue Template . Complete and attach the necessary information. Note: If you are submitting a request on behalf of a TMRG or DIB, please add the DIB DRI as a reviewer. NOTE: For any request(s) that require payment, please be certain to follow applicable ESG & Procurement processes. At this time, GitLab does not offer a matching gifts program.",
  "The Strategy & Legal Ops team promotes and institutes streamlined processes, efficient tools, and centralized program management to ensure LACA remains agile and able to support every area of GitLab’s business. Check out our issue board to learn more about what we’re working on. GitLab uses Brightflag’s Legal Spend Management platform to process and review legal invoices and accruals. See the Brightflag invoicing process in the internal handbook. Accruals submission reminder notifications are automatically sent through Brightflag to ensure vendors submit their accruals on time. This process is designed to enable LACA team members to submit requests related to attending events/conferences, furthering development, or purchasing tools/software funded by LACA. This process does not apply to equipment , Individual Use Software or other personal reimbursement requests.",
  "The Privacy Team is part of the Legal and Corporate Affairs Team. We provide support and guidance to uphold consistent business processes around the protection of personal data as it relates to GitLab customers, users, Team Members, and other natural persons. We collaborate cross-functionally and serve as advocates to ensure that the data privacy practices of GitLab meet the needs of our cross-functional partners and are continually balanced with an ever-changing global data privacy and protection landscape. Slack channel - #legal is the best place for questions relating to our team that do not require legal advice, deliverables, or any discussion of confidential information. For issues that require action from the Privacy Team, apply the label Privacy::Intake . This will update the Privacy Legal Issue Board and allow the team to triage the issue appropriately. We also use the following labels: For sensitive, private, or confidential requests email legal_internal@gitlab.com . Please do not send emails to this address for engineering, marketing, sales or procurement requests. These should be directed to #legal or an issue should be created in the Legal and Compliance project. Tell people what you are doing with personal data and why you are doing it so that the person can make an informed decision about whether they want to allow it to happen. Do not be creepy about what personal data is collected or how it is used and do not change the way personal data is used without first giving people notice and an opportunity to object, or, where required, obtaining prior consent. Make it easy for people to tell us their privacy preferences and honor those preferences even if they change over time. Build a product or service that has privacy-focused settings turned on by default and let the consumer decide if and when they want to change that. Transparency is a core value and every team member is responsible for the proper collection and use of personal data consistent with our Privacy Statement . Anonymization The process of permanently and irreversibly altering personal data in a way that it is no longer capable of being related back to a specific individual. Consent A freely given, specific, informed and unambiguous indication of an individual’s wishes. Consent is captured by an un-ticked checkbox or other unequivocal statement which signifies agreement to the processing of personal data before or at the time of collection. Data Classification A method of determining types of data associated by risk. See GitLab Security Data Classification Standards for more information. Data Controller A natural or legal person, agency, or other entity which alone, or jointly with others, determines the purpose and means of processing personal data. For example, GitLab is a Data Controller is in the areas of marketing and sales where the personal data of prospects and leads is managed solely at our discretion. GitLab also serves as a Data Controller for all personal data collected from Team Members for employment purposes and any administration of benefits. Data Processor A natural or legal person, agency, or other entity which processes personal data on behalf of a Data Controller. GitLab acts as a Data Processor when we manage personal data native to a Customer’s instance or namespace. GitLab acts as a Processor in these situations because the Customer is the ultimate owner of the data it submits to the service offerings, and our contracts service as Customer’s instructions to GitLab regarding the processing of their data. Data Subject An identified or identifiable natural person. Data Subject Rights Rights granted to individuals in relation to personal data or information processed about them. Because Data Subjct Rights are instrumental to the privacy and protection of data subjects, many of these rights are codified under global privacy legislation, such as the GDPR, CCPA, and LGDP. If a business processes personal data pursuant to certain bases such as consent or legitimate interest, then a data subject may assert one of its fundamental rights and a business is obligated to respond under law. The rights granted vary slightly by country, region, province or state. GitLab treats all users and Team Members the same and will respond to a data subject request from any individual user or team member even if they live in a country, region, or state/province without specific data protection laws. Expand the following section for more information about the data subject rights available. Right of Access A request seeking access to the specific pieces of personal data that have been collected and used by a Data Controller. Right to Correct A request asking for inaccurate or incomplete personal data to be corrected. Right to Delete A request which seeks the erasure of personal data relating to the data subject. Deletion requests must meet certain conditions and businesses are not required to delete any personal data that is processed to meet legal obligations, including that data which may be processed in pursuit or in defense of claims. Right to Portability A request where the data subject wants to transfer their data to another Data Controller; typically seen when the individual changes service providers that share a compatible electronic filing system. Right to Restrict Processing This is a request for the Data Controller to stop processing personal data under certain circumstances. This may also include a request to limit the use and disclosure of Sensitive Personal Data. Right to Object A request to opt-out of all data processing or specific processing of personal data based on consent or legitimate interest. Generally this is a request to opt-out of processing for targeted advertising, which includes the sale or sharing of personal data for profiling or cross-context behavioral advertising. Right Not to be Subject to Fully Automated Decisions This is a request that the data subject not be subjected to a decision based solely on automated processing, including profiling, which would have a significant legal impact. An example might be an algorithm that excludes someone of a certain race from obtaining a credit card. DPIA A Data Protection Impact Assessment is a method to review and document identified privacy compliance risks, as well as evalute higher risks to the rights and freedoms of individuals, including any that pose potential for significant harm. Learn more about GitLab’s process for completing DPIAs here . Personal Data Any data, individually or when combined with other data, that identifies, relates to, describes or is reasonably capable of being associated with or linked to an identifiable natural person (a ‘data subject’), whether directly or indirectly. See also, Sensitive Personal Data. Privacy by Default A concept that should be implemented at the product development stage and uses appropriate measures to ensure that, by default, the only personal data processed is what is truly necessary. In practice, this means a user’s privacy settings prioritize privacy in their default state. Privacy by Design A concept which focuses on intentionally designing a product that incorporates foundational privacy principles and ensures that Controllers and Processors are able to fulfill data protection obligations. This may include appropriate technical and organizational measures such as pseudonymisation and encryption. Pseudonymization The process of altering personal data so that it can no longer be attributed to a specific individual without the use of additional re-identifying information. In order to practice successful Pseudonymization, the re-identifying information should be kept separate from the pseudonymized data. Publicly Available Personal Data Refers to personal data that is publicly available from federal, state, or local government records or made manifestly public by the data subject. Under limited data privacy laws this may also include personal data made public through widely distributed media. Sensitive Personal Data Data that is particularly personal and intimately tied to the core identity of a person. This type of data generally includes racial or ethnic origin, political opinions, religious or philosophical beliefs, trade union membership, genetic data, biometric data, data related to health, data related to sex life or sexual orientation, criminal offenses, and citizenship/immigraion status. In some jurisdictions, Sensitive Personal Data includes government identifiers and financial data. Every vendor that handles personal data is required to go through a Privacy Review prior to being onboarded, which includes completion and approval of the privacy due diligence questionnaires detailed in the Procurement process . Certain vendors who are classified as handling red or orange data under our Data Classification Standards are reviewed annually. Additionally, when new product features are designed, there are times when a formal privacy review is required. This section outlines the process for these reviews. For Third-Party Risk Acceptance, any Moderate/High risk requires VP and/or above approval Anytime a new feature or a change to an existing feature is planned, Product Managers and Engineering Managers should evaluate if the planned development presents a legal risk where personal data is involved. If Personal Data is implicated, utilize the Legal Risk Checklist and Workflow ( internal only ) GitLab Team Members are required to complete annual training which covers general privacy practices worldwide. The goal of annual training is to ensure that Team Members understand what personal data is and how to handle it to ensure that GitLab maintains the trust our customers have placed in us as well as to ensure that GitLab remains compliant with frequently changing legal and regulatory obligations.",
  "The guidance for using open source software has been updated to enable team members to comprehensively determine which open source license types are pre-approved (deemed acceptable) for use, and which require prior review by the Legal & Corporate Affairs team (as their use may be unacceptable). Team members wishing to use open source software should now refer to the comprehensive Blue Oak Council license list, and proceed, as follows: Team members must ensure that we comply with all requirements and restrictions associated with the applicable license (these are typically defined in the body text of the license). If you’re contributing to an open source project on behalf of GitLab, you may be required to enter into a CLA. In accordance with the Authorization Matrix Policy , legal approval is required to you enter into a CLA on behalf of GitLab. If you have the choice between a Corporate and Individual CLAs, opt for the Corporate CLA. Follow these steps to obtain legal approval and enter into a CLA on behalf of GitLab: Contributions to a third-party project on behalf of GitLab should be made using your @gitlab.com email address. Post any questions to the #legal Slack channel. Alternatively, if looking for information on contributing to GitLab see here . GitLab has established guidance to aid with determining authorship of academic papers developed at GitLab. This guidance is accessible to team members only here The purpose behind this initiative is to ensure consistent and fair licensing enforcement for breaches of certain licensing terms, in order to support the continued growth of the open source community. Further information on this initiative is available here . GitLab’s GPL Cooperation Commitment follows: Before filing or continuing to prosecute any legal proceeding or claim (other than a Defensive Action) arising from termination of a Covered License, GitLab commits to extend to the person or entity (“you”) accused of violating the Covered License the following provisions regarding cure and reinstatement, taken from GPL version 3. As used here, the term ’this License’ refers to the specific Covered License being enforced. However, if you cease all violation of this License, then your license from a particular copyright holder is reinstated (a) provisionally, unless and until the copyright holder explicitly and finally terminates your license, and (b) permanently, if the copyright holder fails to notify you of the violation by some reasonable means prior to 60 days after the cessation. Moreover, your license from a particular copyright holder is reinstated permanently if the copyright holder notifies you of the violation by some reasonable means, this is the first time you have received notice of violation of this License (for any work) from that copyright holder, and you cure the violation prior to 30 days after your receipt of the notice. GitLab intends this Commitment to be irrevocable, and binding and enforceable against GitLab and assignees of or successors to GitLab’s copyrights. GitLab may modify this Commitment by publishing a new edition on this page or a successor location. ‘Covered License’ means the GNU General Public License, version 2 (GPLv2), the GNU Lesser General Public License, version 2.1 (LGPLv2.1), or the GNU Library General Public License, version 2 (LGPLv2), all as published by the Free Software Foundation. ‘Defensive Action’ means a legal proceeding or claim that GitLab brings against you in response to a prior proceeding or claim initiated by you or your affiliate. GitLab means GitLab Inc. and its affiliates and subsidiaries.",
  "The Risk Management and Dispute Resolution (RMDR) division of GitLab Legal and Corporate Affairs (LACA) is responsible for informing and guiding GitLab’s risk management strategies as well as managing internal and external investigations, litigation and other dispute resolution. We seek to support resolution across a wide range of topics, including responding to subpoenas and discovery requests, drafting and revising legal documentation, managing investigations and negotiating and drafting agreements. It is our goal to proactively address and resolve these matters in support of GitLab’s business objectives, coordinating with internal business partners across the company whenever appropriate. In support of these goals, we adopt this mantra: if you see something, say something! There are times when GitLab team members must immediately consult with RMDR to ensure that GitLab is managing its legal risks effectively. These include: If in doubt, please involve RMDR earlier rather than later – we would always rather be proactive than reactive. You can reach out to RMDR via rmdr@gitlab.com . Privileged communication is communication, written or oral, that is protected from later disclosure in litigation because it was conveyed to the attorney in confidence by a client for the purpose of seeking legal advice or by an attorney for the purpose of giving legal advice . Privilege can also be asserted over certain confidential documents created by attorneys for the same purpose. The terminology differs depending on the jurisdiction. For example, in the United States, the privilege is generally referred to as “attorney-client privilege” for communications made to or from an attorney for the purpose of providing legal advice or “attorney work product” for communication or documentations created in relation to actual or anticipated litigation. In many of our EMEA and APAC countries, it may be called “client legal privilege,” “legal professional privilege,” “legal advice privilege,” or “litigation privilege.” Additionally, the scope of the privilege differs by country. It is therefore likely that the status of a privileged communication that contains legal advice in respect of foreign law will be determined by reference to the law of the country in which any action is taken. If you have jurisdiction-specific questions about privilege, please contact a LACA team member who sits in that jurisdiction. Practically speaking, this means that communications between our team members and members of LACA are not necessarily privileged just because a member of the LACA team is involved. As a threshold matter, the LACA team consists of both attorneys and non-attorneys. Communications may be privileged if the person wishing to assert the privilege can establish that the communication was made to an attorney of the LACA team and the dominant purpose of the communication was to seek legal advice. Conversely, if the dominant purpose of the communication - even if made to an attorney - is simply to seek business advice, it is very unlikely to be privileged. Additionally, otherwise privileged communication will lose its privilege in most circumstances if it is further or also disclosed to third parties. Thus, team members should always be aware of who has access to the communication and should be very careful about forwarding it. What are some tips for team members to follow to help maintain privileged communication? Below are the steps you can take to help ensure that any communications you have with GitLab’s LACA team can be considered privileged: For more information, please see the internal handbook here . A legal hold is the process GitLab uses to preserve all forms of relevant evidence, whether it be emails, instant messages, physical documents, handwritten or typed notes, voicemails, raw data, backup tapes, and any other type of information that could be relevant to an investigation, pending or imminent litigation or when litigation is reasonably anticipated. Legal holds are imperative in preventing spoliation (destruction, deletion, or alteration) of evidence which can have a severely negative impact on a company’s case, including leading to sanctions. Once GitLab becomes aware of an investigation or potential litigation, a GitLab attorney will provide notice to the impacted team members, instructing them not to delete or destroy any information relating to the subject matter of the investigation or potential litigation. The legal hold applies to paper and electronic documents. During a legal hold, all retention policies must be overridden. Your obligation to follow the procedures outlined in the notice continues until the hold is lifted, even if you depart the Company. If you depart the Company, all Company owned devices and any material you are holding in accordance with any active Legal Hold Notice or active Company investigation should be turned over upon your departure.",
  "Trade control laws, which often consist of sanctions, export controls, and import laws, govern how and under what circumstances technology, software, and technical assistance may be exported. Trade control laws vary from country to country but usually exist to protect national security and further foreign policy and economic interests. Under United States law, exports, re-exports, and transfers, can take many forms, including oral, written, and visual disclosure, physical shipment, and electronic transfer or transmission. An export can also occur when technology, software, or technical assistance is transmitted to U.S. nationals abroad, or to non-U.S. nationals located within the United States. The export of certain software, technology, or technical assistance to certain countries, certain end users, or for certain end uses, may require authorization from the United States government prior to export, re-export, or transfer. GitLab Enterprise Edition, related technology, and services (collectively, “GitLab Software”), are subject to the Export Administration Regulations (“EAR”), administered by the U.S. Department of Commerce, Bureau of Industry and Security (“BIS”), and various sanctions programs administered by the U.S Treasury Department’s Office of Foreign Assets Control (“OFAC”). The GitLab Community Edition is freely available to the public and is not subject to the EAR. GitLab continuosly monitors developments to these regulations to maintain compliance and to leverage any opportunity to broaden access to GitLab in a compliant manner that allows everyone to contribute. The GitLab Software has been classified via CCATS G178430 as a 5D992.c mass market encryption product with eligibility for export to most destinations under 15 CFR 740.17(b)(1) of license exception ENC. GitLab users may not export, re-export, or transfer GitLab Software, without first obtaining authorization from the U.S. government, to (a) any U.S. embargoed country including but not limited to Cuba, Iran, North Korea, Syria, Russia, Belarus, and the Crimea, Donetsk, and Luhansk regions of Ukraine, (b) any party identified on OFAC’s Specially Designated Nationals and Blocked Person list or the Department of Commerce Denied Persons, Entity, or Unverified lists, or (c) for end use involving sensitive nuclear, rocket systems, unmanned aerial vehicles, missiles, chemical or biological weapons, or for any other end use prohibited by 15 CFR 744. GitLab provides this information, which is subject to change without notice, to facilitate GitLab users’ compliance with applicable trade control law. GitLab users remain solely responsible for exporting, re-exporting, and transferring GitLab Software and any user-developed content in accordance with those regulations and should seek legal counsel as necessary.",
  "Okay, I have you one. Is this YCD wax, um, me T, 2024, January 17. Um, let's see, I'm going to start from the top. I actually have to move my plate to the bottom. Yes, that's a question for here. Um, and I think, yeah, we start with the, so we do not go out, you know, thank you. Thank you. I have a few updates. One is that I'm almost done with the competitive evaluation for fleet visibility and I did link the issue, which has the FIG jam file in it. Um, it is actually very, I hopeening to see what the other products are doing. I won't mention their names, but if you're interested, take a look or just wait for me to post the summary. It's probably gonna be faster if you just wait. And then a couple of things that are coming up. I wanted to get some eyes on to see if anyone has worked on something similar. One of them is that for the fleet dashboard, we're going to be adding a filter to filter by tags. And it's going to be a global filter for the dashboard. So it will filter all panels in that dashboard. Um, if anybody has worked on something similar, I just, I would appreciate your feedback on the placement of the filter there. And if it's clear. And then the other issue is a CSV export for time period. So we have, and the fleet dashboard, one of the panels is allows you to export the usage information as a CSV report. And then from there, people usually take that and like create internal dashboard or filter using whatever other tool that they're excelled or something like that. And now we want to allow them to choose the time period rather than it just by default being previous month. So I'm going to be working on that in this coming milestone. And that's more of just sharing for awareness. Yeah. If there's a good. Now I'm going to talk about the the filter while the filter placement. I'm trying to find the information in my brain, but I think when when we work on. Was it an environment dashboard was. Yes, environment dashboard. I think so. It's just that too many features. I think it's better if I just write down because I need to put an information in my brain. But I do have some feedback about this and also just some. Social validation that was done before. My question here is you have the the filter. That drop down component. Yeah. So you're thinking the future you're going to have more you know elements there and you why if that user can interact with. If we do it would only be one more and it would be. It was time you could filter by some type of time period. Those would be the only two that would be able to impact every single. Data point on the dashboard. But have you played around heavy instruments and with the that filter bar right the search bar where you have like the tokens and chips. Like what we have in merge requests the issues view. And if so why was the drop down. The option that you. Sorry everybody it's like dinner time. I've been to the computer for too many hours and like I'm like. I'm going to need to find out. But anyways, why what this instead of the Mr. Bar. Yeah, so we use the filter bar on the runners list view like the typical view that you come into if you go to the runners page. And we received feedback that. The experience filtering there isn't ideal because with tags. Specifically with tags every single time you go into that bar you have to add another tag. You have to say filter by tag add the tag and then say filter by tag add the tag and there's typically a lot of like there's pretty common sets of tags. So by allowing them to see that in a drop down. I don't know if I added a search bar in the this the menu. Yeah, so you could search for that tag and kind of like stay in one place and do it all at once. And then we just found we've also heard that it's not as discoverable when it's in the filter component because you don't know what filters are available to you. So I'm starting I haven't really edited what that page is going to look like in the future, but at some point we will. I'm going to improve the filtering for that list view. I don't know when it will be. Yeah, got it thanks for the. Take sort of insights. Yeah, I agree makes sense like you don't know what you can filter by right we repeat that. I feel there to search bar. Yeah. How about you also like show fields like below this. I'm mentioning all the tags that are selected because it's not a crowded space you have space. Yeah, because I mean, because this is the only thing that it's being filtered by and you just not being able to see all the tags which are selected. I would say that is problematic to click on the drop down and then get an idea about what this filter is all about. True, that's a great point. Initially wasn't planning on showing them all, but I think I could play around with just showing like the chips. Yeah, like the actual labels underneath. Yeah, I think that's a great addition as we to come and chin and then I also have just really really minor feedback. I just like I like that you're starting to play around with the drop down because I also could hear from you that the concern of using the current filtering bar patterns. Throke it love and then I know that we have a little bit of minor usability issue on that so I could totally get it. But I think my problem when I saw the design was like because I didn't think that the drop down is connect to the whole page. So maybe you could consider I don't know I'm just thinking out loud maybe adding a small borderline below the drop down so make. More visual higher to keep there would be helpful. Yeah, that's a good idea. I've also noticed a lot of other filter like when you use the filter component there's a gray background. Yes, I could try that out to. Yeah, maybe that would help like I guess, but I need to I need to see but you can definitely play around with it. Okay, yeah, thanks for the feedback. And then the one minor thing I wanted to say is I would be curious if people would clue into that being a filter more if it was on the right side over the left because I feel like filter and sorting place and get lab always seem to be on the right and I'm wondering if that would make it more clear or not. Okay, I'll try I'll try that too. I wasn't planning on putting this through solution validation. But now that we're talking about all the feedback I'm wondering if maybe I should just do a quick on. Unlaterated test. I don't know. Like I don't really know if I want to do that. I think it's I think it's going to be good. I just think if you have something coming up like any research that you have coming up, you can just like plug this question in. Yeah, because you implementing the very first version will not be problematic but later for making improvement. Blood this question into any research that is already scheduled. You don't have to like do something like dedicated to this. Yeah, that's a good idea. Okay. Well, thanks for all the feedback on the spot. I appreciate it. I will give it to Emily now. So I only have one announcement since I spent last week watching all the unmoderated interviews. What we've closed off the Kubernetes dashboard views with our improved preview that we were testing. We found out that users really liked it. They were able to troubleshoot, especially like the pod area a lot better than they were. Before using the single stats. So it's a great addition to the cluster UI. And the next steps are actually to work on moving this UI under the environment details page because we tested this on the environment list page. But we know we're running out of kind of space that page has a lot of problems with things loading and adding a big tree on that page is definitely not something we want to do. So we want to test out moving this over. And then also crafting up a more complex version of the tree just to see how it scales. But overall went really well. And I wanted to give a big thanks to Will who recommended this unmoderated test. We were able to complete the study over a holiday period, which I know is usually a slow period for user testing. And we were in to initially accepting to close this off in 168, but we did so thanks again, Will, for the recommendation for this. Yeah, thanks Emily. I did want to add here that you know, I think this overall process was really valuable to go through because. As I wrote the doc, we were originally planning to develop a whole another page. So we're going to have to like go through foundations team to try to get approval. I think Sengen is probably really familiar with with this process based on everything that happened last year. But through like Emily's design process and like collaboration with the P. and Victor. We ended up deciding like let's just keep it on the environments page instead of like having another page. But we, you know, have to like surface to users. So overall great work. If no one has any questions about this, I can pass it over to Vintka. Thanks Emily. I was looking for the link to your design. When I got lost in the position of the addition issue. Let me, let me post link to the figma actually because there's a tiny figma links lost in the issue, but I think it'll probably be easier if I just link directly. I mean without looking at the design, I would just share why I was looking for it because. You mentioned a good point like overloading a page is definitely not a good idea, but I was looking to understand if there is a possibility to provide a toggle between two views. Yeah, well, that's actually what we're looking to do in the details page. Like I've all between two views or two views you can access the problem with the list page right now as it has already so much information that I think added more information into it. Yeah, me, but it's the table overlocked page. Yeah. I just saw the tree view design also and I think it looks really it reminds me a little bit of the pipeline view like the I don't know what we call that tree. It looks all like a tree graph. Yeah, the graph I guess. I think it just looks very good lab. Yeah, I thought you did that was my first reaction that this looks like a graph from good love. Yeah, I used inspiration from that pipeline jobs graph actually because it's the only one in get love. I knew that kind of related size like I'm going to just use that as inspiration and see if I can work this tree into that design. So there's still a few small UX issues like think we have to clarify the arrows a bit more but yeah overall starting from there I think was a really good starting point. Thank you everyone. Yeah, coming to my point so Miran I you were able to make big changes to the way to boost management experience. I know that we are still calling it beauty find a far UI but we ended up like also doing some back in work. That was long pending which was around adding descriptions for variables so that it's easier for someone to you know find a variable that they're looking for because they might be. One with similar like a bunch of variables with kind of similar names that sometimes users create and it's very difficult to understand it is. Through a glimpse that which one was meant for what and why they were created. So now that's going to be easier but that's just one and we have made improvements with validations added a lot of them. We have improved almost like a bunch of health decks and changed than the interior the behavior of the input boxes made the drawer more pajamas compatible and also like contributed to the guidelines for jars in pajamas as a part of this. Overall this was I think a very successful one and we enjoyed a lot so expect a blog from both of us coming soon. The next one that I'm looking at right now is most dream research. I know that I've been talking about it in a very long time but it was really difficult to get people to speak with but I'm just very happy that finally we do have people and this is for the first time and. I would say ever that we have so many people who are willing to share that feedback about most trains. I'll be concluding this research this week and posting the analysis next week or like mid-off next week and I have a lot of insights that are valid for very different teams so just stay tuned. And that's all. I think I can take over and then just voice a. Bonnie's. Notes here they could use not attending. I think most of you have seen that you share the job token. Designed right in the CACD UX. And oh sorry this is the CACD UX. I'm talking at the Google level yes that's exactly the one so. If you haven't had a chance to provide them feedback. Please leave a comment over here the designs. I'm just dropping a couple of comments here and actually press that you know that they managed to use the design patterns and the whites it's really polished. I think there are a couple of things that we can provide feedback on especially on right. I'm just going to ask you a question on. Ording and then smaller interactions. But overall through nice to see them first prototypes them first design. Issue out there in the wild so. Good job Bonnie and also thanks for being there for more supporting them as them onboarding buddy. they made a. I don't know. The way they like explored all the different use cases for this feature and went into the real depth something that we missed the very first time when this this feature was being built on at project level. And we kind of built upon it over time but looking at how they made all those considerations. Before them very very first draft it was really commendable so yeah I appreciate our work on this. Yeah really cool. I'm going to leave a comment here in the document but also in Asia for them I think a good next step is to you know move that acceptance criteria and the comments that she's living in the designs area to the issue description because right about the behavior about the rules. Just the over our functionality it's good to have that in the Asian description. But if you haven't had a chance yet to review it please support Bonnie. And as you also post some questions and scenarios about the CI job token work. It's in I think is the validation issue yes this is the solution validation issue. I think you had a chance already right we to work with them on this one. Right yeah the noise was a really good question so they still has open now. I think Marcil and I we answered most of the questions almost all in fact I invited Marcil because I did not want this to be heavy on my perspective I just wanted someone to challenge what I had written. And yeah I think they has all the answers but still if somebody feels differently I would really welcome you to post your answers here. And Bonnie let us know because you probably watched this later on. And then there's two more point to additional points here she's also mentioning an next steps for that CI job token issue is to create a validation plan and start from the clean the validation so they can refer to Emily's word for dashboard on this one. Emily do you want me to voice a comment you want to voice a comment I never know how to what to do yes. It was just a funny needed any context or feedback around the solution validation that I just did a feel free to reach out and I can chat about it a little bit more. And I just have and if I know not really it's just a comment to tell them that they should create the recruitment issues so because it's not easy and get in touch with Erica. Is it Erica or will supporting the. I've been to Kyra to do to you. It's Erica. Yeah, it's very fine. Oh yes, since they also worked on the scar card and they referring to the one had a chance to see that. And I've cost more aspect that one of the suggestions of the recommendation that she's the she. That's a very good question. So, I'm also good to see right that. We'll mention an engagement also for customers on the community on the work that she's doing so think overall what they has been busy they hasn't had to just join this call us. But it's nice that she's also able to update us here about work so. So. And then if you don't have any questions, I'll pass it over to the studio. Thank you, Sian. So I have to item so the first one is about the think because I shouldn't that we did last week. So it was really great. We didn't chat about any design relevant elements because it was intended to just become a more of a technical discussion. Who was full of this question around how to use API should we change architecture. But I think it was great that I could hear a lot of from back in engineering perspective as well around this project. So. Okay. So it was really cool and we wanted to make sure that we're aligned on this in terms of the project planning perspective so we're working on the proposal from the back end architecture like planning so Bobbi was working on it and then we're also discussing. Shortly after the call and then we're keep discussing around this so the long story short while we're discussing at the moment is like how would the CIC catalog would like to become is it like get left catalog that includes a lot of different elements inside. One catalog or do we want to speak to become a Gillette CIC catalog that includes the ICD components with the city steps the city template type of component. So it's more of a discussion for the future vision and I'm happy that like we're discussing this so that we can make a proper design and product planning so that we can work on working towards the roadmap. And make sure we're aligned on the same goal so it went pretty well in my opinion and based on this we so it's only the empty issues so far but dove and I started to think about like we should validate if this vision really makes sense or not so today I just really quickly create an issue that the research issue service and deeper now. But I would probably need some of your feedback because we wanted to take this opportunity to test things out that is not exist in the market as well as it's not part of the product but we wanted to make sure if it is really aligning with the users needs or not. So it's that and my second item is also more of an FII that I also went through the solution validation it was during the holiday season and it was a moderated test or went fairly like well and. The good thing is so we wanted to test out the different visual by representing the component so currently if you check the CSE to catalog it shows one project in a row but we are asking ourselves like is it better to show the component inside the project. So it seems like it was well received but I I taught in one unrelated question to this research that how would you filter and find the components that are created by your colleagues to test out the filtering experience and actually we got a lot of constructive feedback on this we need more immediate action item. So yeah, but it was a good test and thanks Eric, if you're listening to this call for helping out planning the research issue. So yeah, if you're interested in let me know if you have better idea on how we are going to improve the filtering experience I think it would be a whole topic that I may want to share in the CSE to review meeting at some point. Any questions if not. I don't have a question you mentioned that you would want some help like some feedback but where do we go to provide that. I don't have a really well described issue but I have created one empty issue so I'm going to link it here. Where is the past sorry too many meetings. Thanks, Bill. Okay. So this is the issue that I'm going to work on in the next one or three months and I want to really revamp the whole filtering experience for example on question three needed tap here. And the second question like how does this filtering tap with the search full working well together and I'm pretty much going to start about using this pattern so I'm happy to explore alternative UI for this. Thanks for asking. So move over to hyana. So switch word updates we're seeing hiring. And the update I want to give to the team is that you may sort of change is to the role requirements that you can find the kickoff issue but also they'll review with you because most of you are part of well they're really team right. So the big change is that we are only interviewing or going to be hiring intermediate product designer so when assessing right in interviewing the candidates and also fill in this for a card. And we are moving them to the pipeline into the interview process considering them intermediate designers and also make some updates in which the priority of some of the skills both the soft skills and hard skills for this role so as you can see here. So 3 and 5 three to 5 anyways top 5 the theory skills is foundational research. I think for switch board there's a lot of have a lifting that needs to be done. And a product designer inexperience in research is really important they need to be very strong collaborators with you know the image and new in all the designers. I'm mainly looking for some on the has worked with the design team before or at least you know had some fair designers. We have interviewed designers that they were like the design team of one they were the only first designer in you know a startup etc. I think that especially for if you think of where switch board is right now where we are getting started with a lot of the core functionality of the user experience someone that has you know a bit the more of experience in collaborating with our designers to get the understanding the insights. All of how to use a design system that they are used to you know just figure in things out with a team think that's essential. If they are not already technical they are interested in the nation and CSS. So I'm sure if in your score cards you're interviewed you talk about that about those skills but. I would like them to at least. The you know proficient in basic HTML and CSS that mean to they're going to have a better understanding of design interactions that they will be able to communicate with the front end engineers right about the components that. When review merge requests they will just be able to jump in and. Do the work that we've done. And I already mentioned the usability interactions that we did to design system and experience working on complex products before I was really looking for someone that had the depth to experience. That's always tricky because it's a little bit such a specific work that you all do here. So complex products that can be like working banking you know and compliance in anything that really requires them to dig into this. And that's a messy hairy problems and showed that they cannot lie design iteration. To you know, propose a motion that they can use research skills that they think collaborate with their teams anyways involve engineering in their design process so. And I'm not sure that's what I'm looking for. And then I still have skills technical skills in general right. Like anything outside the scope of HTML and in CSS. As I mentioned, really nice if they have that tool that's to be experience maybe they're a user of GitLab maybe they I don't know the coming from one of our competitors. And also body and platform workflows or admin platforms experience that's also nice to have I think before one of my mandatory skills was that they had experience in onboarding design. But very tricky to find and combine all of that I think it's we make some tweaks there to make sure that we're not finding the right candidate on paper but that you know we find the right person for the job and that for the things that. They they need to improve this was an engineering proof that we create the opportunities for them to gain those skills. What changes I think to you mainly is you know only intermediate so if you think someone is at the intermediate level but they're like I don't know early intermediate or I don't know a very strong intermediate I'll like you to highlight that in your notes. And also when I say intermediate is like a GitLab intermediate right because a lot of other candidates that's. We interviewed recently especially end of our last year which was like two weeks ago. They had a title senior but they were like not GitLab senior right so it's difficult to not difficult but we have to. Kind of set that expectation with the who the candidates so I'll pause now see if you have any questions and if any obviously since so yeah that can I line. The expectations for the upcoming interviews. I just have one question I really haven't been getting any interviews for this role. So I was not sure if that was something was missing there or I was not sure. Yeah that has been the challenge and it's also why. We were making this changes to the the requirements. I've interviewed only a I don't know maybe. Six eight candidates in the the last four months that's not a lot right because initially the candidates that were being as creative that were qualify. They were and then that's going to answer to will question to expensive or there were two senior right they were good designers but switchboard is an area that. If if we put someone that is too senior they're going to finish the work like in a year and then what. We cannot risk like getting someone. Put is a one there so that's to be born right they it's a it's a problem area where the designers the design and they need to have them for growth right and need to be able to learn also in the fly for example. I think that's the experience in research but have never worked with a research team so they're going to collaborate with will right you going to learn something. So yes for this this role we haven't had so many interviews we had candidates that were screened and we're qualified and they moved to the team interview I think. VT coming me soon during interview a few of them but then rejected. After after a couple of of interviews. I think that's the first question that's great. I think you you kind of answered by first question so I did add a like another question. And then you know you've done you know you mentioned six to eight different interviews with some like more senior candidates. Are there any. Concerns that like intermediate candidates will still be able to match up well with what you're looking for for those top three to five. mandatory skills? Yeah, I think so. And that's the assessment we've made after having these interviews, right? And after also looking at the book of applications that we had. Right. I'm sure if I remember at some point I mentioned like I'm reviewing a thousand applications because a lot of people were applying. And as I mentioned, they are seniors where they're like not good senior. And if you will have the experience working complex products that we need or understanding research methodology etc. I do think we'll find an intermediate candidate at this level because farm and but correctly like Gina. Gina Emily and Tunjoon, right? You, the three of you, join as I think to be the level, right? But you all came with a very strong graduate, very strong experience in various specific things, right? And another thing we have to look at is that this team, you're all seniors, right? So looking at the business when we hire yet another senior in this team, right? There's all you're going to be so many staff and manager opportunities in the company. So we also need to think about that, right? The career path and development and room for the road for a new person in the team. So that's really it to the point I mentioned about, you know, compensation expectations for people there to senior depending on the location factors but also budgeting and career progression in the initial. The, as a two question or will. Yeah, thanks for going through that or me. And of course. And then a question. This is a little bit out of scope and if it's something that we can't talk about in the recording also in time of that. But as you mentioned that there's a lot of seniors on our team and that there's, you know, not every. The growth opportunities are only going to be there if there's like a certain amount of openings and etc. Are there any is there any work going on to maybe change up our. I don't know what the word is, but basically like the growth opportunities for product designers because right now like you're saying you're senior and then you're either staff or manager is there going to be like other opportunities in between that. Yes, I think we have a, I'm not sure if the merger class was version of things, so but I know that Valerie and and better to work here on the principal product designer, which is staff plus. Right, similar to what engineering and also product has that something that Christy was also working on because we need to make sure it's right that. Let's say after. Actually, you get promoted for example to senior product product design manager, you could become a director, but then what's in between right. It's a limbo that. That we need anyway, just this, a clarify and I know that this is part of the discussions. That leadership had at least, you know, last year, I'm not sure yet when this will become a. Let's say something that is in the handbook right of a job. Because we also need to have the business need for that. Right, so we didn't have staff designers in the past, right, we also didn't have senior design managers, so as we grow. We also have to make sure that we are not. We're not only adding people there are too senior because that also means we might lose them right at some point. That's all part of the changes for this particular role and also mainly because I think switchboard is complex, but it's not as large as for example. Verify right where yeah, you have a very well established set of functionality that yeah. That will require expertise that requires more senior designers. So yes answer a question, but I don't know what. Okay, thank you. Oh, the. Um, thanks for your questions, by the way, and I know that we are at almost a time, so I'm just going to drop my question here. I discussed this with some of you already, but. I want to see if you're all can help me. I have to disemble it to switchboard by the way, but I have to complete a UX score card as part of a Q4 okay are. That will require me to validate a job to be done right run a score card on a job to be done for a product area outside of CI to the outside of switchboard. I can pick anything, but I want to pick something that relates to verify in release. I'm sorry, verify any environments. It's an a room number that that changed. And my question to you is. Do you have any suggestions do any recommendations on. Cross stage job to be done that makes sense to be used for a score card that can help us, you know. Experiment or at least a sastic with the that that user experience. Taking that the recommendations will be beneficial to your product areas. That makes sense. It's late. What do you think. I have the same suggestion here that I shared with you and everyone on one. You help me like come up with this. I discussed for the designer with the other team of the other team which is go interviewing that's how like. I did not have a chance to find this for testing the usability of merging experience, which is a very wide experience. And I can totally give you the highlights the tags from the up to you that validate this. I would just need some time maybe tomorrow. I think appreciate it. I was thinking, I didn't discuss this with you, Sunjian, we didn't have time today, but I was thinking of a running job to be done, of the Reyescore card, also for one of the jobs to be done in Create, but related to the pipeline, like the editor experience, right, either the coal suggestions or using, I don't know, VS cold, whatever, using extensions, because I think that could be interesting to you, but they don't have jobs to be done for that category. So yeah, that's the type of thing I'm looking for. So I'm thinking secure could be cool. I think that could overlap with verify, just thinking about security widget, vulnerability, that all can expect to like, commit in pipelines. Yeah, that also makes sense because I think that's the way that you will or the Erica that it's Erica, that was working on a study or I think, and I think you see eye adoption and something about that related to related security and compliance work flow. Yeah, it's definitely Erica. It's Erica, right? Yeah, okay. Anyway, I forgot that, you have wheels. Part of the agenda, I'll just, oh, for now, unless will you want to, yeah, voice up coming here. Yeah, I saw that you added the link to the just to be done. Yeah, we'll file, so I was just kind of like, scrolling through it. And I came across this one. It looks like it hasn't been validated, but it touches its technically under distribution. So that's involved with, like, actually installing, get labed and then it touches on Kubernetes, which is covered by the environments team. So it looks like there's a couple different, like stages involved. So I don't know if that might be another area to consider, but unlike the one that Vita got had shared, I don't know if this is like, it's got the evidence behind it as much as their does. Yeah, thanks, Will. I linked here the, the, the, the objective on the file. Yeah, I'll do some digging. You could just, you must be some additional information in, direction pages or these categories, this area, but appreciate the feedback. And then over to your world. Yeah, I don't really have a lot of updates. I did want to just add to what Emily said, earlier about our solution validation study, that I think they did a great job collaborating with Victor and myself on this work, and they was extremely efficient getting everything done, especially around the holidays. And they also did a great job like running the study in user testing and then providing us with updates on like each participant, asking me for feedback on like whether they needed to like, you know reject certain participants and kind of get new ones in, get higher quality feedback. And then they also consolidated all those individual sessions until like it overall. Summery, which was really detailed and awesome to see. But that's kind of it for me. Thank you, Will. And Emily, thanks for the shout out. Thanks everyone. Yeah. Yeah, I never worked. I mean, we want the, never, we wanted to, never, we want to. It's going to be forever on the internet. Thanks everyone for joining me. And first thing a little bit of time. Hope you enjoy dressing today. And that's me, Linda. Bye.",
  "Okay. It works. All right. Okay. Last team meeting of the year. I just want to start off by saying, it's been a crazy year. Thanks for being a crazy couple of years for everyone. But yeah, I'm really happy that we've made it. We have the people to be working with you all in 2022. Looking forward to much more next year. And I just wanted to ask this is anything that you have in mind that I'd like to share a ball's what you're looking forward to. Work related, no work related in the next year. For me, for example, I'm really looking forward to the GitLab. What are we calling that now? Are we calling it summits? We're calling it contributes summits, right? Yeah. I really hope it happens next year. Making all my string blends around it because it'll be super cool to make you all in person. But also, yeah, just to meet again, some of other colleagues in the subject. I think that's a high light from my GitLab. GitLab was just but also to higher the designer in switchboard. It's been a few months and boredom. So these are like my plans. And also something I'm looking forward to. And it can be quite annoying to have your manager ask you like, What do you want to share? But does anyone have something that they want to share in this last year? I'll have to say, was summits as well. So now I'm going to try and think of something else. But again, that's one that I really want to share. I have met many people from my immediate team. I have met a few people from our team, but it would be nice just to for the first time ever to meet like a big jump because of people I work with in person. Which will be really nice. I'm looking forward to figuring out what fleet visibility will look like next year. So like the first time I feel like I'm dealt with a brand new. Not really a brand new feature, but it's a new category for us. So it's the first time I've dealt with that. And I think it will be. Very fun to collaborate with a lot of groups because it's really cross group. Yeah, so I'm looking forward to collaborating and figuring out what that's going to look like. And some it will get on the summit train. Eric, could you want to say something? I was just like, I didn't want to. I said, no. Okay, then let's just be grateful for Eric. All right. This year and more Erica next year. To. Awesome. I think, yeah, I just before we jump into the agenda, just want to say I was not going to take some time off next week, but I will because it's just today's. And in case. Any of you, I know that Emily asked for my coverage for a few days, but if anyone else, that's going to be watching this recording. If you need a me as your backup simply assign yourself in get lab as busy and I don't forget, you know, to do that and also link to your coverage issue because. Yeah, from not able to cover it, we can point people in their requests to the coverage issue and it can leave comments there if you need it. Okay, then we do a start on top start from top yes, then over to Gina. I have a reminder that I'll be out of office for probably the longest amount of time I've ever taken here. So two weeks. And I linked my coverage issue there. Thank you for people who are covering for me. And then I have a few other items just for some ongoing work that's happening in early will in January when I come back. I'll work on a competitive analysis for or competitive evaluation for fleet visibility and. I will not state the competitors there, but I will be looking at, but you can read them. And then go into that issue if you want to see more Darren has already added actually a lot of screenshots. And then I'm also working on auditing existing research in summarizing that into a video about kind of like the broader pipeline performance or how to optimize my pipeline. Erica has done a lot of research in that area that I'm also going to be just like summarizing in one place. And so I'm not to my surprise there is a lot of existing stuff so we don't have to talk to more customers which is also nice because the problems are already validated. And then lastly, I worked on I worked with Graham and Alana who are two new product designers because they wanted to shadow someone while reviewing an MR so. And it was a really fun experience for me we were trying to get pipelines to be in the right states and I was like googling on the spot and. We were all just like working together they had GDK up on their instances as well and we were working together on it and it was very valuable so we. And I'm hoping that I'm able to add this officially to the onboarding template for product designers. You're welcome to jump in there. I have a comment on this one can I say I left a comment in the merge quest you know just in being a big company for feedback because. I will this help you and board bunny and they are do something similar. So I think they had this week the first merge quest shadow call they also were taking a call for like one hour so they might have some. Some feedback and I think it's a great addition because to your point right now that designers are assigned to randomly assigned to merge requests reviewing something out of sort of your stage. And even can be even scarier so I think in general doesn't opportunity to do that for us to also define when shoot that happen right and who should help them with. She'll have to mean the process so thanks for that mark. Yeah and also I met with be this morning and they was saying that she'll create another remark because I they had some larger ideas of how to. Kind of like time box a lot of the stuff that we have in the template so there's going to be more changes. After that but we were able to connect at least on the small change that I wanted to bring in. Throw on the same page there and she'll build on it. And that is it on for me. Emily. Well, um the first one is just an FI I that we're starting a new unmoderated study in 168 for a tribute on the Kubernetes dashboard. We're seeing like a lot of competitors have a tribute of this and it's a concept that we didn't initially explore when the Kubernetes dashboard was first validated so it's something we kind of want to. Sneak in there as we're working on it and a big thanks for well will's been kind of helping me with the screen here for this as well as the scenario so. Thank them for their help with this study. And then. The second thing I wanted to say is Andrew one of the friend and engineers on the team invited me to a Kubernetes meet up in Toronto last week which was a lot of fun. It was very very technical so a little bit above my knowledge. But it was nice to just go kind of network with some of the people working in this area making connections and we're also talking about maybe presenting our Kubernetes dashboard at one of these in the future. So I'm going to meet up with a Vittica to see if this is something I could do speaking about and partner with Andrew so they can kind of answer all the engineering questions that come up. But it was a lot of fun and I haven't done like an in person meet up for a while so. And if no one has questions move to Vittica who's not attending. So do you want me to voice it for her. So it's I'm talking so them first point is there's a discussion going on in PE on the time we show for duration created at started at and it'll be helpful for some early-career and Gina to for and she's like the epic for improvements. And then. Gina I noticed yours is read only but since we probably have time we can. Yeah, I can voice it. I brought this up. It's actually a really interesting issue. I would recommend going into it because it basically like the users saying that weren't misleading when we calculate duration for pipeline. I brought it up with a flea team just because we have a lot of overlapping features that have to do with duration and runners and wait time and everything like that. And I left a comment for Vittica because we have some specific customer recordings of how they use duration metrics and why they're using them to optimize their pipelines. And the just a summary of what the fleet team talked about. We kind of agree with the user that it is misleading because we're not including pending time in the duration, which is. I'm not saying any kind of weird. But anyways, I just they had asked me to jump in and just include some. User stories, I guess kind of what most like how customers are using this today. So that's what I did. Yeah, I can bust my comment to. I had my one of my new Vittica today and I told them there are some insights in one of the environments. That's where we searched issue. I think I did that maybe last year with summer. That has some explanation about what the release managers and that's up in the years when looking context of the payments right when they are in the pipeline pages. But looking at the point of production. What do they understand you know of that metadata what why are they looking for. When it comes to. The ratio of the pipeline or you know. Well, when it started running etc. So it might be interesting for them to look into that but also connect with you and you might have a lot of updated and more insight in that. But just as a reminder, I think related to what you said there's some connections right about those information that we need to make. For many different personas, a lot of people just go to those pages, especially the pipeline view. And look for something very specific. There's a wanna see one number or one tag. So keep that in mind when making this and coming nation, let's also look at the insights because we have them and if you're gonna find it. Ask me, oh my, oh my. Oh my no where it is, but it's it's there. And then research project is the environments dashboard redesign. I think. That's not the group level environments dash that's the original environments. Okay. Yeah, yeah, I think then you all have been a full school started and I finished that was before you join. And I think that's the main thing. But there I remember documented that in the off jail because we specifically asked about the order of the metadata and also. What they want to do. Why some of this info is relevant. But anyways, mission like one or two insights about that it's the environments dashboard redesign. Also just another small note about this. I know that the issue was about the pipeline duration and. What another thing that I was thinking about is that a lot of these metrics are. I think more useful at the job level rather than the pipeline level because each. Even if the same pipeline runs. A pipeline is designated for a project. If that pipeline runs multiple times the same it might have a difference at a job set run every time because jobs can be skipped in it whatever. So yeah, I think it's it's hard to think about this at the pipeline level just because there's so many changes that could happen with each run. And I can move on to our next point. We're done. And the next one is we need to get started. Butifying our UI in 168 there's many MRs that are merged already and now have validation in the form of that work. And I'll voice Bonnie's comment here if you want to take a look at the list of items. Maria and them are working on. I think that's just a question for Vita go and to watch this. I think it's the opposite. I think it's the biggest comment for Tika's comment to Bonnie. Okay, everybody. Yeah, that makes sense. Yeah, because Mida is in front of an engineer in Tinas. No, no, that's right. Mida is a sorry to many to many teams. You're right. No, so we're confused because it could be that 168 beautifying something with Hayes Lumi. Maybe with Mida. Anyways. Yes, they can take a look at the agenda. That's it for. Yeah, so Bonnie and Sinshu are not here. And then my only update for us which more is that still hiring, Steven interviewing. I don't have an interview scheduled for well this year and more. So hopefully by January we'll pick up with more interviews and fingers crossed hire someone. Good evening. And over to Eric. Yes, so I posted the link to the draft of the report for the runner and CI builders build. Belled feature prioritization survey. So we did a canos survey with 11 features to see how they got prioritized. The signal we would be looking for is that there was like one must have that we would want a champion. Actually, they were all either attractive or indifferent. So like kind of cool, but like nothing like that we have a clear signal to deliver on. Looking across the ones that were in the attractive bucket versus the indifferent bucket. We have a signal that users want AI assisted things like for example like custom dashboards are actually not compelling because it's more work for them. But if it can be like AI assisted then it's like attractive but still not a must have. And so Darren and Gina and I are going to go over that report and depth tomorrow. But I thought I was like give the high level overview that we don't have any must have so I think it means that we're on the right track with like figuring out those little optimizations suggestions which is a read we got from that qualitative study. And we wanted to do this just to make sure that there wasn't something that was like so cool across this bigger sample of users that we missed but we didn't so it kind of doesn't change the strategy in a way. I've also noticed that the not the sample was mainly small medium businesses. And I want yeah which is fine like I kind of had the assumption that they had different interests than the enterprise customers that we're talking to that are self managed. I we did have yeah it's totally right very keen. The thing is I think this idea that it would create more work still holds. Yes. So I agree but I guess my point is I feel like they have different needs because enterprise customers first of all are typically self managed bringing a ton of runners. So when I look at the features profile does attractive. I would imagine runner usage and compute minutes are higher up on that then for small business. Small or medium business. Yeah yeah I don't I I assigned a fidelity of signal score to to kind of help us make sense of it. I think what we can do is really look at the the the five features that are have strong. We have strong of it like a strong signal that they're in different to that's kind of the most. Okay, well it's like okay don't invest heavily in these okay yeah oh that's also so interesting though because. The notifications I don't know it's. Yeah because yeah because it's just it could end up really distracting them the notifications parts. Right. And then I think a lot of their flow. Yeah. I guess that makes sense but they're but then I guess does that mean that we just think about different ways of solving for when you're keep performance stinks. Or when your runners are. Erring out a lot like we have to figure out a different ways to give them that information. Or at least make the notifications like customizable as opposed to like across the board so it's like you don't want to get like all of their runners aren't important to them it might be like only the subset so like allowing them to. Opt in. Yeah but in model versus like opting out. Okay. It's like just we did just be really wary of creating. More things in there already busy. Yeah I don't I totally understand that this is really cool though Erica I'm going to look this is my first I'm actually seeing it so I'm going to look deeper into it and I think our meetings tomorrow. Yeah yeah and let me know like what what to do but I and I think it's a really good critique of the. Of of the sample that's like really on point I just think that. The the problems that we're seeing with those features should also have a problem like notifications for enterprise level but would skate that would scale that finding kind of right definitely yes. I was looking more at slide seven of the ones that are attractive because I figured the ones that are set they're set at week there I would have thought that they would be higher up for enterprise users but the findings that inside a definitely relate to all I totally agree. Yeah yeah so we can decide I think the thing is we didn't get a clear I mean I thought we might get a clear like chibby in here. I'm not really important that we don't want to like sprint towards any one of these particular visions and I think that like this AI assisted is the way that we want to be thinking about it. Like yeah behind the scenes they just wanted behind the scenes. Yeah okay I have some more questions but I don't want to take up a ton of time. I'm not a baro. Cool everyone don't you care. Yeah okay. And let me know if you want to sink today too. Okay. Yeah. And then or just slack about it and then the other study I'm working on is this conversational AI study which is like a team of researchers and I'm working with ops folks. And we are asking we did like an intro interview and then they're doing a diary study where every two to three days they let us know the tasks that they performed with conversational AI and how it went and they're giving us screenshots of those. And so we're getting a sense of how we might use conversational AI in the ops space. Although we think it's not related to performance optimizations like we got a signal that they that's inefficient use of their time. It's more like it looks more like they like it like as an idea generator or like getting the right language to use for documentation searches stuff like that. Anyway, but I'm threading the like really technical example tasks in that thread I linked and we'll we'll be doing the follow up interviews with them in the second and third week of January and so we'll be asking people to like give me follow up questions to ask about those tasks. Sort of put that on your radar and then the next thing is I was initially thinking that we would run that shared resources workshop at the summit, but I think we should do it after the summit because it seems like. I'll be like the only nerd being like, hey, let's work. Look at it. But let me know if you want to go rock climbing because there's a rock climbing issue that someone created. I work our rock climbing. That's all I've got. We can have the workshop while rock climbing. That's a good challenge. Okay, we're too happy with that. Let's go. But I think it's a good idea because yeah, you know, maybe some of us will also take some time off after some it's before right is some traveling. I think it's good. Well, once it gets confirmed, we can play around the summit for that and if not, we just, you know, pick a date in. You don't make you to things going to make you to right. Yeah, and if you work it to, yeah, I think it's safe for. Can we. Okay. Think that's that right we'll. Yes, already off. Anything else we didn't discuss we would like to share before we wrap out for the year. Good. This. Who? And thank you Emily for recording. This. Yeah, enjoy the rest of your day. I'll see you later. Final see you later. See you next year. Enjoy. Enjoy time off. Bye. Bye. Bye.",
  "Maybe you can, yeah. But I don't know where this is getting recorded to cloud on my computer anyway. Yeah, so I was saying that Sunjung, I learned a lot from the process that you were following and other learnings that came from that. And so like we thought that it's going to be good to kick it, kick it off as soon as possible, but not wait for everything to develop first and then think about right up like that. Yeah, yeah. This is the right way to go. Thanks for sharing. So my item on there is free a big and rather radical change from the CACD private catalog. So we we discussed this a lot and this came up and inspired me a lot. I did the journey map exercise and I didn't feel it's right to having this private CACD catalogs go for namespace because we have interviews several self-manage instance customer. It could be really multiple namespaces and not going to disclose to exact number, but it could be a lot of course it depends on the organization's size. And now dove announced this change and this will impact a bit of our product roadmap or the timeline. The design lies that impact is not that big, but I think this is the right way to go. So I was really supportive about this change and yeah, you can see that now we simplify the structure. So we'll have one organization catalog and one open source catalog that is kind of similar to the market. And I have personal updates so you can re-through it. I might need to reschedule some of our coffee chat or 101 if I need to. From next week until the end of September. I wanted to here with you all. So I am so excited. Yeah. Immodges are not working. Yeah. It's mostly common control shift that works for me to show the model or the modus. I use it a lot. I'm not ship story space. Okay, so my Anna is not here. So we can just go through this asynchronously. Maybe we can move to a research them. Yeah. Yes. So I am just analyzing all the sessions from how do we use AI to help users optimize their runner and CI builds. And it was pretty cool. We used like a decision tree process from this book on AI that I'm reading. And basically the idea is if you can put the decisions into like clear if then statements, then you have support that you can automate this workflow a bit. So we didn't know if it would be like I think about this, but that's related to this. And then this is really, but actually it's like pretty straightforward. Anyway, so I won't go into the results. But I'm working on the report. And another come out. I think I'll share with the UXR team soon and then it will come out next week. But there's really good information in there about just the details that people need to make decisions. And kind of the outcomes for optimizing pipelines. And the process is very trial and error. And set it and forget it. That's the main thing. So I think that's like one of our CI learnings just like the biggest thing I've learned. And get lab. They don't think of these things in their minds either. Like they specialize in it. They execute and then they go away. But I think that there's also findings related to how to support teamwork. So I think you'll like their report. And then next week or as soon as I kind of like get my mind into that report, then I'll start working on this. How users are using get lab on the cloud. And we'll do a little bit of a good question. Yeah, could you just elaborate a little bit more on the effort? Yeah. So I'm actually very proud of us as a product company that we're like ready to move into this space. So we're looking at kind of the connection points between get lab and the cloud. And like where the work flowing is. Kind of the tools that users are using. And that'll inform and part of the integrations that we're building. Like, June, which is DCP. But then also like long term what works well in terms of the integrating. And what does it? And that's actually where I think we can bring in some questions about resources and whether or not they should live at the project or the group level like how teams access like secrets for example. Like where does those get stored and get labed? And it is like higher up organization. Because I'm thinking a lot about this new. Chains that's coming related to organizations was to new nodes because that's like the new play with the catalog. Which I think is really smart. Which is to put it up there. But then I'm wondering like what else could we move up there? Like maybe the secrets because they're at their organized at this company level. But that's what we can look into. Talking about that Erica, I like can you like later go in chime into the issue that I've created for navigation. And like share your views there because yeah, we don't want Even though I have mentioned about like taking it out of CI CD. If there's any research that points that we should have it maybe as a part of the main navigation or out to higher level navigation. We should be aware of that and that would help us inform the methodologies that we would like select for research. Okay. I think we haven't gotten that detailed because that's actually like really tactical. Most of the research so far has been like what is a secret. But I'll look I'll look up the lens on there, but I think I won't have much. But I think from this study we might get something. Okay. And by the way, I will also pick this up based on your recommendation. Oh yeah. I'm listening today. You can read that chapter. I'll say it. Okay. I had. No, it's okay. I just have another like full hook question. Are you only looking at get lab. Sassy. There's always trying to read through your issue. And I mentioned that because one of the teams that I work with in enablements. Currently the application performance group is changing their name to cloud connector. Okay. And they're focusing on. Sumpman issues are essentially giving them access to like AI features and things that would be available through get lab SaaS, but at the self managed level. So that might be a good team to connect with. You know, in terms of this research. Okay. Awesome. Thank you. Right. I think we landed on the SaaS. Users. But not. Yeah, but either way, I think they at least be interested. So that's so helpful. Well, thank you. I'm embarrassed that I can't quickly find that in the issue. But it's okay. I'm on the other project. Yeah. So if you want to ping their entire group. I can list the the channel there. And then the P.I. can also list the PM as well. Okay. Cool. I just put thank you. Thank you. Well, go team. And then I have this this note. I have to find it in the, but I'm watching the sessions again and again. So it's coming. But there's just lovely quote about how we made a change. And the details people could see about the type lines completing. And this person was like, I love it. I used it the other day to show my executives that I need more funding for my pipelines. Because I could show like kind of the times and how if we organize things, this would impact the times. And then they used that to find more of our product. So that's like the best. I can look this in your portfolio. Be prepared. Where are we showing the pipeline? Somebody. I'm sorry. Oh, no. I mean, I don't know. But when they was telling me, was that the new way that they can look at the metrics allowed them to make this argument. I need to pull up the, I'll pull up the quote. Okay. It was pre-owned. Any other updates before I go or. Yeah, the data technique. So I don't necessarily have anything. Connected to delivery or dedicated this week. I guess just a. Announce what I'm working on outside of that. Doing some research for organizations that I think Eric had talked about in the US week, the call earlier in the week. So I included the research issue and I've been. Constantly like adding and refining research questions there. And then I'm doing a workshop on breaking changes. Some adding people from different departments at the moment and can be doing that. It's going to be kind of a mix of async and synchronous workshop parts. I was looking for Jocelyn in the list because I mean, given how much they has been struggling with breaking changes and I see them there. Yeah, that was somebody who came up pretty quickly in conversations with the PM that I worked with Sam. they was like Jocelyn definitely let's include her. But if you have any other suggestions on who might be a good person to include. Let me know. Yeah. That's all the updates that I have this week. Okay, so with this we are at the end of the agenda. Anything that someone wants to bring up. If not, then the visit. It's in the meeting here. Bye everyone. Have a good one.",
  "Hey everyone, this is the CICD UX meeting on August 2nd. Starting with our standard topics with the, I had not wanted to mention the Q3, okay, as are being defined and will be available soon. So she'll share more info once the details are known. But two of the okay, as might center around the UX department completing the neuro diversity training and resolving usability benchmarking findings across FaireFI. So does anyone have any questions about those two? Awesome. So then we'll move into the product design section. I can voice Hannah's because there's important notice here about how she'll be providing coverage for switchboard at 20%. So hands on design work, milestone reviews and team rituals. And this will continue until that roll is back filled. And they expect to onboard five customers to get liad dedicated by the end of Q3 which puts a focus on the UX. So during this period, if you need support or she's blocking in any way, please be explicit about what's needed and you can ping DM on Slack where she's most responsive. And then currently working on email notifications and the onboarding flow, gathering requirements to improve design proposals for the deployments page for switchboard. So lots of updates to existing work and that'll be kind of where them focus time will be. And they shared a customer call from Switchboard on Monday. Think I'm next? Yes. Oh yeah. No, no, sorry. But if a no one has any questions, yeah, Gina, you can start. Okay. I have attached a report that I wrote up about the mental model research study. And there's a lot of good insights that came out of it. I'm working on recording like a video and sending it out today explaining a summary of that as well. But right now, one of our next steps is to run another survey, which is in progress and already have some data from it. So finalize the terms that we use to explain the concept buckets. And then we're also asking users where they use those things in their workflows. This will help us get a better idea of really what they think those words are. And then we can also use that to look at like pain points within the workflows that they bring up. And then the next item is that our next big thing for runners to look at runner cost visibility, which is focusing on users who bring their own runners. So we're not going to be touching shared runners on SAS because we use like our own pricing method for those users. And an example of this is like if you create a runner through AWS will provide you visibility and to how much that runner is costing you like the machine and to run the jobs and everything. So I attached the issue there, Darren added like a ton of details. And we're just going to work on scoping it down. But I talked with the team today and we're going to run a think big and think small on this because my goal is to have the process of getting the first iteration for this feature out faster than what we did for the NBC dashboard. So I think I'm next if no one has any questions. So I'm just making progress so so you can update it kind of artifacts for design sprint starting with issue templates and slide decks that you can reuse during the sprints. Just let me know if there's anything design sprint related you'd like to see template it out and I'll try and get to work on some of those. So some of the ones I haven't gotten to are each day of the sprint to those issue templates as well as I'm going to make videos that you can share asynchronously on how to conduct sort of these activities or the homework we were calling. So there's a few things I'm still working on but just wanted to share progress on that. The second thing being our we did internal research on populating a service list two weeks ago now we're doing external research. The good news is the results from both internal and external are really aligning, more seeing a lot of the same feedback from both which is great. So we're hoping to wrap that up in the next few weeks and move on to prototyping an NBC approach from this. Yeah that's a good thing that they're aligning. Yeah that's so we were worried about we were like I hope GitLab isn't using GitLab differently enough but there results will differ but there's a lot of the same thing which is really great. Okay there are no questions I can proceed with mine. So first of all thank you everyone for providing feedback last week on the secrets pucy prototype and you'll allow find our advice design on the same framework profile. The next step for me is to create an interactive prototype and share the link with everybody who has signed up for the OLE Adoptal Program for Secrets and then we'll just keep our fingers crossed to see what kind of feedback frozen. Beyond that I mean that that's a big thing that has that is about to be wrapped up this week so now I can start looking at see a job token and variable work again. Yeah besides that I have a chat with the application secure to team here at GitLab. I organized a meeting with our engineering manager product manager the security engineer from upseq MSF and it was really interesting so I tried to understand how is it that they are currently like how are their workflows related to secrets being fulfilled today with the current set of features that we're offering and they kind of walk through it and also contributed to the user story map that we've been working with. So we definitely have to keep in mind and one of the learnings was that we have to keep in mind the special responsibilities are special permissions that any security uh any any individual responsible for taking care of the security in any team organization would be requiring and that's going to differ a little from what we are going to be thinking for maintainers owners. So yeah this was a really good meeting for us and that's a oh if you don't have any other question I can move on to mine um I don't have so much to share for this week um so we had a meeting last week and this is this is this huge that I learned here is the outcome um so we decided to take a step back and then just like let's just forget about like everything and then we just focus on the user flow and then their context and then see what is the best solution for them and in which context are they starting to author the CIMO plot pipeline or maybe they want to come in to the CIMO and then maybe they want to just make some edit and then probably that's the point that they started looking to the CIME template component. So I'd like to just list down all the possible context and then make a journey map for each workflow as it seems like CINA has one question they want to verbalize your question CINA. Yeah it's a little it's kind of related I guess um I just had a question on the progress of the CISCD catalog and the navigation effort because I saw in that page now that Foundation team isn't allowing navigation changes until January of next year and I was wondering like how that impacts your team progress. I think of of course it's I don't know at this point because we are ready to start to take a step back and then I based on the outcome that I come up with like with the journey map or whatever design that we are going to discuss okay which is the best placement. So the short answer would be no I guess so from mind-resenting so okay probably the project navigation is the best location for this reasons then that means we can still ask for the request for the Foundation Pman and have those conversation back with better back up from the UX perspective and then also from the UX our perspective so I I you didn't think it will be blocker for us but at this point where you just like decided okay let's like a huge step back so I don't know we would be a plug or not just to answer your question. Yeah it does um one of the things I'm in our team call them runner today we were bringing up the fact that our project runners views and settings and everywhere else it's in the navigation like in build or in admin areas still called CICD but same thing and I was like yeah I I'd mentioned that your team pipeline authoring is like going through this and it's been quite a struggle um and yeah I'm just I'm a little nervous considering what I've seen from the stuff that you've gone through already so I might end up talking to you more about it depending on I might create an issue just to get like their input so far because someone actually commented about it on the navigation feedback issue they're like why is runners still in settings for project when it's not in any of the other views so maybe that helps but we'll see yeah so my solution would be maybe you already started to getting more numbers and data and also those verbal feedback from users and then just be ready to have those conversation with the foundation in one year really sure about like okay this is the good place for the runner yeah that'll be my only suggestion thank you yeah thank you my issue is also going to come in so I'm going to join the band in a while yeah and I think to genus one that most things that we are placing in CICD settings I'm not like very aware of what's happening with the other settings option but CICD settings those particular items are facing discovery, discovery, discovery, quality issues and I think we just need to see that what we can do and it's impacting kind of silly person the item that's inside the settings so it's not just about runners and variables it's about like how can we collectively improve the visibility of those items overall and yeah it would be a great chance for us to collaborate on this yeah totally agree yeah I just wanted to add that that are protected environment settings are there and we've had a lot of like user feedback that no one can find are protected environment settings so oh yeah yeah I remember that looking and that one is pretty divin the setting yeah it's deep and it also is sitting in an area that people are expecting it to be in and it's not linking you can't link it to the edit or just we recently started linking it from the edit environment but before you go to edit environment and it wouldn't be there so that was even more confusing I have an observation there that in most of the other products because we did a competitive revolution recently so I was just like passively looking at there are many options our like anything that's placed under settings in any of these sections I actually cascading in two steps and that's not the case with other products they have their settings navigation separate from like everything all together so doesn't feel like everything is very hidden you mean they have it like in a separate like almost on the separate level pile yeah yeah I see what I mean yeah and the one thing I noticed is sometimes we just put things like two kind of figure also when sites the setting and is it really setting or is it something to configure and yet that's sometimes confusing to me so maybe this is a good opportunity for all of us to I don't know just some fun work like settings you why version two yeah yeah well yeah we even talked about just removing protected environments from the settings and just moving it into the edit environments page because we're like that makes so much more sense for just actually in just actually those yeah and then maybe have an overview in settings but like I don't know it for us it doesn't quite make sense while it's sitting there yeah and just some a different kind of semantic grouping like for example verbose tokens secrets they would all of be kind of managed the same way but they'll be scattered all across even if it's on the same page let's see well think of something whoa yeah just wanted um if there any other it looks like research is both away this week so I don't think we'll have anything from them oh does anyone else have any other questions if not we can end this slightly earlier oh that one has a great rest of your day",
  "other. All right, today is the 19th July and we are meeting for CICD UX discussions. There are some stunning topics. There's just one impact which is by me. So in by playing security, we are kind of experiencing something like we are getting community contributions, which are not directly related to any issues. And those contributions, they are not very small in nature. So what ends up happening is it kind of like, and some being in conflict with something that we already have plans for and we try to make changes of that, we can accommodate it like we can take some parts from the contribution and get it implemented. But the discussion has been like going really long on those. And so Jocelyn and I we thought like maybe we should bring it up with the contributor success team. And that's the discussion that's going on there. So probably they might bring in a couple more automations to the process or some additional label that would require contributors to ensure that they at least look up for issues which are relating to the change that they are proposing. And that would also help us make sure that we are not discouraging anyone from making contributions because of course if we just continue the discussions for a month or more than that and just keep on changing, making changes to the decisions. It's not, I would say for someone who's taking a time from their personal time to do this work, it's not a great experience for them. So yeah, I'll keep everyone posted on what we decide or changes the contributors success team decides on rather. But this is this lack thread, but it's all happening. So if you want to just follow up. Yeah, Gina, do you? Yeah, because this happened to me as well and it gets a little frustrating especially for designers because we tend to have to go back in like retroactively design based on the contribution. And for runner we saw it was the same contributor every time who was doing it. So we ended up talking with the contributors success team and then met with them plus the contributor and kind of like talks about our roadmap so that we talk about our roadmap and also what they were thinking on from their roadmap so we can come together on those things. And what I suggested, I mean, obviously we have to like be open to them contributing. So I suggested that if they have like a new feature that they want to create, they create an issue first and tag me and that puts a lot of responsibility on us but I also think it gets us involved earlier. So that's what they've been doing lately and it actually has been helping a lot in regards to like making sure it fits in with the features that already exists or that we even like see coming up in the roadmap. Yeah, that's a very relevant example. So like this is case by case like for your team writing but to make sure that we come up with something that's generally applicable to everyone it's adding. Good for the contributors success team to come up with some new rules probably. Okay, if there are no more comments, let's move to product design. So for the first section which is which for Hannah has some updates that she's working on the backful requirement for all this position and if there's anything that any of the team members have been working on with the relief, they can just let them know so they can work on finding alternatives. Any other, I think NFI for Emily that the bears would be rotated by the end of July and for the rest of us as well of course. Yeah next is Gina. Yeah, I'm going to drop a little bit early from this meeting but also the things that I've been working on are mental model research for runners which I completed and thank you, Erica's not on the call but if they washes the recording, they helped me a ton with this project. So yeah, thank you, Erica. And they gave me a skeleton of a report to be able to fill out with all the analysis and still working progress but you're welcome to check it out. I also added a summary to the issue though. So what came out of it was to find groups of the runner concepts that users think about when they're using runners and then we also had them like come up with their own terms to describe certain definitions which almost all were not aligned with what we use at get lab but conceptually you saw some connections there. They just weren't the exact same terms. And then the final exercise was to have them match the terms to definitions like the terms that we used to get lab to the definitions and most of those they did they were not able to match accurately as we define them at get lab. So there was a lot of learnings but what I'm seeing now is the next thing I want to do with that Erica suggested is work with the team to come up with names for each group of concepts and then we're going to send out another survey to like a hundred participants if we can get that many and have them select which name like they feel best describes the concepts in that group and then ask them where they're using that and they're workflow and what tasks they're using it for. So then we can figure out how we want to make changes from there like if I wanted to make the design changes for that configuration example that I shared last week that would be like a way to help us validate that that is needed. But what I'm seeing now is to have the team is very it's difficult for the team to like understand that these concepts were grouped together because it doesn't align with how runner actually works. So yeah it's people are having a hard time looking past like the technical way that it works and then how people are grouping them and I was wondering if anyone has run into that problem and kind of how you're approaching it with your team. Will do you want to ask your questions? Well I did want to give you space to see if anybody has like thoughts on the question that you just raised because by questions a little bit unrelated. Okay. I think what Emily did with environments was something similar because they challenged the existing ideas of concepts that were like that were there since forever in GitLab. So I don't know maybe Emily any insights like how did the team take take it when they learned. But this was rather something that the team was involved in. So probably that's how it was different because it was not coming from outside but from discussions within the team. So the buyer would have been on the boat somewhere. We also had a lot of research to back that our current feature wasn't working. So I think that made it easy to convince the team because through like the group of environments research additional research before we were aware like this feature wasn't really hitting the mark. So collectively I think it was much easier to move forward versus yeah it coming from someone else. Yeah which is totally different in our use case because people are very happy with runners. It almost feels like we internally because we're touching it all the time. We have a much deeper understanding of it than our users, the majority of our users I would say but also the admins or platform engineers who are managing their own runners have a very similar understanding as we do. So it's hard to get it's hard for me to be like they're not understanding it when we know that they are because people love the feature. We want to think that helped us like I know we are at a very I would say nascent stage in the entire native secrets manager concept but one thing that helped us was the competitor evaluation because like the entire team got to like take a peek into how the rest of the industry is referring to different concepts and that also helped them relate with the research data that Erica had like greater reports on. So everything together they were able to relate one thing to another and that helped in like gain that conviction. I did definitely start with that. I have like a Google Doc with competitor terminology in it but I didn't really share that widely with their group so that could be a bit places to start a person. Thank you. Yeah, I just just certainly back to my question. How did you decide to get survey data or you know target 100 people? Yeah, so Erica suggested that we run the survey for I think a month and a half I think it was or we or when we get to 100 people like whichever one comes first and I they was saying 100 because like that's the average number that they've been seeing trends lately from surveys but if you have other suggestions I am very open to it. Yeah, I've used like survey sample size calculators which I can certainly link to here. So if you know like how many people are using a particular feature because I know that we have like it a lot of internal metrics on like who's a user of this stage group or that stage group. You could use that as a little bit of a guide and to like okay if our population is x. Then we should survey this many people to get somewhat of a representative sample size but yeah I'll include one below. Okay, I think another thing was that I was kind of saying that we need to move fast with this so they probably changed them answer based on that. Okay, yeah I mean that that certainly plays a factor too. Yeah, yeah but I still would like to look at that survey sample size and see what we do. Yeah, so I added a link there. If you click into it you'll notice that it has like three different sections. You don't really have to mess with confidence interval or margin of error. You would just have to enter the population size. So that's the number that you would have to either guess to me or you know get some analytics on to populate that field. Okay, thank you. This is nice. Great, the hearing will. Yeah, so is everyone that I noticed they're still typing going on. We're good. So just a small update with the outcome of the design sprint. We're trying to figure out the best way to go forward with the research here and we've started with a just internal customer interviews with the SRA and delivery team around the service concept getting a better idea of what they consider services. How we would populate that list and after we're done those internal interviews which I think they're like six happening this week. A big thank you to Victor for scheduling them. We'll be able to move forward with like external user testing just an update on that front. The other one is I'll be working on the design issue around designing for external jobs and I believe this touches kind of like the job details page, the jobless page and the pipeline details page. I'm not sure if any of those funnel into the verified teams that have designers but if they do just let me know so I can involve you and kind of feedback for these pages. I can help because I like I took part in a lot of free searches for those pages so I can help with any feedback but otherwise there's no designer for pipeline execution. I can also help because runner has a very small portion in this but there is at least a link to the runner that runs the job and then there's a lot of output from runner in the job log that might be helpful. Yeah this is mostly just designing it around this new like external job thing and what visuals will show but I think it'll touch like all of these pages. So figuring that out as I go forward. Yeah and also please slip mean because pipeline all thering it's not just owning but we are also touching some pipeline details page and then job log as well so I'm happy to help. Thanks a lot. Have you liked can I ask a question just on like what external job means? Yeah so this is one that I'm just starting into so I have to kind of look for it that this is as far as I know a say I job that's being executed outside of get lab. Like like on actions so for example on actions okay. No worries it's so what this is is I'm reading this out is like adding a new job status so it will represent the jobs that are currently being executed by a service external to get lab. So adding in like a pending kind of state as that is being run and then yeah so I'll actually link the engineering issue here that explains it a bit more than the design issue. Um That is helpful. I'm also reading the epic and it says external jobs are not picked up by a run and so it might not even can actually mean and if so you don't have to work me in. I mean I heard about this use case in some pre-searchs and also in the conference lately but I don't know I still cannot drop a runals it I need to read through this. Yeah this is new um for 163 I just started looking into it so as I learn more I'll reach out but it's a fairly new um being more looking into so I don't have all the details right now but I'm sure over the next week I'll have more. Okay. Okay. That's it with me. Then I will take up and so I'm mostly focusing on just one thing besides a couple of our community contributions this week that's the like finishing the POC prototype so I created the user map and flow on big jam it looks so good and um based on this particular story mapping I would be like designing the different pages or the different actions for this flow and that's for this. Another thing that I wanted to share was I'm starting to add like I added a slot to my calendar from starting next week for every Thursday morning. Kind of a time that suits all of us like all the members in the CI CD team at least and this is for like to have a sync time with me to discuss just anything that relates to public presence. We can just like use this sync time to discuss a block proposal to edit a block that's already there to like write abstracts select conferences designing content representations. I mean just anything that comes to your mind if we can use this time for that and to be like how you can add yourself you can just like go to the link add it to your calendar and for a whichever week you have booked it others would not look it for that particular week. Yeah and that is it. Yeah you could just do I like I noticed it goes to a calendar block. Do you get notified or do we have to like book it with you to let you know um let's try. I don't know if that happens because when I clicked on it it just goes to a calendar event but no one's invited to it so. Yeah okay I'm also not a part of what that is amazing. Okay now I am. I'll work further on this. Does it look very full? Well some links are getting it together. Yeah are we can just like sync runes like catch up over slack and just book slots manually this doesn't work. I'm not so good with automating things. And that's it. Yeah so in the new you can take it out. No I just wanted to say plus one but yeah it's my turn okay. So I'll type it later. So things be to Kat um I have two items that I like to share that um dove and mark and myself we're planning the Q3 that what could be the CSED catalog beta and then what could be included technically and also from the user standpoint and it's almost like divine and then we also announce a true to team and then now team is some board so that's exciting that means so just to tell you a little bit of more details we will gather more and made a data from now on so that we just just don't need to hold like this depend on the repeat.mb that user right and then this is your component a and b and c like for now like we have to just let them write everything and then just hold the depend on this instruction but you like to kind of provide a better assistance and suggestion system um so this is exciting for me because it'll open the doors for the further UX opportunities that I like to share this and the other point is probably you might see this in the MR's pipeline path um there is a now failed job number is surfacing in those tabs so that was the idea for this packathon and then luckily we get a lot of positive feedback at the same time we started to get a lot of bug reports. For example there is four failed job but so we just have a zero failed job and with even with the tabo emojis like oh okay we should fix it now so we're also working on this it's just popped up but it's exciting that people had a coming in and as they and then if you also have any other feedback please please let me know I'll also link the app to your issues so that you can leave some feedback around UX and also the UI pack as well. I think this was a really good improvement like a tiny one but so impactful. Yeah I was excited and then yeah we will have more issues. Thanks um every case on leave so moved with over to Bill. Thanks, Andrew. So I just wanted to quickly touch on additional point about the survey calculator that I spoke about a couple minutes ago added some information earlier on but I would just recommend you know working with your researcher if you decide to use one of those types of tools to figure out what's the appropriate like margin of error and confidence interval because if you leave it as is that those things are more like academic standard so tell you that you need probably more than you really do so if you have like tight deadlines or you know whatever it might be you might have to adjust that a little bit to get a better sense of what's a more reasonable number of people to survey. Outside of that I'm going to be a leave starting next Monday. I'll be out for four weeks so I'll get back second half of August. I've created my Q3 prioritization issue and linked to that here. A lot of the stuff that's represented there is just carry over from this quarter that's still active and then I'm going to be picking up some projects that are paused. And then since Ali has departed get labbed, coordinating with the PM Lauren and then Hianna to determine next steps for the solution validation project they was leading. So we're meeting on Thursday to discuss this more in sync haul but we're also having a synch discussions about it. If Hianna's going to be able to drive it while I'm out if we're going to just kind of keep it paused for the next couple of weeks until I'm back those sort of things. So that's what's going on with me right now. Enjoy your family time. Yeah. Yeah I need it. Yeah my wife goes back to work next week for the first time in like 12 weeks. So I'm going to be on full-dad duty 247. So give it them a little bit of a break. Okay so with that we have at the end of the agenda anything that anyone wants to bring up. If not then let's play more time back. Yeah let's meet later. Fine.",
  "All right, welcome every month. Today is 21st June and we are meeting for CICD by VTU. I'll start off with making a mention of some of the announcements that Hannah has put in the agenda. The first one is, we do a check-in in process. It started like 12 of June and it's going on and what we'll be using for these check-in says are regular one-and-one agendas. This process is led by team members. That means we have to go there, fill in our achievements, rent, support needed, and affordability areas since the last formal assessment. Besides that, there's another one. There's a 360-feet back that is going to be launched in July. There's not a lot of tedious available right now, and I will keep this posted. Yeah, coming to questions, I see one from China. Do you want to talk about it? Yeah, I can voice it. And then whenever Hannah listens to this, I'm sure she'll answer. When I was filling out the career development plan, issue template, which was the one after the individual growth one, we included a Q1 portion of it. And we've already completed that this year, so I'm wondering if we should include goals for next Q1 instead there. If anybody has already filled it out, or is going to, I'd appreciate if you, whatever you end up doing, just let me know. I chatted with Hannah this morning about this, because I had the same question. I'm like, oh, it's almost Q3, and the top thing says Q1, they says you could also just break that down into months too, so you'll be doing something in like July and August. they kind of put the quarters there as like a template, but if you want to break it down differently into months and stuff like that, you can do it as well. Okay. Well, thank you. That's a better answer than what I was going to say, so I'll just give. Okay. So what sequence are we following today is like, Eric and Will are going to go first, because I remember we did it the, the different period last time. So Eric, tea. Will you go first? Sure. So mostly my updates are about things that other people are working on at the moment. So it participated in Emily's design's friends, over the past two weeks. I've told them this, but she's really did a great job, you know, from start to finish, and it was great to be a part of that process. All the activities that they put together, we're really great is by first time doing a design's, and I'm really excited that we came out of that spread with a prototype that was tested internally, but also we'd plan to test it with customers. So I'm assisting on the late social validation issue that Emily has started drafting. I'm also supporting Ali with their social validation testing. On the onboarding experience that we've talked about in past meetings and looked at them figure. So right now we're developing the screener. And I think he's also planning to work on the discussion guide, which I'll come up with. And then the last one is that the PN for switchboard, Lauren, who started, I think about month and a half, two months ago. When they was onboarding, they completed some internal team member interviews to get some feedback on the current experience related to dedicated and how that process is very manual onboarding customers. At that time was more of like a limited access product, but now it's we've just gone live as of the last week, we're dedicated, is open to all customers. And she's just recently finished research and added insights to the issues, so I'll link to that one as well. Any comments or questions? I do. I have once, you said we have gone live, but dedicated, is it the same thing that Ali was working on? Like is it live now? No, that's a good question. So this just means that dedicated as a product was previously something that was like limited access. I think we only had like three or four customers that were actually all dedicated and it just went general access or G.A. So it means that like anybody who's interested can sign up. That's a little bit different from what Ali is making with switchboard, which is more like an automated onboarding process or kind of self-guided onboarding process. Which will certainly help people as they transition on to dedicated. And that's something that we're going to build as an MVC in the next couple of milestones. But yeah, that's a good distinction that dedicated is a little bit different from switchboard. It's kind of connected but a little bit different. And they're getting some sort of assistance right now. It is so manual right now. Basically, like our sales reps are kind of walking through, walking through different parts of the process with customers. And it's so much like back and forth over email. Trying to track down information that they require in order to set up that instance. And so that's why like having this onboarding experience will really help. Take off some of that manual burden off of our sales team. Good. Thanks for answering. Good. Yeah. And future for me. Yeah, that's a good question. Any other questions directed over at Erica? Okay. So if you haven't yet, and I think everyone could participate this on in this. Who's on the call now, but this is the first week of the AI user needs and. AI fit workshop. So we're done it. But basically the idea is we're going to answer these questions in the survey. There's two surveys for this dedicated feature that we've decided to like go through the process with. So the idea is just to learn about what's a good fit for AI and the long answer is it depends on how we design things. And so that's opened up. So everyone wants to complete that. There's two surveys that be good. And you guys can if you don't even want to. Hopefully Gina, we think a little bit is a pay, but all all the rest of y'all want to look and to see what we're doing. I think it's like helpful just to see what those workshops look like. And then just calling out kind of just a hey Anna knows that we. I'm so excited about this, but we we figured away to differentiate against one of our key competitors for our. Get lab native secrets solution and we something that we've already called out and I like created a deck about it. And then we like finally heard from customers. This is something that we really struggle with in a key competitor, bless you Gina. And then to me that makes sense like why I would be very familiar and why that like problem we keep resonating with me. So but I we're all like aware that we need to like figure out how to work together on this. I'm sure we will, but we just want to like make sure we're mining it so we don't get back into a design corner by accident. And so I just thought I would put it in the agenda there and then. Tina and I are having so much fun working on the mental models research for runners. And she's going to check in with some new today to see if they can like combine efforts with the mental modeling research for pipeline authoring. So anything I can do to support that collaboration, but depends on availability. So and then last but not least is a feedback request that I was hoping people would fall out and just to help me focus on. Tell me what to focus on in the upcoming quarters. Yeah, me too, Dmitrika. Thank you. Thanks, thank you for. Especially for the deck because I am personally interested in that for the differentiation and I want to make a mention that I have highlighted like there's a tag in my new research project and ductile with the variables and environments and today I'll be processing those into insights and I'll be sharing with them. Okay, so moving upwards. Singing is not attending but they has a couple of items here that you can take a look at it's about the design think that they was. Like there was a design to think around see I catalog and the other one is a solution validation. For how users would want to publish their C. I confidence to the catalog both interesting ones and next up is me. So we interviewed nine participants last week for the solution validation research for variables it was like specifically about the new layout that we were designing for presenting the variables on the project setting page because that's where users get to see. Custom variables defined at two levels projects and who were so group so they inherited and the native or to the project ones. And it went really well like at least I was not expecting to speak with nine participants all in one week. There were no no shows. There were technically but then the risk at you that we still got to talk. So one learning that I would want to share is. I'm a standard follow up email a few hours before the schedule meeting that really helps just like stating that looking forward to talking to you in the upcoming interview and something like that. We shoot and dovetail product link is tagged here if you are curious for what's happening there but I'll hopefully record a video by end of today and post so my goal is to like wrap up this one and wrap up the sketching exercise before I leave for the conference next week and. Apart from that I have also written a blog with Patron on the work that we did for beautiful for you. I good thing is I met with Jackie yesterday and they mentioned that customers are noticing it and they are happy with the changes so. It was scary. I am going to be speaking at Penbot first it's a tool which is an open source prototyping tool but there is simply like. They got funding last year and they have gone GA I think in January they had this official launch. So there'll be a lot of organizations present there like us like Red Hat GitHub. It will be an interesting gathering and yeah. I'm looking forward to it. It's happening in Barcelona and of June. And if no other questions we can move to Emily. So I know I already reviewed this during our feedback session yesterday but the three days of the design sprint we finished up. The last week or the week before I'm losing track of weeks at this point. But we kind of have the highlights from the hallway testing summarized so just to give some context hallway testing is the third day of the design sprint where I get everyone in the sprint to kind of. A potential persona at GitLab and given interview so we have some highlights from those three interviews and then the next steps here is to run around a solution validation over 16 to 163 so currently working with will to put together that testing plan in the smile stone. The second the one some of you might have seen this in the UX channel but we're celebrating not moving forward with a concept so we're deciding not to move forward with the group of all environments concept mostly because we realized part way through these interviews and with kind of stuff that came out of the design sprint that we were moving forward with a concept on top of a feature that wasn't very strong initially. But I think what we want to do is go back really perfect the environment's feature itself and then move forward with something at the group level versus kind of building on top of the feature that's not working currently so that's kind of the outcome of that. And the final one is I'm looking to put together a mini design sprint training guide just wondering if anyone is interested in kind of being a pilot for that. Yeah, I mean, what we are planning. It's going to be very different in nature but I think the general guidance around that is going to be very helpful for us. Cool yeah because I'm going to plan to just put everything into like a template and kind of share that out with like best practices so I can start with like just our immediate team before sharing it out with anyone like further than that. Yep, I think in support we definitely yeah benefit from it. And that's all for me unless I knew and hence any other questions I can give it to Gina. All right, I have been I actually talked about this a little bit yesterday in UX weekly, but the whole runner dashboard MVC has been quite a ride because we have run into a lot of issues performance wise. But I know I've been still fighting for the things that I learned from research when we did solution validation around this feature. So we know that the metrics we're showing are the right ones we just need to figure out how to implement it. Come up with a POC using postgres, which is our database that we currently use and go from there so we'll see how it goes. And then my other note here is about the mental model research that Erica brought up. I met with them yesterday and I'm very excited for this because there's been a lot of like heavy discussions happening internally on the runner team about disagreements on the terminology we use for different parts of runner. So we're going to focus on mostly external users to understand how they think about runner concepts without using get labs specific terminology and will be talking with people who have not used get lab before so will be more general. And then we're also going to ask some just five internal get lab team members. And we set out a plan that has four parts which I can also not update once we update the call trick survey but the first part will be a card so we're. Just like the typical ones that we usually see where you're grouping terms and then you name the groups and then ranking order of importance of concepts I think we have a list of 16 or 18 even right now. But maybe minus the duplicates it would be less I think that's what it is. And then we're going to do a fill in the blank section kind of like madlips but not really and then matching the concepts to terms so we'll have like a list of the concepts in the terms and they'll match them together so today I'll be. Doing that like putting that survey together. I'll read you one of my voice that that's probably yeah yeah so yeah cool. So I guess I'll. I'll. Do my section yeah. So I'm more human finalizing. Our solution validation that we're going to do for the designs that I've been sharing over the past few weeks. So yeah doing proper working on the screen and discussion guide and recruiting and then I'm also working on story mapping. For switchboard initially I'm going to do a pass and then I'll do it with the team next. So yeah hoping to get that done this week. And then I'll share with everyone as well. Yeah can you elaborate a little like what you're trying to achieve through those story mapping exercise. Yeah so we know that. We we want on boarding there certain things that we want customers to be able to do. Such as. For the customizing we are dedicated instance also customizing when they want to do maintenance and then providing monitoring. For their dedicated instance and some logs and there's some also some other tasks that we also want to do so. Which is on a map. And so what that will look like. Yeah and yeah heavy discussion team and see what everyone has been thinking because yeah. It seems everyone has different thoughts on what switchboard should and should not do so I think this will be a great starting point. And this was good because I have never done like specific exercise for like a story mapping exercise for any of the things are just thinking like. Can I. Like getting some inspirations from you. Yeah that's it. Okay. Um anyone has any questions or anyone anything else to. Discuss. What life didn't with this answer to recording.",
  "Okay, everyone, today's the 26th of April and we are meeting for the CICD UX meeting. And today I'm filling in for Hannah and let's start with those stunning topics and FIIs. So, I'll just like, I'll mention like, the important items. So first, just Hannah is going to be back next week on Monday. And next week on Thursday, we have our very first design review meeting. So that's great. And anybody who wants to volunteer to share a problem with the team for design feedback, please add it to the yetto nine advance. The agenda can be found in the calendar invite. So it's probably not the same one. It's a different one. Then the next one is by Syngim. So do you want to localize that? Yeah, it's more like a FIIs or a read-only item. And I thought this information from the CICD Keller project that's like, we redefined this name because I'm working on the experiment phase of the features. So probably it might affect to other maybe AI projects or your new feature. So I think it's just good to know because if we name it using the experiment batch, because I was using the alpha before, but I had to change the experiment. And then there are another node from seed recently that like, so all features are should be just public. I think that part is also added at the very end of the talk. So I just wanted to share with you all. Thanks. My main main feature is that I saw like it was mentioned that it just slows us down. And we release a feature only internally. Does anyone have any other points for the FIIs section? Or we can just, okay, we'll just move on to the origin items. The first one is by Ali. He, I think, is not on the call. So quickly, we're obliged. they has thank everyone for the reviews on the minimal onboarding design flow. And he's working on incorporating all the changes. And I can totally understand like there are so many comments. So it's definitely going to take some time. Does anyone have, oh, Ali, you're here. Do you want to take it up? You're item? Sorry for throwing that on you. Like, so suddenly. Well. I think we were, I think Ali was still connecting to audio. So you might have not heard you. Okay. Okay. Ali, I was just mentioning that I was kind of covering your items for you. Do you want to take it up and work a lesson? Okay, cool. Sorry. I didn't realize again. No problem. That part. Yeah. I wanted to say thank you for everyone for the reviews and it took back. I'm working on incorporating them. And yeah. I'm working on improvements. Yeah. Yeah. Just wanted to mention like great work there. And we understand like how many feedback there are on the issues now and the time it's going to take you to incorporate this. Moving on to Gina, they is out of office today and there will still be holding sessions for the selection validation for runner fleet dashboard. Does anyone have any comments on the runner fleet dashboard? Validation. If not, then I'll pass it on to Emily. Well, so my main focus this past week has been planning for a design sprint within environments. We want to kind of rethink environments as like a deployment tool. So just, I don't know if I'll all of you know, but I still run design sprint and teach how to do design sprint that my previous company. So I'm taking the time while planning for this to help fix up some of our templates get everything organized. So might have waited this a little low given kind of some of the stuff we're having to do, but hoping to have that complete before this will be my next tip is I'll be out of office for two weeks starting May 12th. So my covered issue is there. But yeah. Oh, see, there's Vita Gahez a question. Yeah. Very well. I mentioned about like a re-looking at this whole experience of environments environments and deployment. It kind of reminded me about the secret manager, like native solution for secret management. And I'm just curious that how you and your team have talked about it, that while you are looking at this bigger effort, that's definitely going to take some time. In that mean while how do you plan to look at the smaller improvements to this area? Yeah. So that's a really good question. We're hoping that actually the design sprint lake day one will really help our focus in what we're going to be kind of going into in some of the takeaways of this will actually be to have think it's like three or four. I'd have to double check on, but issues that are ready for development to take on. So while we are looking at kind of reimagining the whole experience, we're also looking at getting out smaller issues to kind of work our way there. If that makes, I hope, is that the question you're kind of looking for? It is. I understand that there couldn't be a very structured answer to this because it's something that we just explore and figure out. But it's good to see another fellow designer in those same position. And just to get inspiration from. Yeah. And I'll be recording all the sessions for this too. So I'll be able to share those out. I think we're looking at hosting this like the first or second week of June and having it run for a week and a half with split async and sync periods. So still trying to figure out how everything works together. But hopefully we'll be able to share it the process and maybe it's something other designers can kind of take on as well. Nice. Thanks. Does anyone else have any questions for Emily? I would just, I would like to be included in those design sprints. That's note. And then the other thing is just, I had created that deck that highlighted the trouble with variables and environments. Sorry, my maintenance people just came. So I was like, okay. Okay. Well, I had created that deck for y'all. And I will link it here. It's just that we have not been like listening intentionally for, but have kept hearing about this paint point related to variables and environments. And so when that happens, that's an indicator that that's something that's particularly painful. Be a part of that design sprint, but also to elevate that deck again, which I will like, assume it's like invited. Well, great. Thanks, Erica. I also keep keeping eye out for that because I mean, the changes that we are making. Yeah, it's related to deployment, it's related to environments. So yeah. Yeah, we want to make sure that this is like the beauty of this meeting and this team, right? It's like, let's make sure that that experience cross-functionally is working. I'll link the deck. Welcome back. Okay, thanks. All right. So the next item is, I wanted to share the progress that we are making on the beautifying UI 16.1, which is going to start in the next milestone. So Patron and I, we met to do a small prioritization exercise because if you look at the issue, there were a lot of issues shared with us many epics. So we just like went through the list and picked up items that we can work on with the time limitation and which also makes sense to work on with all the current priorities. We like, put aside a bunch of issues, then we plotted a chart of like what's going to be impactful and visible because it's purifying UI literally and then we plotted that against time. It's going to take in how much time we have. And you'll see like in the description, I've updated the table of the things we're working on. Most are like small but impactful improvements, but there are two that would require some design work. Which is the last one, the table. One is unboxing pipeline detail page, which is kind of ambitious, but we'll still give it a try. And the other one is rearranging the tabular layout on the jobs list view to match the pipeline list view. If you like compare the two today, there is a difference in style and there's also difference in how we are organizing data. The jobs view has like so many columns which just makes it difficult to have and be like change the viewboard size. It doesn't look very elegant, the kind of shifts that happen there. That's one. And everything else is just like regular street group work. So I'll talk about that in some other meeting. If nobody has a question or comment, I'll pass it on to Sunjum. I have one comment. I'm trying to find it. I'm distracted by trying to find the link. I did this analysis of the suspect for Betham for the last two quarters, which will come out at the end of this week, they report. Part of that was identifying paper cuts. Some of you got pulled into the issue. Yeah, and so I created an issue and the idea was just to get verification from the team about whether or not it was a paper get. But less than if they didn't just start thinking about how to fix those things. But I wanted to call it out because some of them are small things that relate specifically to pipelines and those pipeline views. So Vithika is soon as I could find the issue, it might be worth just kind of scanning to see if when you end up hating do that work. If there's any other little things you can support. And maybe that team would pick them up or you can get a little help. So there's that. Yeah, definitely. So we were a little conservative without planning because, yeah, I mean, we just did not want to over promise. But I have this hunch that we would, in fact, have some time to spare and we can definitely squeeze in some small MRs then. Okay. Singen. Thanks Vithika. So there's one excited moment for me and for the team was the detailed stage of the catalog has been shipped. And it's being reviewed. And I believe you're ready to break things down for the implementation. So the front end team is working on like how can we break things down starting to communicate it with the back end teams just the usual development flow. I think we're good if you check the issue like we got a lot of feedback. Those reading exciting and also the design is done. So I think the team is ready to work on this on 16.1 until 6 in.2. So that's exciting. And the other piece that I like to share with you is like myself. With our amazing tech writer is working on the very initial onboarding flow. Let's say for less experienced user who haven't create any pipeline so far. And then if you check this example review, I have like this is targeted for those audience. And then like I just tried to stop by myself like it was so nice and it was so helpful. Like they put all the like comments just next to it's Yamul's in text what does this means. And then you'd expect to see some failure attacks. Pass after running this because of this reason. Oh, sorry. So we were just working on this type of guidance. So I was excited and I was excited to be in a community. Kinipate because I made some trouble. But yeah, so it's yes, the dark huge starrous and then they have really cute dinosaur icons. So that was exciting when I successfully build a pages on my namespace. Like this is cute like really big dinosaurs were on the page. So I think it's a good example. And if I am for two weeks and coming back on May 15, I edit my cover this shoe and yeah, that's it for me. So I and I saw here so maybe we could use verbalizer comments. If you don't have any other questions, sorry. Okay, that looks like we are clear. So if we are being panked in the community contributions for package, we need to assign that to Pedro because he's I think covering for our staff groups that don't have a designer. And here are the like changes that were made to the magical review in relation to that. All right, with that, we are kind of through with the product design items and now, I was again passing it onto Will and Erica. Thanks for your time. So starting to wrap up and hand off projects to others before my leave starts in a couple weeks. So I've added my parental leave issue above and also to the template since it will go on for a period of time. And then I'm also going to be making a Q2 research prioritization issue. A lot of it is going to be carried over from what has have been completed in Q1 because it has been pretty proactive about letting me know what research needs they have. So I'll you know, paying you know Emily and Ali and many others did I work with when that gets created. Also wanted to shout out Erica for you know supporting Emily with recruiting for them solution validation study. I just wanted to thank you for you know joining in on on that comment thread and really helping support them with recruiting. How's it going Emily? That was one of my questions, Jessica's it working? Yeah. Oh, you need it. Right there. There's a vacuum I think so she'll be muted. Oh, oh, god, I got it. Yeah, that's that's that's that's it. All right. Yeah, I'm I'm like very interested in the progress because I'm going to come to you soon Erica for recruitment. And I think with our big success this quarter will be that you guys can sort of drive that and be more self-serving or that sounds funny. But yes, be able to get access to this participant. So be the give you want to if you anticipate like in two weeks starting to need that participant will we should be asking Caitlin for a push now and then that way you guys are like in a spot where you're like no I need this participant. No you need this participant. Okay. Yeah. And then my last comment is that I reviewed Ali is minimal onboarding design. I think maybe sometime last week and gave some feedback. I really appreciate you know you putting those screens together. I think the kids spoke into this before. But you know there's no prior designs to just clearly leverage. So that's incredibly difficult to just try to create something from scratch. And I think it will spark to you know open it up to be back from the whole UX team and also bring it into the UX showcase to get some additional feedback and comments. That's all I heard. All right. There we go. I did not make notes yet. I can't believe I can't buy this paper cuts issue. But anyway, I want to talk about I am doing meetings with everyone because we kind of like had that benchmark stuff and so there wasn't any way to juggle in new research topics because that took up a lot of time. So I'm setting up one on one meetings like really next week and the week after to make sure that we have scoped out like forward-looking research projects. And then my research prioritization issue will get finalized on May 10. So just to note that that hasn't been finalized. And then what I was out, but I saw that there's a thread talking about naming for the catalog components. And we did have an issue for that, but it's silver. And so that means that don't drive it. So I did it to elect like a little bit of data for them and they can use, but it was very narrowly focused on like how we name the group of components. And so and that's already been done for him. Yeah. So I did that for him, but I don't have the agility to like do it again in an easy way. And it's also silver projects. So they should drive it. Does that make sense? So I saw that there's a thread with Chrissy. Well, I don't know what to like quite what to do. Because you were like, oh, this is happening. Oh, isn't happening. So anyway, I just wanted to note that I'm not driving that project. And I did like, as like that's what I always am trying to help. So I did try to help by getting a little bit of feedback just on those two, like how do we name a group of components. But if we wanted to do other research, that needs to be like loan up and reprioritize and going through that process. So I just thought it would be good to sink like talk about that. Because I'm probably not going to chime in on the fact that because I feel like that's one of those things we don't really need to like surface it. Like that goes, it's in dose court. Yeah. Okay. Let's see, like the right thing to sink about like face to face. Okay. And then another thing that we're doing is we're working on the CI components alpha which we can't even call it that because we've now like changed what that means. So it's now like we're pre experimentation. But we have people coming in and they're using a sample project to use a CI component. And we looked at their satisfaction with the CI templates and then across the assignments. And so they were more satisfied. So it's really good. I was a little nervous. Like, what if they know satisfied? Then we have to do all of this other work. But they're looking like their satisfied. So we now know that there's we were ready to take the next steps. But so new we did like scaffold their approach in that assignment where they use the sample project. So then what we need to do is then like break out how we scaffolded that and then make sure that's happening in the in the catalog view when we're ready. Yeah. Okay. Is there any like for these sessions that are happening very users are coming in and testing the CI component. Is it a video we can watch like of them? Yeah, they're not. We're not. What we did is we gave them assignments. Okay. And once I saw it, like dove has just so much happening that I'm just like trying to make it quick and easy. So I'm running the interviews. But actually we gave them a survey format of the assignment. And we actually one one video where they made a video them cells and shared it. But we kind of are just getting more of their self-report and then we're doing follow up. So it's not as neat as being able to watch them use that. Got it. I wonder if there was a solution like user testing but something that we can control and not truly like go through their recruitment channels. And we can share the link but that way everything would be recorded. Yeah. Yeah. Oh, by having them use it. Yeah. Yeah. The good idea. Yeah. Because they might not be mentioning a few things which might be worth observing is what I'm thinking. Yeah. Well, this was easy enough where, basically we put the links on the include syntax in the ReadMe files. So it was pretty easy. So I think, but the livers on the concept of being able to use include to kind of get the components in your pipeline. But I don't think it would be like white worth it. But that's a really good idea. I keep in mind. Like as we go, as we move forward and include more participants. Yeah. I like it. I just find it funny that how important it includes and like when it was initially launched nobody sort of I want to know what most people did not realize they important. So it's powerful. And Erica regarding the naming research like we have a lot of research already like opened for the just for the catalog project. So we need to discuss the priority. And I'm not so sure about the priority itself because like it's it's own doves. So we can discuss. All right. Does anyone have anything else to mention? If not, then we can just wrap it up and we can get back to our own things. See if sometime. All right. It was nice talking to you all of you. Thank you. Bye. Bye.",
  "everyone this is the CICD UX meeting on April 12th and I think it looks like Gina you have the first point if you wanted to start us off. Yes, I honestly when I was filling out the agenda, I wasn't sure what I should be focusing on. And I was so I wanted to bring it up with the group and just ask how you've been feeling with the structure of the call. The specific thing I was confused with was if we should be focused on a design review or if we are normally like going through all of our group updates. I did notice that the designer view has like a time box on it, but the group updates do not. So I'm wondering if something like that would make it more clear. Yes. Also are the items in the group updates actually read only. I think that's something I haven't been following quite as close to the cabinet. I have neither. Yeah, looks like it says items are really only by the focus that meant read only. But yeah, I think that answers your question, Gina. Yeah. Well, do we find it beneficial to still get group updates now? I think it might be like the UX weekly where if the designer reportion kind of takes the entire time you just leave them as read only and then if you have time we just go through them maybe something more like that. Like focus this meeting more on the designer view and then if you have time we can go through the group updates. That works for me that makes sense. So we would have to do more than one design review than. Oh. I think it's more like if the designer view takes a while it can like we don't stop it from taking a while and then if it doesn't we go through. The updates. But should work pretty well for it. Okay. I think I tried somewhere as you would. All right, that works, that works. Thanks for having that discussion. And I have an exciting. Okay, so the design this shouldn't really shouldn't be. I've been. I've presented in the past how we're updating our runner creation form, which is really nice update because we get to do a lot more things in the UI and and explain what different runner concepts are versus before it was just in the CLI. And then the problem statement that's still kind of around this whole thing is it isn't clear how a new developer who doesn't know anything about runners can create a basic runner quickly to be able to get started to use their CI city pipelines. And then also runner concepts are unknown to these novice users and they're they're not sure how it kind of fits into like the larger scope of CI city. And then the feature that I'm trying to work on now is making the the registration form, which is the the first iteration that I created for the new form. I'm trying to make it a little bit easier to digest, which it's super long right now so it's kind of hard to do that. And this would be mostly for developers who's admin like restricted their shared runner usage so they can't use shared runners and they have to create their own. And then we just one of the known constraints is that we only bring in configure option configuration options that are basic. So if you're an advanced user you can go into your config, Toml file, which is the configuration file that is created with each runner and you can configure it with as many advanced options as you want I linked to documentation about what that means, but like you can make it run more jobs at once there's a bunch of different things that you can do to it. We don't show those in the UI though it's only in the config file. So now I'm going to open up my thing. All right, so to give you just so we have a before state. This is what the first iteration of the registration looks like. Or creation recalling creation. And when you want to make a new runner you have to select the operating system or or whatever platform you want to make it on and then after you select it you get this like form of different fields that you need to fill out a lot of them are optional. But the only thing that's not optional is the tags you have to either add tags or you have to select run on tag jobs. It's one of the one of the other. And then once you create it you get you still have to go in the CLI but you get like this one thing that you can copy paste it in the CLI and then you have a runner. And it was looking at like if we have any patterns and pajamas yet for I would call this like a stepper but I guess you could think of it as a wizard as well. And I saw that we had an issue that was already open that has a lot of work done so I kind of brought that into this view. And I'm hoping by. Breaking these up a bit more I can help. One users understand the concepts a little bit better because. Kind of the most important thing is the tags which I broke out into its own step and then I can also get out the fact that you either need to add tags or you have to market as. So the design is really just like breaking these up so it's not as long so if you click next year you then go to tags. And it's the only one also that's not optional. So you would have to select either add tags or select this and then go next and then this whole step is optional. For details and configuration so you could just go next immediately. And then you get to that same screen at the end of the registration. But I think my main question was. In this pattern is this this pattern seemed like something that any of your groups would need as well and are there other use cases that maybe I'm missing. For me I don't have any other like adoption for this but probably in the future yes. And so I was like looking into the other issues so this is outside of verify but I have seen that there's a new it's behind the feature flag that get left pages team. There having similar we sort. And so I think that's the main thing is on the UI I'm also sure like it could is delivered by any of designers but it was implemented by. I've run it in general sure that issue was like found that and I'm pretty sure a few team needs one of this step or at some point but at the moment they don't have designer so not sure when they can work on this but. And then I think that in a couple of years. Would that be for like installing it lab. Yes and then making those two different sides to making a geo connection so that they can start mirroring all the. Data inside one killer been so's to the other one. Okay. That makes sense. Yeah. Yeah, I was going to say the same thing not so much on my current team but on the growth team these were like the step count like how you set things up I know we're playing around with these quite a lot and I don't think we ever actually came up with a component. But there was definitely a use case in most kind of setups like setting up your account setting up and getting onboarded and certain things so I do think it'll be useful to actually finally create a component because this is something we circled on for months on that team. Okay. Yeah, also in my area. And we're working on the onboarding flow and it's also very form heavy retired ways to kind of cut it down and include some navigation but. Yeah, we certainly didn't. Think of going this step of direction but I think yeah definitely use it. If it's available. Okay. I know when I was working with anti abuse. temporarily we also did like something like this except it kind of went down the page so it was certain like. Clawbsville sections that reopen yeah it was exactly something like this. Okay. Yeah, I think the thought was that and Lee Moore was doing a lot of work on this before so they had created a bunch of different options like vertical. A lot of. Sometimes you can jump between different steps so there's that option as well. And then there's horizontal and then. This linear option as well. I think the only other thing that I had in mind was something. It's more like this where you're it's more it's like looking at a process so in this case it was updating runners we don't even have this available today but in the future we might. And I was thinking we could reuse the same icons that we used for the MR widgets. And then just like give status of each step that you go through because there's something happening in the like I could. And this is all kind of steps that are happening in the back end that we're trying to give the user notify the user where it is within the process. I do like. I do like a green check like I like that visual but like you've completed that step. Oh yeah, that's one thing that kind of felt was missing from here. I like when it's completed it's just blank and I wasn't sure if that if we should add some in the further indication. I think even just like a checkmark icon like so the number. Yeah. Like this. Yeah, I could just the color that's something like that. I'm going to play around that a little bit. I know you have some comments you want to voice them. Yeah, mine is just that I think when you're ready to show this to folks we can recruit from the substance respondents. So I'm just working on a project where I looked at the source for a bit and for the last two quarters. And just as a side note, runners and pipelines popped out as to pay that's particularly confusing. On the bright side that means we can find people who think letters are confusing and ask them to participate in research for you. So that was my first note. And then. Yeah. So as a part of the like such process. We'll create an issue either by self or or Nick. Depending on who's on leave and who's not. We'll be creating an issue for. So I think it's like traditionally called like a says responder. And this will be a mail. This is typically made available to PMs, but I certainly think you could recruit via that sheet like just find any like relevant quotes and you know, pull them into to your study as Eric will say. Okay, awesome. And I have some from the previous quarters that I can pull for you. Okay, yeah, I'll be helpful. And then another thing that's coming up in the. And so I think that's the first thing that's coming up for beta analysis is that users want specifically onboarding content related to like learning and documentation. So we've been pulling out that learning and documentation is important, but it's targeted. So this is helpful, like which you're pointing out there like what you're kind of doing is an onboarding widget almost. Yeah, I think it would be helpful to just remember that I mean you're already thinking about this, but remembering that these people are kind of. And it's just because they're new. Yeah, just kind of layering in as you can. Like yeah, not in a way that's not too heavy, but that kind of like gives them a framework for what they're doing. Yeah, I think I could add more. Like explanation about platform, I think that's. I added a lot about tags, which was nice because we never were able to add any documentation about that. Just that that's like how. The tags determine which types of jobs that the runner processes so then it gets like a little bit more connected into the. The CI city process and then I I like to documentation stuff as well. But I think that there's more that we could add. And even stepping back like it's like, how did they get to this point in the workflow right like how do they know if they should be doing this or not. You know, like shared runners or not this. I'm thinking. So, but like how do they know if they should be using that widget. I see. Yeah, like where are they coming from within the flow. Yeah, yes. Okay. But nice work. Yeah, and I think on the topic of like novice users. There could be some opportunities for like tool tips or something implemented over the course of those, you know, different pages or steps to kind of communicate like this is why you're making this or if you're confused. Like, here's some information that can help you make a more informed decision when you're choosing between radio options or what should I enter here or what if whatever they be. Yeah, okay, yeah, that's a good idea to. And then by next question, like, would you be able to pull out the. The prototype again, because I was a little bit confused about that optional text. I don't you said that like it was something where they had to choose. Oh, yeah, like. So yeah, I think we're in this one. Yeah, that's more. Yeah, this is the current one that we're using and. You have so I think this was just more of a mistake that we made you have to either select that the runners get a run on tag jobs or you have to add tags. So even though we say this is optional. You actually have to do one or the other here, so that's why I was trying to like break those out into their own. Section because that's really the only piece that's not optional. Yeah, that makes sense for trying that. Yeah, maybe just include some language there like if you select run on tag jobs, you will need to select tags below. Um, you. It's so it gets so confusing. Okay, so. You can hot, you can add tags and you can run on tag jobs. So our runner can run jobs with tags and it can also run jobs without tags, but you have to at least have one of. Like you have to say which one or both does that make sense, but I think I could add text that explains. Yeah, my it's like you need an and they're like an or choose one or both I guess. Yeah, yeah. Okay, I'm going to think of how to explain that a bit. All right, thank you a lot for the feedback. I think. My action items are to see what other information I can add in there that helps explain the concepts and then. I'm also going to work on trying to figure out how to contribute that stepper slash wizard design to pajamas. In the same thing. Well, I guess I have the next item. So I am continuing work on the onboarding flow just to finalizing it now. I had a meeting today where we're just going over some of the settings and what what that should look like. And then also working on messaging as you can tell from genus form that yeah indicating things like optional and all that. Yeah, you have to be clear. So I'm working with our technical writer to make sure that yeah, we communicate clearly. Which settings you can change and yeah, once I'm done, I will open it issue for review and I think I can also do presentation here as well. And then yeah, we can take a feedback required to any validation internal folks. Yeah, I'm not sure if you're already wanting to do this book, feel free to you know, tag us. I'm more happy to provide feedback on that minimal onboarding flow. Awesome. Thanks. Yeah, that's it. Oh, is that huge enough? Yep. Thank you. I'm running solution validation sessions on the runner fleet dashboard, which I've shared in the past and I've run one so far. And the early insights were very, very positive to excited about and I summarized those insights in the issue. And I'm still waiting on more enterprise customers to meet with. I guess it's me. So in environments we're currently working to put together a glossary, which I think is much needed just a common terms within like the deployment area. So we're getting that on track, but looking forward for this to help with like a certain communication, the design sprint and just in general talking about deployments. And I just wanted to bring it here. So all of you could see kind of what has gone into it so far. And then just wanted us. I'm also working with Eric and well to test it the common screener for my solution validation study for group level protected environments. So kind of excited for this to test it with self serving option and let you know how it goes. And then the final thing to have is just more of a question. So I don't know has any one ever been assigned a community contribution that's not in their group. I was assigned one for package and I did notice in the experience weekly. Let's like Pedro is one that supposed to be reviewing these, but is this something that. I should take on is it something I should pass off to Pedro. I'm not sure if anyone has experience in this area. And I must say. I have an experience. Yeah, I think it's not similar, but I have an experience like selected by boat automatically. But I found one issue that is dedicated for the distribution team and then I just think that design manager that I can review this so because I already have this that will be my GDK. So I had this discussion with hyana, but from my understanding the community contribution without any designer goes to Pedro. And I'm not so sure like how we can make those change in our boat. But I think that's the process. So if there's no any no designer on that stage group goes to Pedro. So that was my understanding. But if you are willing to have those review or if you have capacity and then you can assign yourself on them. And then P. Pat Broke as well. I was trying to review myself. It looks like it's one that I need to have like packages and images set up and MR review doesn't have instructions on that. So I did ping Pedro if they hasn't set up already. It's quite an easy review. If not, it's something I can look into. I think I just need more instructions on how to do it. Oh, thanks for the feedback. Oh, thanks for not taking away. Yeah, so P. Pat is on leave. Maybe I can move on to my item. So there were a lot of discussion going I really appreciate Erica. We're taking a lot of time for this and moderated research and then I as Kayana like hey like. I mean, the feedback and then just provide some feedback how we can move things faster and there are a lot of discussion on going and then so in the end. Too long discussion to read. But in the end, what, hi, Anna suggested was like, okay, let's just step back. And then we have run the first unmotorated solution validation using the user testing.com and then we got already just one cycle of outcome. So let them decide if it is enough for them to make decision to put it inside the left side navigation or not. And corresponding to their feedback, we just choose the direction if the answer is no from the foundation theme. We start from the pipeline editor and having one small button there to access to catalog. And then in the meantime, we we still gather a feedback from users. Verbaly or also we need some more metrics to use snowplow that also require some front and implementation. So that was the outcome of this research. Yeah, thanks Erica for your help on this and for everyone. I'm jumping into the discussion and the other point that I'd like to share with you all is about syntax actually. So this is something that I try to onboard myself like how to create a pipeline as I want and then how to choose the right keyword to keeping a certain job, for example. And then most so pointed out that like, hey, this is also a very crucial part of our UX like we should focus more on this terminology and also the experience on how to choose the right keyword. Is it easy enough so we started this Tim discussion and then it will lead to another round of user research and also the team discussion and then I'm very excited for all of the team members are. Okay, so I think it's part of our crucial user experience so just one of the shares and we find one with you all. So hi, so packages and these social we move on to the UX research will. Yeah, I'm just going to go quick updates. I created my breath relief issue. I think to it within the doc. There's a couple of sections within it. That I'll be adding to over the next couple of weeks I get closer to the leave where I'll be assigning you know certain tasks or things that I won't be able to cover to different people. But no action items as right now just making you all aware. I'm also going to look over Emily's comments on them solution validation study and I'll respond to that comment. I'll leave today Emily. And then I watch all these video. I think it was with for in and Fabian and a couple others. I thought it was really informative so thanks for recording that and sharing that. And just wanted to let me know if if you need help on any of those switchboard related issues and kind of being pulled out of different directions so just make sure to mention beyond anything. Any comments or questions for a hinted over the air. I'm going to take some time off next week. I'm still taking some time off but I need to be around to send out our CI components assignment three which is also our friends and family day but I'll be in on Monday doing that. I'm going to take some time off because we're finally able to send that out so people in our Alpha program will be using some components and some inputs to create an assignment. So we are kind of supporting that in a way by we're giving them like a pre selected list to select from. So we'll want to remember that if we see really good success race. But anyway, it'll be good to get feedback on that. And then hopefully I think Jackie and I today will meet to finalize the next steps at least for Q. To for the benchmark like our 10 issues to champion for Q to. And then I've been working on a cross functional analysis of those susper betams it was originally an okay R for. I was thinking it would be okay R for Q to but I really like analyzing data and so I finished it. So now we're kind of socializing that around to look at. The next steps for that. So we'll see how that goes and then I think we'll wait to prioritize the research for Q to for Q to until until we think it comes back. And has a chance to like time and with them projects. Thank you everybody who's chimed in for Q to research so far. Well, I think that's all on the agenda because anyone else have anything else they need to discuss. Like a we can have 10 minutes back. Awesome. Well, thanks everyone and we'll see you later. Thanks. See you all. Bye.",
  "Hello, this is March 29th, almost April for our CICD UX meeting. And we don't have any standing topics to review right now. Is anything blocked or at risk for anyone? No. Okay. All right. Then I'll just leave it to Emily. So to give some background on this, I think a bunch of you know, I am now working with the the configure team and release we've grown together into the environments team. And with part of my onboarding was understanding what Kubernetes was, which is, or I've heard quite a bit, but during my onboarding I was reading about it. And nothing was sticking until one person told me about Kubernetes as a post office. And I just wanted to go through that because I chatted with Gina about this. And we were both like, oh, everything makes so much more sense now. So I just put together a very, very tiny little presentation that kind of goes over what I learned during this onboarding. So going back to it, I did a lot of reading about Kubernetes. A lot of it was in a very technical jargon and nothing was sticking until I came across the post office metaphor where it kind of subbed in Kubernetes as like a post office management. And everything here then started to make sense with me. So this is just like a cute little story about that. So a post office as we all know is responsible for processing and delivering thousands of packages each day. And the goal being that the packages are delivered to the intended recipients quickly, securely and reliably. Kubernetes kind of does something very similar to that. To help achieve this, there's a team of postal workers responsible for processing and delivering packages. However, with packages, there's many challenges such as sometimes there's an influx of packages coming in. There's incorrect addresses that need to be fixed. And this is where the orchestration portion comes into play. So when you think about Kubernetes in the same frame as a post office, you can think about containers, which is a package of code as like the mail packages. And when you get a group of containers together into pods, you can think of this as just a group of mail packages that need to be delivered to the same place. So you have now that group of mail packages needing to get somewhere. And then there was no emoji for postal worker. So just pretend they is a post worker. Kubernetes makes each group of packages. Known as the pod is assigned to a postal worker, known as a node. And that all these groups of packages are evenly distributed across workers. So we're not overwhelming certain workers or certain workers aren't getting any work. It also makes sure workers have the resources necessary to handle each pod, such as enough room in their delivery trap, which is the CPU or memory. And then Kubernetes also makes sure that each worker is not overwhelmed by packages by doing the load balancing or adjusts the number of workers based on how many packages are coming in, which is the automatic scaling. And if a worker calls in sick or a node becomes unavailable, Kubernetes can help get the new worker on the job so the delivery goes uninterrupted. So yeah, you can think of Kubernetes as an operating system that manages the processing and delivery of packages in a post office by ensuring they're all assigned to a worker and that the resources are allocated efficiently so workers can do their job. So it's just a very, very tiny story I put together, but when doing my onboarding, this is the one thing that helped me really, really understand what Kubernetes was. And I thought it was cool to share out that with everyone else. That was so great. I love that. And it makes it like so much more obvious, I think, as to what Kubernetes means. And I have one question for you. Yes. Do we, the information that you're saying like CPU and memory that is there and if a node goes down and stuff, do we get that visibility into GitHub or do you only get that when you're using whatever Kubernetes platform you're using? Very good. I'm late on Ali here because I'm still learning some of the stuff that we're actually showing in GitHub. But is that kind of the work that we're doing around the Kubernetes dashboard currently to get more of that information showing into the UI? Yeah. Yeah. That's what we're trying to do with the dashboard. OK. Sweet. Where I've had to deal with Kubernetes a lot lately with runners as well because runners are sometimes in the way that you're saying that there's a coordinator. It like the runner acts as that. And he's like the head and then tells whatever machines to go work within the cluster. But a lot of people have brought up the fact that CPU or memory could be an issue. And that's why the runners aren't working as expected. So I think we're going to start exploring that probably within the next couple of years. And there's going to be a lot more connection between the runners and Kubernetes in general. Yeah. I just wanted to point out, Emily, I love the presentation. Super simple with the emoji family love it. And then what I feel like on this feeling, I made it this like three years ago when I started to work on the distribution team. I, with a lot of peers, we have a lot of time watching the YouTube developers talking about Kubernetes. It wasn't easy. Ali is left because they knows that Ali was reading a book about Kubernetes. So this is what I need to thank you for that. And I also really like the point that Gina just mentioned that this concept could be work like load balancing and then it's kind of made things more scalable. And then I think it could be also applied to runner and many other places. And I think that's the reason why Kubernetes getting more popular. So I'm the person who's in thank you. Yeah, so things seem super simple. I wanted to put together, but when I was talking with Gina during our one-on-one, I was like, oh, this makes so much sense. I think it'll be helpful to share it to everyone. Eric, I think you're next. And yes, thank you, Emily. I wanted to share like the origin story, which I don't know if you know this, but this really helped me in as much as I understand Kubernetes. So I don't think anyone ever will. So there's that. But so it started, this is like an air conversion. So fact will, don't, just we're going with this story or an story. Okay, so Pokemon Go was this big app, right, or everyone was trying to catch them all. And it was this huge craze. And it was a Google startup that was in charge of it and spinning it up. So what happened was they didn't anticipate that so many people would be using it so quickly. And within the first three days, everything crashed because they couldn't serve those users. And then they were like, well, how could we scale things more quickly without having a big overhead cost? And then they bore Kubernetes, which is this idea that you don't have to have all that compute power all over the whole country. But when you need it, you scale it up immediately in this localized way. And I didn't really understand it. And then I had that like, I remember when that went down. But that was like the computing problem that then kind of led to the creation of Kubernetes. And then just one more note that I've heard it also, I think the male person perspective is perfect. Because it's like more relatable to like I can think of a male person like a male, male or male person person. But also like this idea of shipping containers is another metaphor. You'll see a lot. But yeah, I don't relate to a shipping container person as much as a male person. Okay, go. Anyway. Yeah, Erica is the person of analogies. I remember when you told me about the Homer and the scooter stuff about secret management. Wow, it just made everything seem so simple. It was a great presentation. Thanks, Emily. And it actually like, it kind of explained in such a simple way the whole like the environment. When I talk about everything related to Kubernetes like load balancerers, workers. I mean, what's the individual role? I have been through some pretty complex comic books in the past. But they did not do this job. So yeah. I took the inspiration from one of the courses. I did actually it might have been the documentary. You will just mentioned, but someone said this and then that's how that was really my my light bulb moment. So all the things has to go to, I believe it actually was the documentary. I'd link their own thing in my slides, but the documentary will kind of link to your has was great in helping me understand Kubernetes. Yeah, in the documentary like briefly touched on what you covered in your slides. Like maybe the first two or three slides I didn't go like very deep with that metaphor. So it was nice to have a better understanding of it. And then it also I think it's like a two part documentary because the whole thing's posted to YouTube. I think there was a section of it that also touched on what your convention with Pokemon Go. This kind of that example. I do remember Pokemon Go crashing on me that side. I remember being part of that. Thanks for sharing. I came across the story when I was the one getting in Konfigur. But yeah, I think in retrospect though, yeah, the community is definitely going to stick now. So should we move on to the next items on the agenda? Yeah, I'm telling you. I'm the one starting. So I'm working on the minimum onboarding flow for switchboard. I recorded a video of a workflow, the initial concept that I'm working on. And yeah, it's been a great way together. Information also during my conversations. There is some information that customers have to input during the setup process that they can never change again. Or if they lose, they can't access. Get lab. Oh, we can't help them. By the way, is there a way that we communicate this type of info? And then yeah, I'd appreciate any insights. Or if you can point me to something. And then I'm also using that to have conversations with engineers to help me with the scorecard that I'm working on to understand some of the stuff that they do. Um, Ali, I have one question. So how would you um, how would you work on the scorecard? Because I don't think there's existing workflow around switchboard like how would you handle that and how would you approach to work on the scorecard? You're just curious. So today it's something very minimal. There's just the switchboard platform and then um, uh, as I raise kind of pace chasing file in the and kick out and kick off a bunch of jobs. And following. Yeah. So it's around that. But then also trying to understand some of the stuff they do outside switchboard. Um, and because we also building for them would like them to do also the stuff they do outside switchboard inside switchboard. So it's kind of a hybrid of mapping. Let's scorecard. Because the idea is it's like a bit, yeah, it's going to inform the next phase. Oh, god, I think. I have just a question about the, your, when you ask if we have an approach for telling the user about information they can never change again. Is this some, uh, are they selecting like, or what, they're configuring it and they can never reconfigure it after? Yeah. So they're bringing encryption keys to encrypt, um, their keys left, uh, instance, um, and so we're giving them the option of bringing their own keys. Um, and um, I think once, uh, they've put them, we can't see them. And if they lose them, we can't help them either. Oh, I see. Um, this sort of reminds me of, well, actually, no, it doesn't. I was going to say we deal with tokens that are only displayed for a certain amount of time in the UI for runner. And we just tell them like, this is going to be displayed only for a short period of time. So copy, we give them the chance to copy it so that they don't lose it. Mm. But then if they lose it, like, saw them. So that's how we tell them right now. I don't know if that's helpful. Yeah. Um, I don't know if it's, uh, something I think, uh, I'll lean a lot on the writers to kind of help me communicate that. I guess, since these and kind of an approach, it's all linked to my, too. Sure. Awesome. Okay. I think that that that's it from me. All right. Um, mine, the first one, which has read only. And then I'm starting to validate a runner fleet dashboard. You might have seen. I shared it in our York School working channel. And um, I'll just share the issue that I'm doing this through because we have a very hard time finding enterprise customers to meet with. And there's this, um, process in the handbook that you can lean on customer success managers for that. So yeah, if you want to take a look at that issue of that applies to you in the future, it might be helpful. Um, Gina, have you, um, have you reached out to any like technical account managers? I think sometimes they're very like reactive to our request. Yes. Okay. So there now they're now called customer success managers. Oh, okay. Thank you. You know, that's a great. No, that's yeah, that's who I was saying to lean on. Okay. Um, because yeah, they've helped me in the past, just like meet with customers just to hear what their pain points are. Yeah. So we have a list and I'm going to list that Darren and I are keeping. Yeah, thanks for clarifying that. Yeah. That's it for me. Emily, you want to go? Well, um, so I just had one open question about a larger designs that are put in through a lot of small MRs that are kept behind a feature flag. I was getting this question from some of the engineers as if they get a UX review for each small MR or a larger view at the end. Mostly being they were doing small reviews, personal MRs, but we are getting people wondering why the experience wasn't complete. Like you couldn't go through everything because it was just one piece of the MR and then the rest seemed broken. So I was just wanting to get other people's opinions on this. Yeah, this was so relevant to the token like re architecture that runner was doing and our devs did UX reviews for each small MR and then sometimes our front end of will include a table in the description being like, this is where you are and these are the other MRs that are adding these other features or they'll just add like some description text to tell people when they're bringing up the changes and then I linked an example there. Sometimes they'll also just ask me to review the MR before they ask like the random UX assigned reviewer. It's been like a mix. Yeah. Well, thanks. Yeah, I think the problem was we're not like clear enough where it is in the experience because the issue it links to links to the entire future, not just the little portion. So I guess just being a lot more clear about what the MR is changing and what is out of scope for that particular MR. Yes, exactly. We did the exact same thing and linked to the entire feature that MR was not creating. So I think just like that text that that they had added was helpful. It took a did you want to? Yeah, it was very simple to what Gina said. So this I experienced when we started to work on the CI job token. I'm trying to find the MR but like that proposal, it had to be broken down to like a few parts like some preparation by the front end, some preparation by the back end and then eventually they would like club all of that to form the final to work on the final development and implementation. So for that what we the process we followed was engineers used to invite me first to do a UX review and to make things easier for them. Like once I reviewed, I left a very detailed sort of summary for the next designer who will be assigned by the reviewer to look at. So I explained to them that this is the bigger proposal. This MR only takes care of like this small part of that solution and eventually this will lead to something else. So yeah, that's how we did it. No thanks for sharing that'll help us a lot with. There's two big initiatives going on on my team right now and I think we'll have to do this for both of them. So okay. Yeah, my final thing is I'm finalizing the group level environment solution validation study the week. So hopefully this will make some progress after being paused during kind of like the team switch time. I'll pass it to litica. Thanks. I just have this small update like besides all the stage group work that I anyway keep sharing and go working on other channels that in preparation for the UX team workshop that I thought I would be conducting next month. I figure that we don't have a vision documented for a pipeline security yet. So like the very first step for me to prepare for the workshop was to meet with Jocelyn and write down our vision and do that keeping in mind the information that we have at hand like all the insights from Erica's researchers, the existing epics that we have, the highly-witted issues that we have, the market inside the jobs to be done. So I created sort of this homegrown template to have that conversation that discussions are that eventually like we make sure that whatever vision we arrive at it is done like after being informed of everything that else that we have in front of us. So the first mural is where we started from. I just made a copy when I created that so that was the very first state. I brought together all the information that we had on different areas and create two sections. One was for each category. The plan was that we will create clusters like we will be bringing together a item that could form the one team and we ended up with four groups of information or of like the work that you want to do and then we just would it on them. One of the options that we had was to maybe take it forward and instead do a like come up with the rice score but we refrain from doing that since we are going to be doing the UX thing workshop anyway like in future. So this we just kept this very simple and once we identified the larger themes the next step we took was put them like use this business model innovation framework to bring together like who are we making who are we creating the solution for. How we are planning to do that and to achieve that how what is it that we would be working on in terms of features in terms of changes then why do we think like why are we confident that these set of features are going to like solve the problem and the good part with this innovation framework is like you change if you make slightest of change in one part of this diagram you will have to like make adjustments all throughout. So it's always balanced. It's never balanced and it worked out pretty well for us we came up with our new vision which is re-enable organizations to adopt good practices for secure handling of sensitive information and I just thought like since it worked so well maybe I'll just document this and share with the team. I've been done that yet. Yeah that's all. So for the pipeline offering we are focusing on the solution validation for the placement they were replaced this catalog feature is it the left side navigation or is it more embedded version on the pipeline editor and there were another option in the navigation but inside the export tab which is the same level with the organizations and new tab but it's slightly behind that so it's the second step. So we just tried to work on another round of research so things are cut to jumping in so we just started this conversation how can we proceed with the new test and then once we got the result probably we can more confidently say okay so we should play the feature to where so I'm also working on that I will work on that also the rest of the week and there's another very critical discussions which is mostly about like how we architecture how we create architecture for the back end which will eventually impact the York's work so the here is the MR like I just don't want diving to the two details but it's more like if we design API this way then there's two there's some impact to the user flow and then if we design API in the B-way then it also changed the user behavior so I'm I'm really glad that I'm participating in this discussion and just trying to follow up what they're saying and then if we make this decision then we our MVC scope might be slightly changed in terms of technical perspective but we not a lot from the UX perspective so this is still ongoing just wanted to share with you all and yeah that was my agenda that you have any questions feedback you know then I'll pass it over to Erica sorry do you want to verbalize your call yeah so I think that this has been like really good work and we're trying to figure out like the way to approach this solution validation but what we're going to end up doing is having an approach that other teams can use and basically like what we've arrived at is having two groups one that has the side nav in a prototype one that doesn't have the side nav in a prototype same tasks different entry points performance comparison maybe satisfaction question in there and I think that's what we're going to need to make an argument for the side nav so just wanted to like say that this was a hard problem and to give props to say good job being patient and getting through it and then I think like we're going to have a model so that next time it will require so much thinking as much thinking to get to how we approach it yeah thank for sharing that Erica because it wasn't easy but the good thing is that it's a full-the-first front of validation we could eliminate like a list one option and now we have two so that's good and then I think since it's a new process like there are some challenges it's just not our team there's some other team also going for a similar process at the moment and then I think once we design nicely and then come up with the results I think probably we can also add it to our handboot and then provide better guidance on how to test these things around the placement yeah I've been thinking about this Erica and like wouldn't it always be the case that whenever we present to users an navigation option that's placed at a higher level it will always end up getting a better score yeah and exactly I've been calling it the bagel problem or like we could ask them to search for bagels in the product and if we had bagels on the side map they would always just go to the side map so yeah so that's why I think having the two groups there where one they just don't even have that option so it's not like bagels on the side map then we can see if they can still perform or not and that's why it's with it that's why it's two different groups yeah I'm just you go to see like if you figure out that let's say without bagels being on the front shelf if there's still happened to find it how do you still just to buy to the team that you know they were still able to find it yeah and then you think they found it much easier on the higher level so that's why I think it's like if performance is the same with those two groups we don't have evidence to add it to this okay it's only that if they are failing without that side map option then we can say hey it's important okay and we don't we don't know with the other approach I was like it's bagels like they'll just go to the where it says bagels but so with this approach we don't feel like we have a sense of which one will work and then that's when we should be doing research yeah thanks good question yeah I just had a question about how has the rest of your team been responding to performing this research and if there's been any learnings that you could share with us if we have to run into the same situation so first of all I think they're excited that we're validating this because the input is coming from the user then like we could really be more confident to say like we're placing this menu at this place because of the this outcome so I think they like that from that perspective but on the other hand like I think they are still having this own and more behind the future flag and I'm not so sure they I don't think they haven't they have merged it so of course there are also some frustration because they are ready to make this change happen but at this point they need to just wait for the result as that was the reason why the timeline and timeframe was a little bit tight because it's working progress so there are kind of like mixed feeling but in the end like the industry like of course like if you have more strong like I bet it is that's good and then I just try to like illustrate okay what's happening in this version that they know what's going on for this research so that they could be more patient nice yeah thanks for sharing you're ready um so we've few don't have any other comments then I'll pass it over to Will thanks Ajahn so just a couple quick updates um I was on a customer call Emily led a couple hours ago to learn more about users and pressions of the group level environments you concept that's going to feed into them social validation study and my thought they did a good job handling you know multiple users or customers on the call and like trying to pivot to get to the questions that you really wanted to know given some of the time constraints that we had on the call so good job there I'm also working with Ali and Pedro, the brainstorm just be done for switchboard so we've started that in an issue and now we're moving over to Google Doc to put that in there I've also drafted an issue based on an initial team call that we had with switchboard that that group as I was writing it up after the meeting I was a little bit confused about the request so I'm going to need some context on how this you know fits into the foundation research plan that Hayada has drafted so I've Ali had I've all used to attack you and I had offer for some additional feedback Erica see your writing did you want to elaborate well maybe see if the team wants to respond first anyway so I think we just need to see how this fits into what we're doing now and make a call because yeah we're trying to find out what the customer wants but then also I mean how some of our users are also internal folks so I kind of feel like we're trying to do the same thing and then maybe my not been necessary to have two issues to do that but that's just my feeling maybe we can get some feedback from Hayada as well yeah and that's why I want to clarify that before like jumping in because if this is like duplicate or redundant like let's not do that in that case um so my first note is just that there I wanted you to know that there's this foundational research happening like in the back burner but we've run some participants on the life cycle of an image and understanding how those are environments and how that relates and kind of whether or not people want one pipeline for them or how they kind of structure that so that's happening and I can I'll just catching up on life but I will put I will link the dub tail and it might be helpful for you to watch some of those videos that's my first point but then looking at the foundational research I think we have a bunch of this just from the the secrets and security related research and I think I'll look I'll take a note to look through all of that but one thing I can just quickly make note of is with the the personas we know that there's this like shift left with security so even developers are that was like a big thing that we keep finding is even developers are getting their hands on security and compliance stuff and we did in the secrets features prioritization survey we asked uping up the finding but we asked them if they knew what compliance requirements they were working under and surprisingly they had a sense of it even the developers building me pull up that work but I can dig in here and help you piece together what we will we know because it might feel like this is coming from nowhere but it's actually I didn't really even this is a nice summary I mean it will to be like oh we know some stuff about this so good job in framing the issue there everyone but I can be helpful so let's figure out the best way for me to help you guys okay okay awesome well and then the last point I'm working with Erica they may speak to it a little bit below but um helping them pull some like customer emails later in the week from our past us analysis so that I think they and them team can do some like future like follow-up calls tell me guys that for me yeah since we have time I can actually just share my screen and show you the benchmark report okay so this is a one side slide summary of the performance and I set up a benchmark finding epic that has like videos of overall like so this is all detailed and in a way like we issue a ties this report um but basically they're we're in good we're doing good but with each of our workflows that we tested that to me mouse there's like one sort of really painful task in each of the workflows that we need to address so it's using includes in tax identifying the failure the reason for failure and understanding the test unit tests and then the first of the find and six pipeline error workflows and I so this table here kind of gives you a sense for each of those workflows this column gives you two to three two to three word summary of each of them so for the author of pipeline workflow we found a barrier to entry with the includes in tax which is important because that's the entry point for our CI components catalog um and as a companion analysis I looked at all the sus verbatim for verify for the last six quarters and did an alignment exercise to see how much the pain points for each of the tasks and then each of those UX themes in that table below mapped to the sus verbatim and and it was kind of astounding so 13% which we're calling a significant amount of those negative sus verbatims were related to this author of pipeline workflow and then looking down here at these UX themes one of the themes across all the tasks was just general confusion about yamol and that was 8% of the negative sus verbatims so we really want to focus here on making that whole yamol related workflow easier and then it occurred to me that we now actually have all of these people who gave us these responses about the yamol experience so we can actually that's great because they're hard to recruit right it's hard for us to find good matches so here we have this lovely sample of 43 participants who gave us feedback on the yamol experience there's like all these different categories where they made points but basically it bubbles up to having a hard time with the yamol experience so we'll do thank you will it's going to give us the yamol context for those and what we can do because we can't really have although people would probably try we can't really schedule 43 call but what we can do is run them through that CI alpha components program and those assignments and then we'll get a nice read on the satisfaction scores and how those are increasing right now we have like a dedicated really well rounded sample where we can do deep dive interviews but actually they're pretty satisfied with the current template experience when we ask them so it means that it's harder for us to raise that bar in terms of their feedback and getting them into a really satisfied place so these folks who gave us the sus feedback and were negative we should get a good read on if we can move them up to positive then we're really doing a good job so that was what we was talking about and that's the author of pipeline workflow and then understand and adjust unit tests two words for that is their hidden treasure so it was amazing I just so one of the things we did at the end was we gave them this job to be done rating question where we had them agree or disagree on a scale of 1 to 7 where one anyway so we had them look at their agreement and they gave us really high scores here and they were just they were delighted they were delighted once they understood they didn't understand what was quite happening with the unit test at first so it wasn't discoverable for them because they went into the logs and they weren't looking at the UI and they did this like circular thing where they were like oh I've arrived at this artifact don't know what that means and then they would like arrive at the artifact in a different way and they're different way and and then like when they then we kind of like pointed out they the artifact is going to be helpful for this task and then they were like what is this artifact how does it relate to this error but once we did the reveal they were like thanking us at the end of the session like they were like and so this rate in here this 6.1 is very high because they were just so pleased about it so I think that there are some small I mean I'm not a designer but if we can make it more discoverable and a little bit more obvious to them how those things are related like an example is so in the in the unit test it showed them a screenshot of what the website would look like if it displayed as per the code and they weren't sure if it was what the website should look like or does it look like so we just need to like connect those dots for them and then they will make them so happy and here we saw only 3% alignment with the sus verbatim but that actually like tracks with this hidden treasure idea right where they're not quite under that's not discoverable for them and then for the fine and fixed pipeline errors 9% they're waiting too long another way to tell the story there though is that they learned which is really cool so if you look at this so we had fine and fixed pipeline errors in build and then one in test and one in deploy and so you can see here that they kind of bombed that first one but then on the second on the next one they were in green again and basically what it was is if they could step back and look at the patterns in the yaml file they could use that to quickly come up with the solution as opposed to like digging into the logs so if we can figure out how to help them take that step back maybe it's AI I don't know but to like look at the discrepancies in the different stages of the yaml file we could really help them succeed there but here you can see that this job should be done rating is not as high and that's because they dinged us because they get really frustrated waiting for the entire pipeline to run after every fix so that's a theme that came out here and it's actually that specific theme is small in terms of the percentage of self-provedem I think that's because we logically have a smaller percentage of enterprise respondents and that's a particular enterprise problem that we were getting feedback on so another thing that will and I yeah we'll know like our working on is to track business size in the self-responsive now and so I think that will be helpful but yeah so this frustration with running the entire pipeline for every fix I've heard that before but it was really resounding and we see them bringing down our satisfaction scores or agreement with the job to be done ability and that's because the user behavior in terms of fixing a pipeline is just to throw a fix at it and see if that work and throw a fix at it and see if that work they're not like now with this work let me check on the documentation like they're not doing that kind of thinking and so then it becomes really frustrating they're like I just wanted to try that thing and now I have to sit here and wait for however many minutes and I asked so I did my like due diligence follow up like tell me about your tell me about your work flow and and like is this really a big thing and they're like yes yes yes this gets me out of my flow gets me out of context I'll go watch a YouTube video and then I'm totally distracted and not useful um yes so that's the benchmark stuff in a quick summary and then I let's go to comments and I'll stop sharing I had a question about how you determined if the suspect back percentage was significant considerable or small yep so there's documentation also that's already out there I can just look at that no it's totally no I love that question so um we I put that in the report and it's just mapping percentages okay so it's and it's just because later on if it just has 3% or not like I put the percentages in that report right like 90% later on and like Jackie's question was like is that good? so yeah so we wanted to be able to characterize it in those ways and so just being transparent about how we're doing that but I think it's legitimate because if we look at like kind of the other findings 33% of them were not categorizable right so like something about inconsistent behavior and CITD pipelines like I don't know how you map that to much but so and you might think like that's a lot is truly be concerned but actually like it's pretty to me and legitimize it because it means that there's a bucket of things you can't force fit into these benchmark paint points and themes and so like having the assert of it right be around number is good um yeah to answer your point we just came up with these kind of categories um that's just opening it playing it well thanks yeah but they're mutually exclusive so it has to fit into one of the categories and only one of the categories and to do that I ended up having to parse out a lot of the verbatims because they'll talk about all these things and so to get the one to one's boring and I think it was 317 verbatims that we had. Wow okay. And Erica when you pull those sesame for betams that I'm gonna you know find emails for later did you only focus on people who indicated they were open to follow up conversation? Yes sir. Okay, cool yeah just try to make sure I wasn't at first and I was like oh what's this oh that's important but I thought I did I got it. Yeah and then I just pulled out just to try to make it easier so it's not lost in the issue verse. Here are these four pipeline authoring design related tasks that we have at a critical severity level so those are the ones that are critical so like a way of like prioritizing all of them those would be the most important and then yeah the one that I pulled out I can lead is bringing those people into the yeah I have to program. Erica I just wanted to say really that this was really great work maybe because it related to things that I was working on so I was like more interested in it but yeah this was this was awesome and I love that we're seeing feedback across like the whole pipeline experience it's not just one section. Yeah actually when it touches upon the places that we have always like talked about there's never been like enough evidence that we should really focus on that work and improve those experiences so I'm very hopeful that the results of the benchmarking would like help us prioritize things which are really going to make big changes. We're good. Yeah that's us analysis was pretty amazing but I didn't expect to see that much overlap and and some people kind of just did like a one sentence like that we found this also in the sess but I'm just extra so I was like I need to know the precise precise percentage of self-provenants that are mapping to each of these things but it away is good because then we oh yeah because then we can make this statement that 45% of the negative sus verbatim overlap with those like as an aggregate is 45% of them which is more than the 33% of the edits so I think it's good stuff. Thanks thanks guys. Yeah and then later this week I'll work on the Q2 research prioritization and so we'll start pinging in that to start getting threads of what we should be prioritizing and then the last but not least is I isn't linking yeah I put I put it a deck together after kind of now having this idea that we are focusing on environments. I was like oh I can think of a key problem that I've heard so like across these four studies we keep hearing this problem related to coordinating variables across environments so I just pulled out slides from each of those reports because I would say that that's a big problem. We're not even asking about that it just keeps coming up. So I linked that deck. I'm glad to be back I missed everyone I was just only working on that one study. Thank you for sharing all that too. Very excited. All right um does anybody have anything else? No, all right we'll have a good rest of your weeks. Talk to you later. All right buddy.",
  "So, thanks everyone for joining. I wanted to get in touch with all of you all because I am working on a project with the permissions. Some of you may have already seen the work that I've been doing, a couple of you all have already provided feedback. So that's great. Let's start with this real quick. So just a brief overview in case you're not familiar. I've been working with Amelia and Andy on re-doing the entire permissions architecture in the platform based off of the idea that we're going to make customizable permissions. And so Amelia had gone through and the first round of organizing the permissions based off of what they have here in our documentation. On this table here. So, you know, analytics would be grouped as one and then we would try and find out where does the clusters be gone to, what is container industry, et cetera. So, that was the first pass at it. After we had started looking at some of this, we started seeing a problem where as an example, where if I were to look at a individual permission called view project code. And I was to disable this permission. If we were not looking at what it links to which just look at this table was like, well, individually doesn't do anything, but logically in the platform, it would turn off all of these other permissions without noticing. So I've done is gone through and mapped all of the permissions to try and see if I can find these sorts of mappings. And tied to this issue specifically. The devs are looking at consolidating this permission to view pipeline and view pipeline details, so that instead of it being two permissions is just one. Kind of like consolidating line item here in this table or the same thing here. The idea is that if we're going to move forward with having a prototype where an admin can go and toggle individual features. If we try minimizes complexity, that can help on the backend with the problem that they describe here that. They would have. There wouldn't be a need for overlapping overdund and permission so in the case of here they have the same access level. We wouldn't be less confusion here and then. It would make things easier to undo or to make changes within the the code based without being so catastrophic to where's like if you allow something. But they shouldn't have access to they wouldn't necessarily cause any sort of negative impact in the in that sort of. Permission if you were to enable that permission right so the reason I need y'all to to sync with me today is to kind of go over this. And I think it's more or less okay, but I just wanted to kind of go through as part of this work that I'm doing here where I go through the entire. Permissions architecture and meet with every team that kind of has insider impact with this to make sure that I'm thinking of everything in this model and then also see if there's opportunities to merge or group things that don't necessarily. Need to exist individually. And so with that I will open the discussion for anyone who wants to kind of jump in and provide feedback. The first question from me Daniel is like. What is the like basis of consolidating I you thinking of in terms of. Page access or is it dissociated the primary permission is dissociated from the page access and. Something and entity of its own. So the idea would be specifically in this case where view pipeline page and view pipeline details page the question presents itself why are these separate. What is the use case where the job to be done to have them in dependent permissions and so if there isn't a real realistic or definitive purpose for them being separate. And then can we merge those the benefit is that it reduces the code. The amount of code that we have and reduces things that are it's touching. So kind of improves security and that instead of having to watch or monitor to code code points you actually just looking at one. And then that sort of logic would apply to the whole project that I'm working on right. So minimizing the code base or reducing it in terms of complexity as well as what sort of improvement that will present on the front end where. An admin doesn't sort of toggle two of these features and you can just toggle one instead of and receiving stuff to and then make it simpler for them reducing the UI. Got it. So one thing that I just noticed in the link that I just shared is just a realistic permissions which are specific to pipelines on the docs. And while looking at this table if you see the we have separated out view pipeline details feet you pipeline page and view pipeline tab in MR and when I kind of cross checked dot with. The documentation that I have in my personal excel sheet the only difference that I see between. The pipeline list few versus the pipeline tab in Magic West is that guests cannot access the pipelines in a mode request. So that's something that I think we should also cross check with the team if there is a possibility to consolidate that with these two permissions that you just talked about. And there's a very compelling case of not allowing guests to visit that particular tab in a logic West but I'll see in time be able to look at the pipeline list. Well, so one thing I guess I should probably clarify is that this change doesn't necessarily have any reflection on guest reporter developer maintain them owner. The idea being as that as an admin I can then customize all of these two offer those individual permissions so if a guest only had access to one particular permission. I could then enable that they would no longer be a guest so any sort of restriction that you're seeing. By a non member or a guest not having access to that doesn't necessarily play into this particular problem I don't think. Apart from what do they have currently and can I enable or disable that. The idea being that an admin would then give custom feature or custom access to something. The impetus for the project is our permissions are either too permissive or too restrictive. So somebody can come in here and say I want a guest role, but I'm going to also give them permission to run a pipeline and then disable all the other things that could be possible. But we're looking more so is that. Any sort of problems that might arise by making those changes by if I do allow that does any sort of security concern arise or. Again going back to the idea of merging stuff if I merge a particular permission does that have impact elsewhere. Right in that case I think it definitely makes a lot of sense to kind of treat the pipeline tab in merge request the same way as which we treat list view. The only thing that I don't think I have the I've complete knowledge to comment on is. How that plays with the access to merge request page. Because eventually like the same pipelines are also going to appear on the pipeline list view so if somebody is able to view that then should also be it would if you this is just presenting the pipeline list you in a context in the context of a particular merge request. Yeah, I guess that's also something where I was looking for your knowledge is that so as it's grouped so far we're just looking at this CICD screen, but. It doesn't necessarily just stop there right so it touches merger quests and you know parts of the repo right so are there connections there that I'm not linking correctly because there and these little boxes right should this be a more. Connected prototype or not kind of prototype but connected mapping here. Right. So I mean I'm just thinking of this habit critical scenario that. Is there a possibility that I might as a user be given access to view pipelines but not take part in merge requests. That's that's possible that's the the whole idea of that if an admin wanted to create that custom role or custom policy then we would offer that ability. And how would that play out if I still have access to like if I if I can still view all the pipelines just that not through that particular location. Right and that's exactly where I'm like thinking okay what sort of problems as I'd create like like the one I used here of view project code is like well I turn that off but then everything else below that should turn off as well. Yeah, if you don't have access to merger question, I. And have. No, but you should you should have view pipelines to have an MRs right that would just automatically be shut off because. Okay, so that's kind of what I'm wondering is that if you would be able to see a merger quests or if you don't have merger quests you wouldn't be able to see pipelines. No, I don't think so. I think the whole logic if if I'm not allowed to look at more requests, the enough course I cannot access that page and the tab is. And even not going to be available to me. But I don't think that impacts pipelines page by itself right that would be a separate permission. Yeah, it's separate that's what. So I don't understand if you if you can't access pipelines though can you access jobs because I was wondering what the difference between the pipelines list page and the details pages and I'm wondering if it's because the details shows the job so then a pipeline and sometimes. Specific users can't see the jobs because they contain whatever sensitive data. Yeah, so it's a question that we should ask back in in general that if there's a let's say if there's. The asked a asked like compliance related jobs that are running as a part of pipeline are they inheriting the same access as the parent pipeline or can that be controlled. So Maya assumption was like you'd look here. At the pipelines and then look at the pipeline detail. Or is that now this is the comic page commit. Of course. That's the thing is that where is the detail shown. Would that be in the MR. This is the detail here. Okay, so this would. Yeah, and the list you can see like there's a list of jobs down there too like and it's own tab that. And then you could like view jobs but technically I guess you could do that from the pipeline many the little mini graphs as well. Here. Here. So the information for. Yeah. Like the list view pipeline. Yeah, I'm here is the. Yeah. So it's kind of tied if you're able to view pipelines and view the pipeline details and just view pipelines you can also view the jobs and job details. Okay. So this sort of all these screens then need to be looked at as a linked. Linking from all of this right it's not just independent. Time back to here. Okay. And that's kind of what I'm also curious is if you were to disable that what does it look like. From the UI is basically just disappear as these navigation items against. That's how we are handling thing right now. I mean we've just basic permissions right if you just don't have access. Nothing will appear right this tab would go away. This theoretically would go away. I don't think that goes away that ever goes away. So anyone who has access to the more requests has access to the pipeline widget. There is one restriction that they cannot run a pipeline if the pipeline feels but I think recently we made a change that if a pipeline feels. Developer should have access to the run pipeline button which is inside the pipeline staff. I mean. Let's see where it's about. Yeah. So there's run pipeline button. Oh, that's one. Yeah. Oh, that's our leap placed. Okay. That was not available. I think to not maintain it as if I'm not running a check. And so if a pipeline feels for a magic question here, the developer working with that you probably could not run the pipeline manually. Okay, well, I mean so right now I am not a maintainer on the get lab projects. I don't think so. And you can see the button. Yeah, I could probably push it, but I don't want to. Okay, yeah, I don't know. If that you think that should not be that way. I think I'll find the issue. Anyway, what I'm thinking is like, let's say if I don't have access to this merge request. Would these pipelines not be visible to me on the pipeline list view as well? Because they are there as well. Hmm. That doesn't happen today. Right. So that's where I'm curious. Is that what amount of data would be visual? So like you wouldn't see the MR. But for some reason you could see. You could see the pipelines. If for some reason they were turned on, but you just couldn't see this content. And maybe the changes. Yeah. And the commits. What what, how would that information help? Like what what would someone use that just the commits pipelines and changes for an MR for? I'm just wondering if it even makes sense to just show those without the actual MR description and everything. So I'm think let me jump in real quick because I want to make sure I'm understanding. I think this might just be a bad example. And I think there would be an admin would disable that or enable that to allow that sort of flexibility or to to prevent seeing something here. I would look at it more holistically across the platform as like saying, is there something that would negatively impact it. The example we're using I think is is a fair example. But it might be like an outlier case, right? I think the more relevant example might be. I don't want an external contributor to see code, but they can look at the the merger question. Whereas on merger question. It's a thing of a matter of security perspective, whereas like, okay, I want a contributor, but I don't want them looking at something. Potentially problematic, right? And so disabling pipeline might not be an example that would be valid. But it is the one that they're working on right, where was that issue. I lost the issue. Oh, here it is. Where do they want to emerge these two? Yeah, okay. I think for me that the problem is I'm more afraid of what happens when they go to something that is actually. More likely to be played with and disabled. I think that might be why they're going after this one in particular, because it probably nobody's an MS with that, right? Like there's no reason to disable this. Right. But this would be these two are available for every level, right? Guess, does access to this. Yes. Okay. Yeah, so I think that might have been why they chose is because it's super low impact. Right. But jobs like permission for jobs are definitely tied to these. So if somebody has access to view pipeline, they can definitely look at the job details and job list. Mm-hmm. Okay. So I think what I want to do next is talk of my engineers and ask them. What do they see? Impact wise from that outline example, if somebody did for some reason to disable that permission. What does that look like in this case? Because I'm not exactly certain how that works because as you described, it won't just be contained this pipelines page. It actually goes all the way back to the merge request. And this whole screen. I'm understanding correct. That's correct. So like out of the 12 trigger sources that initiative pipeline, come it is, I think one of them and come it is again related to the changes and Like for security purpose, if the changes have to be hidden from someone what happens to the pipeline permission. Okay. Mm. One other thing to throw in there, it's not fully live yet. It's only enabled for this project for get live projects, but if you go to CI CD, we also have artifacts now. And that depends on jobs and pipelines. So it's like the way that we had to go was just saying if you don't have access to a pipeline, you don't have access to the jobs. And you also won't have access to the artifacts. Gotcha. It's like a whole tree. Yeah. So I guess that's something that I need help with over here is. Can you help and fix this tree with me to make sure that the links look correct from your perspective. Yeah. So I think that's my my ask for the team is that just double check what I have so far and make new links or break links as you see. Related to your group. And then also just you know feel free to note or call out anything that might be weird or problematic and we can talk about it async. And like I said, I'm also going to pass this through with the devs so they can see what we're talking about in our concerns. Okay. Okay. We can do that for runners now that I'm just looking at this is I never even know add runners to name space is an existing permission. Sorry come again Gina. I think so let me. Find it no only six links. It says add specific runner to project it's listed. Yeah, okay. Oh yeah, product will be in namespace. It's limited to one those inventingers. Yeah. Okay. I'm just trying to think about all the things I'm going to think about it a little bit more runners are like the permissions are so complicated so I'm going to take a closer look into into that. Cool so yeah this is hopefully been helpful and I hope. I have explained what I'm trying to do correctly. I guess if you all have any other feedback. For the meeting today, I don't want to I don't want to keep you longer than our schedule. Okay cool so then yeah like I said when you have opportunity. I'm going to add some changes here or links that I'm missing that you think should belong here especially like across. These sorts of features. I think that's mostly where I'm lacking is where they touch across the platform. There's definitely an overlap with security and compliance in the create features. I mean that and also package I guess. Yeah, my assumption was there should be a lot more links but that's kind of what I was hoping to have these chats with everyone to see which ones I'm missing. Yeah. Awesome. Well, thanks everyone. I appreciate your time and it was super helpful for me and I'm glad I could catch everyone in the same time slot. I hope I could keep anyone up to late or wake anybody up to early. Cool. Thanks again. Good to see you all. Have a good day.",
  "I can introduce the date also. This is the CECD UX team meeting on November 16th of 2022. It's almost 2023, which is crazy. I'll just leave it to you. All right. Thank you, Gina. I can see the agenda of my iPad is acting up. Anyways, I think my first item, it's about severity labels. I'm not mistaken. So just a heads up for product designers. Please don't forget to add severity labels to your UX issues, especially the success impact you want. The bold will annoy you if you don't. So it's a good way of reminding us to label us issues. It's just the core and we can have the public prioritization. But also I've seen some issues. You might see in Valerie also doing. Some housekeeping on those issues and living comments, but there are some issues. There are not labeled UX, but they do have UX requirements. So I leave you one for verify today. There was a back and issue, but it changed settings and it changed UI copy. But it didn't have UX or a severity added. So just a reminder for everyone to please. Make sure that the issues that you and your team are working on have the correct labels applied. Let's one. And then my other updates about hiring, you have any product designer joining the pipeline insights teams. That's a lot of will be joining us in the coming months. We don't have a star date yet, but they will take over from pipeline insights to the Gina can transition full time to runner. So it's going to happen. And then once I know more about that, the star date and yeah just to onboard you all that everybody know. So that's that for me, Gina you have a year of next. Yes, there's an issue that I linked to you that Amelia had brought up about discussing how to better collaborate with. With designers, but I think it also applies with research and even technical writing cross stage or cross group. I just mentioned it to this group. I think a few of us have already jumped into it, but I think this group does a really great job of collaborating. So I was encouraging you to share your thoughts. If you have time. And I think we were going to try to start from the bottom of the agenda today, Erica, if you wanted to start. Well, that's a great tie in because working backward from my list. One of the things was that we have. I wanted to like celebrate the my my updates today are framed around welcome back. Hannah. What is yeah. Welcome. I wanted you to know that Gina and well, I actually got sick and like tested since that I couldn't go to the conference. And so Gina like stepped into this leadership position will really delivered. And we have like four reports, I know, amazing. I coocon four reports that we hammered out because of the data they collected. And then one more on the way, which which ties into the issue that Gina just showed about cross stage collaboration. So we we set up like a game we came up by the interviews instead of running them. Just like in a conversational way. And we were able to get great data and I think we got actionable data. So we're still working on that report and it's setting up a secret scan. Which we kind of all want to be working on anyway. So that was really good. And then I guess my last item to is just asking if you all would like to open up the H2. And I think that's the first thing that I want to do is to verify and package common screener for solution validation studies. So I just wanted to check if it's worth like making that change with Caitlin. And I linked a PDF of what the questions are and as long. But we have like 500 participants in there and some of them have been used. And so you can just mention me in the notes to give you time to look at it and just let me know like how many studies. We might recruit for. And then my other findings are kind of just an overview of the Q flight Q for prioritization. So the big one is that we have a pre work issue set up for Q three. To do some pre work related to a benchmark. There I say for package like a nervous but but anyway. So yeah, so we'll hopefully do a benchmark for a package in Q one. Fiscal year 23 starting in 159 waiting for design and PM resources to kind of come online. But before then I'm just doing work to like kind of crosswalk the stages that have done this before. To kind of like make sure what we do is on par. And then we also kind of related to package resources we put on hold this like cycle of an image study until the new PM joins. So that would be the study that we did kind of with secrets and see I templates where it's like how does it start what happens next what happens next. We would use that kind of framework. And so that's on hold for the new PM we think it's exciting for the new PM to work on but I think the benchmarks would get priority and then it would go to that study. One more thing to mention about that is Gina had it's a really cool issue a research issue around. Looking at the life cycle of logs and so part of me wants to try to pull that into this study that's now on hold. But I think we're getting feedback that we need to like scope that. Gina's study down a little bit so but does note that I have like a plan because I think it's really smart like in foundational research terms. Sorry, I didn't wait for Hannah to say that things for the heads up on the benchmark. Yes, sorry, my. The Google Docs keeps getting. He's crushing on me and it's like enough finish writing down my question. So yeah about package I have a question for you Erica I cannot open the issue now. Okay, so on my iPad but the pre work issue is for the benchmarking right for package. Yes and correctly. Yes. And it's it's all things that I want. Thank you like I'm comparing the screener questions because I think that and I worked on benchmarks it a lot of previous roles and one thing we want to see is how much we can compare across stages. Which kind of means looking at how comparable the samples are and do we all use the same screener and like what's the alignment. I mean that so that I'm not making us be too rigorous with like finding a counterbalance table. But like whatever we've done is an amazing iteration. But we just want us to track it explicitly so we kind of know. So it's mainly me doing the pre work of like what were the screeners. What's our screener looks like I actually have our screener drafted and so whenever Tim had time they can kind of come in. And the idea would be we would treat it like a common screener and then being a position to run the next benchmark like we would have 500 participants that we could select from. If we kind of start rolling it out as soon as we're ready and so we're just waiting on 10 to have time. Yeah, so it's like what were the screeners how are the samples comparing. And it will help us know like I think when I'm seeing so far is like we're mainly small medium business samples which is fine to know right, but it means we don't have to then stretch to do an enterprise recruit kind of doing it beyond part. And we're just looking at all of the mechanisms that different researchers use to like get feedback like will has this lovely. You're all approached that they used for jobs to be done like deciding on the tasks. And then working with them to figure out like what the team has capacity for I feel like we'll go more bare bones which is also okay, but. And a couple weeks when we get back from the Thanksgiving holiday, I'm going to meet with Tim like give the updates and then see what their thoughts are related to the. Kind of the next steps and I my plan was just to use those little half hour meetings that I have set up. So I don't take too much time and then the last thing I'll look at across those stages areas is just like the level at which the tasks were framed. Because that can impact performance to. So I think we'll also help like elevate the research program with this work. Okay, I'm taking some of the thank you. Thank you. Thank you. I'm pretty overview that that clarifies my next. A couple of questions and then I just want to say please keep me in the loop because if we know. A bit ahead of what the the UX I'll say like hands on design work for package is going to look like. I need to know to prepare the UX coverage because we won't have a US location for package until next year. Q 3 q 4. Okay. Yeah, we've made a decision to because team and Victor also Michelle. They talk about the deliverable plans for package now and that's for everybody. Kind of me aware that they're hiring developers right so they don't have for example front and location to build UI right now. So we are not going to be hiring a designer for package right now we move the headcount to a different team and we won't have a designer for package until next year right once development pays up. But if there is a the research is going on and we have. Well, the hiring for development also picks up and we know that we have a need for package UX deliverables then yeah, I'll need to know ahead of time so that we don't work on a coverage or borrow plan or hiring plan for the for the design backfield for Katie's backfield. But just so you're aware that yeah we're not planning on deliverables for package because we don't have anyone allocated there. And maybe we were backwards from that to. I don't know I like really wanted to contribute to the okay are so I don't just care or we're trying but I think I think it would be okay to get those findings and then have the designer as they on board like pick up with them. As long as we like set the expectations. So everyone knows that that's our plan. It seems okay, but we'll let Tim guide us. But thank you for that. Okay. Keep me keep me to go. Okay, well. Sure, just got a couple updates. I'm using the same prioritization issue in Q4 as I used in Q3 and kind of trialling. Just a longer term prioritization issue that goes to quarters as opposed to one just to see how that process goes. I did add a comment in the issue. And with just some updates on like what projects we accomplished in Q3 so if you want to check that out you're welcome to do so. And we're still collecting projects for Q4 but I've listed within that comment what projects we're expecting to take on this quarter. Aside from that, also met up with Emily on Sunday, which was awesome. We got to have dinner. It was nice to just meet another person. I've met like half the people on this call, which I never thought would contribute. Got canceled for the year. But yes, it's just been great to slowly meet more and more team members as a bit of part of the company. And my final update is that I've been reviewing scenarios kind of often on for Emily's CMS study on environment management and she's been really putting a lot of effort into that over the past couple weeks. Any questions for I turn it over to Emily. Okay Emily, you could take it away. I'll go a little out of order because I'll get to my critique one at the end. But I was mostly curious now that I'm kind of getting to the stage of conducting the CMS interviews with internal users just kind of some of the things you did to recruit people. I know we're using some people from the delivery team who've already offered to volunteer but just any other options to get people into the study I'm looking for. So it's like will and Gina both head to comments and in that. Sure, so I haven't done this at ton. I did this probably the most like after I started to get lab just because I wasn't sure how to get internal people to begin with. And I've like posted on Slack channels just I try to keep like a very concise message of you know who I'm looking for what dates. You know just any like kind of a summary of details about the study. And just try to post them on different channels. Sometimes I've like leaned on the PM to try to help direct me to like the right channels to go to. But since you're working a lot with the delivery team they might you know help you know where to post or you could just talk to the participants that you have signed up and say like hey. You know now that you've gone through this study you know is there someone else on your team that might be good for this. Your mind was just plus wanting to what will said I have posted in what's happening at get lab before that has gotten traction. I also found that linking to the research issue helps cutting allow them allowing them to get context around with the research is about. Awesome. Sounds good. There's a high enough left to come into those of hustle and. Yeah I'm trying my. My. My. My. I think I've rolled down about yeah looking of course in the head book right we have a section with the internal customers for each stage group. And then on which pages that on the handbook product slash categories. And if early stage you know it's the delivery distribution team security so just echoing what the will and genus say. Most of those groups they have other own select channels so can drop that same message in there and then we reach out directly to people. I think for the release of the delivery team to have super active so it's easy to find those connections through them and also ask like. We someone that is working or has used X Y Z feature functionality and then can contact with them. But in Gina and we'll they are ready to get the best tips so just echoing what they say. Cool. Cool. The second update I wanted to give is we adjusted the deployment snavigation so now when you click on the high level deployments you land on the environments page not feature flags because there's a lot of data showing that people would land on feature flags and navigate away as well. There was comments about this and are like benchmarking study so that is now in production which is really great. And then will I know see you at a comment on that as well. Yeah, as you talked about we did see this a lot in the benchmarking study and I would just constantly see participants like click on the link and know that they weren't going to go to the right place and then they would just like get frustrated and then have to you know use that extra click to go to the right page so I know that's going to be a big time change. And to do a time to do a little kind of design review I think we still have like 20 minutes so I try I will I'll try not to take of all the time but just wanted to make sure I wasn't taking it too much time. So the background of this this has been a long standing issue that's opened that basically we want to design a page that for MVC starts out to hose production environments across a group. So we've seen a lot of research showing that users are having a lot of problems tracking environments at the group level and they're having to go to each project bubble page kind of track statuses on there so kind of crafting up a page here that. would be able to see like a high level summary and then be able to click in and take action if needed. So I hi on I think you for telling me to kind of just step back and start from scratch because that really got me started quite well but I'll go ahead and share my screen. If I can figure out what screen I have to share this on. So how I started this is I really took a step back and went through the research and kind of this page is interesting because it's not a typical flow users wouldn't be going here to complete like a typical flow but I did want to get on to paper kind of like some of the tasks that. the research showed that they were looking for I don't know if I should go into this if this is going to be public but you can kind of take a look at kind of some of this and then just taking a step back and really figuring out. So this is like a high level view what do users really want to understand and starting from like the information architecture point of view here not even designing something just trying to figure out. from a high level what we should be showing on this page so with some like a team feedback I've done some like St. Calls with the engineers on this I kind of landed on something like this where. we would show kind of production environments a top level summary of like your environment health across the group so like how many environments are healthy are you having problems with any of them. So a little TPD on what to show in this area but really having that high level summary and then being able to dig down if you need. and then when you go into the environment showing an environment level summary of that production environment with like environment health project the deployment the most recent deployment health and then some actions you can take. as well as showing that most recent deployment so you've like context of what is going into that environment currently. I've cut it down quite a bit from some of the original proposals because this page we really want to show them. what is going on and not bog them down by a bunch of different information that they could kind of dig into and find. So I've kind of moved it into this again as you can see this is like a really big TPD I want to create a nice summary at the top. but kind of showing this into like group name environment at the production tier kind of like is the most recent deployment success tools or anything action you have to take and then kind of showing details about most recent deployment underneath it. with options to like view and project and kind of debug problems going that way but by condensing this information down reducing the amount of like page nation you need so being able to show things all on one page and being able to show just the information you need. to kind of take the action that you need to take so open to suggestions on this but that's kind of like how I've started this it still fairly rough. so there's like I do want to run this by some users as well but but I would get a first feedback look even though this is kind of like a rough low fidelity right now. I had one comment just saying that I thought it looks great I didn't even see this the one that you're showing right now I only saw the I used up so I think it I think it looks. like with the all the information that you have to deal with which there's a lot like you did a really good job of summarizing that. and making it clear I also the thing that came to mind for the summary view up top I think that this single sat component is always good for that type of stuff. so if you you could consider using something like that for up there I've seen other pages do that as well. and what runner does that and then there's a like a DevOps report type of thing that kind of does it. awesome and that's called the like single stat component yeah I'll link it to you from pajamas also it's pretty flexible to which is great. then my my only other comment was. for like this would be adding a new navigation item for for groups and. we have to do a similar thing for runner so I would consider talking to the foundation team just to make sure. like I think you have to get approval from them now I can link the handbook page but I also know because they have all their navigation efforts going on it may change. and I think I have. yeah I know when we're trying to do building on the growth team we're running into similar thing there was like a process in adding something to the nav so I will definitely kind of communicate to this idea with them and see how to go from there. okay yeah. okay. Erica I think you had the next comment yes thanks for sharing this I love that you're sharing that's on this in this meeting. so one thing that we're we've learned about the workflow with CIC e variables and a subset of those are secrets is that one of the reasons people want to use the unprotected values is because. they tend to use the wrong ones pro environment because it's confusing so yeah like a subset of that finding is that they use different secrets values. for the different environments. so I think you could add clarity here if there's like a light touch way to even like allow people to dig into the different variables that are being used in different environments. and they do I think that would help them and it feels like if we're going to do an overview like that that that would be something. to highlight. or think about. I can definitely think into that I'm still in the process of trying to figure out what is the best information to show here what should be still cut out because of this deployment information is just kind of copied over from the environment index page so. to do a bit more exploration on like is this the right amount of information and if there's additional information like you said the kind of secrets. how do you show that here so thanks for bringing that to my attention. well thank you all for the feedback by the way I see will is next. yeah I think for the summary view some of the things that could be helpful is just being able to identify not just like how many. environments are in a successful status but also if there's ones that are like pending or if there are ones that are like failing or having some sort of like difficulties just so that people can like easily. like click into those and be taken to those somehow. and then also like how in your design you're not like using that carrot Sasha accordion style menu that that we currently have just because it's nice to be able to just see. some of the specific data that users were having to like really stumble around to try to find. awesome yeah thank you that was kind of I know a lot of our research has shown those. so that's a lot of the things that we're going to do is to be able to see the actual appsable sections when you're trying to find a high level view of everything was just not working out right so. I was trying my best not to even collapse any information and just if there's information you. that needs to be hidden maybe that's not the right place to put it on this page so appreciate. then hi and I think you have the last comments. yeah so just personal what we'll say about. showing information without using that. also pattern all for your spending collapse and it just hides all the info. that it's essential in this page very much in both in this case the group but also the project level environments view. and also plus one in the summary blog I think this will give users that at a good understanding of how their environments are doing across the board and that's important for us not only for the. developers but you know the managers the director of why a person anyway is anyone really that is interesting how. the application is doing at the higher level so. I think that's going to be a great addition. and have a couple of full of questions Emily and sure if you have the time but we may think those the just things I was thinking of while looking at the prototypes. what type of environments will be displayed on this overview just like the protected environments production only and also was only how users set up this overview. has a team considered if this would be something that will be just pre populated you know coming from all the projects. that had the environments that meet a specific criteria or we users be able to let's say managing customized that view based on what's important for them to see you know for each project. yeah so this is a good question for MVC we've really landed on we want to show just production tier environments and then. figuring out later we'll need to scale this up to show the different tiers but kind of pre populate this based on like tier of environments. this is actually where the empty state comes in I don't think I'm the one that designed the 70 state I kind of grabbed it from one of the others but. if you don't aren't using environment tiers using this empty state to kind of encourage users to kind of set that up so that they can pre populate this page but also use this feature which we don't have any area now where we're really encouraging them to do so. so having the empty state as both a bonus of setting up this but also encouraging people to use environment tiers. so hopefully that answers your question for MVC we're just going to stick with production tier environments and to set it up would be enabling tiers in your environments to show them pre populated on this page. yeah thanks that is my question and I think following up to that so. and thank you all too is that when you have the chance look at how the MVC plus would look like because for example if I'm not mistaken you can have multiple. and the environment some of the same tier right so if you set it up in the CI file you can have I don't know different production environments label production but for a different name right so how would that look like. in the UI how would users customize I think will be a good next step for you there see how this page would grow because we're not a challenges that we had with the current environment pages that. while designing and also coding it we were not the they off how that paid would scale so I think you have the the ability here. to predict some of the edge cases but also to avoid you know unhappy unhappy bets in the in the UI and the real UX. yeah for sure yeah that's something and where I'm early enough into the designs here now that I have to kind of build out what does like unhealthy environments look like what happens when you have like a lot on this page so I think we still have quite a bit of time to kind of build out all these edge cases and then. what's we certainly more than just the production tier how do you set that up what does that view look like and everything like that so. think there's lots of work to be done on this page but it'll be good work to finally get out to users. yeah. awesome. then my next point is about that. this seems like if you move forward with this approach right up we could potentially deprecating for our room. dashboard view. or what are the team's questions about it if if there's any discussion going on around the deprecating or replacing the page. if we're often this group view. yeah so there hasn't been any like official conversations going on but I have chatted with like Andrew on the team of the reasons of moving away from it and just. the performance issues on that page were so hard to sort out that that's kind of where this idea has come from and kind of like why we might be moving forward with this and some of the performance issues are really hard to kind of solve or but I think we need like some written out kind of conversation with. the PMs and everyone on that team to figure out is this a like a replacement can they work together. the main pros of this pros of the environment dashboard and really sort that plan out as well. also yeah I think that that will be. you know a good. but I put that. just vacation right for deprecating or moving away from that environment dashboard view. so that's a lot of the traffic there as far as I know and see your point there's so many. restricts in terms of the. also for that performance of that page that you know. I'll say. if you can it also when you have the decisions that are positioned the team putting the proposals to right that how this this page will or the group potentially solve the jobs we don't solve the problem that's the environments dashboard view does not solve right. thank you and. no sorry. no I just thank you that's like something I definitely have to think on so we'll get that right now. so. and then my last thing back to just about it's a spot the y and then if I. I'm assuming here yeah. when I did the validation for the environments. page right. I learned that that information about the commit message it's irrelevant to users or it's usually relevant. when that commit is merging or branch or anything into master because they often shows that same you know the fog message. merge branch and then branch name and that's not we readable and that's not a very far motive to users. so my feedback here would be like. check in the. the insights I don't remember correctly now what what's the most relevant information that that we should be displaying here to users and see how you know and if it could be. we if you could replace the merge branch message because that's what we have today in the environment stage is just the same message everywhere with the link. and as I already tell much. can use that space in now in a more a different way. rather than just showing this message. yeah I agree I think there's stuff we can. and that if we can say real estate on anything in this page and take out things that they don't need at this viewpoint on something I'm working towards right now so that's great to know that something we can potentially take out of this. awesome. thanks everyone for your feedback I want to give Gina enough time for them points as well but I appreciate kind of taking the time to let me walk through that so. I think exactly I have one other question for you I noticed that the. the way that you were like. for each row there was kind of like many columns of metadata in there like the job like all that's up. have you heard a gotten any feedback around how that impacts. like scan ability I guess across different deployments. yeah because it is a lot of information so as like a side note I've kind of added these in here to this I'm still considering it like a low fidelity I've just added in what we have on the environment index page and I'm planning. in like the next version of this to kind of go through see what information we can remove from there see what information is like the most important. based on conversations with the engineers to kind of for MVC start with what we have and kind of take away from that if possible just so we can read. just so we can reuse some components so yeah I agree those sections I think need a bit of cleaning up and figuring out what exact information should be placed there. like the merge branch message taking that away but yeah I agree I think there's some simplification we can still do in that area. yeah I'd be like if you end up testing this I would be interested in seeing if. people say like if they if they if they want to scan I guess my column like does. formatting the information in that way impacts that because I mean we're doing that in runner and there have been like two. or two customers who have been like I don't I don't want to see it in this list view I'd rather see it in a table but the majority have been saying it's fine to see it in the way that it is. but the view I that we're using is very similar so I think the insights would carry over to them. well good to know and yeah I'm planning to run this through some solution validation as well when we have a more finalized design because there's such a big. lift to do so hopefully we'll get some of the usability comments from that as well. and then I have one more point there like jumping in before it's but it would be that when you do the solution validation I think try to include like more personas than you might not like try to push yourself to include more personas so we make this work for more. people because I think it can have a bigger impact. yeah because I think right now the dashboards are welcome is not working for some of our personas so opening this up to a bunch of other ones is probably like the best. Well thanks again for all that I will pass it off to Gina. I think I have two updates one of them is for pipeline insights. we're dealing with this problem of flaky tests forget if I gave a definition of these are not but the general one is that when you run tests they can sometimes fail but the failure isn't real like it wasn't actually detected it was because like the test is actually written incorrectly or maybe. I think it's a test that that test relies on failed something like that and if it fails more often than it succeeds without actually detecting a failure it's considered flaky. So we're trying to understand from users like their pain points right now they're work around with how they're dealing with flaky tests and then definitely how they define them. So the insight for us here is how they define them because we want to be able to when we define them for this MVC want to make sure it matches their expectations. So I included some of the insights that we've heard so far we will we met with three users so far but we have a few other participants coming up as well. on the runner side we this is like one of our our bigger. Well it's a small feature but it's it's an important feature that we've added for runners. It kind of has to do with the whole runner fleet monitoring slash Q stuff I've been talking about for a while so it's it's difficult for users to be able to see if a runner is running a job at the moment and the reason why that's important is because. If they were to update the runner for example. It would stop that runner from running any jobs so they don't want to impact any current running jobs or else the code just it doesn't go out. So they want to know if. If that runners actually running one. So we've added a badge to be able to see if it's running or if it's idle is the other word that we're using. The tough part about this is that now we have two status badges we have one about if the runner is online. And if the runner is like running a job so our next iteration would be combining those into a single status something like active where it's running a job and it's online. And then that will allow for less less load on the user. Make questions or anything. I just want to say I love the idea of combining status badges into one thing that works because I've seen it quite a few time and I think it's in like the environments area as well or there's like multiple status badges and you're not quite sure have here really to each other so I think that's a great idea. Thanks it will definitely be a tough thing to do like because even with runner because everything through the CLI to people can now get a list of all like online runners but all those statuses would have to change so it's going to be like a long long process. I think that's it. I think anybody has anything else so. Thanks for meeting today. See everybody soon next week.",
  "Hello everyone, this is the CICD UX team, we're meeting stay on October 19th and there are some manager announcements in the beginning. I'll just go over and then we decided this meeting that we're going to switch up the order so we're going to start at the bottom of the agenda. So there was a there's a thread for OKR's progress if there's anything blocked or at risk. I don't know was asking for you to list it but it doesn't seem like anything's there. And then there's some upcoming family and friends days dates that were released for November through January. And then the talent assessment officially kicked off today. And it's open until November 4th to get that first down. Maybe it's a specifically to fill out the performance sheet that we were all given. Anything any questions or anything. All right, so then if we scroll all the way down, will do you want to start off and maybe if you know if Erica has anything you could go over that too. Yeah, sure. So I know that Erica is also going to be a CUC on all be there as well. I think you're also going to be there. So be great to actually see a handful of people. I've met just a couple of get lab team members before but excited to actually meet people within the department. So we'll be out doing some research. I think she's doing Erica's doing like them own survey. I think Gina you've been working on them with. I'm doing something for distribution team. Like a survey and some interviews and then Erica and I are also teaming up with the market insights team to do some like buyer persona research. Where we're trying to identify people that are likely to buy. Get lab and then talk to them for a little bit and then see if they're open. To like focus groups later in the week. So we're going to be quite busy with research next week. But what I have time. I'll be checking like messages and things to to keep up with stuff. Aside from that. About halfway through the benchmarking workshop that I'm doing with the release team. Basically the team is come up with solutions to the identified pain points and is giving feedback on everyone's ideas at this point. And then tomorrow I'm actually opening up a final part of that workshop where we'll essentially vote on the top solutions that we want to work on as a team. And then I'll create actual insight issues that will incorporate into the roadmap. And then I've been helping Emily out with some research that she's doing related to them CMS scorecard. So that's it for me. One of the stuff that you're saying that Erica was doing for Qcon. I only have insight into one of them which is about secrets but she's been there was like some previous work that. they had piloted and ended up getting like a lot of data from around. I think that one was specifically secrets within the workflow so kind of figuring out like what developers were expecting. Ideally for how that would work. And she's going to continue looking at that for Qcon but focus on finding out metadata that's important around secrets. Okay, yeah, thanks for the context. Yeah. So Kevin, I think you're up next. Sure. Thanks. Yeah, for the null of dates but mainly one of the issues that I've been working with and that I shared a last time is about not really massed for our eyeballs. And kind of what I've heard is after discussing with the team we kind of agreed to. They can not have and kind of diverge on this and decorulate secrets and variables. So I've got to sketch out some really broad wireframes of what that would look like. If anyone has some thoughts, please wait in. But essentially the next steps are going to be to they have a bit deeper as to what this would look like introducing a whole path for creating and managing secrets. And that is the first point. Second point is you may have seen this in the UXCICD channel. But there is a proposal from an engineer for it's to be designed by applying over view. Bage. So instead of having a like the pipeline I think it's by plane graph you go with the different jobs and stages that would be a different visualization that kind of highlight. More like the number of fail jobs and also like higher pipeline is is doing. So I'll be helping them kind of iterating on this design but potentially also good through research and validate it. So that's it for me again if you have any thoughts on that. Feel free to wait in the issue. I think there's a link to their video that actually created an issue otherwise you might able to find it in the slack thread. So yeah that's it. So this one looks to I was going through a proposal and like crunching the comment box to a small area and landing straight on the summary tab. Did you also I haven't read through your comment that was the discussion around removing the fail jobs or that's something that's. The fail jobs happen you mean. Yeah yeah. Yeah. I kind of mentioned that we kids instead of like showing all of the fail jobs just showing a bunch of them and then sending people to the fail job tabs. I think that's on direction. Instead. Yeah. All right. It's good. Oh, sorry, next is me. Dave and your friends are okay. All right. So for us to have had enabled tracking for a bunch of pipe and related actions and now what we are doing is. I will quickly find the size and slink. So we are able to get a lot of information about what are the actions that our users are performing more frequently and which are the links they're clicking on on the pipeline this view. So we had our monthly check in around that information here's the dashboard I'm thinking get here. And this is the dashboard and we kind of thought of different ways in which we can act upon the information that we're getting from there. So for example, we already had a bunch of systems related to speed. So what our front and team did was they used lighthouse runs to figure out like which are the areas which are costing us the most in terms of performance. And how can we tie in the insights which are is we were saving through this set of data and come up with you proposals. So if you go down, I have proposed a new method to kind of just suggest a proposal. And that would be broken into three parts like proposal the assumptions that we are taking into account and the evidence to support the proposal which can be a combination of both. So metrics we're getting through the data tracking as well as the insights from previous researchers and at the end like once we have a bunch of these we would do a round of voting. And so far my idea is that we would vote on the basis of like which one has the least risky assumption associated with it and it would be most impactful. So keeping those two parameters in mind. Yeah, and then we would just get started on the like design proposal for that particular proposal that kind of gets the highest number of words. Coming to the next point which is the PNPS and sus feedback assessment. So we revisited feedback from past four quarters because we really wanted to get to like work on such impacting issues that would have the highest impact. And we were desperate to take a different approach than just like going through like burning down through list of S1 and S2s because we wanted this to align with what our users are feeling and how they're like sharing their experiences with us. Now this assessment I was able to understand at what are the most frequently emerging themes across the feedback and the two that were most recurring were one was definitely around improvements to talks like adding more examples and improving the instructions around CACD. And the next one was reducing the number of steps for primary task this was like a lot of users mentioned like share feedback around. It taking a lot of time and a lot of number of clicks to get and to perform an action order to perform a task which we can say is primary to the area. So we want to make a collection of all the existing issues which are sus impacting which relate to this insight and bring them together in an epic with a very clear exit criteria. So we are actually able to go through like going down all the issues inside the epic and also close it by the end of the quarter so something very realistic. And that's the next step for me like I'll be creating that epic and adding issues to that. What next quarter yeah. Any comments any feedback. Do you want to voice. Yeah, I'll voice that one. This was related for your first topic. It was it's just interesting that that that issue is more about like load performance which makes perfect sense and that we have an in pipeline insights. We have a feature around load performance and it's supposed to help track those types of things. But it's clear that the products that like that your team is using is showing results or data and much more depth than what our feature does. So it gives me some ideas of how we could improve. That area we're not even focused on performance testing as like a whole in pipeline insights but something for the future at least that we could think of. I was at a very rough this so far and this is the first time looking at once it's proud that provides these informations. Yeah, they do a good job. Yeah. All right, so that was me passing it on to Emily. So last week I was kind of wrapping up our solution validation around pending deployment approvals introduced and just there was some interesting stuff that came out of the study that kind of correlated with some of the competitive reviews we've done. Slack seemed to be the most popular tool users we using to notify each other since we didn't offer it. They were figuring out with bots or doing it manually through Slack channels. But interestingly enough like users were looking for a way to integrate pending deployment approvals with Slack so they didn't have to depend on the get lab UI. And we saw this when we reviewed harness as well harness was using Slack integrations for pending deployment approvals more so then their own notification system. So good takeaway of this is we were going to implement this within to do's and we think now a good MVC is just to allow users to kind of integrate these deployment pending deployment notifications within Slack and within their current workflow so that's kind of the way we're going to take this. They did like the idea of being notified they thought to do's made sense but there was warnings about how to do's are pretty busy already. We have no way to prioritize to do's they kind of just land in the list. So I think there's additional work we'd have to do to get them in the to do's but they would also see that workflow as being something useful just there's more work in there so our MVC we're now landing on is focusing more on integrations. And yeah I linked the research as you and Deb's till study if you want to read more but yeah a big thanks to Will they helped and supported with like all stages of this so big thank you for all that support. And then the second one is one of the bigger design initiatives we've had is supporting multiple approval rules on the project level protected environment settings UI this really gives users a lot more flexibility into setting approval rules so now people who can approve don't always have to deploy you can set who can deploy who can approve different types of approval rules on which we're really excited about because this gives them a lot more. Configuration options within the UI whereas before we only gave this through access with the API so excited for that to come out as well. And then my last thing is I didn't mention this on Slack but I will also be having lower availability next week I'll be using my visiting grant so I might be just lower availability for sink calls might be late responding to messages if you need me so that's more just an F. an F. And no one has questions I can pass it off to Gina. All right the first both actually both of these items are on pipeline insights I have work going on for run or two but I felt like these were more interesting. So the the first one here is about the new artifacts browser design. So also kind of goes along with what we've got was saying about having basically like allowing for less clicks to get to the thing that you need to get to. So the browser right now if you were to go into a job and browse artifacts from there you have to click into each folder it's like it's so there can be so many nested folders to be able to get to that final. So what I ended up doing if you look in the design there. I kind of consolidated all of those folders if they're empty into a single file pack and then I just. And then did the the file with it like underneath there so it's clear that it's still part of that folder. But if you have any feedback on that design I also linked video if we're going through the proposal in the York's coworking channel. If that helps as well. And the other thing I had here was we are we did a bunch of research to understand for review apps kind of like we heard that the view app button wasn't prominent enough people couldn't find it so I wanted to understand really why and what was going on. And we found out that it it's really just not integrated into the review workflow as much as people were expecting it to be so what we're planning on doing is adding. The view app button into that code drop down when you go to a MR and this also allows it to kind of be like stuck to that global header because that. Um, adding is sticky when you scroll down the page and it also is closely integrated into the the review workflow there there's some. Some specific things were like only certain projects have review apps enabled so we won't be showing that. Many of them all the time unless they review app is enabled for that project. I want to say I love this change because I don't know how many times I said trying to scroll down page looking for that like view out button so having it just in one place would be so nice. Thanks yeah we're definitely I think it will definitely be better because like we're saying like people were having trouble finding it and said that it doesn't always show up in that widget on at the same time so. I think this will help. And then my last thing here is I'll be I think I might have done that with zero is why did they look like that I did. All the out of office next week at Q com. Uh, but I'll probably be on. I'll have my computer so like if there's anything that you need just feel free to reach out. So 50% of the population here is going to be a. Yes. Yes. 50% yes. So since Erica is back maybe they wants to go to them items. No, did I miss Kevin. Kevin. Yeah. Okay. So yes we're gearing up for a coup con and then just if my personal life my child has been sick and then yesterday we tested them for Kobe. We all have come in. So it's just so I'm like oh no, the conference. So I'm going to have to figure that out. So we'll continue to tonight. So I just am like waking up because that's why I was a little late but miraculously I woke up. I was like oh there's the meeting. So I'm always just glad to see you guys and I'm glad that I showed up because those were great updates. So we now have to kind of figure out what to do about. My potential absence but maybe I'll be healed. But what I can do is show share with you this deck. If I can do it. Um, hit well. Let's see if. I can just give you the general updates. So we are going to be at coup con doing a lot of things will and I both have a survey that we're a fielding. Um, and then we are going to be doing some interviews with buyer personas. The pms from ops, especially have kind of come up with. Some questions and so people will kind of come up to the booth. And if they're a buyer persona, they'll get put into this buyer persona interview round slash recruitment for focus groups. And then if they are. Not a buyer. Then they can either come to the area where we're going to have a station at the booth or there's actually two booths. They might also just complete these interviews from the pms. So it would be good to get your eyes on that stuff. So I'll link that issue. We have like seven questions that we're going to ask. Um, And then we've set up. Um, Qualtricks so that all those questions and then questions for the buyers and all the other questions are in Qualtricks. And then that'll be a note taking mechanism where attendees can kind of take notes on those interview questions. And then James was going to kind of lead those pms in a sus like approach. Where they kind of summarizes it in an issue the findings and then we're going to invite people to follow up with those participants. So I'm sure it's all going to work out and I'm miraculously not have COVID in like a day. So, um, And besides that, we're working on the prioritization stuff for Q4. So if you haven't had a chance yet, We'd be good to link the Q4 projects that you want and we're going to put that through the research prioritization, But because also our pms are going to be at coupon. We're going to wait to finalize that until everyone gets back on November 8th. And another thing that I'm probably going to pin you about is I'm going to try to do the research prioritization of the research center this. I forget we have like a longer name for it, but basically we're going to go back through and according to our research questions like summarize. All the research we found for this quarter. So that will be something I can link. So I'll just be asking you to include studies that you would like. Kind of included in that synthesis and the idea is our great Jackie will return and we want to like have that as a way of helping them come back is to kind of summarize the research so far. And if I don't go to co-continue to be really bored. I don't know if you'd like please tell me things going clear. And then I think another thing to put out there is I and I posted this on Slack, but to help us like digest the secrets findings. As a team we've set up. Three meetings the first is totally, it is optional and informal and will be me reading out the. The secret to features prioritization survey responses with some pms like cross functionally pms so they'll be like in that report we can do like a Q and a and then the second we give no member. We have a think big set up. And so y'all should attend that and I will. Maybe easier from me just to post the slack. Meaning but there's two such the slack of the slack comment where I linked all of these things which is in the city channel. But there's two time zones so that should hopefully work out for everyone. Hopefully you guys can attend that and hopefully we're getting this on your radars brilliant enough. So that you can plan around that. Okay, there's the slack. Sorry that I'm a little just can buy really did. No problem if there's anything that we can do to help for Q gone at the very least let us know. And take time to rest. Sorry about the coven situation that's tough that really bad timing. I know I was like well I'm going to I was like it. I have a whole plan for like quarantine when I go back. It's just yeah and all of you I think even if you make it all three of we should be really cautious during the event. Yeah, though at the last one a lot of people kind of had it. I was like really like OCD about it and I put the did you know this story I was I didn't get it I could kind of traveling to Spain and then I came home and my partner had it. I don't know. I remember that. So it's just you know whatever we can do. But but yes so Gina we're going to need to kind of decide today like a backup plan for that yeah the inner. It's stuff yeah okay we're going to probably need to meet and we just because I can scope down that whole project. Okay if we want and so it just depends on what you want to do I had even talk to James about. When we met to talk about the thing big James and I had kind of talked about scoping that down anyway and just making the question could we do this research. So it kind of scales it back but yeah hopefully we can find some time to meet today to go for that stuff. Yeah definitely I'm around like all afternoon. Okay it's fully fully available so it's do that. Right well I think that that was all of our items anybody else have anything that you wanted to talk about. Now all right well have a good rest of your week and talk again soon. Bye everybody.",
  "All right, so this is the CI CD, UX-NET for September 21st. So first of some standing announcements, right, and update. So okay, our progress, get left a comment here about the SUS, the S2 issues for package. There were plan for 15-5, they're not going to be able to deliver that. they already left a comment in the mirror, you're lying on this. Emily, I know that for release, everything's already done. And I think we have a couple of issues open for my client, authoring. But yeah, what I just want to voice is, if in the future, or we are looking at, okay, ours, right, they relate to SUS or issues that, yeah, we have a commitment to deliver. I think it's good to align with the stage groups with the teams that if you're not improving, they use the ability to the commitments of the OPR, then what is the improvement that the team is proposing, right? So that's, we make sure that we're talking about a trade-off and not, yeah, we de-prioritized things happen, but what's the plan to later prioritize those issues or the usability improvements or what are we, what are we training off, right? So I just want to put this comment there, but thanks, Katie, for leaving the update. And then next up is the tele-assessment. It's coming up soon. We're launching media October, so October, October November, we're going to have the tele-assessment. Logistics, closed, exact dates, TPD, so I link here, the handbook page. And I think they're also changing something's in the process. I think the team member assessment is going to be optional. The manager is mandatory to member optional, but that might change. Now that we are under product, things might change in the process. So now, I am an manager is going to update the team, but keep an eye for what's in the page. And Katie's not here, but I'll make the announcement since this is going to come out. It's going to be a public knowledge soon. So Katie is going to transition to the machine learning team, so they applied internally and she's changing team. they would be managed by Tory, starting October, but they was still recovering for package half the per capacity and team in backfield, the low. So she's transitioning out in 156, because this milestone she's doing beautiful in my UI, but once that ends, she'll start onboarding the new team and then she'll be able to run in the backfield. Which I'm getting very very set to see Katie go, but it's such a great opportunity and I think she'll do great in the machine learning team. Right. Gina is not here today, but I'll just quickly go through them updates, run it to sleep. Was it evaluated at complete? That's huge, right? You can check the updates in the VOMTL project. And then they has an async comment here about browsing a job artifact file. They're going to be working on making easy to find the file in this particular issue, which isn't here. So we can find more details. Is that, oh, it's an actual, it's an insight from fully disability. That's for pipeline sites. Right. So that's that. Emily, do you want to take over for four please? Yeah. So I don't have much of an update. I'm catching up on like a little over two weeks of pedo today. So not much from the product to update, but I will be hosting another really office hours tomorrow with some focus on like UX and front end. So how community contributors can get UX feedback, front end feedback on their contributions. So I'll then update on how that goes on our next meeting, but excited because our last office hours had a pretty good turnout and people were open asking questions and everything. And looks like the ticket is also not here and is on some time off today. So. I think I've got a Katie's comments as I mentioned, she's working on really finding out why this milestone and now that also the Nadia left, like it, my is less than what's yesterday. Katie's taking over some of the issues and some of the proposals that Nadia was working on less milestones, so that's super cool to. And then which is working with on right from then engineering. And then we have a comment here. Do you want to put a stand. Yeah, so I came up with a bunch of ideas for beautifying our UI, but our login page ended up taking the whole milestone. So I kind of have a bunch in that issue that if are any are interesting. Katie can kind of take them over. And but yeah, our login page went out, so that's really exciting. So yeah, you'll see a different login page down. I was like, oh my god, it's in production. I saw that's a earlier this week was nice. Yeah, thanks for that. Thanks for the heads up there. And then I'll just read out Katie's second point. They're working through us adding a banner to the product UI as a call to actions for survey. Costing was stretchable for recognition, reaching five. Erica, that's a good. I hope with that one. A little bit. Yeah, so yeah, so tell me more about that. It's with you. So they're working on the integration for package and how they're going to support that. And so they wanted to get just like tactical quant data around which packages, which types of packages people wanted to import and then like how large they are. And I think they're asking like a background question on company size. But that's when we don't want to do via email campaign because it will be hard to find that. But I think that'll be really helpful and because we asked it in a more global way, which is not like just that that one are the factory migration. It should kind of help the team if they can get quant data around like which one might be next to be somewhat people are asking for. So that's right. And this is going to be rolled out in both sass and self hosted. Great. It should be. We should follow up. Thanks for making it about that. Yeah. I'll be here. Erica. Thank you. Julio. And then the else. Yeah, the comments on the on this one. You know, just jump to I'm reading out all the. Where's our team like people are going back to the you know, all in different times on so it's funny because they had everybody like. Like for a period of time, that was nice. Nadia. Let's get lab and then Kevin Kevin commodity brought design it for growth. He's going to be covering for pipeline all three as a borrow until 158 so we'll be allocated 60% of the capacity. And I'll link here the transition. For a pipeline all three so you folks can see work. And so Kevin will be tackling mostly the some of the research items Erica there were a pending order on going for pipeline all three so they starts looking to this tomorrow and they started onboarding on the production next week. And there's already a plan with dove with a PM about what are the priorities. So we have. And there's that issue. Other than design issues for the. CI CG templates. There's a social validation for the catalog like the the EAMO syntax right breaking down that one and also secret management. Not if finished off the jobs to be done, but then. I know the dog wants to prioritize secret management and then start to design work so you have to ramp up on that. So if they asked for anything and then. Now just let me know also so I can we can help them out, but I think it's good to triple check if there is any research. Fendi or if there is any if the plan so if I quite already have changed. So if you need to know was an F dove because he's on this making those decisions right now. And then there's also solution validation for retrying trigger jobs. I don't think they has started. Mother has started with that only yet. they just has the issue. So I I was thinking I'm I'm so glad that was so quick. Yay. We have the plan out there. Yes. And I was thinking that I would create an issue to like walk people through all the research to date. Because we have like a research program around some secrets. So good to know I don't have like three weeks. But I do that next few days and then. Like link link Kevin and then I will also invite them I set up a bi weekly meetings every two weeks with dog. So I can also invite Kevin there and then if they has any questions I did get some resolution yesterday when I met with dog about. It's a double negative but it's like should make the unmasking. A default the ability to unmask or unmasking. I default and not yet had paid. And and was wondering if we needed research and you had suggested to them that we might do it at KubCon. But I resolved with. Do yesterday that we don't need research on that. In part will be my announcement later. We have we were able to. Because of Gina's help and not to be able to use those pilots to just get great data. So we ran 11 participants in our secret workflow and we saw that. Debugging specifically. They're using unprotected secret value. And they're using the CI CD variables. So. While it's not good practice. Yeah. We also got one alienate users. And one thing that one of them said. So the prompt in the study is like, what if we encrypted these values, what would you do? So that would be like the proxy for if they're inaccessible. And one of them was like, I'll file a ticket with get lab. So. Look at the full of like we don't really need a mass amount. In flex and our tickets. Right. So yeah. So I'll find this. I will make a note to comment on the Erica to find issue. And make a note. We need more research. Like we're just going to make it configurable whether or not the default. Yeah. Okay. Yeah. And I think that when Jackie and I did the initial Robin validation for sequish management like two years. Baty it didn't have years ago. We all I also remember hearing this type of feedback that's why it's experienced materia like checking with Erica, maybe you want to ask people. It's not anything has changed. And then indeed like vault and then sequers management that's why we want to separate CI variables from city could. OK. Yes. Yeah. We're all right. Pat and then also with our. With some of the changes in the survey. that that one person was like, I'd file a ticket. So we want to like in place have the like documentation around what to do and like think even the release program. Okay. Awesome. Um, Binging issue. I'll give some of my updates after everybody will with sort of their items. But I also have context and I worked on secrets management before. So I can help out there pointing out to whatever. Uh, maybe historical knowledge that that you are in the team might need. But I'm not sure. I don't remember exactly don't told me yesterday that I'm before yesterday, but I don't know when they wants to scan to any of the secrets work for design just yet. Uh, they has a the team has all this large stuff to work on like the secrets and then catalog and the trigger jobs and then trying to make a business face for something else. But I'll double check. And what one more update around this would be that I saw that the engineering team was like wanting more preparation for being tagged and needing to respond to stuff. So to be proactive about that, um, I asked them if they could set aside a little time for plan. As we get who come secrets research like that should be big juicy stuff. So we'll want them to like digest it. But we also want to ask too much. So the plan is the first or second we can November to set up a thing big, think small. Exercise around that and those might be. Uh, with the idea that that's like low. That are we think just brainstorm. That's like, I was like, should this be a thing? But it's like, no, do it sting everyone can brainstorm and then they can like let go and walk away. But okay. Yeah, have you talked to to Mark about that Mark. Okay. That's close. Another, but these are the three loops to close. Okay. Yeah, that that has been a challenge because they are they're working on a lot of stuff and then. There's no buffer to to into a lot of things. Yeah. Okay, let me know. Ping me in slack and then there's a thing around that and I also try to. I don't know, do my magic. Hey, yeah. Okay. Awesome. Um, Cool. You know, um, Erica, do you want to go over the research parts go through wheels and your full such you? Um, and so I know that they has a draft of their benchmark report, which is really good and exciting. And so I'm sure he's really excited about that. Um, they also had the assistance performance workshop. I think I was sick. I was sick. And so I missed it. But one of my things is to watch it. And I'm excited to see how that worked even with our team, right, of doing the sink. Um, because I have been like, no, no, no, we should never do that. But I think it went really well. So I'm excited to watch it. We should maybe link to it. And then mine are just that we have these four reports on the secrets secrets in the workflow. So it's the debugging workflow creating a CI template. Creating a secrets policy and what happens when your block when your MR is blocked because you have a secret value in there. And so with TLDR debugging, we need the unprotected values. And so we need to solve for that. And it will create a bottle of vekin that workflow, especially because the ops are usually the only ones who can access those values. So there's going to be a back and forth. That will add time. And then we know that that's don't sell a bad best practice because they don't have time. And so we just want to solve for all those things. So that's the debugging. And then the MR report, we want to look at. I want to look at what happens when your block and now we want to look at when we go to a coop con. We could unblock you in advance. Or like I'm one of the things was that the team said. The the create team said that they have something already in place to prevent secrets from going into repository. But I don't know the values there's new about it. I think why. So I think that there's like a grander thing we need to do, which is as we roll out our secrets solution, we need to be like, and have you. Do you know about these things right like so I won't just be like with dove and team are working on it will be like our suite of offerings. So we can put those together based on this finding. And then part of the creating a secret policy which I think is helpful for us and will be helpful for. And so the thing that we're going to do is to know that is like obvious, but it's important is that as they're creating a policy. So there's like three drivers for the policy developers. Ops people or compliance requirements. And so because the ICD variables can be a secret or not a secret and explicit step for them is like, okay, is it a secret? I thought it's a secret and I think that's really helpful because. It's not on my rodeo, but for the design calcable who are like, we're trying to figure out what the elegance solution is. Maybe building on that decision process until we have a long term secret solution will be helpful, right, so like if we think about how to clarify all that setting stuff so it's not. So, complex, maybe you surface the complexity after there they have about decision, but this way I'm always happy to hand off. The artistic or like the like design problems, but. So I think thanks, Eric, that's really helpful and. I think. And I didn't have to get them also like some like there's an issue with like a list of things around secrets that they thinks that they found relevant rights to onboard a ramp up on that. I think we can follow up that when that with with Kevin. See if there's anything else that they can consume many other information that it's more up to date, like what you're saying about the reports or yeah just stuff that will be relevant not just to build that like. That knowledge of secrets, but here the things that we need to address now. And as you say, building that decision process. Make a nice. Yeah, actually simple. I remember I did the UX I did a mature to spark art with the Jackie on vault integration. Many moons ago and no one to complete this core card with this scenario, because it was so complex to set things up. So yeah. We'll see. Yeah. Awesome. Anything else? Any only cool stuff. Actually, I want to I have a comment. That's what I learned to talk about. So we're going to have three openings in our team team for out of the line opening soon. So for packaged for five planning sites behind another designer. Gina will be moving to runner full time. So now she's played right right inside the runner. So Gina will move to runner and we have pipeline altering. So I'm working on the requirements for the roles, but once I have those open and we have the jobs postings. I'm going to share with our team and you know someone already that you think you're not going to fit for our team right. And in my way, we got to tell them to prepare their. We're thinking profiles and the CV. Um, because yeah, so we'll be three new hires this year for all teams really and we are back training. So yeah exciting times. Any questions any other thoughts anything want to share. I just have one thing to say. I was going to say one thing about. I was going to say one thing about. Just learning the transition from packaging I'm thinking that I have like a gold project. That looks at the lifecycle of an image that we wanted to. We were planning for me to start executing on then I'm meeting with. Can't until today to talked about research priorities but maybe that's another reason to wait. on that because we could bring that designer in and Tim said also that they might hire a new PM for that. Yeah. So maybe that's like those two things are enough to make me, I mean, on the other hand, if I can pull it off myself, that might be helpful for that. As they land. So, but I just wanted to say out loud, but I, that made me debate more if I should meet on that. Yeah, checking with them and then I'll say also make your recommendation, right, because she's she's still now she's they won't be covering for integrations anymore. And then I also don't know when we're going to back feel the role right and maybe that when doing a liking a monitor to or for months hopefully not. But if it's a go project and yeah, there was a plan to execute this year, let's say. Right, then team, I think they would only to make some tradeoffs because Katie will only be at 50% of them capacity. Okay, this and then the common milestones. Just exactly why I said it out loud. Good job. And then also just to say like, I think it's really great that Katie is like worked with us and then we'll do the ML thing because it has really good. And I'm tired of our team right. Yeah, I also thought it was I just saw them integration step because they had a question in Slack and I thought that was smart to like, oh, I think it's just make sure that we're. Really touching all that it's good it's good though, because I think like we want to figure out how to cross policy. exciting. It's a huge product, you kind of have to do it. I'm leaving now. I'm looking for a complete different area different perspective also. It's difficult to know. No one knows everything, but it's also difficult to be in that context without actually working on the further area. Yeah, awesome. Right, I think. I think that's that. We can let we'll know that we finish without him. We finish the scene. Yeah. And then I'll see you. I'll see you next time. See you. Good. Bye. Bye.",
  "Okay, hello, it's August 24th almost September, which is crazy. This is the CICD UX meeting and there's just one announcement that we have friends and family day on the 29th, which is Monday. And then, hi, on ahead, I'll also ask for us to just give progress on, I think this was supposed to be for Q3, okay, ours, but it says Q2. Does anybody have anything that's like blocked or at risk? Any other announcements or general stuff? Okay, all of, I'll move into the items for PYBITN sites and runner then. For runner, we're running a category maturity score card for the first time ever for runner fleet. And going really well, where I've done four sessions, I have one more scheduled for next week, I think. And we're shooting to be score that complete. So right now we have that for those four participants, we've gotten to that score, so we'll just see if we stay there. And then for PYBITN sites, this was something I wanted to share with the group, because there's a lot of overlap with our groups. I'm just going to share my screen. Okay. So we, in a test report for a PYBITN, if there are child pipelines that are included, then the tests from that child pipeline will be aggregated into the single report at the parent pipeline level. And the problem is the jobs, aka the tests that are in those jobs that are listed here, we don't like tell you if they're from this pipeline or if they're from the child pipeline and there can be many children. So what I wanted to do was indicate which jobs are from which pipeline basically. And I, I, I added this child badge here because it looks like we were doing that within the pipeline visualization, I'll zoom in a bit here. We use this like child badge. I thought it would be a good way to reuse that pattern, but what I did differently was added the pipeline ID because I assume that there can be many children and I wanted there to be a difference. And then I was thinking that you could just click on that badge and it would bring you to that child test report. So it would be like a similar page to this, but it would just be for that pipeline. Is there any feedback that you have anyone has on this? I have a little questions, which I think you both human not yet could answer. So when I come to this tab on the pipeline overview page, I expect the experience to be like very similar to what I would see on the job stage on the job style, right? The next. Now I have added a link here in the agenda. That's for one of the pipelines that's running currently for good luck. And when I navigate through the multi project and the child pipeline like upstream downstream, so what I see is the list in the jobs, it doesn't change until I go ahead and select the pipeline in the craft. Like for example, if I'm in the first pipeline, the job list is not going to have the items which are in the child pipeline until I go and select the child pipeline. Okay. So we don't show the jobs for child pipelines, basically at the parent level. That's what I failed when I checked this in the morning. I'm not, I wasn't aware of this actually, so I had no idea that we have some kind of connection there. So just to confirm when you expand the child pipeline in the pipeline graph, only then we show the jobs from the pipeline in the job staff. If you just look at the badge and see when you know clicks on the on expanding the downstream, it doesn't change. Like the number of jobs inside the tab, it doesn't change at all. It doesn't expand and it doesn't give a new list until you click on the pipeline, like child pipeline numbers to express the name that now I'm looking at the details for the child pipeline. Oh, like that. Oh, yeah, make sense. Yeah, I mean, it sounds like an issue. Something that we have to consider separately. I have never heard this come up. Yeah, it seems reasonable to update it if you expand the graph. Right. What you guys doing is they experience that's expected, but the overall experience for the job staff, it needs to be a bit as well. Yeah. So I have a comment around the, around the usage of the badge for this. So currently the badges on the child pipeline are not applicable. They're just meant to show you the type of the item that it is and oftentimes the badges in the UI are not applicable. So they don't have a strong visual affordance that you can click on them and users are not necessarily used to that. So my concern would be that this link would not be discoverable. And also placing the ID inside the badge makes it kind of more difficult to read in a way because it's together with the child, the world child and makes more difficult to copy and paste it. If you need to if you need to search for the pipeline for example, so these are just like my thoughts on using the badge here. I do like using badge for the child just to show that it's the child pipeline because it's exactly the same what we show in the pipeline graph. But maybe the ID could be a separate link or yeah something that the user will immediately see as an navigation item because we do use a linked IDs a lot in the UI to navigate between five clients and jobs. Okay. Yeah, I can play around with separating it out. I agree. I like the reuse of the badge just because it's already something that we use. And I was thinking of maybe doing a full other like column that includes the pipeline ID. I'll all think about that more though. Also, this might be unrelated but we were actually finding the opposite where we had a success badge on the deployment, the environment page and people thought it was clickable. Even though it wasn't because it was a badge. So I think I know why because of the badges on the pipeline page. They are pickably. Yeah, so I'm wondering if you could style the badge a little bit differently where it looks like it's clickable. I don't know what options for badges there but I feel like if there is like a badge version that looks like it's clickable. And when we made it not clickable, people are confused. Is there any guidance from pajamas about whether or not these should be interactive elements? I think they can be both. Okay, cool. Yeah, I'm trying to think of like issue labels. I guess those are technically labels not badges. But those are clickable. And sometimes not clickable. One other question I had was just thinking about like the idea of having the sophisticated simplicity like thing that's going on right now. I was wondering if it made sense to maybe keep this like the child pipeline test collapse by default because we do that in the pipeline too. Like you have to expand it to show the child pipeline. So I'm wondering if it will be worth exploring, I don't know, a show hide like toggle to show and hide them. It was just a thought. You mean in the pipeline craft? I mean like in this all-moved list. Yeah, I mean like a toggle here. It sounds like something that could be also a filter like a drop down because you would filter by a type of job sorting mechanism of sort. That's for the bridge or triggered jump can be expanded. Like the job that's calling the downstream or the child pipeline that can be a little that can be treated differently visually and can be an option to expand it and go up. Am I just be something that I need to like, validate a little bit too? Because yeah, I feel like I don't feel confident enough to be like let's complicate this by adding filters just yet. And I think you know what what you can do too is come up with just two or three key tasks and then look at the impact on the flow. But I'm thinking that we want to optimize for when they're debugging. Yeah, that's a good idea too. This stop sharing. Thank you for the feedback that was really great. So everybody in the on the call. Like live brainstorming. Okay. I know it's very useful. My last thing, I mean, we can take this ace a sink if we wanted to. But I just feel like I'm taking forever creating issues with the feature that feature detailed template. I feel like there's a lot of different sections I need to fill out, label the ad. I was wondering if people had best practices around making it more efficient. Or yeah, I just wanted to say that I can relate and I don't have any specific tips because my process manual, but I see that we think a mentioned templates and I can also second that is very important to use templates. Yeah, but it doesn't really solve. I mean, it applies some labels actually, some basic labels. So yes, mostly you have to delete what that is. But I'm talking in terms of efficiency because you highlighted that Gina, I would say, it makes you efficient, but you're not very different. For example, it really makes people think about the problem statement when they're filling out this detailed template. So in pipe and execution, when we started to get to the like very heavy battle of that first therefore, it doubled about 3.5,000 issues. So we started to be very careful about like what exists there and what we need to process duplicates and what is not valid anymore. But we started to notice that people open issues with just two lines like, this is what I want, this is how it should be done, like with proposal. So we started to like discourage that and every issue when it used to come to me for charging especially through that body should that's created over the weekend. I always used to make it a point. I still do to like request people to like apply the template and fill out the questions. And that has really made everyone think about the proposals that they're trying to put across. Like what isn't that they're trying to solve? Who is it for? Is it just for that one person? So even it feels very heavy to do that at that particular time in pipe and execution that has like really helped us. I just had a comment that I don't currently use templates. I totally get what you're saying though, Vitaca. I guess I don't know if it's required to use the template, but maybe for me the reason why I don't is sometimes we'll just have like a small bug or something that needs to be addressed and I never want to be barrier to document that in an issue and it can always be refined later. But maybe I'm a bit lucky that when my team is creating issues they usually have a strong why and like a you know like maybe I'm just a bit fortunate in that regard. In terms of expediting I set up a keyboard shortcut in system preferences that will apply the kind of like 10 most common labels that I use. So then I just literally just have two two key strokes and then I have like a the labels and then I could just edit them from there. That has helped a bit. That's really cool. I didn't even know that that was a thing. I can make a like a like a one minute video of how to do it and I'll post it on our channel or on you access something. Yeah that'd be great. Please. Yeah that would be awesome because I remember when I had my coworking session with Vitaca I was asking about like how do you copy and paste labels are there a more efficient way to add the same labels to issues like creating UX themes all of them head the same labels but I had to manually add them so if there's any way to speed up that process. So I use a very bad little bit so I keep like the shortcuts to create those labels that I have to based on the issues handy on my note that. So I just copy based. Very cool. Okay. Thank you for all the conversation Emily. Do you want to go through your items? Yeah so my first item is release held office hours last week which was actually a huge success. It was a surprising how many people we got to attend the first one and during this time office hours we really walked through like how community contributors can contribute like open the floor to ask them questions. One of the engineers kind of went through how to find issues that are available to take on and to centrics on how to get help on MRs like how to tag us in those. So it was like a very helpful thing to do and we're planning to hold another one in September around UX friend 10 which hopefully will increase the amount of like community contributions we get since I know that's a big thing right now. That's the exact thing I pointed out to my product manager. I guess within the meeting that Chinese had an office hour I looked at the agenda. They clearly they highlighted the opportunity for a contribution so well and like I'm not sure if we have the bandwidth to do this at this moment but yeah I'm really happy to like learn from it. They call that meant. Yeah and I'll just some of the like main questions we had were just how to get feedback. So people who wanted to contribute or curious about like when they should like create the MR can they do it when they're still working on something if it's still a draft and how to tag people to get feedback on things. Tips on how to find issues in like the languages they're comfortable working on and all of that and we also had some questions too that were a little topic of how to look for open jobs at GitLab. So I think just be aware that some people might be coming in because they are also interested in working at GitLab and having answers to that. Yeah that's true. I just I'm sorry. Oh and I'm just saying like a closing statement like I hope more teams are able to do something. I put the question what are office hours in this context but I'm gathering that you you had kind of an open conversation with users of GitLab is that what it was? Yeah so what we did was we created a I think is on needup.com just like a release office hours added in an agenda and then the release team kind of advertised done LinkedIn are we kind of went to our older community contributors until then that they could come to this. So we like advertised it out a little bit to just the broader community who would be interested in seeing how really worked to come with questions and yeah it's like open to absolutely anyone who's interested. The only thing we did learn is that it's important to have this figure like meetup or another site and not just have an open zoom link because apparently zoom bombing is like a big thing if you just randomly put a zoom link out there. So having kind of like stricter access into the meeting was important. That's also a such a good idea. I'll have a closer look into it. And then the only other thing is on release for planning and onboarded onboarding focused improvement to our empty state pages because we realized kind of across the board of the empty state pages they're not very like they're all slightly different design-wise they could have more helpful like CDAs for people just landing on them. So this is like an idea we're doing but just wanted to share it out because I think it's helpful like across all stage groups on that get lab. Well then that's it unless there's any questions. I have a quick suggestion for the empty states just like a quick study that I have I don't know if there's probably reasons why you did this way but it would be interesting to see the buttons placed underneath the empty state you are copy and say the station so I think this is how we usually deal with those empty states but I guess there's gonna be like a table here or like a list of items and then the buttons will be at the top but I think usually what we do is we have the action buttons inside the empty state and then once there are items to show then the buttons are placed above the items list. Yeah I think right now all three of the empty states and deployments are at look different so I think the first kind of like getting that CTA the CDAs to be consistent across the three of them getting like a good CTA and I think some of like the copy so I think there's some really simple things we can do to fix it up but yeah the main yeah sure the main thing is yeah like you said I think the CTA on the release page is up in the top right corner which is kind of the things so I think just ending the CTAs in the empty state would be an interesting one to do. So um if that's it I can pass around to VitaCath. Okay yeah so I wanted to update about the insights that I've known through a very recent validation exercise that I it was unmodulated on user testing and it kind of gave some good like beeping to how users are using their username space today and what kind of projects they host under their how many projects they host. So it turns out that users usually put just like small personal projects directly under the under their namespace and those are not kind that are very engagement heavy like it's something that they only interact with by themselves and um that's the reason like they also don't have much concerns around what's the CICD minutes consumption because they might not even be a biophype like for this project and there is no contribution related to consumption for sure for these projects and that applies that there's little or no need to control the mid-consumption and user namespace related settings. I mean we did provide them with certain tasks in the test but it was really difficult for them to go through those setting options which are existing today it was not even something that I had added on the top because something that had been there since long for example we are still able to see the usage code for the projects which are directly under your namespace through the user settings usage code page but the way we land on the user setting is kind of here because today if you click on your profile picture and you go to preferences that takes you to users settings preferences and for me that has been my gateway into the settings and that's how I always land there I don't know if there's any other way that someone else uses and that gives everybody an idea that maybe it's just something that's used for customizing like how it looks for you and like not do anything deeper than that. So some resonated with this thought and the others expected the quota to be controlled through the existing usage code app because it was fairly clear for them that this is code and I'm getting to control my code as it has to be through this data but the interesting part is we never do that like so far we have never done that it only kind of shows you like what a consumption is like but that page has never historically been used for allowing users to kind of set any configurations as far as I remember it's mostly just to view and like consume the information about the consumption. Now apart from that what else? Yeah so this feature as we had already expected is as mentioned that it would be more useful for groups and subgroups. Now I'll give a little background about like why we had started with using namespace so this feature is a cognitive only allow users to control their minutes consumption. Minutes is the resource that's required to run the pipelines. The only allowed them to control this through the admin view and all that is done today is the admins are able to add like standard cap to the usage so they can define like all the groups can only use 2,000 minutes like they cannot exceed that but at a more granular level like when it comes to projects because a group can have a lot many projects it can have subgroups and some groups under that and under that many projects so it gets pretty complex. Now at project level maintainers who are working on this projects they would want some like some sort of control because not all the projects inside a group or a subgroup has the same amount of importance so they would want to make sure that this particular one that sees a lot of contribution from my users and is very critical to our business should always be able to run its pipeline and it shouldn't happen that this other project that's like badly to it which is not as much of a priority to us. It ends up consuming all the CIC minutes that's a lot into the screen. So to avoid that situation we started working on this feature but right like why we were having the discussion on how we would make this happen. Be realized that the moment we like touch upon groups and subgroups this is going to get so much complicated because then we have to take into account like how there'll be the subdivisions of the minutes and how we'd be surface the quota like this much out of this much is used by this group. This is what you're left by this is what you can do with it. So we started with the easiest path available so that we can also validated. We started with using namespace because that's like flat that's just one never like the projects which are directly under your business space and once that happens what once we get good insights from those we decided that we'll create issues and we'll make improvements and then we'll move to group subgroups. I see that we're very close to Daemon Naniya also has to go so I'm having the day questions on Slack as well and I'll pass it on to Nadia. Thanks, Vedika. Yeah so this milestone I'm participating in beautiful fine UI. There's an issue where I'm gathering all of the ideas that I want to work on brainstorming some solutions. So I'm trying to keep it like one thread for one improvement. So if there's anything that you would like to work on but maybe your team doesn't have capacity or anything like that feel free to drop your ideas into that issue and I can't promise that we'll get picked up because also the engineer I'm working with they has a limited capacity this milestone which is unfortunate because I dedicated half of my time to this and I think they dedicated like 20% of their time to this. But we're going to choose like the top impact maybe also easiest to do kinds of improvements and run with that so drop your comments there. Yeah I see that you have a lot of suggestions but let's give it only for the sake of time and another thing I wanted to share is a result from the secret management. Chopstead we done research so it was meta analysis of existing research that we have around secrets and there's lots of great insights that come out of it that you can check out in this issue that I linked. I summarized everything in the issue description so you can just skin through that if you want and one at the point out that secret is our top priority right now for pipeline authoring. Generally as far as new features are concerned our main focus is secret management and the SISC decadalic work that we're doing and there's NMR for updated secrets jobs to be done and there's a lot of overlap there that we're finding with compliance and security so we'll be collaborating with those teams to make sure that we connect the secret management work flow to management of your security policies and compliance policies and so on. Yeah and that on to Katie. Cool I first once just an FYI in case people don't know we have a dedicated product analyst named Nicole who we can make UX data requests to they can help with structuring the kind of the right questions to ask to move bias and implementation in the code for tracking things and then also SISC and Stashboards. they is across all of ops so they also works with many pms so she's got quite a lot of competing priorities so they did request that when we are creating issues to kind of indicate the urgency and the priority but I just didn't know that we had this relationship so in case anyone else didn't and not much else to report for package I'm still covering ecosystem until like November but we're working on some process improvements in package about how we refine and how issues come into the milestone and I've also got to the opportunity to speak to a number of enterprise customers because our pms on parental leave and you might have seen Eric and I discussing in Slack yesterday now that I have those relationships with those enterprise customers and tms I would love to recruit them but Eric all wisely pointed out that you know we should be really strategic these are very valuable and hard to find research participants so I just wanted to have a group discussion in terms of like does anyone have any ideas about any light process that we could use to make sure that we're utilizing these to the most high value research amongst us or maybe amongst Gidlap in general. I mean one thing that I can add is that I've mainly worked for tms or worked with tms to like get access to large enterprise customers I think we maybe have some within like our first look panel but I'm not entirely sure I think it's relatively low. I think it's important whenever scheduling these research sessions with large enterprise customers since there's not a whole lot of them trying to not overuse and you know over leverage that group because you know they are so hard to access and you know we're essentially talking to like the same handful of people over and over again so that can influence the design even if we do see them this like well they're you know they're representing a large company that has a lot of users we tend to just talk to the same individuals from those companies they're representing their set of users. Yeah that makes total sense. I wonder if like if we have some kind of process that we can also track who spoke to which enterprise customer at which time so that we can kind of avoid what you are mentioning well in terms of biasing for three customers or something like this. And and that's why we have kind of set up this in part one of the reasons why we've set up the enterprise company profiles and personas is that we kind of just want to cobble together all of those interviews and touchpoints and then like abstract them away on what if we can so that we can really do the like small business enterprise comparison so just advertising that we have that so if we can get consent through that forum that's linked in Slack to record and then put those in the Dovetail that's linked there. I have it in Q4 plans to begin to like put those together and when we start on that like in a more full-fledged manner which isn't cobbling I'll be like really asking for help on recruit for that but they won't be in Tokyo for. Oh that's all good. I'm just wondering um you know because I have these relationships now and I would love to just speak to these customers anyway but I'm wondering if yeah should we even just have informal conversations amongst ourselves to make sure that those customers aren't needed or they haven't been kind of overused as well as mentioning or should there be a more formal process or does anyone have any thoughts on this? We could start in that um Dev Kelishu just a table where we mark who we've talked to and bring them I have an intention to bring in the one company that I won't name so we can share this out but that will and Hiana and I think Tina has also met with so maybe there's just like an informal table but if it's in Dev Kelishu maybe it's like accessible we can cross-reference it with the videos we have. Okay cool so I'm happy to create that table. Did you say you wanted that on the get-live issue that's about that at the process or do you want to end off tell itself? I think Dev Tail because then we can okay video. Okay cool yeah I can set that up. Cool thank you. Katie can you tag us once you do that too because it's in my to-do is I'll do it. Okay yeah sure. Cool um I can pass over to Will. Yeah and I guess before I talk about my section just to add on to the group discussion I think this is something important that we could bring to the larger UX research team to talk about like logger term strategy for how we deal with this but I think Erica's on board with for on point with like how we're gonna dress it in the return at least. Sounds good. So the only update that I have is that I'm halfway done with my benchmarking city so I've got 10 out of 20 sessions complete I've added the videos to Dev Tail I'm continuing to get them tagged and I have another 10 sessions of around the next week it might bleed over until at the end of next week but hoping to have it done pretty soon and if they're not any questions I'll pass it over to Erica. I guess so I didn't make a little point about this but my Q3 and Q4 plans are kind of set so in the way that's good for my life but that also just you know that if possible like for example with notias like job to be done stuff I can sometimes fold in or like build out a research question if there's pressing needs um so just remember that we're locked but we have some freedom within that structure and then what I'm working on is just the secrets feature survey and that will help us to understand the trade-offs between the developer needs and the SRE slash maybe security compliance depending on who we can find and we're going to build it for quite some time because to reach those two groups we know we need to go to cook on and then we need to have a long email campaign and so I'm sprinting towards that and then also just say yay for the team that Nadia and and Gina maybe in reverse order I are working on a really cool participatory design activity where we're going to kind of map out this secrets workflow so it's still emerging but it's exciting and and I think we might have a new paradigm there. I wanted to ask about that so I know that there are some feedback there that I was trying to provide and I wanted to ask what's the latest time that you need this by because you mentioned that you want to run the pilot next week and I'm taking the family and friends day on Friday so I'm a bit short on time this week so maybe you can just message me let me know what's like the best way for us to collaborate on this or point me to an issue comment or threat where I can jump in and help out. I think you can come in after the pilots we'll use the pilot see if it stands and how it looks more okay okay sounds good yeah with the secret features we might need your feedback I think we're actually good on it with the survey stuff and I think we'll okay okay you've got well I will catch up with you on select about that as well too Erica do you want to do your oh they they're read only. Well did you want to voice it we have for us yeah so um there was a paper posted yesterday in the security research channel and they said it wasn't a good paper which I think means it's like not we can maybe follow it because it's not that it takes to be complex and good but I actually felt that thunder really helpful because it takes a few competitors and kind of talks about their different roles in access and then gives an overview and it gives not in a way that we should feel like concerned like I wanted to be careful like it's not that they've found a new idea about security breaches but they basically go through and explain how in these different platforms also including our competitors secrets might be leaked so yeah so that's not like a pressing need but I think it's a nice resource and I also did this thing where when I first started and kind of saw that security was one of our product focuses I was watching like devsuck up conferences and taking notes that's kind of you really to remember anyway I turned that into a plus white papers and so I'm like taking notes in that issue as well thanks for sharing I think this is exactly what I need to dive into yeah if anybody ever wonders like what do we mean by hard-coast secrets it's like right there so we can I think this will be a good resource for us moving forward yeah I have one one other question for us before we wrap up would we I'm just thinking about the order that we go in when we're going through this meeting would anyone be interested in swapping maybe next time so that people near the end have more time in the beginning seeing more of head knots yeah I think it's a good idea how would put in please search for a student that is in quarantine yeah we can swap the order of design too so I'm doing it exactly the opposite of what it is today go ahead and start okay cool all right I'll do that next time thanks great thanks everyone have a good day thanks",
  "Hello, this is the CI CD UX team on June 15th. The America's thread and I'll just start by going through some of the manager announcements. So hi, on ahead, added if there was OKRs that are blocked or at risk and if we need help with them, but it doesn't look like anyone added anything. Cool, it looks like we're on track. Unless does anyone have anything for that? OK, sweet. We have a friends and family day on June 24th next week. And then high on ahead, send out a message about midyear check-ins and created a tracking issue for these. They are team member led check-ins to assess how things are going from both the team member and the manager's point of view. And then share feedback to help inform performance and development plans. And just make sure to dedicate an upcoming one on one to do this before July 22nd. And welcome Emily to the team. We're very happy to have you. And then Katie says that she'll be in Europe for around two months starting late next week. She's taking a few days off to deal with jet lag. But that could be a good time to set up coffee chats if your time zone doesn't overlap. I know for me it's like the last hour of the day. Anything else in the announcements that anybody wants to say? OK, I guess I'll just say thank you for the welcome and excited to be here. Oh, yay. OK, so I'll just go. I have more of like updates on my side for pipeline insights and runner. I'm going to go through them quickly since there's four of us. The artifacts page research that I've been talking for. I think for a week now about is finally complete. So we're moving forward with creating the page. It's a project's page for artifacts that list all of your artifacts in that project. And we got really great feedback. And so I'll be making on updating the list view based on that feedback. We also sent out a survey in Erica helped so much with this until to Caitlin. We sent out a survey to validate artifacts jobs because we have no data really around. The jobs related to artifacts so we just wanted to quickly like get something. So we did the survey route. We've gotten 80 responses already, which is great. So we're just going to cap it up that and start analyzing the data there. Erica had asked if I saw any differences in self managed versus SAS responses. Because Jackie, who used to be the PM for pipeline insights had asked that. I haven't looked into it yet, but my assumption is that self managed folks are focused on storage related and management jobs. When it comes to artifacts versus SAS is probably focused on using artifacts to debug jobs and pipelines. Because we have like automatic cleanup for SAS. Beat up. Do you want to voice your question? Yes. So I was looking into build artifacts as a category and I know that both job and pipeline artifacts are a part of that. But since we had stopped using the world built many different parts of the product is there a plan to maybe rename this category. Rename it to build artifacts or what do you mean? The name currently is build artifacts. Is that a plan to stop using the word build because that's confusing to many users. And even internally, it's a very confusing term. So just wanted to ask about that. Yeah, we don't. There is there's no point. But I completely agree there's been so much confusion about if artifacts means like packages or build artifacts. So we need we need to do some type of clarification there. Maybe. I get a lot of we call them job and pipeline artifacts. So there's potentials are just call them that. Rather than even build artifacts. Right. Okay. Well, do you want to voice your question? Yeah. I guess going back to Erica's question when you mentioned that there were 80 responses. I haven't really dug into the data. But how many of those 80 responses were self managed versus SaaS users? Do you know that off hand or have some. And so I don't have I don't have numbers, but when I was scrolling through it looks like about half it was half and half. Which I wasn't expecting I assume we would get more SaaS users. Yeah, usually we've had a little bit more difficulty getting self managed users. So that's that's good that there's more of a balance this time around. All that you know next time we meet all. Bring like the actual numbers and is all exported from. Quaterics to. Okay. Well, I have a question for you that. Ideally. What is recommended like how much of a balance should we keep the sample size for SaaS forces self managed. I know that in our direction as an organization. I don't know if it's a very important thing for us to focus more on SaaS. But at the same time, we also want to understand like. Why self managed users. I mean, what would it take for self managed users to become SaaS users? Any recommendations there. Mm, it's a good question. I think like I mentioned before, we've traditionally had a lot of difficulty finding self managed users. I think at least if you're getting like. You know, 20% self managed when you're specifically targeting a mix. I think that would be fine as long as it's at least 20%. Anything lower. You might like want to like talk with your team about and kind of look more closely at before you. Okay, I say, this is what we found from. Self managed users. But it depends on a lot of different factors. Right. Okay. Thanks. Yeah. I promise. Okay. Last thing was the runner list view. I was also it took on three research projects this month, so I'm which was a mistake, but. This one is the last one. So I did unmotorated research and so on. We've got that and it was purely quantitative data. So I wanted. Hold sessions and I'm holding sessions with customers to get qualitative data to make sure that they can complete their jobs with the new view. And I'll have some updates around that. And we added an upgrade icon if anybody has the upgrades within their area. So this wasn't specific to get lab runners, you would update the version and now we have an icon to represent that anybody. Emily. So I don't have too much to update on just like the next few weeks in 15 to my focus. Really be on onboarding. So I think I've sent coffee jets to everyone on the team just getting to know like the release area really well and all that. So that's going to be my focus for the next little while. Hi, Anna has also given me like two tasks to help me with onboarding the first one, which I'm going to tackle. And of this week, start next week, which is really deploying a project and creating release season get up. And I will be I will be we're chatting gene and I will be taking like if you notes throughout it, I could capture some of like the screens and what the flow looks like for those interested on but yeah, but focus will be kind of just getting used to the journey. And I see will because the question. Yeah, for the first task, I'm definitely interested in whatever you learn. So feel free to share once you have gone through that experience. I'm just connected with so many teams I haven't like dug in deep to understand like that end to end experience so. I'll be interested to hear how your your process goes. And then for the jobs to be done task, is that a plan jobs to be done study. So that one, I'm actually going to get into I have to do task one for so I ultimately haven't read into this one too much. It'll probably done in like the second half of 15 to but. I don't actually know I have to read into this one a bit more I can link. I'm just going to ask you a question with hi on our there's a bit more detail so you can read into that. Okay. There's the link to. The conversation around it. And I think the point of this is really just to I think it was one of genus first activities to onboarding onto the team is like a good onboarding activity. So we can do that. So we can do that. And then we can do that. So I think it's going to be to go through the process and ultimately and growth. We didn't do a lot of jobs we done because we were working cross stage. So it will be my first job to be done here at get lab as well. So kind of just get in process and that. More about later, but. We use it to essentially just prioritize like how researchers are aligned to different projects and how we prioritize the work. Sounds good. I'll keep you up to date with this. I expect to finish the first task pretty quickly. So let you know like give you a heads up when I'm finishing that up and starting with these so we can figure out how to go. Okay. Sounds good. I think that's it. Mine. Okay. I have a lot of follow points from this discussion, but I would like mention those points and context to what I'm about to share. So I recently wrapped up a foundation research for PIPEN execution. I created the job. For continuous integration when I started off, which was about more than one and a half years back. And in the course of this time, what I realized was it was very much based on my very. Like initial understanding of what those teach group was all about and what this whole thing is like what verify stands for in the DevOps process. And I understand that the jobs to be done the core jobs to be done is not something that kind of evolves, but because my understanding evolved of those jobs, I thought, by not like take up a very basic foundational research, which is not very much tied to any specific area in the three categories that we look at. But something which is generic enough from where we can extract the core jobs to be done, the high level ones. So we did that. And because I deleted the history of my browser, I couldn't find the issue. I'll do that and add this here. But in the meanwhile, I, so will you mention that you have taken, like you have kind of did a few JDBD studies. Now, when I went through the whole process, when I read the description and when I wanted to like do things from scratch once more. What I figured was in the documentation we have mentioned that a problem validation is a really good way to go, but from the recent problem validation that we were engaging within the pipin execution team, there are two focus. They were very focused on the specific capability of your specific requirement. So I was not able to get much from that. And that's why I connected a whole new research and with Ericos help we did this in a very different way, which I have mentioned in the YouTube video that I've added. And this is the outcome from that. I know that that's a lot to consume and provide feedback on in this short while, but in case all of you get time, please look at the video and I would love to hear your thoughts around the process. Because I'm also planning to write something about this process and like document it and publish it as a blog. So any feedback is helpful. Nice, thanks for sharing. I'll also add a link to the issue as I find it. You can see that it was all for me for today. Do we cover the April threat, so. How much time do we do have a lot of time? Well, well, do you want to cover your stuff first and then we'll like go back up to the effect that. I did have a quick question for Vithika. I opened up your merger class and it looks like you know you've added your jobs to be done to like a more extensive yearnl file. I think when I've been a part of some of the other jobs to be done studies. They typically will also like. I think add a section of like a product direction page or category page and like add tables with you know the jobs to be done that have been determined based on the research. So that you know once you do like a category maturity scorecard, you couldn't like fill out the scores. Are there any plans to do something like that? Yeah, so the reason I picked it up was I was working on the maturity plan for the categories with our PM. And when we laid down the plan, I figured that for each one of them, we are very heavily relying on the job should be done and the other ones that I don't have enough confidence on. That wasn't a good place to be. Uh, yeah, so once things are finalized and based on that, we will kind of make plan with the research like what's research is. We need to do in the upcoming months like the very close milestones that we need to hit with the maturity. And we usually, I mean, this is how I have been documenting jobs to be done specifically and we don't replicate that to the direction page. We only mentioned the maturity plan. So this kind of shows up in the pipeline execution you a job stood down page that I've set up. And I mean, I myself not aware of there's a different way of doing it. I'm based in the link here for the page where I added. We do the same on the runner and pipeline insights team to document the jobs. I just added that will. Okay. With the links. All of the had a bunch of them, they did not like some of them just never had just to with anything. They were very hypothetical and they were just one person thought to God it as the DVDs. So I wanted to get rid of those as well. Okay. So just to clarify like are. So once that you like the high level jobs to be done are those like mentioned on this link that you just added. Yeah, so the. How they're documented is the high level ones they appear as a once with a more bold title like the shot and one and following that are like what are the sub. And if you can also point me to the researches that you have kind of that's like and also get inspired by the process. Yeah, yeah, can we to the. I'll do reminder, just so I don't have to try to find that in the middle of this meeting. So yeah, I can talk briefly about some of what I'm doing in release within the UX research thread below. So Emily for your. Information. Erica and I have included our prioritization issues. So minds. Right here of highlighted it within the doc. And I basically have like all of the research that's going on in the teams that I cover so I cover all the enablements and then roughly half of ops and Erica covers the other half of ops. And so I've tried to specify to the best of my abilities like which projects there's like a column kind of in the middle of that particular prioritization issue. That says. If certain work is connected to release. So there's about three or four projects in total that are listed within that table. So some of the things that I'm working on currently I've got a research epic for my usability benchmarking study. I've made a lot of updates to that just in the past couple days. So I've added a timeline checklist to that epic. And I'm also going through a set of tasks that I worked on with hyena and Chris. And then. I'm also in the process of making a UX cloud sandbox. So I'm following the handbook documentation. And that project that will eventually create will be used for data collection. Taking users through very specific tasks for release. And that study. I have as close to solution validation study. I think within the past week or so. And then linked actual insights. Actually, I'm going to say issues to that research issues. So if you're curious to see what came out of that work. And what's on the backlog. It's all there within that link. And then finally Chris the PM for the release team is wrapping up interviews for their project that kind of covers a couple different stage groups not just release. So it's focused on Kubernetes deployments. And he's conducted about six or seven user interviews in total. And hopes to have insights later this month. I'll pause any questions on any of those things. I know we put some time aside next week to the Open usability benchmarking for release. So in preparation for that, I'll just make sure kind of to read up on this from any like specific questions along like. Kind of I noticed this is going into like September probably when I'm going to be fully onboarded. So like what I can need to help in the featuring all that but this is great. Thanks for all the links into this and how definitely took a look. I know Chris has talked about the interviews been doing as well. Yeah, I think one thing that might be useful to check out. Within that epic is I went through like a sink mural activity with Chris and hyena. As referenced in the comments section of that epic and we spent about two or three weeks. Going through different parts of the study set up trying to determine what tasks we were going to do. Who we were going to recruit what metrics we were going to look at. So that mural board is linked and available in there. So I think that would maybe be a good thing to look at ahead of time to. Awesome yeah I just is the one that's like part one part team. Yeah yeah so I just I just want to. Awesome. So I think that's all that I had. If we want to go back up to the apric. Stuff. Yeah I think so. They also had already when they have the meeting they took a recording of it if anybody wants to take a look at that it's in the channel. Katie completed a solution validation for container registry cleanup policies and they links the dovetail and it allows the user to set up roles or policies to delete items from the registry to save storage space. Their them findings were in line with them assumptions, but they did discover that users want a way to make a template of their policies and apply them to other projects which sounds cool. Should I read like the whole conversation after that or should I just move to the next. I can move to the next one. Okay. And 15 to there'll be implementing a feature that will release to a small subset of users and monitor with snowplow the reason is despite two rounds of validation we're still not certain if this is disruptive to people's work close so we are taking a cautious approach and this will be. Katie's first time at get lab doing a phased roll out and she'll keep us posted. And finally she's oh no that's not true she's consistently hearing from customers that the integration with package and release could be better. An example of the feedback this captured in that link. Will and Emily that might be good for you to look at. Okay. And yeah they said I would like to work with Emily to share my findings and see about improving things. That would be great. I know I've set up some time with them next week as well. So we can check it out this. Yay. them focus for 15 to will be mostly research heavy and will be improving the package detail page and then she'll also attempt to implement the missing front end events tracking so we have better data of how customers are using the product. That was in sure I don't know if anybody had time to. Think Katie either recorded a video or shared in a previous UX meeting but they found all the front end events like. Through the browser and tracks those and that was really cool. Especially if your team is lacking to the geometry right now that might be a good thing to look into I know we we are on runner and pipeline insights. Not yes said that this milestone and 15 three should be focusing on pipeline components mbc as we're starting to get as we're getting ready to start the implementation 15 four. And we've recently added the guidelines for in product reference information using drawers. To pajamas and that's a good thing to check out as well. I think that. Lots of links so if you have time would be good to do it. Yeah, I'm going to share the issue that's created by. By the product manager of package for those snowplow tracking events. I'll be sharing that with my PM as well because. Yeah, that. We are also facing some issues of making a decision regarding filters and similar functionalities where things are not so black and white. I mean the insights are not so black and white that we've received from the validations and still we have to make like we have to proceed in some direction. And we have to do it cautiously. Yeah, that definitely would be nice. We're seeing like similar trends on both running runner and testing and also even for purposes of like investigating bugs for developers. Sometimes it's really difficult to recreate the bugs. So having more like insight into what was going on with the event at that time would help them move faster. Well, do you want to read through air gust of. Sure, I did notice that we didn't touch on this general update. I think Erica must have added this but. I was like we're seeing a higher overall no show rate for sessions so just ahead so. I think this might just be too to like seasonal variants so it may be. Related to like you know, a lot of people are out or you know have holidays or have you know summer vacation or you know whatever it is depending on where they are in the world so. I just want to provide heads up about that. I don't know if anybody's noticed that with with any of the studies that they've done recently I haven't noticed the ton. Erica and I discussed about this I think the previous week that this has been happening a lot for us. It never happens when we're actually through platforms like respond in but if we rely on the our internal recruiting process. And out males and like nobody responds to them for a very long time even when they do it's like pretty much after them whole analysis is over. And I am not very sure if that's for. That's that's due to seasonal variants but I mean that's how I have been trying to. The reason by telling it's because there's some hindsight bias that's going on here that I'll maybe it's what it's because this was going on they didn't turn up because this has been happening throughout the year and our when we internally try to record users. The reattach which we get a positive reply it's always pretty less so I think we need to figure out what's happening there. Yeah, and I know that we talked about it, which is like the research team last week and some of what came out of that was that, you know, respondents and like a lot of reminders and also has, you know, that panel of users that are more likely to, you know, apply and attend for the things that they sign up for. So there's been kind of some talk within the research team about ways that we can try to remind people a little bit more. So like maybe getting Caitlin involved and reminding them to just send out reminders or if you have the ability to like have the list of emails. And then you can reach out to people before sessions and say like, hey, just ahead's up, you know, sessions going to happen in 24 hours. Can you confirm that you'll be there or you know something like that. Right. Yeah. I definitely don't send reminders. So maybe that should be a practice that we should follow. Same, I don't either and this I can for the artifacts research that I did get a few no shows and some of them like if they missed it, I would email them and be like, oh, sorry, we missed you do want to sign up again. And then they sign up and then they wouldn't show up to the next one. We're done. I had a afternoon a few on morning and to these on growth to where someone wouldn't show up, I'd email, they'd reschedule and then they would show up to the second one. Yeah. Okay. But I can provide some updates once we know a little bit more. I think Caitlin and our other new research ops coordinator. I'm going to try to work on some of that to see if they could build some things into their current workflow to take into account that people need to be reminded ahead of time. Because I think there's a way to set that up through calendar. So that it just automatically does it instead of having to remember to do it. But I'll provide some updates once I know a little bit more. Any other comments or questions for I go to Erica's points. So let's see. She's asking if anyone can link and add the studies that they would like to be included in the verify and package research registry and synthesis and this issue. She's 1738. they is also asking to update studies in them prioritization issue. And then some specific updates related to them prioritization issue. they closed out the ops product direction survey where they went to cube con. So she's included the link to the report there. And has a link to the a sink discussion issue. And I've looked at that report to us very detailed. So I highly recommend checking that out. And then she's got a couple of read only updates. A thing else. I'm not good at pivoting or transitioning. Nothing goes. All right. Well, we get 10 minutes back. So yeah, good rest of your days. Bye.",
  "It can record. And we don't have a ton of items to get to. And I might be able to do one that might be fun if we have a little bit of time. So corporate events, I think I saw a little, I put this in Slack and I saw a little bit of noise around it, which was good. You know, the nutshell here is as we've kind of restructured and tried different things. The event support that we need is as nailed down as it needs to be. So the current tactic that we're going with is go to market team, signs up and kind of sponsors that event. So you support it as a PMM, your campaign manager does the campaigns for that event, et cetera, et cetera, et cetera. I don't see anyone in the, maybe they're comments in the issue. I don't see the header updated yet. I thought we had in Slack sort of farmed each one of them out. That's so I guess the next, so it looks like Ty put in some folks. It looks like this looks good. So that, let's see, needs support from GTM teams. So I guess the ask would be to work with your GTM teams. So let me ask it, I saw some Slack, I think, at R Slack. But did you all, were you all able to connect with your GTM teams? On Slack only and on the issue actually. Yeah, so I'm not in real time, but it sort of speed back. I got one person responsible, so it may end up sending you being you and I just pick in the one that we want to do and then they can back us up. If we don't get anymore feedback. Yeah, do you all, does anybody have like a regular sync still? Are those all been canceled or is there? Okay, so we have a real sec. Yeah, we're on like the two week cadence. Okay. Yeah, good ops has been canceled, huh? After the in-event. Cool, so I'm just trying to catch up with the thread. So it looks like maybe a platform on reinvent. CICD on Google Next and get ops on coupon. Is that sound right? Yeah. Yeah, that's where we were last to hurt. Cool, so then I think, I think we can help the core events team. They do a lot of like cat hurting and you know, keep on tracking people down. So I think if this team can take the mission to like try to help track that down. So if you, you know, get the commitment specifically from your campaign managers. Hey, like we're we're signing up for a coupon. You know, can you comment on the issue that yes, I can commit to this. Excitor, et cetera, et cetera, et cetera. You know, so that just so that they can get that that event support. But that looks good and I appreciate. Thanks for the link to you, Simon on the rules. Product announcements. So I appreciate, Brad, for adding this. I probably should add it, but. I had two questions about that one. One is over what time frame are we looking at. So so in theory, this is this could be the same as the get that 14 lunch where we're saying basically since 13.0. What has, you know, what kind of big improvements have we made. I'm not totally sure of all of the kind of the interplay here, but I think I think this is kind of the general just of this assignment is okay. We have commit coming up. We have some amount of like we have a stage of you know, a metaphorical stage to say something to the world and we'll get some amount of press attention because we're having an event. Right. What kind of announcements do we make? I get lab it's really, really tough to make product announcements because our entire roadmap is completely public. Yadiyada yadiyada. So the thought here is to. But also because our entire roadmap is completely public and because we ship these tiny little NBC's and a new feature will come out, but it'll be not really usable. So it takes a while. So if you look back over the past year and say, okay, like what started in the last year, but now is really like a full-fledged feature, what would we would be coming out of beta, right? Like if we were any other company, then we started a beta program in this last year, what would be announcing is GA at commit. That's kind of that's kind of my thought on it. I see a couple of head now. So what do you think, Sydney? Yeah, I think I was inclined to group some things to say like vulnerability management because there were tiny little NBC's all the way along, but if you look at over the course of the year, we went from this to this. It makes more sense to me to highlight that than in any individual little thing. And then the other point was. And then we had 14.0, can we just can we reuse some of those things and plug them in here? Absolutely. Yeah. Yeah, and vulnerability management was was one of the, you know, the features out of, we looked at 14.0. So I think, yeah, this is like, let's take this stuff from 14.0 and then maybe add a few more so that we can hit kind of different areas of press or give, give our PR team. This is kind of like input into our PR team to give them fodder to go out with. And are we trying to hit three per stage? Yeah, ideally there's just, you know, three, you know, three times per stage. And then I think y'all made some good notes that okay, we need to add managed. So I see manage in here that's excellent. And integrations, we don't. Well, actually we have some pretty exciting integration. So we have the ju, the ju is stuff is pretty good. And there's a couple of things and integrations with the S code that could be notable, but. I can I show what I did for S.C.M. just to get some general feedback on it and if someone hasn't done it, maybe that'll also. Okay, I was going send this direction here, which was thinking, you know, a lot of, and then I call them key iterations, the sort of specific things. I listed a few specific things like three or four per each as key iterations, but I called them something overall. So we would have like one thing to call them. There's several key iterations aligned with a get a little cluster. Several that roll up to a research based user experience. And I was thinking like. I was shy and calling out UX as a bucket except that actually that's that's super typical. And when those 11 or anybody who's rolling stuff out, they almost always have UX is one of their three things. And then they mentioned a longer list of stuff that aligns to that. So I, you know, forgive myself for doing something that first I was kind of like shy about calling out UX and and bucketing some things there, but actually when I looked around, that's pretty much how everybody does. I've been in a big launch in terms of the excitement level that also like I went in thinking this is just a two. I guess part of it is like I feel like we're almost fixing something that's broken and that's not something you want to shout from the rooftops about but actually someone that's really cool. So I gave it a three excitement level. I wasn't sure exactly where to place things. But I figured I wouldn't like the Cindy's comment have something that's a one here anyway. But I the only way I could make sense of like the iterations was to to bucket them with something. And so that's kind of I came down on it. Yeah, I've done a similar I've done a similar approach for monitor with incident management. And there were a bunch of things that we released. Although the core of incident management was part of 12 or X seconds. So I did the same with the address. Yeah, I think that makes sense. Yeah, that would help me with the plan stuff as well because there are some things like epic boards that people have been asking for for years now and they finally have and that's great. I think you know like the the milestone burnout charts. That was a two and that's that's helpful. And I was looking through features and you know actual monthly features that were released in a given month. I mean, that was one of the more important ones, but it's not very exciting, but we can bundle that with some other things and and up level it. So that would be helpful there and also Brian about the VS code thing. And other cool thing about that is both of those VS code integrations that hit the same month or community contributions, which is also kind of cool. Is is that is do we have VS code listed on here. I didn't add it, but I can I can add that in. I've got good notes on that. The extension that had been around for a while became official was kind of one of the things, but there's some other stuff that's a little juice here. And that I think that's a that's a pretty good note as well. We did that with like our our terraform module right there was like a community module, but now it's officially supported so that's that's the you know that can be the line yes or no, whether you can use the thing. For a lot of businesses so I think that's good one to. I think our, we've measuring excitement levels one through and three is. I was I was looking at. Maybe a me you as a metric to perhaps measure whether customers have started using that are interested in using that or not. We probably may not have it for everything, but at least some of it, but it will be have it, it's probably something we can use there. But of course, we need a benchmark level of what MAU we want to call one to a tweet. I think it's a little bit of a judgment call. Like, you know, for example, a core mac was saying, like epic boards is something that was asked for for a long time. Now, depending on how it's asked, or maybe that means it's exciting or it's not, right? But usually when something gets a lot of upvotes or when there's a lot of demand for it, then I would bump the excitement level and something like that. Certainly, it's something shifted a lot of people are using it. That's another good measure. I don't know that any one of those needs to kind of be exclusive. I think it's a bit of an individual judgment call and just like what is, what is your feeling based on, you know, could be anecdotal like you talked to customers and they're like really hyper-excited about this. Or, you know, maybe not, which is okay. I think it's okay. Okay. Our responded to my comment about why we have the low excitement ones and just said something about having them sort of stacked ranked. Yeah, I might even constrain us is we're not going to highlight low segments, but we want to see the full spectrum, especially in the case we have slim pickings. I'll turn it off if there are double j's and entries, then we can stack rank. So that's kind of also helpful there. So if we have three for all of these, that, I mean, it looks like we're going to get there and that's going to be quite a lot of features. I might constrain it like this. Maybe you can only give out one, one, two, or three. And, and it treat as a stack rank rather than just like a raw excitement level that doesn't necessarily somebody's one or somebody's three may be more important than somebody else's one agree on that. And I had, I started by just looking at what was considered a key feature over the past year from from create and, you know, there are at least a dozen things that I'm not mentioning that could be on the list. Like I could give you a 10 things instead of three. Give it, give this just like the rough like pass and like a lot of things recently have and moving between docs and spreadsheet and the spreadsheet to docs. This, you know, this might end up or some iteration of this might end up in a spreadsheet where we then try to stack rank and PR is like okay, out of all of like the, you know, there's maybe 10 boxes here, three, each out of a 30 items which are the top five. Actually, let's just, let's actually just do that. Let's just do this like top top five overall. We did this one is a PR driven to some degree. What about like the new pin christie's product keynote at commit or we that have telling this with that? So this is exactly where this is headed to. Is the idea that during the product keynote and then ideally during kits sits keynote as well that there's some level of like an announcement that we make that then the press cares about that announcement. Again, really tough to do at GitLab because it's all been around for a while. So this is, this is our time to head to that area is to say like these are the announcements of the event. And yeah, maybe it's some things that have been around for a while, but like now they're at a level of maturity. Now, you know, there's, there's reason to be excited about this. So that's kind of where it's coming from. So I put a top five overall. Maybe we can just, you know, you should reserve a spot for plan, but I'm going to have to go through and do some aggregating after this. Just go through the list of what's shipped in last 12 months and re-buck at that. So maybe we can do like five minutes kind of like um just just time box and kind of look at these here. What do you all think is a top would be a top five out of the list of, you know, 20 or 30? I would say if UX does end up being a flag that we fly, that's an easy one because there's more than just maps to the creates stage that would be considered UX improvement. What else? We need to have a security one in there. We could have either vulnerability management or um, I was when I've been struggling with a little bit as I'd like to kind of group the some of this proprietary scanning. Um, without making it sound like they're scanning improvements because that sounds like they weren't good before. Yeah. Let's let's go with vulnerability management for now. I, I know I would agree on that one and I would say that some of these we did do some pressure on and the the fuzzing acquisitions was one thing. So we might not be able to do that. I mean we did the fuzzing acquisitions a long time ago. Was it within the past year or longer? I thought it was within the past year. I think it was in the last the little oversie expanse or so. It was last summer, right? So it was like last spring or summer and then we did another set of press around the integration of it because the PR team liked all the I mean we got a lot of attention on the acquisition. So they had me do a follow up on the integration. I just feel like it's kind of worn out now. Right. That's what I was saying is that the fuzzing we already did press on through probably will not get into the chomp at that. So I think let's go with vulnerability management. Unreviewed. Well we also have in terms of proprietary stuff we've also got we replaced one of our scanners with some group and we did we have our own proprietary DASTS scanner now that's in it's in beta called Brouserker. So it's our answer to scanning single-page applications which represent a unique challenge. I think I mean if I could have to it would be vulnerability management and Brouserker probably. Okay. Some bref that get picked up as a online item in a couple of pieces about get back 14. Which between the proprietary ships of the vulnerability management which would you pick as number one out of those two? Probably vulnerability management. Okay. I'm just kind of scanning the list. Our probably say like Kubernetes agent is something that we've made a lot of investment in. Although. I think the Kubernetes agent and from an integration point of you perhaps the DERF of integration I think we have a lot of customers actually using that already. So I'm going to agree. I'm just going to call those get-ups and I'm going to put like KDS agents plus S-E for integrations. It's like a bucket of capabilities that is probably worth talking about all. Oh well let's you know let's try to this is due Tuesday I think we want to try to get this done. So maybe just review this kind of like let's just kind of pick top five maybe something from plan and something from CICD. I plan editor's definitely an option for CICD. Yeah there might be a sort of a no sorry there might be a value stream analytics story and there too. If we aggregated all it's a bummer that customize the value stream analytics is 12.9. But yeah there might be. Cool then I'm just going to I'm going to ping you on this core back to add add a top five plan. Cool I think. person that's a pretty heavy pretty heavy list just looking at it you know it's pretty cool yeah that's okay. Yeah and this is something we don't do which I agree like we don't we don't often stop as a company and just kind of look at our winds. I was really I don't to jump did anybody miss what do we call it assembly the assembly. If you missed it I recommend I last because like what assembly reminds me of it like when I was in like a early-career child in grade school. Yeah. They'd have this big stack of like carpet they're like squares of carpet and you go like you go into the gym and you pull off this square of carpet and all the children would sit on a square of carpet and that was like you'd have an assembly. Yeah. So that's better to put everyone in the gym or like convert the half cafeteria gym you know take away the big barrier between it so that we can have more room and then take those carpets that have never been cleaned and just you know sit on them and it'll be a regression. Besides it's not from the goofy name I thought that and of course we can't talk about any of it on YouTube but person that was like a really awesome look back at like here's some winds and here's some exciting things that we've done and so I think we just need to get better at this as a company. I think this exercise and opportunity that I think we do a little bit of that but this is doing it more. Sami you have some questions here. For a leader. Yes. So in the competitor sheet I was I mean I've already had the features for CD configures as well as monitor getting it reviewed from PM. My guess was or my understanding was that we wouldn't have the same set of competitors that we're going to compare to but the right competitors for that particular stage. Right so for example for for monitor we may not have like a get-up or something like that we would have monitor based competitors or was that the understanding because we identified that entire list of competitors and partners for every single stage I thought we were going to be using those competitors to compare against. So I just wanted your input on what because I saw some of you have compared with the five competitors already listed over there. They may not be relevant for example cloudbeats may not be relevant for monitors they did all. So that's what I wanted to check with you. Yeah that's the case I need to go back and look at this spreadsheet. I mean if the competitor does not apply that particular stage it shouldn't be on that tab in this spreadsheet. So are you saying that you see competitors on that tab for a stage where they don't apply? Yeah I think it was copy-pasted. Yeah so each sheet has has a DO at last you can get up Jenkins, J. Frog and Cloudbeats. Right so that may not be relevant for monitor for fun figure it would be a different set of competitors that we need to include which I think we already identified the while back I think we identified top three competitors for every stage. Yeah we feared that's only tiered them right tier 1 tier 2 tier 3. So this this where we are right now is we're looking at just tier 1 competitor. That's probably our current pace then because those are all tier 1s. We're just focusing on tier 1 right now. So if they don't apply to that stage don't worry about me make sense. So just pick the ones out of that list that apply to our stage. Right I just you'll be just use the tier 1 competitors and if they're not if they don't are not applicable then we don't have to fill out anything for that. Yeah for now but then we'll move to tier 2 and tier 3 then we'll look at those other competitors. I mean to add a line item for good lab as well because I think that's the same over here. Yeah we should I think you asked me that before right or someone asked me. I'm just looking at it for I announced. Yeah yeah we're good. Yeah because the way without it you're basically putting all the things that we're better at. Yeah for sure. Yeah right so I did. I deal with when we pick like the 10 to 15 features. Those some of those were like get lab only some of those were like get lab didn't have. Well I thought it was supposed to be not none of them should be get lab only right it should be all through the lens of. Yeah yeah exactly yeah it should be like market lens so an ideal world most of them are like this is what I'm shopping for this solution. Some like some of them might be get lab only. Some of them might be like competitor only so that is so somebody would look at it and be like this looks honest and trustworthy and like a accurate assessment of the market. Yeah yeah yeah and then yeah I think some of this just to kind of reiterate for this one we're only doing these five competitors so we'll do the other ones in a later stage and then for if the vendor builds like builds themselves as a platform then we should specifically call out that there you have a zero so GitHub builds itself as a platform it sells itself that way same with Azure DevOps same with J Frog. Well they all do for Tier 1 except Jenkins and Cloudbees. Yeah Jenkins Cloudbees is not you know but what is it you know Azure DevOps GitHub had Lassian and although at Lassians we still need to figure something out there. Right is it matter of fact that I go back and add a Lassian because like based on conversation I think we have to do it I don't think we can leave them off. I don't know if they'll look at this version. Yeah it is Lassian is on that sheet. Okay yes I went back and added them because based on a discussion like my my I wanted to take them off completely and just leave them as a partner but we can't do that so. So yeah so for for those platform you know we should call out the fact that they don't have any monitoring if they have like zero out of 15 we should call that out same with the configure stuff or this you know any those kind of capabilities. Maybe I think if we took that step further to be good to even describe it in the way that doesn't just say our product stage and use like a value based description of what monitor and configure mean because otherwise we're just no that's not going to do any good force. Have you guys seen the new infographic that the design team has come up with? Yeah. All you have it. No. Do we have a link to that quickly? Well you know I don't know. Or you can throw it in the notes if you find it. Let's see who can find it faster. You're taking it with the amount of tabs on how open. Oh my god that's one light it's probably will be made. So this is why I send everything to a Google Gmail because the search is so fast so I literally have a folder where all of my get lab things go and I can just search for infographic. We could look at first. I found it. Oh. So this is the one so there's two. This is the one for the single two comparison. But then for the but it's gone look the same but then here's the one for I just put it in chat for you guys. And here's the one for the platform player. So we made it. We made a couple of decisions along the way. So one of the decisions was to just go with stages. It would have been like a whole nother layer of okay if we don't go with just to get lab stages the 10 stages. What do we use? That has to be determined, reconciled, all those kind of stuff. I don't know how long it's been since we updated those pages have like not been updated. But we kind of are just trying to move towards some sort of towards some progress. So I concur that maybe configure and monitor is not as descriptive as it could be. But the place where it will so actually I'll share one thing. It just seems like the right time to do this, right? Manage is is even less descriptive than that. So so the way this is the way this is getting built is off of a data model. So in theory these could change or be updated where the underlying data remains the same. Right? We're separating like presentation layer from data layer. So that's it's not going to be as hard to update in the future as it currently is where it's a messaged spaghetti code and it's like really really difficult to update. And then the other component is then not in this iteration we need to get something live. But in the next iteration you'll click on verify and it'll go down to the list of the 15 features. And that's where you can describe what the heck of verify is. What the heck of manage. Okay. So we'll have another two that because I was going to say you're not hate me for this but like we're you know what you're being sick of go about the same problem we have. There's been the audience coming in and not knowing anything about these specific stages. Yeah that was one of the big things it's like okay 15 15 features what what is what are they why do I care you know yeah to say that means nothing to me looking at this this page but quick questions. I could be a dummy down there. No we're on top of it. Actually if you read the thread I brought that up but they said we can't do it like right now which I understand but it's small thing what do you just think about the colors. I usually the designers handle that stuff they they like I like I like the fact that there's no red on there. Oh you like that that's what was going for you like it's not it's not obviously negative so we're saying because it really does lend itself to being a comparison more than a competitive piece because of that because there's no you know these folks at 37% are in the red and they're terrible they just have less green or we would have less green so it's not it's not being worse it's being less good. I think that lends itself to working with you know all the co-optician situations we're talking about. Yeah that's that's a goal is we want we want to generate a helpful asset for the industry. This is not just like marketing skill this should be like a helpful page so yeah the goal is to have a comparison comparing DevOps tools not a you know this is all of our competitors and why we're better. I think it's one of my competitive mindset and for me I'm like let's go hard let's put some ran on there and go on that's just the way I think right so but I'm going to have an effort now we're going to stick with the green they work a minute we stick with the green and see how it goes but I don't know I was feeling the creating yellow too. It is easy on the ice it's you know there's something to be said to that so. Yeah cool well we're almost up on time but I did want to share one thing that kind of came in like basically Friday it was like late Thursday for me. So this this is another thing a lot a lot of these things that I don't have a katana context on. I just know like we're just trying to up our game so like hey let's move fast and do the best we can do. So this is a little bit of a messaging framework and then the assignment that was given to me was to to fill in these boxes here. So and then this was the fodder that I got. So there was these ones that said like from roadmap to company vision we are transparent and a single source of truth countless possibilities. Honestly I think this ones are like really pithy and catchy. I really like it. I really liked what's the other one that I like that not everyone does. All in one for everyone so I just I don't know I like I just think so there's something catchy about that but I know that kind of catchy is not everyone's cup of tea. All that's to say is the assignment here was that I took it was to write a pithy. Few words like punchy statement for each of these three things. So I ended up with a single source of truth countless possibilities. I like that a lot. This one I went with end to end control over your software factory. Not as pithy but like not as catchy but I think what I'm like what's the difference between that slide and the next one that doesn't have anything under security. This this was the honestly so that so when I first got this it didn't have this and it didn't have anything under security. So this was like it was your iteration of it. Yeah this is this is mine. This is I just put here so that I would have like the original statements like on it you know automatically anything collaborate on everything. I kind of like that but I you know this one I hate scale up speed up test up. I don't think anybody tests up I think that's really goofy so I hate it. This one I love that for me it's I love it or hate it. Right. I love how I love that. I just like bam. So anyway the what I love in just in five minutes now and then we can kind of and the call is kind of like anything that you would change this out for anything that you would change this out for and then actually let's start here. This one I couldn't pick and I'll I'll tell you why it's because I like things to have parity in in their tone right so this is maybe like super nuanced elements of messaging but I hate it if it's like a single source of truth that's like in noun phrase and then control that's like a noun item so you could say with GitLab you get a single source of truth with GitLab you get and then control and you can't say with GitLab you get move fast with confidence. No but you can't say more speedless risk I like that one the best. I don't like the last one. That's that's why I put this one at top was for that parity and then so it sounds like kind of y'all's thoughts or that's that's good as well. Yeah the only thing it might be missing but I don't know how we do it without over complicating it is well I guess less risk also implies higher quality I guess that kind of bundles the quality and the security together so I've talked my self out of not liking that so yeah good. Cindy's last blog post that a really good turner phrase that I stole for give that 14 and they was out it was like secure the software factory and all the stuff that you're making it. they said it really elegant and it's just kind of deliverables secure the stuff that I created still deliverables. Yeah I thought it was good to emphasize both is that mean yeah. Yeah so you could like secure and control your software factory and it's deliverables maybe that would encompass both thoughts. I'm not a big fan of the it's deliverables is there can we say like your software factory and your software or your software or in your product. Product sounds like car manufacturer. I can say IP here but it just doesn't sound right and that's always to always shorthand. Yeah I like this one better. Yeah I think I do too but I appreciate the thought. And then the last thing I'll say is so this is this was my attempt to channel my inner ash withers. I think ash is like super good at just again just like the catchier like the turner phrase element to it. And so like this kind of idea like single source of truth countless possibilities or all in one for everyone there's something catchy about this so here I was trying to like envision what is it that we actually give you with this velocity. It's like it's velocity with confidence is what is what you get. I like your move fast with confidence better than increased speed and stay on track. Yeah so I was trying I was thinking of like you know if you're like if you're in a race car and like you go like super super fast you're like let's say you're in the bike race right and you're going so fast and somebody holds up the sign and then like half of the bike race collapses. Right. So it's almost kind of like you can you can keep going like faster and faster and faster and if somebody like jumps out with a sign in the middle of the race track you're not going to get tripped up by it because get labs you know it's testing is going to catch that and security is going to catch that all of those elements that allow you to move faster but to do so with confidence because otherwise you're like the faster you go then it gets more dangerous. So trying to encapsulate that in this like that was kind of where I was like stay on track but I didn't like it either. Did you just open the door or mask car partnership? Hey, mask car goes fast and turns left because it get light. So you're a new Ricky body and immigrant. So yeah so I don't think I like that one and then I think between these two I like move fast with confidence but I don't like the the verbage of it because it doesn't have parity so that's where I think. What do you want to add? Maybe we can add high confidence or more confidence at the end of your first statement so it kind of combines on a fit so more speed less this high confidence. I'm just going to overuse anyway like I've seen a lot of competitors of vendors using it I really like the first from just more speed less bam no tradeoffs you know just how it is. Who no tradeoffs? That's one of my go-toes I love that one. This this gets into the no tradeoffs. The engineer in me says there's never no tradeoffs that's like saying like 100% secure. Come flick this is why you always say mitigate right we always risk or less risk you never say no. I'm just more speed less risk with confidence. What yeah. The Ricky Bobby and me really did like the no tradeoffs that Parker. Yeah. Oh is that was that from no but it could have been it's something Ricky Bobby would have said if anyone hasn't seen Telodega Knight's they should. That's the homework for this week and that's definitely not super back and we talk about it on Monday and I live so support. I often say thank you little eight pound five ounce baby. This is the goal of this I say that's so often that I just think everybody has seen Telodega Knight's but then I realize that people that haven't are like things really weird. And then next week we'll watch stepbrothers. That's another great one. So much room for activities. Okay I'm gonna go with more speed less risk. I think this is due at the end of the day so any additional thoughts I'm actually gonna untie. Me it's strong all those are you know. Those are good. Awesome. Well this was kind of fun to do a little bit of like working together session and keep it around as you dance. Keep it rocking. You'd to see everybody see you soon.",
  "So, sorry, this is a good question, Samya, that, hey, as we're trying to plan things out, like, you know, will Harsh have their own take, they almost certainly will. And so my thought is, I don't think that there's going to be any objections to, we dug into Salesforce, and we looked at what the win loss was, and we looked at the reasons for our wins and we looked at the reasons for our losses. And then we even dug into some of those deals specifically, and we found out that these were the commonalities for deals that involve CI. These were the commonalities for deals that involve Agile, right? They tended to use this type of message generally. They tended to go to these personas generally, like, no one is going to object to that work if we can do that research or we can document that. That's my strong feeling, although, like, I'm kind of, like I said, I kind of welcome challenges to any of this kind of thought process. So that's kind of my thoughts, Samya, is that I think that Harsh very much wants to see what's going and kind of keep the ship moving. I don't think they wants to pivot the ship. they wants to keep it moving, and any new changes in insight, wisdom, experience that they brings, I think they wants those to be kind of incremental, and that we all make those changes together. they shared a story with me that was very similar to that kind of language. So I feel pretty confident that we can make a decision now, and hopefully we don't end up in too many pivots in the future. How does that sound? Yeah, I mean, if you feel confident with, because we haven't had a discussion with Shiet, so I think it looks like that. Yeah, and it can't really harshen I have not talked about any work stuff. So I haven't talked to them about sales plays, I haven't talked to them about, probably the most I talked to them about work stuff was I kind of said, like, this is a little bit of the history of the team. You know, this is like, you know, what things were like when there was like just a few of us, it was like Cindy and John and me and Ashish, that was like a whole team, and then a little bit of the evolution. So that's what we haven't talked about specific programs or activities. Just imagine to Quentin Tarantino film where we all have our like characters and we're like all coming in at different times. Right, right. Where is that sniper film where that you do that same thing, but it takes four hours. True. That seems reasonable as far as the research is that mean kind of from here. Do should we should we get a lead at a share some of our insights on how to navigate through these things and use the, you know, we're going to be building our own reports. Or we're going to have access to someone that can help us with that data because in my experience, that stuff takes forever. And it's tough and they usually know what's what's the best way to go about that or do we just kind of dive in and enroll with it because I've done both ways. Yeah, so my preference would be that we NBC iterate this. Right. Like let's, let's not wait until we have everything perfect. Let's not wait till we have perfect knowledge, like, you know, let's make decisions with imperfect knowledge and make the best educated decision we can. And I think some of you and I've had a little bit about this where you've done and done some research already, but the data isn't there or it's not accessible. And so I think as we go through this process, if we can document those shortcomings, if we could say, like, here's a link to the Salesforce report that I ran, right? Or like, here are the fields that I'm looking at. And this is why they're inaccurate or here's an example of why it's wrong. Therefore, we need to change the process or we need to slice the data better. Maybe we need to do this. And honestly, if you watched the last call that the one that I'm linked in the notes, I think this is a point, Sakamoto brought up where they said, you know, yeah, we know there's some work that sales option needs to do to maybe even Hong said it himself. Like, like, this is something my team needs to do with the data needs to get better. But that's kind of my ask is that we start with the data that we have and the tools that we have. And when we run into these roadblocks that we just document that, so then we have like a reasonable way to ask to say, like, okay, this is the outcome we want to achieve. And we can't get there. We can't get X because we need Y. Okay, let's get together with sales ops and marketing ops and the data team and say, okay, like this is what we think we need to function as a mature business. What's our path to get there? Do we want to have an app? Do we want to make an epic and then link off some individual issues for us to do our research and our respective areas? What is there, or do you have an idea of how we'd want to do that? That way if people come in or if we do need to pay like, you know, data ops or something like that, they can at least understand what's going on. I love that. Parker, would you be willing to make an epic? Yeah, that is, you know. So, I'm your product marketing that cool. I don't think you can make an epic under product marketing. I think it has to be under the epic group level. Good call. But yeah, if you just make an epic and then we can, you know, have an issue and then at least have a sink point of collaboration. Thanks. Make this big. Oh, there they is. Okay. Thanks. Cool. I think that there are a couple resources for us. I saw that there was a competitive who added um, that was me. Okay. I was hoping maybe you'll lead to added it because they has, uh, so. So I think there are a couple of resources. So one is the deck Parker made. Um, do you want to just like bring that up for a moment? Yeah, I was going to add the link, the feature list link as well. Just one second so that we have that. Because I know that the leaders, uh, or I know that we committed to that as well, let me, oh yeah, you got it cool. You want to talk to anyone, we talked to you. Uh, I'll, I'll just give it a brief overview. Cool. Um, the, the, the, the, not all of this, it part of the correct me anywhere I'm wrong is that, uh, this is based off of a process you ran at your last gig. When you ran an end and sales play process that started with research and then culminated in sales plays and those sales plays were data informed. Rook. And so some of the idea here is that we need to define like ideal customer profiles, uh, you know target tactics, create some hypotheses, and then, uh, you know, viewing, you know, through the data that validating or invalidating those hypotheses, or even through testing, right? So at some point, we might say, okay, well, here's some things we know are successful. Here are some commonalities in some, uh, you know, some messaging like we saw these 10 actual plays and they all kind of went to this persona in this message. So we think that that is where it's going to go. So here we've developed this play and we want you to run it. And when you run it, tag your ops with this thing and sales force that we can track it, and we're looking to look to create like we created 10 of these types of ops. At, you know, uh, 50k each for, you know, there's these small land ops. And we want to create 10 more. Right? So our hypothesis is if we run this play, we'll create 10 more at this size. And then you could run and validate, you know, like shipping a feature. And so, um, anything else that you kind of would want to just highlight in here, Parker or anything from your previous experience? No, I mean, I'd say take a look at it and you know, ask the questions about it. But you know, this was all built around and exercise to identify what an ideal customer profile looks like and then uncover, um, the repeatable pieces or the people motions inside that that the reps can go out whether it's, um, the talk track or the scripts or, um, the paints, right, or the current situation they're in. So, um, you know, it's about you building the data and then building a model where, you know, using these characteristics or the indicators that we have, you kind of get to the bull's eye. And that's kind of the sweet spot. Um, and so yeah, it should be something that we can, um, at least leverage from process perspective and help us kind of guide us to the research stuff. So, um, yeah, and we'll probably have to adapt it if you got ideas on how we can mesh this better with, with what we're doing as we go along. No, please, um, please do let us know. So, I think Parker's kind of document in some of the process here, like some of the steps, uh, just at a high level. And then I think Alita has a nice, like, this is the output of them report and let me honestly see if I can get some of the deeper, um, notes from her, but if you look at like, here's like, they does this quarterly. She's done this a few times. And them sources were Salesforce, deal stories from the GitLab team, um, alignment points actually. I don't know what that is. Um, they chatted with analysts at some points too to get, but this is like, and we've done the same thing. So, I think some of this stuff we're already doing, right? Like, you have a sales play and you have a concept, you have an idea and you schedule some time with Gardner or Forster to review it, right? Like we're already doing that kind of stuff. So, it's not, I don't see this as like a hard pivot. This is more of like, uh, augment and just, um, acknowledgement almost that like, hey, there's a research component required here. Let's make that a little bit more formal if document what we're doing on the research side that's inputs into the output of the sales play. And so, you know, you can see here where Alita has come up with some of the data. Um, and, uh, what I can do is, is maybe on Monday, let me, let me see if I can ask Alita on Monday to walk us through them sales force report. I'm happy to do it as well. It did work one on one with anybody who hasn't like, mungent in sales force before. Um, I think this is a muscle we need to build. Um, you know, I think there's, there's a lot of things that we're like have high competencies in. And I think this is a competency we need to, that will serve us well as at this stage of the company and moving forward. This kind of data rigor. Yeah, I think that'd be an awesome idea. Any, any other thoughts or questions on just kind of like, what I view is more of like, additive than a pivot on taking a step back and doing some research. Um, I was just looking at Alita's report. It's brilliant. Right. I think she's done this before. And it's, it's a great report. Um, we tried to do something similar for, um, take off. And I see a few challenges for the one is the ability to identify. Peels that are either the cops lead or even have the tops is very, very difficult. Right. Um, so for example, there are chorus calls where you know, some of the keywords that we have identified as related to get ops are being hauled out, but we never see anyone who was in SFC. So there will be deals, but then they're not necessarily that the skit-offs, the notes on SMBC, don't call it out as skit-offs. So, um, we can perhaps, we, I mean, I have most of the data create a similar such report for the dogs, which I also need to view SDRs and Sall's, some of the bin stories and some of the lost stories that can be put together. But it's, it's a very small sample size because we weren't able to identify enough these that were that had to be. So, um, at what level some is this like where, so like before we, in just a waiver with that, with that slide deck I showed you before we went through that process, I had a data person tied to my hip and like we went and put a field in Salesforce that the sales reps had to go and retro actively fill out that like basically our value drivers are our use cases and they, and they didn't like it because they don't like to have a making extra click, but we had to do that in order to get back and exact these deals with what we needed. So we may need to consider putting that asking early to date and so we have we actually have that theme. So when we started the use cases last year John worked with there, I think put that theme in there. Some deals have that, but not all of that, right. I think 90% of the deals don't have that information. So that really makes it difficult. I mean, and the way we've categorized those use cases also is very difficult to, I mean, we have a complete DevOps platform as one of the entries there. So it is possible that, you know, a sales rep doesn't want to put in all the effort and just tags everything as a complete DevOps platform. It might be a security, it might be a digital free, but they just tag it as a complete DevOps platform. So I think that is, that is at least from a data point of view, that is one challenge that we will face. It's possible to kind of work around that by trying to map a course call with an SFCC. If there were some conversations on your product, your topics, on the course call, then potentially assume or maybe reach out to the sale and find out that this deal actually have CIC or the course of whatever. But it's going to be a manual process So that field isn't mandatory then in SFCC and the same thing. It's not. That was what wild the feathers in my last year. They did not like that, but we had to do it. So yeah, so this is essentially I feel like if we, if we just show up now, we say like, well, we can't do research because we need the field to be mandatory. We may or may not get somewhere with that proposal, but I think, some of you, just even had like a word document or an issue that just like had written down, these were the deals I uncovered, like this is how I know it's a, it's in course it's a get-up steel and SFCC it's not. And it doesn't need to be detailed, but just a little bit of documentation. Then we could, yeah, then we could make a proposal and say, look, we think for the rigor of the company, this is, this is a way forward that we need to have some documentation about like we need to have this field, it should be mandatory. What's, what's solution is that is the land solution and it can't just always be a platform play and needs to be like, what are they, what are they starting with? That they started with security or like, what's the order of operations so we can document it? Cool. Yeah, and I don't know that I would recommend trying to do a mandatory game going through it once. It was not, it was not a great process and it did take some time and all that kind of thing. So I mean, it wouldn't be efficient. So I think I think this is the best way for sure for now. Let me ask this since we all have a gut feel or even, I've experienced exactly what you've had done some munching and I know the data is rough. Any other suggestions for work arounds or anything else that you kind of have done at another place that was successful or and just any ideas on, um, hey, we want to try to get some data around this is the thing that works and we need to categorize it so we could say this is our win rate for X. Any thoughts or ideas on, you know, we could like, you know, make the field mandatory? When we've seen a very similar at least high level stats, you know, in McBees decks, we've seen things like this percent of our deals are this kind of a land. Who did that work? Do I know? So we do have things broken out by land or expand. And we do, I think it's in this tab here, let's take a peek. I've seen stuff like 29% of our lands are SCM deals or stuff like that. So I have a rough idea of what the numbers are, but who did that? Yeah. Okay. Do you have a link to any of those decks? No, of course I don't. I'll have to find it. Okay. But the other out there. Yeah. I definitely have seen like this is our 25% of our deals are land. I don't know that I've seen 25% of our land is SCM, but if that exists in a deck somewhere, then we can go and ask like where did that number come from? That might have been on the product call recently, Brian. I think I was on that. I think it was Taylor McCaskin that mentioned something about like 50% deals SCM, I don't know. So I might have to go back and look and see if I can find that as well. It may be the same thing. That was recent. I think it may have been on the last, one of the last two products called. So I'll go back as well. I think I may have seen Donk with those numbers in hand too. I'm looking. And yeah, I think part of your point about working closely with the data team, I think that is an eventuality. I don't. I'm not optimistic that we get a resource like that ahead of time. Sure. And the rationale there or my thinking is that I just know where our data team is like our team. They're very, very lightweight for this high-sqm company we are and the demands on them. And so I think if we can build an ask, then I think we can definitely justify like we want to do this thing. And like this is what we could do on our own. And we need data resources to do X. Completely reasonable. Yeah, it was the SVP at the last place that basically said you're working with him. And so it wasn't really a choice. It's a hard ask. I completely understand, but I wanted to caveat it so that everyone knew. Cool. Then this is, I'm just going to put it to do like Ryan to do track down slide. Cool. Then I think our next steps here are pretty clear. I'll have a lead to do a little demo on Monday where I'll ask them if she's willing to. If not, I'll give it. And then I'm also happy like you know, feel free to schedule one-one time with me. And I'm happy to walk you through SFDC. I've done a fair bit of munching. Anyone else have a razor hand at Samu or a new one? Would you would say like I'm I'm familiar with SFDC and would be happy to one-on-one coach other folks on the team. It's okay. I didn't do it to some extent, but I don't call myself an export on this. Okay. Fair enough. I don't just, I think a lead is probably done the most. And so I'll see if I can report. I mean a lot of ones that are broken. That's for sure. So just to William, you mentioned about best practices from earlier. Yeah. I was just thinking about that. And one of the things we used to do was every quarter we used to track the top 10, 20% of the teams, which playing the maximum revenue. So between the team, we have actually kind of slipped by region or you know whatever to actually be developed. Specific deals go through the deals. Maybe work with the sales sales reps for those deals to understand what's driving these deals. So one so in general, it used to be probably around 20-30 deals. So spread across you know a team of five or six, it would be about six-70's each person would be handling. So they have to in depth information about the productivity. And then we can use that information to punch the data on both for the paper and for the paper. So that's one thing that we need to do in that field. I love this idea because we already do some of that to some extent and this shows up in KBRs. I mean that this is effectively what KBRs are. Each rep comes and says, these are my top deals with the quarter. Now that's for like every single rep. But there's also honestly I know that pile has a meeting where he's, it's like pile's top deals meeting. That is the dashboard. See it's for dashboard. I'm just going to put it here. This is the SIPC dashboard. This is the SPS dashboard which tracks the top deals for the quarter. So we can, you know, we can even use that team. That's a pretty good. I'm I'm going to add this as an agenda item for like the next call. Let's keep this as, I don't want to set an action point forward now. But I do want to set an action point on this by the end of the quarter. Like I think like going into the queue, I think at the end of Q2 we should have a list of Q2's top deals. And I think it's related to this effort. They're not, they're not completely separate. I think they're related. And honestly, maybe this is even a way to get at it. Maybe we just look at like the top revenue. Well, there's one other problem. This is why I don't want to get too much into it now. We should chat more about it Monday because forget lab, the most important deals are not just the top revenue ones. For us because our net expansion rate is through the roof. We have this like best in class like customers. If customers buy us, they will grow and they grow a lot. So what does that mean? That means we really need to focus on new customers. And all that matters is getting that new customer even if the deal size was very very small. So right now the focus for the business is this first order logos. And the reason for that is because our net expansion is so amazing. So I think there is value in getting the top deals with the quarter or the top deals just knowing what those are. But there are, it needs to be coupled with some amount of like that. And this is how we're getting new customers. Yeah, I think we can slice that based on what we want to focus on for the quarter or you know, let's say, you know, for the quarter, we want to focus on mid-market. And say that maybe one on this time, how we are going to market in the mid-market segment which in cases are working well, what we can do to the market. So it just doesn't have to be the top deals. It could be a specific focus area for that particular quarter. And then the key does be shared that and then shared experiences and then come up with some kind of support that's required for the best type for the cool cool. The next thing I want us to chat about today, we're starting around short on time. But, Samya, you had you and I had chat and you made a proposal to say like, let's focus on STRs because we can drive immediate results. And this is exactly what I want to do. Like this is exactly what I think we all want to do. Like the idea, this is kind of what I'm saying. Like, hey, there's some balls up in the air. But like we can make decisions within perfect information. And we can just look around and say, what's best for the business? And if we're always acting with that kind of bias, we're not going to have too much trouble justifying what we went and spent time on or, you know, aligning like other people are going to be like, oh, good. I'm glad you didn't have to go and get a bunch of alignment. You just did what was best for the business. So one of those ideas is that the STRs actually, I'll just kind of like, would you mind just kind of pitching your idea that you just do the team? Yeah, I can do that. So one of the things I did as a part of the GPM motion was also in the view of the US VRs as well as some VR managers, right, to understand what is it that is kind of missing from their workflow? Are they able to have a specific conversation? So let's say a big ops conversation based on some of the bonus balls that I had listened to. None of the STRs were able to have a big ops conversation, obviously we've never probably enabled them on it, but every single time they just used to kind of go with the standard pitch of single application or security, without really listening to what the customer's point of view was, right? So the customer might be saying that, hey, I wanted to infrastructure automation, but the STR would say that, hey, Hitler is great for everything. It's a single application. It can help you forever. You know, not really actually talking to the customer's main point. And I first listen to those calls, let's go to the STR managers and I understand that they don't have a standard process. They don't have calls groups across the board. They don't have outreach sequences that can help the can help STRs. And this becomes important specifically for commercial STRs because they have to turn customers normally the only fast, right? They don't have the luxury of an enterprise STR who's able to then call in a sale or they say for a conversation with the customer, right? So an commercial STR needs to turn a lot of customers' owners in the room. So if they have to actually start, you know, reinventing the week or every conversation that they need to have, then it becomes a bit of a challenge. So finally, the conclusion of all of these discussions was that, yes, there is a gap. They also have a gap in terms of subject matter expertise and that's where we've been coming. And the immediate ask was, one to go over the outreach sequences if there are no outreach sequences, then maybe come up with some of the people's staff who have helped. And the second thing is to help them with some false groups that will help them have the relevant conversation for what if you're doing this. Now I have a discussion with William. I think we haven't prioritized this for you to be mindful of the doing with the TTP. But I'm going to kind of try to work with the SDR team to kind of pivot the pilot this to actually see if if that is a problem that we can solve and what to say to the camera people for that. Right, what is there and kind of that too? So that I have been hesitant in the past to like double down on this and the reason why is because every SDR team kind of managed their own things. Just like Samu said, there wasn't like a unified, this is the template we use everywhere. And they wanted that to some degree. They wanted SDR to be able to do what it took to get the deal done. And to adapt to regional differences, etc. etc. The challenges of course is if there's a hundred different outreach emails being sent, you can't really say like this one's performing well or it's not. But if you have like these are the four standard templates, you know, or these are the two standard templates for get-ups, which one performs better. These are the two standard templates for CI or for security. Right. So, can't know if you've chat about this. Who Parker, who were you working with on SDR? I mean, I've worked with a lot of SDR like Matt Malcolm, Hannah, and most recently Megan Thatcher. I've been replugged in with them. Megan is really passionate about this. So she's recently gotten into a new role and part of them charter is this exact thing, standardizing the outreach templates. And so I think this is an opportunity for us to get involved. This is kind of we didn't plan this at the beginning of Q2. So I'm not necessarily saying like go drop everything and go start doing outreach sequences. But 100% Samu is doing it as part of them sales play. And I would encourage anyone else like this is probably a place where like if you're just looking for some extra time to go do something working with SDRs, this is probably going to drive some results quickly. Right. So if you want to drive some results quickly, this is a place where we could probably see like a quick improvement on you know you can just imagine going from customer conversation of like yeah I'm the you know director of platform operations at sessions such a company and I'm really interested in infrastructure as a code of infrastructure automation and then our responses like let me tell you about our all-in-one for everyone like it's going to fall in death years. But if we can enable them with a call script that's like you know oh let me tell you like here are five things to say about security to the security person or to the security minded developer director of DevOps or whatever right here's your call script here's your outreach sequence like that will prop like we could have a strong confidence that that our hypothesis that that would drive more immediate results so anyway just sharing that as well I think Samu is doing their part of your sales play but wanted to raise it to the rest of the team. Cool any other thoughts or questions on SDRs? Just wanted to add a quick note there with them I think there are two people on the SDR teams I think Nathan is dealing with nine motions and it used to be Elcher was dealing with the expand motions and they are both trying to standardize that so we probably need to work with both of them who's on land? Elcher is Megan is on that. Megan. Okay cool. Let's come back to this in a moment I'm just going to jump ahead to contribute I just wanted to point out that if you haven't read the contribute FAQ here's a link to it and I don't want to lead you straight definitely read it on your own but my synopsis of it is I don't think that there's a lot for us to do or I think there's only one to do now and I think all the other to do is are like once registration opens then they start booking your flights for you and that kind of a thing but the one thing that you might want to start looking into is a travel visa for the Bahamas so it's in FAQ's a reference is it? Here's a direct link to the site and anyway I just wanted to raise that to the team. The competitive feature list is anybody still I know Kormax still has to get their in is everybody else done or is anybody else still working on adding their features? I'm still working on it I'm making progress. Yeah mine's open too. Cool. What is a reasonable timeline to have your features in the spreadsheet? Oh I was doing an innovation. I just need to be cloned I'm sorry. Yeah yeah and I know that's just like this is why I want to ask like what's reasonable. I don't even know anybody now HTML that would be a big help for me. Somebody can out debug some HTML. I do. I'll help you out Cindy I've I've got some open slots today let's do that. How about any Brian what do you think will be a reasonable due date for you to say like I can get my 15 features together by date x? The only thing that's slipped is I was actually going to talk to my product counterpart about it and we lost our call so I can just call it. I can also do that. So I can get it done this week if you just want to get it done. I'm very deadline driven at the moment is the fact of it so if you set it that mine I'll hit that. Okay can you put in some features and we can always iterate on them but could you have 15 features by end of day Friday? Yeah totally do it. How about Parker would you be able to do that? Yeah and Samja? Yeah I can. Okay cool and then Cindy and William. Next week. I work. Yeah and I know I've added more things to your play and I know other folks have too and so I've asked reasonable let me see how much I can help you. I can definitely help you with the HTML. You can have me in the batter's box if you need extra help I mean help with that thing too. Cool. Who added number five? Maybe core Mac. Maybe core Mac yeah. Okay so we have an alliance with VP that's exciting I didn't even know this. This is so cool. I just saw it roll in but yeah otherwise I didn't know either. This is awesome. Okay so what's confusing to me is I thought the alliance was in marketing but Nemo is actually reporting in June like me. So that's that one of the moves? No so alliances has for a long time been part of sales and if you remember Brandon Young. Yeah I think they initially reported a Sid for a short time and then they reported to McBeat for most of their tenure at Get Lab. So this is the new Brandon right? Yeah this is this is exciting. So Colleen is ahead of alliances marketing. I believe or ahead of partner marketing. Yeah I had a tech that funny cheese cream. Yeah yeah Colleen is awesome. So they is partner marketing for both alliances and channel and then Nemo who looks like is our new VP of alliances. So this is who like my own team will report into Nemo and it's really exciting that we have a new head of alliances. It's super cool. Um Brian. I just liked that somebody did that. This funny pile on the product team call they starts out by saying I'm looking to narrow the guard rails of what sales plays look like and like you're talking to the product team but that's how much the sales play issue was on their mind. they opened their discussion to the product team by talking about sales plays but they was actually there to present what's in this this deck and I thought well that was like a nice to have take away from all the QBRs. Was here everybody in product here's what you should care about from QBRs and it would be nice if we're going to do something like the QBR task again if for the marketing team there was something like that too where we could just say in five or ten slides here's marketing team what you should take away from the QBRs. I just liked that they had it you know nicely summarized for product. So I'm confused in my apologies if you can back up one step what was the context was this was a call or this was a deck that pile put together? The latter and first it was in Slack and then it just got added to the product team agenda. So it became about ten minutes of the product team call. And so you look pile made the note in Slack? I think actually Scott made the first note you made it note in Slack. Did I go to it? they made it actually quoted pile I mean I don't know that's sure but I just liked that it was done I thought it was a nice amount of take away it was actually like a useful amount of takeaway for everyone in product I thought everyone in marketing should have some amount of takeaway from the QBRs which maps to what we were saying earlier as well which is like especially for going to go through that whole process let's surface the really important stuff or the stuff that we heard ten times and that kind of thing the patterns. So this is you saying in in piles QBR in their leadership QBR they had a summary. Scott attended that QBR liked the summary and linked to it in Slack and then added pile to the product call. Yeah and I thought it was useful even in some follow up conversation with folks in product I thought it was actually useful to them. It's good whether they're going to change decisions or it was validation or it was a little bit of both but it just felt like something we should also have in marketing could be the same list of stuff but like this is take this is a summary takeaway from QBRs. Yeah and it's something we could do we're sitting we're sitting through them we kind of do it anyway just chatting to each other but I don't think we need to do it if it's sort of being done we might just need to filter it or a resenter it for marketing purposes but. Okay I my thought on this is that we make this a Q3 goal that with Q3 like I don't want to go back to our Q2 QBRs and say like let's derive a summary out of it but I think that looking ahead to the next set of QBRs and how we engage with them as a team this would be an artifact we aim to generate does that sound right. Yeah is this public news Parker can I put this on YouTube? Uh, we were making our public announcements be a press release later today at 9 a.m. Pacific and that was yesterday so as long as we announced it. All right awesome. Which I can go check our link there or something in confirmed. We were planning to yeah. Yeah awesome. So I thought that was really cool I just checked out their website and saw what they were about looks really I mean it looks right in line with where we're investing you know and that's either the product cast as well. And look at that it's on tech crunch love it. Okay cool so we're good. Cool. Yeah just so you know you were saving your speed. Yeah yeah so yeah so this is should be the notes and all this there should be public right. I don't think I I didn't click on like when I clicked on the sales for support I didn't show it. So hopefully everything in this video and that I put on my screen is YouTube YouTube or the I think we're okay yeah so uh hey thank you team Let me just take a quick peek your Cindy do you want to look at that Hey you're HTML stuff? Hope you all mute. Sorry I just pulled out the offending part and got it to run and I will go back and re-added as a separate MR I updated the deathseq offstage and needed to get it out there because MQ stuff just went live this morning. It's driving people to it and it was making me nuts because I couldn't get the stupid syntax error but so I just removed the offending part and we'll go in NBC too. Re-added. Hey looks like sounds like a plan. Do you uh cool I want to let everyone else on the call you know run and let me stop.",
  "Now we're ready to report. So commit boost duty. Simon, thank you for I saw you already posted on the thing and Brian did too. And so Parker, if you could grab a slot on the schedule there and a leader will need some slots as well. So we can cover that. And then core max indie, I recuse the three of us from commit boost duty, given the other scope of stage architect stuff. Thank you. That was nice. So now I'm going to try to do last year. I did the entire 24 hour live chat. And that was a mistake. But I'm going to try to be in there for as many chats as possible because I don't think that I think there are there any other spaces for the speakers won't be able to make it. Or be sure there's someone in there to answer chat questions. Yeah, I'm doing the same thing. At, you know, this is one of those things where we knew this, but we didn't account for it. So cover, you know, cover commit chat for a pack hours. That is an agenda item. We should add to our architect sink. And we're like right up on the heels of it. So I don't know if there's out of who, like, who we can get to sink that time. But let's add that. I would try, you know, in some cases, it could be the speaker could cover it. And the other case is depending upon their time zone. And the other case, you know, we could probably get an essay or a PM that's in a good time zone for it to cover. Right. I remember doing that last time. And I remember they're being a scramble around it. And there's just no, like, these are all the things we need to do for commit. Like, that's a thing we need to do, but it's not on a checklist anywhere. So, so. So, Q3, okay, R's. So let's, let's chat a little bit about some of these things. I have to do, do, do, do, do, do, do, do, just gonna share my screen here. So, well, I'm pretty excited about as we are marching towards doing this in a more organized fashion that, you know. I don't think we've had this, this level of rigor. And we should be looking to do more rigor as we, we've on. So, I did a link here. These are CIDs, okay, R's, which are a GitLab managed future. Conversion from free. And even Prouder to Work here. So, what's nice then, is, Craig's, okay, R's, roll up to that. So, Craig's, okay, R's are increased funnel and drive pipeline, elevate marketing messaging, positioning, and maintain high team member delight through strong prioritization and collaboration. So, what's nice is these are rolling up. So then, when you get, and then I link to Craig's, okay, R's deck here. If you wanna poke into that, there's more info there. So, when you start to look at our, okay, R's, they roll up to that, to those, you know, they are, R's, R's really roll up to harsh, harsh as roll up to Craig's, Craig's roll up to CID. And hey, alignment. So, that is exciting because in an ideal world, the projects that we are planning aligned to the strategic goals. So, and some ways this dock is still in draft. So, for example, I need to get, I have a call with Ryan this week. I wanna get like a full list of what can we expect for R, F, and reports so we can plan that bandwidth, right? Similarly, there are things on here, like our corporate event support. I think these ones, we've, are well documented, which ones were covering the field marketing events. So, ideally, what we're doing here, and I see that some folks have already kind of started to chime in and been like, yeah, I can pick up this one. And I think that that's good. What I wanna, what I basically wanna avoid is I wanna avoid like, you know, Cindy has like eight webinars this quarter. And like, I have to, or, you know, that kind of like imbalance. So, between field events, demand, and webinars, partner webinars, and ABM webinars. Like I wanna make sure that we kind of balance out that load. It's kind of what I'm aiming for here. Here, we have a list, and then it has a link here, and this is the list of proposed field events. So, we did this last quarter and got, I think, eight. This quarter we got, eight team, that gives me a little bit more confidence that we're more, a little bit more comprehensive. So, I have an action item to go through these, and try to, I might pop up in a spreadsheet or something, so that we can look at like, how many's part are doing? How many is core-mack doing, you know? Any questions on just the webinars for the quarter? Cool. So, sorry, I missed part of that. So, you wanna us to put in what we signed up to, or you wanna, or you're putting them in based upon the request. So, ideally, we kind of like batch process these, so my current, what I'd like to do, if I ever get like five minutes of extra time, that I'm not, you know, just jammed with stuff, commit is taking like a meaningful amounts of my time, but in an ideal world, this list here, like, what do I want? So here, so, in some ways, this is nice because this is, you know, all in issues. So, but it, it's kind of makes it cumbersome to filter. So, ideally, what I could do is I could look at this and I could say, like, are these evenly distributed across the team? Does everyone on the team, are they picking up a few, right? Rather than like one person being loaded with a ton? And are they, like, you know, is field marketing asking Parker to deliver like five events, the same week that, you know, major project plan is due for really marketing or something like that, right? But when you might think we can manage this by, like, we could sign up, we could look at the list and assign ourselves to the ones that we think are appropriate. And we can self manage if we've got too many in one week or big conflicts, we can say, hey, I can't do this, can somebody else do this. But yeah, that's a good call. There's, the managing conflicts versus your projects is, I think, good. We need some type of all team collaboration. We need, we need some way. So for example, like, I looked at the ones from last week. I don't think everybody put them in there. Uh, wherever we had. So Brian had none. Cindy, you had, I don't know, one, two, three, four, five. But I had too much going on. But I have a list there. I'm sorry. But I wouldn't want you to look at that and go, you have too many, you need to farm some of those out. Because sometimes I'm unable to reuse content from one to the other. Some, they're all devsickups topics. Some of it was on, I was the spokesperson for the devsickups survey. So a bunch of them came from that. And honestly, I just tend to have more of those and of the customer meetings than other people. But that's OK. And maybe I have less of something else. I don't, I feel like it's a little bit artificial to feel like you've got to distribute it for us. We have a list somewhere. I'll put it in. Thank you. So what we, what we need to do though is so we need to make sure that for all of these projects that we're signing up for, and a lot of them are like horizontal. So for example, like launch plan development, right? Like this is a new practice that we haven't really done before. So we'll need to look as a team what launches are coming up. We need to be done. We just need to make sure that our priorities are in sync and that we have coverage across. So I'm not necessarily saying like we need to kind of artificially flow it. And I definitely don't necessarily want to like, dole out like you're doing this one. You're doing this one. But I do want to make sure that there's some spread across the team. Right? Now I do want to make sure that like everyone's at least doing some events. And I do want to make sure that we cover them. And I do want to make sure that we are getting to our other priorities and content generation. So those are, that's kind of like the, we got to get some kind of wrangle on this. And then frankly, I like, I personally lost a meaningful significant chunk of my time last quarter due to field last minute asking me for stuff. And I had to shift things around. And I had to like chat with FMM managers. And like I don't have time for that this quarter. So like the last quarter we did like everybody just grab events. And I prefer that. I like I prefer to centralize. But like I cannot commit the time this quarter to like what fell through the cracks and what are we miss. So I need to be injected somewhere in there to say like, look, we have a plan that I feel good about. And like my time won't be sucked at the future. Is that kind of fair? Yeah. But I think I think if you tell us what we, what, you know, you've told us the objectives, I think we can. Sign up, manage ourselves and. Yeah, like. Let's let's do this then we just we need them to all be covered and we need them to. Not so so what we also need to do to is across these 18 events. Like some of these are for PMM and some of these are for PMM. Mm hmm. So that's that's all it just it just needs to be managed like. Um, I am not comfortable saying like go and sign up for stuff and then I'll just see what doesn't get signed up for. Why not? Because that didn't work last quarter and it was extremely painful. What didn't what part of it didn't work. We didn't sign up like we were supposed to or they didn't give us the full list and And banks like they were supposed to. I think that the full list wasn't there. Um. That that's a good question. I can I can kind of go back and maybe think about like okay what what went wrong. What what were the kind of problems? What are we trying to optimize for like. Like. Where I'm going to do this. I'm going to review all of the items. That's what we're going to do. So if there are ones that you want to sign up for. Go ahead and comment in there. This will be part of the process this quarter. Instead of commenting if we'd assign it, then you could. You could do a board or a filter for which ones are not assigned or. Uh. You know, which ones are assigned to each of us. I'm just a thought. Well, there's problems with that too though like like some of these are already assigned to people, Some are assigned to multiple people like they're not they're not consistent. So like. We need a consistent view. Uh. What else. We question on. Yeah. Are we going to take up all the feed marketing events or are we going to prioritize. Based on the OKR that we have actually said for us, which is. I think pipeline and. I'm going to. I'm going to. Yes, yes, 100%. So. Um. In your intuition, I think it was though. Well, so there's two there's two elements here. We got it. We should chat in a moment about KPIs. So KPIs are going to be different than OKRs. But they're they are related. Um, so here that the. The OKR. The key result for this objective is to increase funnel loss the indirect pipeline. So yes. Bye. Uh. As we look at these events, if we can't pick up all of them. Then ideally we're asking questions like like which one of these. What's the revenue potential? And we make sure that we pick up the ones that are the most. Um. Like we're prioritizing we're prioritizing based on the key result we're aiming for. Absolutely. I don't know that we're to that level of sophistication yet. Um. Like I said, this is like a step of maturity up from what we did last quarter. But eventually like ideally what we would have is for these KRs. We would say we're doing this and we expect this out of it. We're doing this and we expect this out of it. So we we haven't done that level of rigor. I don't we're not going to get there for this quarter. But certainly on any of these that that is the lens to look through is like hey you for this we're trying to drive pipeline. 100%. Cool. So we're trying to get these finalized point else. So one other element to this is. This is the. OKR epic. So this has Craig's three OKRs. And then if you drill into one of these ethics. For example, like you know increase pipeline that we were just looking at. This does have some specific results like you know web direct purchases. That's super interesting there. And that's on dunks looking for web direct for first order. So I just anyway I just noticed that. But here's an example of like product marketing reposition. Free product individuals and paid product of business. Honestly, I'm like just double clicking at this now because I'm trying to think like how does this align. To this. I'm also wondering if that is that public. I don't know yet it's not confidential. OK. Yeah, it is. Yeah, these are all public. I think this was this was one of the ideas that Craig had for the pricing paid a bit. And I think that's the first. Where they said that message on the free should say for individual users and. For the paid to say for business users or enterprises or something like that. So that's in our pricing page back up. But I don't know if they had something else in mind as well. Just topic. So that we we might need to add something here that aligns that. Honestly, we didn't like you can see in our doc we went from like Craig's to the teams. So we didn't go to that level. The one that I know about. Is. For example here. In this messaging and positioning. We have. This golden pitch right. So this is something that. Core Mac is leading up, which is like. Golden pitch certification. Honestly, some of this also needs to be. Rethought now we were going to run this at contribute. So maybe we I don't know if it's still on the doc it and we run it at. You know, a single probably actually this is a good agenda item for a call later today. The reason I want to point this out to the rest of the team is this comes from Craig and then Craig has one that cascades down to harsh. And then for this one. Harsh will probably just like a cascades right down to me. And then at cascades right down so core Mac will be the assigning here like this will literally be. Core Mac is leading this project. So ideally for each of these things we have. An okay are issue in this marketing planning. Project. And that's on my like Tracy's already logged all of them so this is like a to do for me to do the reason I want to bring it up to the team now is because it means we'll have some duplication. And then you have like your normal issue where you're working on your stuff and you can you can organize it in structure it and then you know if it's an okay are there'll be like some accompanying okay are issue. And what I don't want to do is I don't want to impose the constraint like you must work your project out of this marketing planning project. I think that'll cause all kinds of mess so the the way to roll it is. So it's there'll be some duplication so in some cases like I might have an issue and you might have some projects that are under that issue. And some cases like you might yourself might be assigned to that issue and it kind of questions on just this the okay are sort of planning the way they're trying to roll it up. Cool. So KPIs. So we had a. A solid discussion on this last time and we've iterated here this has been reviewed with Craig and so the idea here with okay are those are quarterly right the KPIs are intended to be like longer term. Right so in theory like our okay are should be driving our KPIs they should be related but but one is a femoral and one is you know intended to be like a longer like we're we're tracking these things as a team over time. So some attributes of this. You know, perk on a samus comments last week and Craig wanted this as well and harsh to and I. Me too I've been sold by the team that we want more of a direct line so as an example if we say okay well we're delivering web content. So we're going to be responsible for something like you know inquiries on the website. So if we're just responsible for the total inquiries number and we share that with say Danielle's team. A there's some confusion because like who's actually responsible is it us or Danielle and then be like we don't contribute to all the inquiries so. That doesn't really make sense. Is that we as a team should be incentivized to make them successful so that's kind of like like we as a team work horizontally across teams and marketing. So that's why you see like alignment with Danielle's team alignment with dunk's team alignment with Evan's team. And then. So that's the horizontal and then here is the ideally we're trying to tie it more of a straight line to the work we do. So quite frankly I don't know how we're going to instrument for this we start to get back into. Some of the some of the goofiness we have in the past. I think there's a few guardrails here so one is I don't want to track views for like. All content ever when we have some complicated matrix and we're trying to like track down every YouTube video and all of this kind of goofiness. Like we should just have a web dashboard that is like this is the web content that we put out and we can track things like page use and voucher it. Yeah, what do you think so. Yeah, I think this is great it's moving the right direction. I think page views is still a little bit goofy because like like we discussed previously as well we. We don't have control on how people are going to land on this page right yes we can do a little bit of SEO but if the solution is not let's say at the top of the. On our home page header then no please going to be able to find a particular page. I think that goes back to the question on whether we need that page at all for not on and whether we should invest. And we need to keep going on that I think bounce rate is a better metric on the content itself. Because it shows that if a customer is not engaged on that content then they would move bounce rate is one and time on page is is the other metric that we could. Look at rather than having page views that's. So we we need to track if the. Content is valuable for example from an essay of perspective if if people are searching and landing on that page. And if. A pages valuable in a customer journey we need to be incentivized as a team to collaborate with the rest of marketing to get it into that flow. Yeah, so what metric would you propose to incentivize those things. So if it's about collaboration and working on the journey then page views is a good metric right because then we need to think about it from a customer's point of view. Customer lands on the front page then wants to say let's say wants to know more about CI CD or get off right how do they land on that page. Today for example, get off they cannot land on directly unless they land directly on that page through an SEO search right because it's not part of the top header. So we don't expose the solution that any more on our home page other than at the footer of the page right and there are many, many such pages like that. We don't have one specifically for continuous delivery as an example. So. We have to work with the extended team to get that in place then we need an overall. Information architecture like we spoke about right like, well, how do we how what's the journey that the customer needs to take and what use cases that we want to. Showcase to the customer and only those are things that we should. As a team pick up and work on rather than putting together 10 different pages like we spoke about this team pages for for pricing themes. There's no way that a customer can land on those pages except through SEO right we wouldn't link to that from the pricing page we wouldn't link to it from the main page either so. That's mostly going to be just an SEO effort. Right so that. The. Essentially we want to have a decision matrix like I'm going to have page A or page B which wouldn't should I invest time in should I generate a new page or should I refresh an existing page. Like overall views is something that like page views on PMM page content is something that we want to track. The other end of this two in Craig was like really open to this was. Ideally these are long term and then we're try we're tracking trends over time. But given that our KPIs have been so goofy and ridiculous for a year plus. We just need to try something right so we need to measure and get some baselines and maybe there is a better metric or we iterate next quarter. But this is kind of like a quantity and a quality metric so we want to look at both of those we both want to get like. And then again it's like the idea here is like if we generate more top of funnel volume. In this case we're measuring that by page views but that the assumption there is that it's a leading metric to feed things like inquiries and self service. Right so that's what we also want to track not just so we want to track that like this is a page and this page individually performs well like when people land on it they don't bounce. But we also need some type of measure that we are contributing to the downstream thing which is the inquiries and the in the self service signups which page views would also get to that so. So similarly here for demand and content. I don't know again I don't know how we're going to instrument for this it is I'll tell you what it will not be is it this will not be us architecting our own separate. Machine like for all of these like whatever we measure like this measure like these measures need to come through Google analytics like do the whatever the web normal measurement thing is. Like our measurement for our KPI can't be like what we need some other tool to do that we have to like work within the. The confines of what we're measuring so it's the same thing for demand gen right like demand gen has this. Dashboard which I wonder if I can like pull it quickly actually I probably can't show in a public video but. You know they're already tracking things they're tracking things like inquiries and saos and mqls and they track linear attribution so that's already a metric that's being tracked we just need to segment out the pm content. So in theory here and this should be like everything that drives a pipeline like if it's in. And it's pm contributed like we should be able to pull a number out of that I don't know how we're going to instrument for that I'm open to ideas. Think about that one. This one should be a lot easier. Essentially like a while back to you all remember we had this thing and there was like five levels of like you know did you write the content did you review the content did you all like it was really really goofy. Like had this extra job of going and like coding all of that we really need to avoid that like as much as possible when we measure stuff. It should be like a normal part of the process like we went and did this work and then like there's not some meaningful significant extra task to go and measure it so. For field events they already track the linear attribution it's already part of the Salesforce object so this should be as simple as as like looking at you know this this list of events and saying like these were the events for the quarter we know what the list is let's go pull the linear attribution these ones are like pm contributed events. So that's kind of the the thought there any any kind of other thoughts on KPIs. Measurement. Yeah this might be a little down in the weeds but eventually are we going to get to the point where we are. Like how we attach ourselves to that is going to be interesting to figure out like our we attend percent of that are we 20% of that are we competing with other groups for that like what is our. Because you know we could we could contribute to something that's hugely valuable for the company but. How much time is still worth putting into that if we're going to budget the needle 5% versus 60% and something that's ultimately less value for the company but we might actually be generating more value. Is there a lot of that small or whole. For instance I kind of comes to mind. Yeah so feel. A reinvent. Maybe we put in 50 hours. For reinvent and reinvent hugely successful but there is really another people working on that as well and like how do we determine how much impact our participation had and that versus the other folks and then judge. Yeah so that that is not a KPI that's that's on here now corporate it's a corporate events. Is not on there that's a great call I don't like this is not comprehensive right this is like a stab at. In incentivizing meaningful things that will move the needle right so we know that this team is team of subject matter experts this team knows the customer knows the product knows the market. Like no one else can generate the quality and type of content that we can for the web for demand generation or for field. And so and we know that when we do that we know when we put stuff up on the web like it drives outcomes for the business we know that when we do field events it drives outcomes for the business and that's why those are on the okay ours as well. So this is like kind of picking the low hanging fruit of like we know this does drive business outcomes that's why we want to track it. Yeah for something like a you know commit. Like you know we're putting amazing amounts of time in the commit how do we get credit for that as a PMM team I'm not I don't know yet. Yeah and how do we know like part of it's getting credit is part of it is just knowing where to spend our time. If we can if we can double if we can double the value of something that's worth a third as much versus increasing it five percent you know somewhere else. Yeah so I have to have two bits of guidance on that right so. Well let's think it through that for the most so I would say one if if there is. Some projects something that you believe if I invest my time and think x it will meaning fully move this needle like to your point is like this could this could like increases by 50% not just 5%. Like that should be easy to justify even if it's not like a specific KPI and maybe let's like have a conversation about it right like let's bring that maybe the whole team should be doing that thing whatever that is right so like if you see that thing and you're like. I can have big impact here that should be like the overarching. decision matrix right like as you're looking at like all the stuff you spend your time on like what's driving meaningful goals. The other one is like we got to keep the lights on right or what we call like business as usual so like this doesn't track like sales and able. The city lives on the government of the governments that doesn't track messaging and positioning really like it's OKR for us. But we're not like, you know we don't have like a KPI that's like. How many people used our messaging, like there's things that we need to do as a team. product marketing association or something. Do they have like a library of things that they share where you can see examples of what other people do? They do. So the challenge is how, so I've looked, I looked in all of that as well and they do actually there's a PDF they put out. If I can track down the PDF, I'll share that with a group. The nutshell of it is that PMM works really differently in different places, right? So in other orgs product marketing owns campaigns, they don't have like there is no Jackie's team. Like this team is Jackie's team and like this team has an MQL number that they need to hit. But like that doesn't make sense for us because like we're not the campaigns team. In other orgs product marketing might even own like a revenue number, right? Like when I was at Chwillio, like every product team function pretty much autonomously like the PM was a little CEO and the PMM was a little CMO and we were responsible for our business. Like we had a weekly meeting where we had like this is the revenue that our product is generating and the marketing plan had to contribute to driving revenue for the product. But we don't work that way. So in like the things that other, so these are metrics that have come from other. So there are things like other PMM works, they track their web content performance. They track their webinar content performance, right? They track like the demand, Jen content that PMM generates. Those are those are things that other orgs do. Some of those other metrics though are like don't make sense for us. Is bounce rate? Set and soon do we know like what pages that focuses on? I've always been a little iffy on bounce rate in general, right? Because if it's a page, if the intent is to have multiple views, if you have if the intent is to go to multiple pages of multiple views and a high bounce rate is good, right? No, no, so bounce rate measures like if somebody lands on the page and that content's like not meaning full to them. Like because you have bounce rate and you have exit rate. So this is the page bounce rate buffer. Okay, all right. So we want people to, we want to know about it. On the page in because if we want people to view if the success of the page we're talking about depends on people going to more than one page than a high bounce rate is bad. So we want people to go through a journey and go to multiple places. But bounce rate would be like I came to this page and I backed out of it. Right. If they come, if they come to a page and they're like this is the next step of my journey and then they go to that next page, that's good. Okay. All right. So but you both the, okay, there's just some pages where bounce rate wouldn't make sense, right? Because we don't want them to go anywhere else. No, no, no, let's let's you and I think on what bounce rate is because it's it's a little more nuanced than that. Okay. I will share with the team that another potential metric here might be like clicked through rate. So the idea would be if we identify it on every one of our pages, what's the next step, which might be go to another page or it might be fill out a leak app or there might be some action. And then we would measure that action like was the action taken. The reason I don't want to do that this quarter is because we're not instrumented for it. Yeah. So in order to instantly attempt them, right? Or you know, a lot of times it's showing like a symptomatic piece of a journey as opposed to being indicative of value. But yeah. Well, we're just, we don't have the measurement for it. So Google Analytics can do that, but we don't have the trackers on the page to measure. Not to okay. So this stuff we can measure today will do that for this quarter and maybe in the future we get more granular or more specific. Cool. This is cool. Sales QBRs. Optional. This this quarter. We probably should chat about what kind of a sync review we would want to commit to or we need to think about this. If anybody has a proposal, I value like a proposal. Like I think the team should do this. And then the an idea here is kind of if you think about it in like an 80 20. So like you want to get like you want to do enough research that you understand something and then you want to generate like more content out of that thing. So ideally like we're not like you know diving 50 layers deep into like customer stuff. We're just like we understand the customer we know the customer or the customer expert. Okay. Now let's go use that information and drive our all of our other content we need to generate. So thank you for the time team. Let's catch up a sync on some of these other threads that are still going on. Cool. If anybody happy Monday. Thank you. See you.",
  "That's right. So do our weekly agenda. Coremea, move gears up to the top. Ooh, I feel so fancy. Thank you. Do you want to kind of show you that? I will verbalize. Yeah, so I was just wondering, we have this strategic marketing off site this week. And I was looking at the agenda and I dropped a comment at there to a leaders point. Just that we should involve product. I think in that discussion about Azure boards. But yeah, it was just wondering if there's anything we should perhaps You bring to it. So we, you know, we can get the grant run. Nice. I noticed that we had a a one to three year portfolio marketing strategy conversation planned. And I thought, well, we could read our three year get lab strategy because that is something relatively fresh. But then I see a counterpoint. Well, you. So, um, It should be, it should be no prep. There's no, there's no ask like go and do this and show up ready with things X. It's, We're going to have some fun time and we're going to have some, Think about different stuff time. If there is a conversation you would like to have together as portfolio marketing. Did I share with this group that we're now portfolio marketing? Well, yeah, I don't know what call that was. That was the dot. Tod. Yes. On our last portfolio marketing all portfolio marketing calls. We efficient things that need to portfolio marketing. So, If there is anything that you would like to chat about in longer term format, Please do propose that. Tie and I were kind of like on one planning track and then. Tod was like, hey, I had all your ideas to do so. I think Tod may have some ideas and I'm not necessarily including those, But. It sounds like there's no prep and there's just like if there's something you want to discuss is all portfolio marketing. Like add that to the doc and so it looks like a leader you added. Something about Azure boards. You know, so we may or may not discuss that, But if there's something that you wanted to discuss, We will have like a two hour block. I have a feeling that if we do multi topics, we'll fill that up quickly. So, bye. Yeah, we don't have to talk about I just added it just so everyone is aware of kind of what's going on with that. But we don't have to talk about it. Well, so I would my request then would be if you if you want the team to be aware of it, Please add it to the like the portfolio marketing team agenda so that we do all see it because I don't know if everyone looks at that doc. But if you want to specifically say during our off site where we have like a two hour block of like extended discussion altogether. And you want to discuss that thing then add it to that doc. And that makes sense. And so no prep, but if there's stuff that you did want to discuss one of the proposals is, you know, We do some game storming if you all the purer that on strategy. So any other questions on just the off site. Cool. Then focus Friday's a permanent that is very exciting. And this is something that like it's for us or is this. So company wide, you should make every effort to not schedule meetings on Fridays. So Fridays Friday should be a catch up day where you work asynchronously. And so this is a. This is a nod towards having a bias for async putting fewer like we should just always be working like this. We should always think like how can I put fewer meetings on the calendar. How can I have a sync workflows and it's just the it's just the most inclusive like we're a global company. Everybody's in different time zones across all the teams that were on. And the most inclusive way to work is asynchronously. Of course, sometimes we need meetings like the one we're on right now, but we should keep those to an absolute minimum. And so. And in order to do that more Fridays consider Fridays basically no meeting day don't schedule meetings on Fridays. And this is a company wide guidance doesn't mean you can't have a meeting on Friday, but. I find that Fridays are my days to catch up except for last Friday where I had a big plan of catch up and then like five things route to my plate of the last minute that's okay. Brian was involved in one of them. I didn't want to talk to the call. So. Cool. So cute so cute to goals so. Ideally. What I what I wanted to do was ahead of so welcome to Q2 we're now officially in Q2. And this is the. YouTube goals issue which I don't think is anything confidential in here. And so I structured this around a little bit. And I want to chat about these ones a little bit more in depth, but. Ideally, like so in the next two months we'll still to execute on this. And then as we start getting towards Q3 or even now if there's anything big that didn't fit on this. Let's talk about as a Q3 goal. And so ideally when we start Q3. And so we have an idea of the work that we have planned. And I want to do this for several reasons ideally if we know what our priorities are that way when last minute stuff comes on the plate. We can say okay well if you want me to do X. What like this is this is on our plate what do you want us to drop to make room for that so that we can have that negotiation. I don't know that we this is one way to kind of show all these things that we're doing. And the other one is. You know common. A common area of struggle I think both for our team and for other teams. I think it's felt both ways is in the in the ways that we were cross functionally trying to negotiate that work. And so in the past sometimes it's come in ad hoc or last minute. Okay I'm working on this thing but all of a sudden this analyst report that if I would have known about a month ago I would have planned for it. But now that I dropped on my plate I have to shove other stuff off the plate or this field marketing event. So the goal is to try to avoid that like shove off the plate and plan for this stuff. So I asked the field marketing team to tell us all the events that they need. And they did. Has anybody looked at this yet. Yeah, it was a few days ago I think but it was that we're like three big events was what it really well found to you right. So yeah the other thing I'll mention just for because maybe you'll answer it or not but if I ask it first you will. Just so that it was a little hard to tell which ones were already taken. And each of them that I clicked into ultimately I found they were taken. I've been after reading through the thread and that kind of thing so I wonder if we could assert a label or is there a label that I'm just missing is there's some way they indicate we've already got the PMM for this. So that is a good question so honestly I asked for these and like a spreadsheet and I my thought is that field marketing would have asked us for like 20 events. But in reality it was only 10. And so among a team of like six people including myself so five PMMs in me that would serve as like speakers and moderators for these events. And Alita if this is something of interest to you we could also like include you in the loop and get you kind of doing this kind of thing. But that that feels kind of reasonable that's like less than two each for the quarter does that seem like a reasonable amount of. And I think we've done much higher than that. I think we've done much higher than that. We've geased in the last quarters. I would say I've done a case four to five. Yeah, I feel the same way too, but. Yes, I'm surely I thought this was going to be a lot longer list. Is there an allowance for some number of them that will still pop up. I'm guessing are ones that they may not have thought through my my explicit ask was. Is can you tell us everything you need for the quarter and is that reasonable if it's not reasonable then let me know. But is this a reasonable asking I was assured. Yes, this is a completely reasonable ask. So to your point, maybe everything's not on here, but at least we have a we have a framework so something else comes on this quarter and it's not on this list now. We have some kind of a framework to say we either can or can't support that. So to Brian's point. I am wondering. If there is like a way to sort by like a sign. So I think a two ways to do this Brian one week, like poured all of these into a spreadsheet so we could look at them and say okay well there's 10 of them but seven of them are security and they all want Cindy right like that's that's not going to work right so I don't have a good way to sort these to now. I just kind of wanted to just wanted to chat about it as a team for ideas. I asked for them to spreadsheet there like no, no, we work out of issues we want issues as I guess use this fine. We'll work with the issues but to your point Brian this view doesn't help us so any suggestions on how to like. Parts this. We can just have a of you know two state label got a PMM don't got a PMM. Something like that. Oh, I'm looking at a summer quest as sign with me. Yeah, that's what I was thinking but that is. Are they each individual? Yeah, that would work. Okay, so could be like label. We've we have one. Yeah, we've we've got it there. That's right. Yeah, I hate these labels but hey if it becomes helpful. So let's let's see actually it would be label so it looks like two of them have that. So we would want label. Does not equal. SM request. Aside. Yeah, well then is usual it's label hygiene so like if somewhere in the comments someone says yeah, but I got it. But then the label doesn't flip then then it still whatever. Maybe it wasn't a big enough concern to like to engineer a solution. But yeah, I mean it it would be. Well, there's a May fifth event there so hopefully that's the sign to someone. Yeah, look at that that's like tomorrow, right? So that's even tacos and tequila. Is there anybody else? Oh, for for furnish take in this one and this one got moved actually so yeah, that's from last quarter and it or maybe it did guide move to the 20th. So. And I was I was thinking of Emily's comment on the issue that where they mentioned that they need help on Google next. Cook on and AWS. So that's that's also on the list and that's under corporate marketing. So so field marketing and corporate marketing are like they both run events. Right, and so corporate marketing are large company wide global events things like coupon Google. That's where it seems like there's more because there are more. They're just kind of derives differently. Yeah, so like big events like reinvent or coupon those are corporate marketing. Field marketing will be like a regional event and in a non and noncoped times. This could be an in person event, but it usually would be for region. These days are, you know, they're all virtual because of COVID. So that might be some of the reasons why we feel like we're doing so let's chat about corporate marketing in a moment. I'm hesitant to prescribe the label because it's like just an extra step if there is a way, but if that becomes helpful, maybe like being go through and try that out. If you pick one up. Maybe change a label to a signed in. Since there's only 10, I'll try to go through these by the end of the day. I think my ask would be like, can you go through. Maybe that's maybe that I just need to do that. Maybe I just need to go through and see if they have a signed. For field. So what's. Do that. Okay, so field. I'll do this today and I should have done it for this meeting. To see what's not assigned. If I ping in the Slack channel for folks that picked stuff up, does that kind of work? And if you see any that are like, oh, this is my topic. I can take this and then please do pick it up. Common. And every. Yeah. And I only commented because everything I checked was ultimately assigned. Like somebody had somebody had picked it up. And there wasn't a way to see that. And it was like 30 or 40 minutes of reading, you know, comment threads to go through the list of them. I was like, well, we probably all don't want to do that. So it's it felt like, oh, what's. What's a better way. You know, better ways you do it. Yeah. So a 1% does it. Yeah. Maybe I can make a board because you can put these onto a board. That's good. And then you can have like. And then you can have like a lot of columns for people. And so if it's not if it's not assigned to like somebody on this team, it'll be in like the not assigned column, right? Maybe I don't know. Let's. Let me think on that. It's there's a reason why I didn't want issues, but. If it's user easier work with. Okay. So corporate marketing. This one's a bit more complex. So field marketing. I asked for the quarter. I was told these are all the ones for the quarter. Let's see how it goes. And the corporate marketing has been. A lot of back and forth. And. So one there's kind of two ways proposed to approach corporate marketing support. One would be. We as a team ahead of the quarter. Assign a PMM to each corporate event and say you're the PMM DRI for this corporate event. This is kind of what we have done historically. The other way would be for GTM teams to adopt events. And this is what Todd would like to see. The idea is your entire GTM team, not just you, but the campaign manager, the content manager, like. Everyone on the team is responsible for that event and supporting that event. And so the way that you would think about it is we are a GTM team. And this is the case where I think I've said shared some of you and I have had this conversation where like the names of the GTM teams are increasingly become meaning more meaningless. For example, like the CICD team, are you really a CICD team? Probably not. So just think of you because like did you, you know, did you run a CICD sales plan? Well, kind of. It's really a just like off sales play. So. And then the point is, okay, if you're the CICD team and you pick up, let's say, Coup con, are you going to only do CICD stuff for Coup con? No, I would not expect that. I would expect you to like run the meaningful campaign for Coup con that needs to be run. Right. So I would just think of yourselves as we're across functional team. We have a PMM and a content marketing manager and a campaign manager. And other folks too, I forget everyone was on the court team. It's the team of person and everybody, right? So the point is, as think of yourselves as we watch across functional team. And for the quarter, you have certain activities. So you're going to generate a sales play. You're going to support a certain number of corporate events. You're going to do something with campaigns. And hopefully we have enough time on this call to talk about campaigns because that's confusing to your changing or something. So does that kind of make sense? Is that a good framework? Like, we're a team and we're just going to execute a certain amount of activities together. Is that kind of like a good framework to think about it? It is because I think because, well, first of all, thanks for, thanks for addressing me, I'll fit the room and saying that that, you know, there's a disconnect between the names and how we operate because I think that's getting potentially a little confusing. You know, we talk about things like, Get up, take out like, if we're doing a platform, get up, take out, do we avoid the, you know, CI, CV showdown and talk about the benefits platform when we're doing that, you know, so that that's really helpful. I think that also helps to when it comes to how we were the sign ourselves to these things because. Yeah, I, Cubecon is like a good example, like if I'm going to get up, team, I look, Cubecon. But well, that seems like it might make sense for you that AWS might be too broad. So, I still think that those three events might not necessarily. They might not all be equal and that we might have, I mean, our sponsorship level might be different, so we might have different demands. I could see I could see two teams maybe wanting to be part of something if we really need the. We got put and we have other things going on. Yeah, like, I mean, just case and point, AWS is the biggest event of the year, like whoever picks that up, there's more stuff to do than with other events. But they're all a big lift and. I think it would make a lot of sense say, Samya with the the area of things that you work in if you're team picked up Cubecon. Now, just, certainly, I mean, there's a lot of things in cloud data that have to do with security, city road, whole book cloud security, like. Since you could your team could totally pick up Cubecon, like Brian there is like you could tell anyone on the team could totally pick up Cubecon, I don't know that it's constrained like that and to your point, core Mac. We should make the choices that are best for the business, right, like you should not look at like, okay, here's a campaign we're working with, here's a sales player here's an event we're supporting. And I'm going to constrain my set of activities because of a name that somebody picked, right, that's like the worst possible reason to do something like you should be thinking like what's the thing we need to do for the business, what's the what's the talent that we have around us, what are the skill set with the knowledge base and what is the business need like that should be our decision matrix in my opinion. The proposal here and so it would be another issue so just like I have this field marketing issue where we're going to pick up things there would be like a corporate events issue and it would say these are the corporate events in Q2 and. I can drive that because I have the list of all the events I think and then the ask would be for you to go to your GTM teams and say we need to adopt. We need to make sure all the events are covered this quarter so there's four that means one team needs to pick up two. Right, and then if you're the team for that event support and the whole team is on deck to to do all the things to run the campaigns for that event, you know, the the pre campaigns to get people to go to it to follow up all that kind of stuff. It does it have to be a single GTM team. A particular event called. The topic of a particular event puts fan multiple GTM teams and we could have potentially two GTM teams working on the same event for example like AWS if it's a big event we could have to GTM teams working on it. And and maybe sharing the responsibilities of campaigns and so on as well because I think it becomes very difficult to draw a line and say that this is only in this particular GTM motion, especially if we are saying that. At the end of the day the metric that we are going to be tracking is you know the number of leads and you know the number of. A R R and so on for a particular GTM team then you may have multiple GTM teams wanting to life for a particular event. Right, your team's in my so some of the feedback from the events team is we're struggling to get support. We're asking for support and we're not getting it not necessarily from this team I've really really asked okay like is it a steam not a celly from this team but it's they don't have everything they need and so. Teams the GTM team should be fighting like everyone should be saying I want that event because these events are high they're like generate a ton of leads and a ton of pipeline. And like all of that those metrics get attributed to your team. So when you look at like the success of your team if you are the team that's supporting an event you get to count. That events success and so so some yeah maybe maybe I would say reinvent is not this quarter so. This quarter let's try the framework of one GTM team per corporate event and kind of see how that goes and what the workload is. But as you're thinking about as a GTM team what's the stuff I need to do this quarter I need to get out of sales play I need to do some stuff with campaigns and I need to support corporate events there might be something else there but I I think those are the big ones. Cool then look look for that issue as well I think that that needs kind of like probably a dedicated issue hey GTM teams who will pick up which event. The next one then is sales plays I think that one we can do quickly. There was a due date where there was an ask last week I don't know that it was super explicit or I don't know if I asked everyone on the team can you commit to this due date. But the due date was Friday. So now I will ask is there anybody who. Like so the ask would be today can you put in your proposed sales play and the proposed delivery date. Not the date when you can run any campaigns around that sales play but just like when can you deliver the document when do you think you could have a usable document. Is there anybody who thinks they could not put in that proposal today. I don't understand the process so are we proposing what we think it should be and then gets vetted and agreed with sales. Exactly or are we having that conversation with sales and you want the vetted answer. So that's a great question Sine because I think it could go either way I think if you've already talked to sales which I expect you would have I know saw me you. And when we say sales we mean everybody from the sales and the AEs to all the way up to to make be or whoever you need to talk to you. So this so my expectation would be that you have already been having some amount of conversation with sales. But this would be a formal proposal to specifically pile and oh now. So last quarter there was. I think it worked out okay it was mostly done in like. Phone calls or was phone calls and there was notes from the phone calls and if you wanted to understand what happened you had to like go dig up all the Google Docs. So this is trying to kind of like short circuit that and say okay here's one issue and these are the proposed plays and we're going to get explicit. Approval from pile and oh now to go forth and if they think no don't go forth on those or know those are not good dates. Then we can have that conversation it's a documented so that's the the idea is that like yes you'd be talking to sales but this is the documented official place does that kind of something good plan Cindy. Yeah it sounds like a great plan. I guess I must have been asleep as a wheel and not realizing that that was what you were asking for so. I could give you my opinion. For today and I know Parker was there was there were conversations in the CI series flat channel among us and the marketing team about what they might be but I don't think anybody's better than the sales yet. Gotcha and you were not asleep at the wheel you got like a ton of last minute heavy hitting projects put on your plate. And then you were out of pocket and I did talk explicitly of Parker and Parker did tell me well Cindy's not a Cindy's out for this and I'm out for this. So. My expectation is that you would get input from whoever you need to get input but that you are the DRI and so if you can't make if you don't feel like I have enough input to make that call today which it sounds like Cindy you probably don't because you want to collaborate with Parker. When it. Yeah we need to collaborate with sales. Gotcha when do you think would be a reasonable date that you could give a proposal to pilot oh now. When's there a Thursday but I mean. Yeah when's there a Thursday okay. Is there anybody else who needs extra time or could you put a proposal in today. So. So you go ahead. No, no, please go ahead. Yeah, so good. I think we discussed this last time as well. I don't believe that we need to have another place specifically with get off one place should be sufficient. So. I'm happy to work on some other play if there are 30 plays and we can work with this particular GTM team on on those places. So that's what I would propose because from what I'm seeing what I'm hearing from from SDR managers that I'm talking to and with the sales I'm talking to you know a single play on get off which kind of covers I see and so get off and the whole infrastructure automation concept. Should be sufficient it should not it I mean if anything more might be overloading them but additional place. I completely agree you know I've said about this a lot so I mean I 100% I think I've mentioned everyone individually on the call but I'll say it here and I want to try to document this I've got to think to back love to document but. I asked by how many plays do you think there should be total and they said like nine and I said now how many plays should there be total and they said like four and I asked. You know so where's how many like if we have eventually have a bookshelf how many plays should there be to pick from and you know so where's like less than 10 so. At some point yeah we're just going to. This should be the sales plays that we run and we update them we keep them fresh but we're not just adding to the library so I agree on get off some of the do you already do the enablement on get ups. No so that's blind end of this month. We hope make 24. Okay let it you get me a favor do this let's consider that this quarter sales play then and then for next quarter. I don't even understand who made this commitment of like the GTM teams are going to deliver a sell a sell like each team is going to deliver a sales play each quarter. I was never asked for that commitment so. I'm just running with it for Q2 because that's what I was told but for Q3. Let's re evaluate that so I'm kind of trying to dig in at this to figure out and this is the so where's as asked for this so where's it what's a total number plays that we want let's prioritize them. So, so I'm going to consider your get us play that this the one for this quarter if you could put that in the issue and then core mechanism right and how much time would you need to be able to have your proposal to pile and oh no. So we have the proposal and they're already but we just don't have the dates. So that was. I if I pinned you down and said what's your what's your best guess at a reasonable date that you would have like. 80 90 percent confidence I could deliver something usable by that date. Well let me ask a clarifying question on that when you say deliver the sales play do you mean just the play itself or do you mean all the campaigns stuff around the play just the play itself. So it is the finish line the enablement session like is that it's done now or what's the. The order of operations and I try to outline this in the issue or in that epic is these are great questions and candidly this has been very confusing. I think these are all super reasonable questions and we sorted all of this out and one of those like 20 mRs that we did. So it's like if this is not clear it's I think that that's it's reasonable so the order of operations this isn't the handbook somewhere now. The idea would be you propose your play and all you're saying is this is the date I can have the handbook page. And not that it's even like a completed finished handbook because nothing that gets ever finished everything's in draft right but this would be a usable is what I would call it usable a sales person could take that page and run that play and it would be usable. clarify that here in this said that summers and anybody else that's looking at this understand everybody that the same common understanding for what it means to be finished delivered. usable. Nothing. Or we call this is what usable web page. You that's this is a great question usable handbook page. So everything at get lab is. And draft. Will always update. Iterate it's never finished but we want to see something we want to see a handbook page delivered. And then we want to see what is usable by sales by the due date. And let's let's think offline and refine this if it needs to be more clear but the idea is you this is like the date you can merge to the handbook. And like a handbook page is live. And then based on this date of when you can have your handbook page then this is on somerson team. And then you can see that there's a lot of them to then decide when do they want to schedule the enablement. And it might be like the day after. In which case like there's high pressure for you to get your. You're thinking delivered on time and it could be like a month after it just like this is like when you can deliver a usable handbook page. At which the assumption then being. Any point after that. So where's could schedule an enablement. And then you can just go to the next one more stupid question remind me what q two is what months. So it's this month it's a may June and July. Thank you. That's a great question is that's not a dumb question but it's they're awkward because January is last year. I always forget I know it's one way or the other and I can never remember which one. I count on my fingers like every word March April. It's cute. Cool so so the idea would be like you say the date you can deliver and then somers comes along and says this is when I can enable. And then pilot or now say thumbs up we approve that and if they don't. There's a means by which we can have a conversation. Again ideally this would be have already happened but. I feel like this is better than what we had last quarter so we're making progress. So what's the date so the get ops one is may. Right so essentially we could say get ops like get ops could be like get ops the play would be like get ops. And then the date would be like you know. Done. Got that no so I mean had the date why not put it right here you put you put get ops and here you put. Oh five dash 24 so the enabled. The reference that the top means a date to. Is it not done is it does it's it's it's it's it's it's it's it's working progress. Okay so so that so put a date here. XXX so so yeah so put whatever date you think you can deliver that by would you be able to put that in today. Yeah. Cool. And then. Some are already has a schedule so when you all get your dates in here. Then they will then go and or their team. We'll schedule here and then we'll ask for approval. One more thing on the dates. So I'm you have your release manager for me and you're doing this on the 24th. I highly recommend swapping with someone. It just about killed me to have. A release for this and the release post at the same time. I think the release. Well thanks for letting me know in Sunday. I think the release post is the week before. I think the Saturday before the week of the. Uh, say it's an ablement and the enablement is I think on Thursday. So I will have about four days. So I'm guessing I should be okay. But we'll see. Um. Think thing about us on me on it. If you do decide you want to swap just let us know as soon as you decide that. And if not, we'll consider you on deck for it. And. So core Mac and Brian, when do you think you could add your proposed. Play and the date you think you could reasonably you're like 90% confident. You'll have a handbook page up. And we're talking about that now. So I think we can have it done within 10 15 minutes of the end of this call. Okay. Perfect. Then. I'm the whole doubt. Sorry. And no, you're you're you're good. I think we're way ahead of the game. I think last quarter we didn't have this at all. And, you know, you had a lot of last minute stuff last week. A lot of heavy hitting. If you all didn't see Cindy's blog post. Um, they put out a pretty significant blog post last week. Um, I tweeted it, but you should share it as well. So it's a really great. So in addition to 24 hours or less. It's a lot. If you look at that post is a lot of content. There was like, how many how many different people around the or do you know how many people contributed to that. I ping probably they're probably six or eight different because I had to get AR. Approval had to get Jonathan hunts had yeah, it was. And so this was something said wanted. And thank you, William, for pointing out that Todd really wanted it published that day. So it was on vacation Monday, Tuesday, come back Wednesday and see. Uh, that Todd had been had signed me up for doing this thing. And William, including me in that, oh, and they wants it like tonight. And you got to done so. So I'm from my perspective. I think we're firing all on all cylinders. We got out of great blog posts. And you did and then like we just get to like bask in the glow of that success as a team. But in terms of you and Parker, I think it's completely reasonable to say by Thursday. You and Parker will have a proposal in there and that's completely reasonable. Would they see them in that. Uh, thank you for that. No, it's right. You do it. Yeah. I've been in the other. Please got. So I have them out today and tomorrow is what I'm showing on the calendar. Yeah. So. So Thursday is reasonable. Cool. Then. Can you leave a comment on the epic to just say like. Parker's on a pocket, but we'll have our proposal in by Thursday. Yeah. Cool. Uh. Moving right along. We did sales plays. Really good discussion there. Corporate events, field events. Um. I don't know that. I'm going to have time to get through all of this stuff. So. Uh, at a really awesome call with a new. Melissa for new. I think Emily Kyle was on the call as well. And. Normally I'm not a huge fan of this like we went and had this tiny little call and then we're going to go tell the team. But in this case, I think it was helpful because we kind of just did some bigger dreaming about how marketing and product collaborate. And so out of that. Uh. We realize like there's bigger surface area and for this team in particular. Um, I think that you and all of us, we already coordinate with product a lot. But. Um. Something that the PMs may not have top of mind is that. Um, when corporate events are coming up. So we talked about a lot of things, but one proposal was okay, here's these Q2 events. The ones that your GTM teams are going to pick up right. Um, if we tell our product managers, hey events are a time where you could promote capabilities. And you may want to adjust your roadmap so that something ships before the event. I've done this in the past. When I was working with Orie, they shipped our first. AWS container with the command line of the container and that capability they moved up in the roadmap and we shifted before reinvent. So that's could just kind of one opportunity is just to have a conversation with your PMs and clue them into the hatey's events are coming up. Um, that's one area. I'll talk about that on the product call as well as just like, hey, this is one way that we can have more collaboration between product and marketing. And then with this with the product marketing team being the focal point of that collaboration. Uh, we don't have a ton of time to discuss, but any just thoughts or questions on on that kind of collaboration. That's fine. Is that related to the tool I haven't clicked on that one? Oh, and then also yes, then during that conversation, um, our newshad like these two tools. I know that like Brian you've already been using some of them. they has for feedback on them. And so what number is that? Number six. So I logged in issue. I pinged everybody in this you. If you would please, you know, take some reasonable amount of time. If that's 15 minutes. If that's an hour. Play with the tools and then think about how what I use these and then just provide that feedback to far news. Um, would you all be able to do that this week? I think Apple already provided feedback. Done. Cool. That also came out of that conversation. Was that for you said, Hey, I've got these two tools. Here we go. Feedback. Cool. So, Uh, Technically the call is over. And so I'm happy to end the call. Really quickly. I didn't want to interrupt you when you were kind of flowing. Um, Back to you queue to play and at the beginning of the setup. I think I have a document where I've already started kind of outlining kind of what the plan is for upgrading them out tools. I was planning on sharing it on our next call with I think I'm more concerned. But some of the we may have to tweak a little bit of you know what I have in the document or what they had like for example I don't I don't see this training kind of me but I think you had a lot of things. I've been up to five ten features or something and a document I have I had five prep category but we can do ten you know whatever you guys think is best. I would say a little you or the DRI and my ask to you would be can you make an explicit ask to this team. The same way I've asked field marketing field marketing can you just define what do you want the team to do for the team. I would say two your teammates can you just say PMM this is what I'm asking of you this quarter can you commit to doing this. Okay. Yeah. Whatever you. Yeah. Yeah. Yeah. Yeah. Yeah. Yeah. Yeah. Yeah. Yeah. In that. It's you that you have. Yeah. I added a that an item to our note start for this meeting for that if you can link to whatever it is we're talking about there that would be great. Okay. Which number are we on. Well, I added 11 item 11 for Lita. Oh yeah yeah, rubbed it into about tools see doc. Okay. Okay. Okay. Okay. Yeah. Yeah. And I would say if you could just summarize the ask a Lita like I want you to commit team to doing ABC. If you can put that in the Q2 goals issue, whatever you whatever your ask is that's what we'll see if we can commit to. I am on mute. So yes, I will add it. Cool. Then ask for some feedback and several of you gave me really awesome feedback. I was so freaking happy. So I know I asked everybody in this anonymous so I don't know who gave me the feedback. But man, y'all gives a really really good feedback that I like super appreciated and if anybody else has any other feedback. The link is I think it's like number one from last week. But what are the things that popped up was, hey, we are the product marketing team. But whenever a new partner marketing manager gets hired that reports in the dunk, the team member updates channel says new product marketing manager. And that's a little bit confusing. And can't agree that that feedback form is not the only place I've heard that I've gotten a bunch of slacks about that. So hopefully in 60 seconds I can clarify just a slight bit of that. And if there's more questions, let's talk about it more. Feel free to ping me. So there is a difference between the name of teams and how somebody's job is defined in bamboo HR or HR system. Right. And so in bamboo HR, we have job families. And the job family does not necessarily relate to the team that you're on. If that makes sense. And your specific role could be defined as any number of different job families. And so partner marketing in alliance is marketing is actually defined in bamboo HR as a product marketing role. And so that's why when the people last group makes a new announcement and says this person is hired as a senior product marketing manager. And you're like, well that's their title, but they're not on the team. Well that's not really their title. The real title is like, you know, senior alliances marketing manager or something like that. But it's just defined in the system as product marketing. And there's a lot of reasons to do that. You know, that takes probably more than 60 seconds to go into like why you would define one role, but like you can kind of imagine if you created like unique job families for every single role in the company. You might have too many of them. So it kind of makes sense to consolidate this one of the reasons. Any just clarifying questions on that or any this other questions on that. You're really good. Good point to bring up.",
  "And, Sam, are you brought up a really awesome concept last week? And, I said, hey, let's have some more discussion this week. Do you want to kind of chat through your initial thoughts on kind of item two there? Yeah, so this is what we used to do at HV. So, in my group at least, so one of the things we used to do was track the top 10-20% deals which bring the maximum revenue. The goal there was likely different basically the goal was to maybe assist the sales teams in case they need help from product management, I used to be in product management at that time. And then we used to work with the sales teams, they don't accustomed calls if required and so on. Now, I think the goal that we have here is probably slightly more different. It's more from addressing a messaging point of view, you know, perhaps improving our messaging based on the deals and the conversations that sales threats are having. So, we might need to listen into a few protocols, talk to the specific sales threats. So, I'm guessing it might be 20-25 deals. I haven't looked at the dashboard lately, but that's what I think. And we might also then want to look at some commercial deals as well. So, in total we can my thought was we could take an overall of about 30-40 deals and split it across the team. And then, you know, maybe in a few quarters we have a good understanding of how the conversations are flowing all the sales threats, where the gaps are from a messaging point of view, working cases are working, what's not working and so on. Right? A lot of that information might be qualitative. Things that we are not going to get directly from SFTCs, we might want to start working with the sales working directly with the teams and getting this information from full assist fund. So, a couple things on packed there. So, I think that our goals as a team in keeping a breast of and reviewing some segment of deals that we are involved in. And let's chat a little bit about like, is it top revenue deals? We have to have commercial in the mix, which, you know, any one of those is not going to be like a top revenue. So, we have to let's talk about that a little bit. But even ahead of that, let's chat for a moment about our goals in this exercise. Because I think it goes far, far, far beyond just messaging. And I think it's about the charter of our team like, what are, what are we responsible for and what, what should, what should be within our domain. And I would say that there's kind of two things. So, yes messaging, 100% and then in addition to that there are two other elements that come to mind. So, one is to assist in the deal. Also, I would say that in most of the places where I've been a PMM that the times I felt best about my role is when my PM has said, you could swap out William for me. Like, I've had enough of the relationship where I understood the product, the roadmap. We had a tight enough relationship that if I was in a customer call or an analyst call or whatever, whatever feedback I receive, I could convey that with high fidelity to the PM. I think that level of type PMPMM kind of relationship is difficult when we have like a 1 to 10 ratio. But we have almost a 1 to 1 ratio at the group manager level. So, all that's to say is I would say that I think Cindy, you do this really really well. I know I used to do this a lot more and candidly I haven't done it a while, but especially when there used to be a lot of Kubernetes deals. I would join calls and serve as the expert on that. The market knowledge of how the industry was working of what other tools were and how the ecosystem played and in particular what Gillab's capabilities were. So I would say that idea of like helping the deal move further is absolutely something that this team. Should have as part of like honor you know that utility belt. I don't know that analogy makes sense Batman had like a bunch of stuff on their belt so they had like you know if there's like 100 things on the PMM tool belt. That's at least one of them. I'm kind of rambling. The second so help the deal alone and then also for us to understand the buyer's journey as a core competency. So I would say a goal for the entire team should be if you gave us a whiteboard we should be able to construct from memory. All of the steps that a buyer goes through from like haven't heard about get lab yet to talking at commit. Right so from like acquisition the our pirate metrics like you know to referral to becoming like a referral customer. What are all those steps in that journey and we should be able to whiteboard that and so in order for us to understand. The the half of that that is like okay now there's an opportunity and then sales goes and works out opportunity. We should be familiar with that process and know it and understand and deal the whiteboard it and. Be able to help optimize that that's essentially what a sales play is at all though there's we're still defining sales place. Just an aside harsh has done a ton of sales plays that I think is really really going to help us out here so just want to side there but. I'm going to stop talking. What do you all think about those points helping sales and understanding the buyer's journey. So in my previous role as PM. I used to do a bunch of those things right so like like you mentioned but here since we still. Defining the roles accurately I that's why I call that messaging but I absolutely agree with what you say about us being point on. Because talking about the market knowledge about where the industry is going what the competitive environment is and so on so don't agree. Cool and and then city I know like I said I think you do this really really well. And they just kind of thoughts now on as we just kind of set goals or directions. That you share with the team on just being engaged with the sales process. Well in terms of what where how I contribute and what. The purpose I guess of my participation in the sales processes to. To accelerate the sales cycle. I'm by by removing barriers really overcoming objections at kind of a deeper level than what the sales team can. I do give feedback to the PM's as well and pull in the the appropriate PM for an even deeper discussion when appropriate so I'm kind of doing a little bit of triage to try to shield the PM's from. Generic discussions. Yeah, but in terms and in the messaging yeah absolutely I try out. Messaging on them and continue to refine it you know one of the things that. When I was shadowing said they was doing the. The non funding funding tour you know we're seeing is talking to investors without trying to raise money yet this was just to plant the seeds. And every investor meaning they would go into they would come out and say what could I have done better every single time. And I've tried to do that when I meet with customers what could I do better how can I evolve the message. Where's a miss you know. So it's it is to some is point about messaging it's an opportunity to continually refine the message. So yeah so I think there's there would be like four goals on us being involved in the in the sales process and the sales cycle. Primary research of our messaging and Parker I think you listen to this call as well. I think I linked it maybe last week. It was a last call I had with pile somers onel hung anyway. Oh, now I'll said this they just said this concept of it was like. Our research on messaging should be research on. So, so one of the things that they wanted us to avoid was something like going to the sales team and asking the sales team what messaging works for you. Which would be like a step removed instead we should be directly talking to customers and prospects like Cindy. Is doing to try out that messaging directly on the market and get that direct feedback and I just kind of think of this like. Yeah, and the same way that a PM would iterate on the features and what you know they ship a feature and who's using it and they get feedback on that same thing with us on messaging doing that primary research. So it's not just sales calls to you can get that from events and round tables and totally agree. Yeah, so our our higher level goal is like market tested messaging. And then there's several places to do that one of which is the sales cycle. And so coming out of from the other end we're saying like why should we be involved in the sales cycle. And while it it hits that goal of of market tested messaging. So say market tested messaging knowing the buyer's journey assisting sales on key deals as a subject matter expert and. PM tree treeage assisting. We get feedback and pass it to them there's like an element where we we act as a the hand of the PM or an assist you know assisting them on on their direct feature. Work. The same way that I think that that they do that they would do that for us for messaging as well that there would be a team. Cool. So quick questions in the on the calls that you do get on with with sales is it something that you look up on S. PC and talk to the sales rep and get on to those calls or visit the other way around you get invited to those call. I get invited. Yeah. I was going to say it's organic for you right you you naturally get invited nothing probably to much higher rate than the rest of us. So my question was going to be Leo get up buddy. Sorry. How do we get the same. You can get a you. You can go in there all going so I want to add one more net new we need to be looped in. Well you need to if you establish yourself as a subject matter expert in a particular topic they will naturally loop you in because if they're talking was somebody. It's going to go deep on that topic. They want to have someone there to back them up on questions so. You know whether it's CI or Kubernetes or. You know what to get off spick a topic and just become the expert on that and part of it is knowing not only what we do and how we do it better. But what the the rest of the market so how to how to our competitors work. And what are the common objections what's going on in the market. You know, let's get all that I'm just I had a week but had like it's still there's a forcing function that we need to be invited like I think we're you know we can't overnight change the perception of other people and I think we all have a pretty good. I think we have a good area where we're subject minutes. But what how how can you get holding is it's like. I'm like once a week but what I so the other thing that I do though. I have relationships with some of the reps and I've done a few things with them so that they just naturally like David Wells calls me into every security deal. He's found that they can go faster when they does. But the other thing is in QBRs I look for big ultimate opportunities and I have kind of a running list that I keep and then and I. If I don't hear from some of those reps. I have on occasion not very often but on occasion I've reached out and I could see where you guys definitely would do that. To get your foot in the door with them so find some big sea ideals and reach out to those reps and say hey you know. You could either offer to help or offer to just ride along and listen. For feedback either way I think they would they would welcome the engagement. Yeah yeah I think all all of them and coming back to like all there's so there's kind of three bits of advice there. I think one is. Being an SME and deepening your your subject matter expertise and maybe even in some ways of angelizing that. So it's like being the SME. Building the relationship with sales and keeping your list of top deals. I would say that that's been the same thing for me when I've been invited to calls. It's been because of those things like folks in Vittanyan because I knew about a technology and. Candidates is probably why I used to get invited in a ton and it's tapered off a bit because all the reps I had really good rapport with or you know maybe left get lab so. It's now incumbent on me to continue to build those relationships and I think all of us. And then towards the keeping the list of top deals that's like the topic at hand right so the sum is original point of why should we keep a you know keep a breath of. Kind of top deals with a quarter. Yeah one of those ways is we can do a reach out we can reach out to that. And so I think that's the way to get up and be like hey I saw that you're working on deal X and that deal is a sea ideal or that deal is a good up steel. I do actually I got pinged on get up the other day and I think I sent them your way. So I'm yeah. And it's not coming to mind what the specific deal was but I think those are kind of the three. Three ways so this is like. I think that's the one I miss because personally I throw myself out there a lot like every QBR every every interaction I have on what they work here uses and very very serious about it so. Yeah happens it's not the way that I would like. I think having that list would be good. You might need to be more specific about how suggesting how they can move so. You know, you're very specific information about. You know I can help you overcome concerns with Jenkins or I can help you over you know that. Yeah. The thick. And as otherwise it's a great idea. But they may not. I think to engage you because it's abroad. Yeah. I'm not sure what what they could do. I think I think if you think about it as well like what are the. I think if you think about it as well like what are the biggest challenges for our sales team right like they want to. Excell ultimate that's like their number one goal right so. Security is the is today is the way that we sell ultimate. It's a fabulous quickly becoming a way that we do that. So Cindy and Kormack are they're just going to be pulled into more deals. Yeah. And we're going to see that the so on. And we know that we can. We can't wait to say. But there are a lot of emotions, there are land with get upsmotions. There's a ton of accounts where we. We can't get in with the dev team but we definitely get in with the off steam and the folks trying to do I see. Yeah. So that land motion I think so me is one that you could get looped into and then Parker I think that. There is a lot of either land with CI where they want S. C.M. NCI or here's this SCM account and we need to get them to adopt more things. And see guys the gateway to security, right? So in a lot of ways, if before they can't even have C.M. You come and help them and talk about how great security is, they really need to get on the C.I train. So. Yeah, great point. So anyway, I think I think C.M. You coremaker always going to get pinged more just because that's where the that's where the dollars are. Because they rock. Well, and because they're great too. So coming down then to chat about keeping track of some number of deals. Each of us has like this is our list of deals that we track. Let's see. I'm going to start like a new item here item F. Which deals to track. So let me let me ask this Cindy, I think you probably do this the most formally where you have a list. I had had a list in the past that don't currently does anybody else have a list or report or any thing like that. But let me ask this are all of your deals Cindy enterprise deals or do you track any SMB or mid market. Just enterprise because up. I'm I only get involved in the really big ones honestly. So. Let me raise this question which I think I think makes sense I think when it's a very high dollar amount that's warrants our time. And also those sales cycles tend to be more complex and involve more stakeholders and require our time. But. What do you. How do you all think we get involved with SMB and mid market what's what's our path to surface area there. I don't know I mean honestly I don't have time to get involved in ones for 30 seats or something I I'll only do the small seats if it's. A big company that has 2000 developers so they're just trying it out with 30 seats but they have a potential to grow to 2000. And it's just not worth my time. To get involved in all of the the ones they chooses I think it would be good to understand what challenges they have and I if I had more time. It would be nice to go listen to some of the chorus calls to see. Sorry to see how those conversations go and understand what barriers they come up against. Don't have enough hours in the day. And I would say the way the way we should look at like the structure of it is not necessarily like if I had more bandwidth I do acts but like what are we prioritize. So the so in my thought process is if if we're only involved in enterprise deals. And it's not that's I don't think that that works for the business and there should be some amount of bandwidth so maybe there's like if we're like have our top seven enterprise deals. Maybe we only get involved in the top five and we just saying no to those other two. And the reason we do that is to have some surface area to mid-barkiness and be. And the thought process there is that if we. Consumably like hone the messaging and hone the process right we understand that buyer's journey and so we can. Understand the process that's working and we can proliferate that and help to proliferate that. You know if you have a good SMB motion and you can you can figure that out with with some small surface area but then you can scale that and that can scale really really broadly quickly. So we could similarly drive a lot of revenue right. So we can use the SMB and the micro just proving grounds in certain areas right enable better and like you know I don't know get things ready. There are certain years where enterprise is going to be different right it's going to be different messaging with different pains level blah but where they aren't as far as enabling our team. You know like spending time to make sure the SDRs and then marketing and SMB are a rate or rock and then performing at a high level right and like doing things to kind of like. BRIs and years while we're there to be able to focus more time on enterprise I agree with that. So I've got. Yeah got. I've got two two thoughts one one is that some in market deals can be like enterprise deals where you know it's has a little bit of a longer cycle and there is a key stakeholder cell. And I would say even though the dollar amount and the eventual vision of any individual mid market deal you know so you might go into enterprise you might be like what we're selling we're landing with 30 seats but there's the potential for 2000. And you might look at this mid market deal and it's like well we're selling 30 seats. Welcome, core Mac we're selling 30 seats but there's the potential for 500 so the potential is in their high. But the potential for all mid market right if we can hone that process and hone that messaging potential for all mid market can be can be huge so it's it's worth it for us to be in this. One mid market deal where the potential's only 500 seats because we're going to scale that across the mid market. So so I think mid market is a place to get involved with deals and then SMB again it's kind of like understanding their cycle and and for them they're just like they have so many for kind of counts they can't reach out to them all personally. So they're looking for like templates and scaled ways to reach out whether that's like scaled emails or content they're looking to do a lot of like automated processes. And so again it's kind of like working with the rep to understand their process helping them hone their messaging it's almost like the work you're doing somia with the STRs. Whereas the R similar like they're they're reach out to so broad they're templatizing it and and you know I think everybody probably gets like crappy email templates every day you know how painful those are. So if we can help our reps with some you know some good messaging and some good emails to send out I think that that helps out a ton so. Brian and Quarmac what we were just discussing was the teams cadence on being involved in sales deals and so. So I'm brought up the the concept of maybe each of us tracks the top five to seven deals by revenue. Let's say over the quarter we are saying these are the top deals with the quarter and then that seems like a good idea and then the only caveat I'm adding to that is that. I think one S. and the dealer any one mid market deal is never going to be the top deal by revenue but we need to have commercial as part of like the work that we do. I mean, I think it's also looking, and it's in the shared folder, we're just looking at that agile, the agile opportunities reported, I pulled. I was surprised that there were a few opportunities, mid-market opportunities that were actually really big and in the top three or four. But I don't know if they were initially tagged this mid-market growth to enterprise. So that could have happened over time. Our numbers are awkward. I think other companies, I think, stop it like a thousand. If it's a thousand and above, we consider a thousand to 2000 mid-market. Right, which I mean, we are a mid-market company, which is, so yeah, that's... Right, we're as to other companies prospecting to us, we've been enterprise. We'd be part of their enterprise segment. Yeah, I think I've worked at places where 500 plus was small enterprise too. So, and if we're landing and not, if we're landing small with vision, some of these enterprise deals, could be initially smaller than the mid-market ones too. But one of the things that I've been looking at with the agile is I've been trying to make sure that I look at the top and from each segment. And I feel like, maybe that's wrong, because maybe the bulk of our revenue is still gonna come from enterprise. But it seems like hedging our bets. A lot of this mid-market deals is going to the SMB QBRs and the mid-market QBRs. People are always upset that their deals have grew them and moved up to enterprise and enterprise stole it from them. So, you know, the wins key deals can slack. Is there any magic to it's algorithm and what appears there as a key deal, or a key win that we could use? There used to be a link to it in the channel description. It's not there anymore. That is a good question. I was just gonna say, let's take some time to get to like tactics or to do's. As one to do, Ryan, would you be willing to try to track that down and find out who calculates that channel and report back to the team? Yeah, I'll find out. Okay, because I think that that could help us out. How do they call it, key deal? So, I was just filling around with sales quotes right now and it looks like you can search using your keywords and you might be able to find specific deals which have those keywords in them. So, I looked up for infrastructure code. Normally deals are not tagged as the dogs and so on but I found a bunch of deals tagged as infrastructure code, not tagged but had it in their comments. So, that might be a way to find deals that you want to get plugged into as far. Is that a report that you can run for that keyword? It maybe we can. I haven't tried that. I just tried doing this search and maybe I can export that. I'm not sure. I'm not an expert on sales force but at least there is a way to for people like me which is not that to a particular tier and not a popular news case to find deals where I can get plugged into. Yeah, so there's a couple, I have a couple of thoughts here so I know that pile has, I think even like a weekly meeting. So, I'll take, you know. Last time I showed up, they were really, really happy. And Canada, the last time I poked into this was, if you remember John tried to get us on this train a while ago and John was like, hey, each of you, these are your stable counterparts and sales. And I feel like that was really heading us on the right track and I like to get back to that where we have things we're focusing on. My thought for it is like, all of us should have some expertise across all segments. Mid-market enterprise SMB and Pubsec. So that way like, you know, like, it's not me if you're only focused on mid-market and all the get-offs messaging is mid-market and none of its hits pub the Pubsec market or none of it hits the enterprise market, right? So all of us need a little bit of knowledge across and then some of us need to go deep in some of those areas. So I want to get to that. All that's to say is I know that pile used to have a weekly meeting and I'll take it to do the poke back into that to find out what deals are on that because yes, QBR is a great way because they'll, they'll, they'll, they'll, they'll, they'll, they'll, they'll talk about in the QBR what they're top deals of coming are. But I think that this also evolves. Uh, I'm thinking for a moment, I was thinking like, is it worth like somebody or some, some of us going through the last set of QBRs and just pulling out like making a list of those top deals? Does that sound like a good use of time or should we like curb that and use other tactics now and wait until Q3? But then when we attend Q3 QBRs, that will be a list we want to generate. What do y'all think about that? I mean, I'm not needed to start. I'm keeping on because dog is back here chewing away. Like I think about that during them sometimes I'll like tag myself to come back for stuff and and sometimes I do sometimes I don't. And I know before we had gone to the point where I was like, open an issue for everything. Maybe we can meet in like a middle ground. And like whether it's all of us or one of us and sometimes I'll see something that's like security and I'm like, my head I'm like, oh, Cindy, and then a lot of times you've already seen it to be completely honest. But yeah, I don't know. I think it'd be good to keep our eyes out. Yeah, I think an issue for everything we hear wins in much, we're not gonna do that. Yeah, yeah. But I would say in the next set of QBRs, an artifact that I would like to team to generate out of those QBRs is a list of the top deals. So whether maybe we put it in an issue or maybe a Google Doc, but the idea is like, every rep puts their top deals in their slide deck. And so we would then just like pull that one bit of information. So you could just look at this one doc and be like, these are the top deals that people are pursuing. Right. To generate that artifact on the next set of QBRs. And then my question is, is it worth going back to the last set of QBRs and spending some time to generate that artifact? I don't know. I'm just kind of asking what you all think about that. Since we've got to do some research on here, I think it may be, especially since there's ancillary circumstance and we're gonna be pivoting a little bit doing some research, I think it may be a good idea. And we could just do like one issue, right? For all of us like QBR takeaways, have like a section for top deals, maybe a section where we each add a few bullet points on something we found interesting. That would be kind of easier to wield. But I think that's a good idea, personally. Okay, opposite that's like an optional idea. If you find yourself some spare cycles, open up a couple of QBR decks and just pull out that one slide and just that'll give you some stuff playing of like top deals. I don't want to task it because I think that if I poke into what a pile is doing, and I'm pretty sure that maybe Onell has something similar, I'll try to figure that out. But then almost certainly they also maybe have like even just a dashboard where then we can just pull out the top ops. And I will set it to do to myself. So, a text of dashboard in the last rating, there is a sales dashboard that gets shared in the worldwide sales cards. So that captures the latest. I'd like to get to it. Boom, Simon's already got it. So this dashboard has the top opportunities by dollar amount or let's just poke into it for a moment. I will not share my screen so we can put this on YouTube. Good catch, good catch. But, okay, so this is a helpful dashboard. If you look at this, there's like a forecast, there's top by region. So here's a place to start. So I'm gonna put this again on next week's meeting so that we can kind of keep this conversation going. But I think we have a few action items out of this meeting in some few tactics. And I'm gonna set to do, to do team review this dashboard and to do William add the four reasons to be in sales motion to the handbook. So I've got a document of summary of this conversation. I felt like this was really valuable and worth our time. I realized we're now like five, we've got like five or 10 minutes left. And there is another important thing. So on point three, here is the issue, I think. Yeah. So here is a question from the UX team who was working on redesigning these competitive pages. And their ask is for this part of the page, right? So they're gonna redesign the graphic that are working on this and you all have done your 15 features. And so then the question is for the bottom part of the page, what should this format be like? Now I probably know for a fact we don't want this tearing information because we don't want to put every feature. We want to keep that list consumable. So in order to make it consumable, it's gonna be summary in some fashion. So that capability may include functionality that's in multiple tiers. It's not always gonna be neatly in one tier. Does that make sense? So we probably have the tier information. The question is, does this look like a good format? Like there's the name of the capability and then some descriptor of it? What are you all envisioning for the competitive sections? We were talking about rating them 0, 1 and 2, which may actually be necessary when you think about things like, well it's in their product, but it costs money, it's in hours and it's free or something. Where there's some meaningful difference and we want to pick a 1 or a 2 instead of a 0 or a 1. And so maybe we need indicators of that here. I'm not sure that we do, but maybe we do that. So that is, that will be in here. So green check mark is 2. That's a yes. Red X is a 0 that's a no. And then we have another little symbol that is, that's a yellow or orange thing. It's kind of like partial. So that's, so our 0, 1 or 2 will end up on the page as like, so we'll basically should have the name of the feature. This is like 0, 1 or 2. And then maybe some descriptors. So this is kind of what I think. I think in phase 1, we have just a description of that capability. So you have the name of the capability and a generic descriptor that that one descriptor could be put on any page. In stage 2 or in some future iteration, this paragraph could be specific to the competitor. Right? So you could, like this one says, like bit bucket cloud was built with cloud in mind, although this also seems like it could be generic, whether they're comparing us to get lab or anybody else. But does that kind of make sense? So that you would get attached as a team. You've now selected your 15 features. There's then a step where you mark 0, 1 or 2, 4, I don't know, the top three tools. And then a future ask soon, probably still within this quarter, would be draft a description of that capability. And it would be like a two sentence, a one to two sentence descriptor. And then the idea is it'd be generic for all the tools to start, but then at some point in the future it could be specific. Does that sound like a reasonable ask? Yeah, it does to me. And it sounds like we're going to meet that, especially when we start looking at these, it's kind of partners where they're going to have, we're going to want this to be fairly friendly. So I think we're going to want to set that context. We should definitely be sure to inform the design and the teams and anyone else working on the site that we do want that to be very both eventually. Good call. So is there any other information that we would want in this block? Any other, like a link somewhere or another section or? I think we definitely want at least the ability to put in a free text link, because if the partners will probably have a partner page that we're linking to. So the partner page link will be like at the top of the page or it'll be like for the whole page. I don't think we would link it for every feature. Like if there's like 15 features or 15 capabilities, we wouldn't put a link on each block there. But what if we had a video or something that we wanted to. Yeah. Yeah, definitely for our for our technology partners, we should have a comparison. This is not a competitive. It's a comparison page because somebody's going to have the question. They're going to be like, well Google has Google Cloud Build. How does Google Cloud Build compare to get left CI? Well, we should have a page on that. And that page should link to our partner page, as since here's all the ways that you can use GitLab and Google together. What we did kind of like Lego is, you know, like had all these elements, but it kept it kind of lean for the actual visual, just capability, name of the capability that could be an anchor link with the score. And then if it has an anchor link, we click down that goes into the detail where it compares. And that way, if we build it, first step is get them all together, score them, have the visual. And then when the descriptions come, they just anchor link down to where they're at kind of all in one little area that I'm not like a long, laundry list, right? Like it should be wheeled. Right. That's exactly how they're designing the page. So they'll be like, cool. It'll look like this or maybe like this or, you know, there. Yeah, so they'll be this kind of graphic with the scoring at the top. Okay. Um, actually, uh, what is this? Like, there's a chart that, um, anyway, like it could look like this or it could be like the way I think of it as like a D&D chart where there's like a circle and it has like, or like Pokemon. If you all know what I'm talking about. And do, uh, so one of the ideas was instead of visually representing it with these like bars, a good way to represent this is like on a stats chart, um, like, uh, anyway. So that might be a cool way to do it. I'm just, I'm leaving it up to the UX team to figure out the best UX. We're just supplying the underlying data. But there'll be some visual graphic because we found that those are helpful to people. And then they can click. It's like to see like, well, how did it come up with this score? And then that'll go down the page to like a list of features. Like, like we have today, but instead of being like a list of like 437 features on one page, it will be like 15 consumable capabilities. So the summary takeaway that I'm going is, is we're basically saying use this exactly out. You know, feature comparison, uh, name of the, name of the capability, a little descriptor and then zero one or two for that section. And then we want to tell them, hey, we want this to be variable because to start, it'll be the same for all of them. So we can just get it up quickly. But then we'll want to iterate to make it specific so we can see something specific. Uh, that's super helpful. Let's chat for a moment. I'm apologize, Cindy. Let's come back to number four. But I just while we're on the topic of Somia, you're asking about, uh, configure. So the summary of this is I've asked this every single time we've done the exercise, like do we get rid of configured because it's a little bit weird. And so, I've chat with Kenny about it and Kenny was like, has been of the opinion that yes, we should keep those. And so my thought is as long as we make the page clear that we don't do what, you know, I don't know, chef, like chef does. So if it's, if the set of 15, if the set of 10 to 15 capabilities, like there's probably seven of them that we don't have. Like we just don't do that. And so when the little visual graphic is clear, like chef does like their point on this part is really high and our point is low. But we do do some infrastructure configuration stuff. And we should have that kind of listed. That's, that's my thought on it is yes, we have it, but we just make it clear and in the description, it should be clear like chef does this, what does this skill, I've doesn't do that, you love does this people use get lab with chef. If you've got Kubernetes, you probably don't need chef. Does that sound right? It does. And maybe we can chat on our one-on-one with more about this because I think there is some level of overlap between what we capture as features and CD and configure. So we might want to be very clear about everyone. Let's follow up one-on-one. I'll just say quickly on seven that harsh will schedule a call with this team. So I know that's on their plan, so look for that soon. Send you, do you want to say anything about number four? Just, I would encourage everybody to follow the conversations in the security channel that I put there. So 30 second background on this, said challenge me to see what we needed to do to grow security and so I put together a plan that's very broad and need to make some short-term progress on it. Some of the items are a lot longer term, but put together a Slack channel to start pulling in Mel since a lot of it is messaging and other folks. And Danielle asked on Friday, why is security important? And so I kind of got honestly irritated and said, well maybe David and Sid would like to answer that question and pile jumped in and answered it as well. And so he's got a document, yes it's outdated. I've asked them if there's something more current, but it's still pretty insightful in terms of where he's coming from and how they thinks about sales plays and sales progress and growth. So I would just encourage you to look at piles document and follow along in that channel. Because the other thing is Danielle kind of, I think had an aha moment that there's more opportunity on the inbound side to guide people towards security as well. And part of the genesis for the urgency of the web piece and the messaging was we went out with the MQ last week where we did really well. And so we're talking about hey we're a devsac ops company and we're really good in security and then you go to the homepage and there's nothing about security on the homepage. So I was trying to resolve that quickly and unfortunately didn't reach that resolution, but at least there's there's a lot of healthy discussion at the right levels about this so follow along. Cool. I'll just say one note we got to we got to hop off, but Alita put together a wonderful Microsoft report. they has a list of suggestions. So those are things that we can take as a team and start to follow up on. Core Mac and Brian I will share in particular to the top items actually in Parker I think this covers your space as well. So one of the ideas is that Microsoft is doing more agile stuff with agile. So as part of our agile, our agile play and our agile thinking we need to involve some competitive intel there. And then Parker and Brian there is obviously always a GitHub and GitHub actions content. So review that document, look for the stuff that applies to you. Think about what should my follow up to the speed and maybe the follow up is I don't do anything with this, but maybe the follow up is okay I created an issue and I add this to my plan or I think it's up to you. So review the document, think about what your next action would be and reach out to Alita to coordinate with them because I know she's also for example in another some prime I hate this stupid freaking group channels. I don't know if you all get these people slack you and it's like you know I have like the Cindy Somio William channel, the Cindy Somio William Cormech channel, the Cindy Somio William Cormech channel, was this a song that would involve me I try not to do that. You are really good at not doing it, but I'm saying like I currently have like four or five conversations that are like this but any and one of these dumb channels I have a message and it's like Alita's like yes and it's not Alita's fault either other people have started these channels but she's working on Angela and they said yes I'm working on Angela yes I'll loop in Cormech so anyway that's my final word on that so thank you.",
  "So it is the sick, meaning secure and govern growth and data science, meaning applied MLOPS and anti abuse team meeting. That's a big mouthful. We might get a better name over time. That's from meeting for September 14th or 15th in APEC and High Allen. Glad you're here. Why are you here when it's midnight? Glad you're here. Don't make it happen to come to this meeting. It's really late for you. So, but I'm glad thanks for coming at least once. So, News and events. I've got a lot of things on the agenda today because of some of this. So apologies for that. I think it's going to go down over time. I'm going to work to do more summarized communications to improve the signal of the noise rate ratio in my communications to groups of people, including this group of people. This will is written for this group, adding more items to our staff meeting in agenda, often as we only, rather than posting to our Slack channel. And I am going to be showing that people leaders on the team will attend this meeting or read the notes and not rely on the Slack channel. So keep that in mind. Let's see, B is a read only item unless anybody wants to discuss it. And see, Phil has. Hi, and welcome, Alan to the meeting. I've asked Alan to step in as acting full-stack manager for security policies while I hire any in. Alan has graciously accepted this wonderful opportunity. So welcome, Alan. Thank you. I've got the next item as well. So anti abuse is moving product sections from seek to data science. Data science is a thick of the arenaing of the model ups section. So data science will include two stages. The existing model ups, which has three groups in it, which model can talk more about. And the anti abuse stage, which only currently has one group, which is called anti abuse. I think at some point there are plans to split the anti abuse into two groups, but that's not a near-metre. And we're working for all the changes around that. There's lots of work there changes, lots of handbook changes. Mom, do you want to say anything more about data science or model ups? No, I think for us we just roll up to data science, everything else stays the same. We still have the two main groups applied MLM a lot of and then later data of spot of it. Yeah. So any thanks Thomas for continuing to compile and report on across all my teams that I'm responsible for. In the engineering allocation meeting on error budgets and reliability and security incidents, so really do appreciate that. Gotta give Neil a high five, too. they does have they does half of these now and. And apologies for the noise if I keep asking for what's the story behind what's going on with reliability or secure or error budget stuff just trying to provide your cover. And I apologize Neil, I totally forgot you took half of these since Thomas has been doing it for so long, but appreciate it. I know you volunteered for it and we discussed it, but I just forgot. Kind of a thankless role. Honestly, and I volunteered because the exposure I get from it, it's nice to be part of a different collective group of people. I was going to mention I'll type this in here, but Tiago had set up for container security a bot that weekly. I think it's Monday, sort of like Sunday, we'll ping the team and ask for volunteer to go and resource that data and we just enabled it for thread insights. It's actually pretty sweet and it just does all the work for you, Thomas, sorry, we don't have to think people ourselves. So I'll put a note in there for that. If it's read only and Bill, you've got G. Yeah, I'm talented, so we had an update from Juliana. The date's haven't been confirmed yet, but it'll be sometime from mid up to about three to two. I think Egroge sign offers it's gifted to be mid to same but before everyone goes on holiday. I've got a lot of time line in there, but the dates that dates on in the handbook yet. The big changes we're moving to do at the work day, that's not available yet. I haven't seen a demo of it, I don't know what questions will be asked, but there's an optional self evaluation, which will move into work day and the work that we do has managers will also happen on work day. At the moment, some people are using the Google Docs, which is listed on the talent assessment page for self evaluations and for the manager work. I'd suggest continuing to use those docs until we get access to work day and see what the exact differences are. Each I am Jay or read only, listen, anybody wants to discuss them. Yeah, I don't care. Yeah, apologies for the late like live addition to the agenda on this one, but one more note on a change we're rolling out within secure pretty lightweight and that we're establishing a second team with a dynamic analysis. And so issue is there folks want to play that would like to follow along or ask any questions about it. The plan for right now is I'm going to be serving as a team acting in the EM and in addition to normally scheduled. Some duties for lack of a better way of putting it and so happy to answer any questions to folks have about that. And we've done the same thing in three to the side, so it's worth both Neil and T. I got a teaching team. What are you going to call your team or how will you differentiate. Was not planning to differentiate it. It's just a second team with end dynamic analysis. We're not creating a new group. Which I think is the same pattern that's happening within threat insights as well as just two teams. No, I know that there's tangerine and there's navy. I'm not a tangerine person myself. I'm so. What a figure. Out. Yeah, another two color Thomas. I started a little down. That's the team. Tomato. I don't know. We'll figure it out. We'll figure it out. The differentiation. I think the difference between was happening in threat insights and with dynamic analysis is that this is a split or long feature category. I don't know what even though we're keeping that we're not super in the group. It is the distinction is a long like who's working on specific feature categories themselves. So the the where the fracture point is happening is a little different but the approach and effect are the same. I hope. So I want to big rocks and how they she's section. I know if it's a big rock that kind of blown here perhaps more or less. I did that survey where you got an awesome response rate from the team on how to improve the MR rate for the team. And then I took a really long time to go through other results and summarizing make initial recommendations. So the loss a little bit of momentum. I got caught up in other things across functional prioritization and a customer escalation. So since then this is actually used by John Hope to lead a discussion at the development off site last week on just improving engineering velocity is one of the things you used to power it. So an add value there. But I've seen no comments. I think for anybody in this group, which means either no time for got about it or it doesn't. We looked at it and it doesn't have much value. And any of those things are okay. I'm just curious where the group landed. And if you haven't taken a look yet to see if there's anything you want to take away from it on things you might want to consider. And I think it's not needed to do so. And to consider not to do but to consider. Anybody got a chance to read it yet. And think about it or not just yet. It's not my guess is perhaps everybody's really busy and they haven't looked yet. I mean, I don't know how I live with Jay Neil Tiago. We've talked about changes. I think Jay you've already looked at some changes around refinement that was near you. That you were going to look at for interviews and report back by the end of the quarter. Tiago and Neil, I can't remember me before we ended up with the three-dimcy. Yeah, I'm hoping that MRA does kind of an artifact or a byproduct of some other actions. The big thing we're doing is more whole type of work like random miscellaneous, not schedule, not PM necessarily actions or issues. And that actually is creating some more additional work like it's just like free time work. I want to change a pace, move to something else momentarily. And these are generally smaller types of issues too. So they're quicker to knock out. Therefore, they create more and more and more. So it's just been a cool byproduct. So I think that's helping us succeed there. Yeah, we're also seeing some success with a, we've implemented like a weekly refinement meeting. Kind of check in, make sure that stories are broken down tiny enough. I think with smaller bits of work, we're seeing a higher MR8. It's debatable whether or not that's like just inflated, right? Like you can work on a small piece of code and ship it. And you can do that a lot of times you're going to get a higher MR8, or trying to be reasonable with that. I am noticing a nice pickup after implementing the refinement meeting. So let's hope for the best. I was going to say one note on refinement in second analysis. We're experimenting with a new section in our finding feature. So it's called looking forward and in there. We're using that is like a baseline where we add issues that we think need refinement. That will most likely be worked on in the next one to three milestones. So we're using that as kind of a base cover. Or some engineer to go in and add their issues and make sure they get refined within this milestone. Not necessarily any implementation, but just refinement itself. I should stop talking about. Thank you. I'm going to talk much more about your comment. Yeah, I'll verbalize and then I'll say more and then I'll finish writing what I was going to put down under the under the just moment. So this was an agenda item with the EMs and I can say it now in the secure stage. That is the analyzer team. So this was a topic yesterday. So my I would assume that this is more like as a part of the industry, but not digest the content. And what I was about to write and I am cautious in saying this because it almost comes across a smug and that is not my intent. In that looking at the MR rate within sec compared to company wide historically runs two to three MRs per engineer per month higher. And I think I could do that has to do with the. I mean, I mean, we've taken this to be true or at least asserted it to be true in that it's smaller teams that are working closely together in less busy projects. Then we see within the Rails platform itself, which does have an impact. It's some it that's not to say that we cannot do better, but the comparison. The favorable comparison. At least to me and I will knit this at times will lessen the urgency on this particular topic and I have to admit that bias that I have. Thanks Thomas, thanks everyone. So I didn't say any comments in the issue that's totally fine. I'm going to answer some of it useful, some of the analysis useful and that's great. So I will close the issue accordingly and just link to these notes good stuff Thomas you've got item B. Okay, all right after this done. All right, bed wrap and the can't or won't fix security issues that way, maybe we may be having there's so we've got them. For security issues that are subject to federal. If you have a feature category that is dips compliant or certified, you're probably going to be part of the fed ramp evaluation and any security issue that has a CVE associated with it, which means dependency or container scanning specifically. Has to have a remediation plan even if they're false positives. And so that's led to a separate discussion on what do we need to do. For do we need to do anything related to security findings on the commercial versions of analyzer specifically that was the catalyst for this particular conversation. Because on container scanning we get a lot of findings related to Debbie and based images that that project has not shipped a fix for or won't fix ship a fix for. So there is a separate thread that is going on here about what to do and can we do deviation requests in bulk for this classes for this for this for this class of findings. The the TLDR is below and there's a couple of all there's a couple of all of options that are being brainstormed within this particular thread. So I wanted and I know I've said a lot of words and I didn't say it well, but I wanted to bring it here in case there was any questions or commentary. Nothing directly related to what you've just talked about, but sort of related for Fedram, you're looking at. Any tips related security issues that have been raised. How far are you going in terms of ordering to find anything that hasn't already been identified. There is of all link in another issue a separate issue that is get that has instructions from app sec on what they would like us to do is for setting up container scanning. And they have set up a project with a single configuration that they're asking us to use there is some debate as to. How that project has been configured because it uses a combination of three container scanning tools rather than one. And so they've asked us to use that as a baseline for the fifth images specifically since there you be or the UBI specific images. There are there equivalent in my mind even though they're there different meanings. The TLD talked to Epseyg. Got it. And the key of the kill the images are stable counterpart of folks on no at least in sec. And so he's been quite responsive and keen and they is also helping chase down answers when they doesn't have them so that's a. Big he's been a big help thus far. Yeah. So in thanks for bringing the set thumbs. So in the hallway so from the development staff main meeting. I copy and paste of the notes. And there but net is the TLD are is can folks request to me and our peers was can folks check with their teams to see if the approvals are affecting them. So this is the new approval rules on segregation of duty so last this group not that everybody's here but in the group and they should be is only exceptional but have has this had an impact these new rules. And I think that's a few cases where team members have realized that they need to get an additional approval. But I'm not aware that it's actually slowing things down. Okay. No loses good news on this problem unless we're miss unless it's happening and we're not noticing it where this long things down but just keep an eye on it in case it does. Everyone please so good stuff. So. With recent changes. We've done a couple different changes with the secure and governd broken out from secure and data ops team and this net is I just compared the name of my team with my peers. My peers have one word team names ops dev create etc mine is sec growth and data science which is pretty worthy and also makes it like when I go into dashboards I have to pull data from multiple places across my teams so I was thinking about one word name. I've started to get feedback on it naming things is really important at get lab to name the right kinds of things and give them good names so I'm going to run it by David to Santa and Marles he's great person to balance such things off of. I landed on enrichment because what we do makes other things better we enrich them. It's kind of a very broad term but there's cover what we do not sure we're going to change the name which I'm not sure it's going to affect anyone but me. That's actually my intent is only affects my stuff not everyone else's. But maybe everyone else is in a small way or others in a small way but we'll see. Any thoughts on this good idea bad idea. I'm not way to there something looking for feedback. I'll just see the other examples relate to either sections or stages whereas enrichment doesn't so you'd be introducing something new here. This it fixed me as well I have a subset of enrichment so it would be kind of enrichment light. I have I'm not sure I'm not sure we I think you and I'm so kind about this I'm not sure we would get approval to introduce a new. Our name or department level would certainly have to talk to people up about introducing that and work day. Okay. So a couple of things you know these kinds of things that would sometimes put in slack and I'm trying to avoid number of distinct slack messages of putting it in here. Making a lot read on Lingon possible so I've got see that which is not really so I would like to add announcements of work anniversary's new hires to the meeting template. PM does this I attended the PM staff meeting I was a guest speaker there this week and like the comradery we have good conradery the comradery there was pretty good. You know firms shadowing me today I think they said in on the PM meeting as well is it's a pretty good field just people really knowing each other well and getting well well. So this is just two things I took away from it is anniversary's new hires. Oh verbalized chagos comment. they always know who you had to hear this thing. Great idea for new hires don't care much about working in a series where it's about for that. But I'd say I always think the miss the bottom announcements as they cover the whole company versus the bot just announcing for our team. If no interest in that part I'm happy to exclude the Neil you like the idea of adding. Yeah, I heard them shows you don't. I mean just last week I think it was Phil celebrated their third year and I congratulated them but then I missed another team member in sec. You know because there's like 20 people in that list plus it's team member updates that's not a channel I look at a lot like weekly I just happen to see that the day out so. I do like being able to celebrate together in this group. Maybe also thanks or praise section like thanks for doing this and great job on that. So remind ourselves to do that as engineers we tend to focus on the problems and challenges and not the celebrating the wins. I used to Jason that we add new content into this document for that, whether we just link to the slack announcements that we've already made. I think it's nice to verbalize them. I think it's nice to link to the slack announcements but also verbalize that I know it sounds like a bunch of busy work. I don't want to do things twice. They've we add the sections and people use them as they see fit. Whatever that happens to be but please don't over don't create busy work for yourself either so so judgment call by each person. Thomas looks like you had a Thomas and Monjots and thoughts on this too. I like it. We don't celebrate enough we don't do enough for team cohesion team celebration across the board that just as a corollary. I think that I haven't communicated well. I was hoping to add this kind of thing to a monthly section of why I'd retrospectives new hires discretionary bonuses work anniversary's big feature releases. I mean anything that we want to announce and celebrate we ought to make sure that it's. I don't want to say shop to the rooftops but we're shop to the heavens but but if we if there are big deals and we don't make it and we undersell. Oh sorry and hey for Nando here so I'm shadowing Wayne now just to introduce myself from the technical marketing team and I'll send the document right now which is something that we do for our kind of get together team meetings. And they kind of shows you can kind of you can look through it. I shared it to everyone on the zoom chat but you can see that we provide updates the new team members and small little introductions. And you can see sections that we have on gratitude and actual spotlights that we show like who we're actually praising what they've done that's good. And how we can highlight that because that will motivate employees kind of really increase that morale because what we're seeing is. These employees are actually getting recognized for good work that they've done and they're getting shout outs because a lot of times. At least in our team the cases we don't really although we work with each other where we sometimes are kind of siloed busy working on different things so it's good to know what everyone's working on and what everyone's doing so somewhat related to to what you were just speaking on but just that I'd share that. Going D so I'm not going to read all this from Daniel Kroth but summarizing they is looking for feedback on the development vision and mission and direction. We're brainstorming on in the development department on mission and vision in particular the approach is one everybody open up a little spreadsheet and follow the instructions anybody interested so if you're interested in doing so to take about 10 to 15 minutes please do it in the next week. Good stuff there. So we covered also the development off site that Daniel sick as an action item follow up on Thomas you've got. Right because I cannot remember and so I'm hoping to crowdsource a little bit of just configuration or keyology so to speak. Is there a reason that we don't use reviewer relette. If folks can remember the reason I'm asking is is contained within that thread but that is a tool that actually is being used to track if we have enough maintainers and any given project and since we don't use it we're showing up. And so that's why I was that's why I'm asking. Yep. When you say reviewer will that are you talking about on the individual analyzers yeah. Speaking only for dash we haven't had a need for because we have six engineers that's just going to rule that the same six people. Yeah, rule is excited that's what I kind of think it is but it's from a larger like if the company like where do we need more maintainers is the question. And so what is the question that the organization is asking. I mean, I'd suggest they're looking in the wrong place if they're looking in real lead to see. To find out the number of maintainers that's the wrong place the engineering projects page should list all the maintenance if you set up your team yearma in treats correctly. What if thought that was more the single social choice. Apparently not. So yeah. Yeah, I'm looking at the chart I mean I think on somebody's project we just haven't filled it out. All of you like we have the API. We've got a couple of us we have one engineer that works on it. So we haven't gone in out of the maintainer there because they know who they are. So if we are trying to maintain that as a single social truth we need to just go through and scrub it. Sorry, Seth and reminded of this. I'm reminded of this spider person meme with the two spider men pointing at each other who's the reviewer who's the author it's it's the same person. It's been a long day so I'm a little punchy so but yes that makes sense. So say, you saying you haven't added up to individual team members, yeah, all files in the handbook here. Right, I believe for that one we there's just no injury for it. I mean we could. I think I think I think that should. Yeah, well, it sure it shouldn't be using just really it. They should be using that as a social search and I think if you've listed my kindness. That should cover. I follow up question would be does that does that cover the senior plus engineers must be a maintainer requirement or not. How much you buried the lead the what you call buried the lead. I took the read only off your comment there so you can. Well, we we're going to end with the best things of the day. That's all. But, but. But the maintainer requirement. Yes, it does. Oh, maybe because it's required for senior plus. Going back to that is just maintaining one or more. I think it's a requirement for a couple of. So I was talking about the Olivia and is. Yeah, no, I was just making sure we were finished with the topic above. All right. And we're at time so I will move on to take that as a prop. Yeah, Olivia has a pop back on slack to announce that the newest addition to their family is here and happened over the weekend. So I so big big congrats to their family for that and I will be. Hopefully to celebrate him. Later when they does when they when they returns to work. Google. Thanks everybody. Have a great day.",
  "So, as the sick growth data science staff meeting, in development for September 21st. So, I've got a number of read-only items and apologies that I have a number of a lot of items in here. It's a function of trying to do fewer updates periodically in Slack as things happen and more in the stock with the way that they get red or not. Got a couple read-only items in news and events, A, B, and C. And I guess I'm going to jump down to the item D. So, instead of changing or creating a team name that covers my teams of secure, govern, data science and growth, to something else that is not for words or five words, as my peers have teams that are named with one word, that just makes a little more verbose and challenging to communicate. Instead, I decided to update my read-me. And now, as a correct list of my teams, and also as sections for dashboards, I use it, I mean, the ad for well, goals and priorities and OKRs. It's a much simpler MVC and a simple boring solution as well. So, just mostly heads up on that. So, it's not me doing most of the talking, even though I put a couple items on here. Any volunteers to verbalize E, which actually is from Cheryl, from the development staff meeting. Yeah, I'll do that. So, Cheryl mentioned the development staff meeting that team members and who teams have been experiencing delays with laptop orders, new highs and refreshes. You can see the details in the document. She's also, they has the spreadsheet to pick. I think, curious to see if that's happening across these groups as well. I have this historically. I don't know if she's understanding collecting historical data. It's been resolved now, but I had one laptop those six months. I think she's collecting current challenges, but it probably couldn't hurt to put in this any historical ones that are likely to recur. TLD, with what she's looking at us, how do we, how do we resolve these? It seems so, the doesn't seem to be a really good process for keeping team members up to date on on their laptop. When it's coming, it seems that you just have to manage the team and keep asking, and get some of the examples they gave was that they reached out directly to someone and said, what's happening with the status of this laptop, and five days later it was ordered in a way, which is not the best process. It means you have to be noisy to get them down. Somebody said, we got to do the laptop refresh. I filled out the Google Doc form and they need a follow up on it and I see that I was going to approach it. I totally forgot about it. That was about such a non-saga. I'm still only able to act up. TLD. Somebody want to volunteer to verbalize Nick's item. Some of them also from the Dolan staff meeting. I can take that. Nick in the development staff meeting, he's following up on last week's item about group software subscriptions. they opened up initiative and put on what services people are interested in looking at this. Chris and I think Grammarly are the top two. Thanks, Mar. On T. So please review and provide feedback of interest did on the greenhouse nice to have. Sure, we have the best list. I came out of the development office site from two weeks ago. I got a shout-less two weeks ago. As one of the action items, I think to optimize those. So we have the we're evaluating candidates on the right things for next steps. Also, please take, if you haven't already, take the training which is being renamed to TeamOps. Meck is managing efficiently for something. I guess TeamOps, that's a better name than it's not an acronym. I can't remember left to guess. Might be good time to do it. I took it today and also to re-advertise it to your teams. I took about an hour. I learned a few things and overall I think it was just a good experience. Managing so everyone can contribute. Thanks, Seth, for putting that in that for the stands for. I think that's one of the reasonable change in the name. The TeamOps sounds catchy or too. I'd actually think it'd be really cool if we actually release this to the public. It's just a video by said introduction and then a rehash of various handbook pages that I'm updated and also some quizzes. So you can test your knowledge. But you'll see. How much you've got? Hi. It's a good segue in that. I'll wear it at the end of the release, which means it's retrospective time. So, section retrospective is going to be scheduled for next Wednesday during a currently scheduled coffee chat times and I'll be pushing those coffee chats back a half hour just like we did this past time. The one change we're going to be making with invites this time around is instead of just putting it on the stage calendars and advertising and slack. We're going to be using the group email addresses that we have to invite folks to attend. So if people click yes, they'll get the reminders. And so here's the the ask here is, which please check your Google Membership to make sure they're accurate. We've had some changes and so it's just a good time to check before we start actually trying to use them. So. Thomas to add to Google group. We have to open up a access request. Unless you own the group, you might have administrator, or privileges as a manager. I got some group people can self sign up for as well. This is seeping them in the group. Yeah. Thomas, you put something on slack this morning about naming conventions. I haven't just the follow up on that. We looking at renaming these groups. IT is looking to standardize how groups are named across the entire organization. Right now it is something that's only rolled out and for structure, but they're looking to roll this out everywhere. And so one of the IT managers that are linked to the handbooks showing what the convention they're moving towards is. And so I'll go grab that link and I'll add it here. Thank you for the for the reminder on that. So in the big rocks and hot issues section. So sec and data science. P.I. Performance indicator review is next happening on September 28th. It's week from today. Everyone is invited to participate. Sink are you sync if they're interested? I've been participating in these for a while when they're primarily about fraudic management metrics. I got some value out of it. There's more about what the questions were than what the content was. But it was interesting. You must have the two product managers. It's added the cross functional stuff that we've been tracking things like percentage of issues that are in various categories and error budgets and bugs behind a various types that are passed SLO etc. So it's kind of a good combination of those things. So Thomas and Neil and others have specific action items in here to help fill out some of the content. But everyone's definitely invited to collaborate on this. So number five. Somebody who has been verbalized somebody else's stuff already. Maybe verbalized this one by the. I can take it. Let's see. From Chun in this week's development staff meeting. Every team's information is updated in a single get lab issue on a weekly basis. Issue creation was fully automated thanks to Sam's script. And here's an example. Thanks, Jeff. So something we would want to do as well is summarize on a per group level, you know, for managing engineering manager or senior engineering manager to summarize and and tag the state holders that are interested weekly feels like a little too often to maybe every other week would be better. I'm planning on doing something similar at my level and tagging anybody interested. Christopher and Andrea are interested. But that doesn't just because I'm going to do it doesn't mean everybody else has to. Yeah, I think anti abuse is going to give it a go. Fill and I were talking about it. We're going to do it on a weekly cadence to at least try it out. Is it is it that Christopher's requesting this or is this just to get better visibility overall or. Christopher's actually is not requesting it. He's but he's in favor of it. And I've been in favor of it as well at a high level at least of just knowing what's going on so that I can know what the risks are and we have to date on the status of the things, et cetera. But definitely weekly seems like too often. And monthly seems like not often. Maybe every two weeks. Christopher's comment on it was if some teams start doing this he'll read them. But then they won't be getting updates from other teams so they wanted to make sure everyone hit the opportunity to at least do it so that he's not focusing on some teams over the other overalls. I see most people typing. I'll I'll I'll talk while I try to figure out how I'm finishing up the thought. I'm honestly struggling with issues or the best delivery vehicle for this kind of information. No, it's it's really intended. I mean, I like the summary because it kind of forces a thoughts on what's happened where we I mean what's happening now where the current risks now. But is there value gained by having the historical information that these kind of update issues would provide? That's that's the question that I keep coming back to on my head and to me that helps determine medium appropriate medium. If that makes any sense. And for what it's worth, Montez's updates in Slack. She's been doing this a while with it. It was good way for them and I to collaborate and they tags me and Ella Ron on the business side. So it was good way to stop getting questions from Ella Ron when I end doing the updates. That's why they started doing it. And not everyone reads all their emails often. Like it's easy to assume if you have you subscribed in box zero, like you read most of your emails in and within a day or two or getting them you may assume others do too and that may not be the case. Yeah, you could update the same issue. Thomas just create one. Some of our PMs do that for priorities across three insights and security policies. They've the same issue that we keep updating. That sort of removes the historical context that you'd be introducing with a set edition. I'm going to risk brainstorming life then. In Slack, there is a weekly created issue for features and hyper-arty bugs that have been closed in the past seven days. I would add additional content as as adding additional comments from each group around risks or praise or anything alone those lines would be worth adding on to that already created issue that we have. Wordsmithing needed. I'd say sort of that. That's worth trying. The only way that's going to fall down is a Christopher wants all of these on one issue board and it doesn't fit because it's you know it's different. Yeah, but we're not being asked to do that. I actually quite like your idea. Why summarise what we've closed when we already have an automated issue that does they just stay at an additional information. That is fair what makes sense and try it and see how it works and then let's hit rate on it. Am I? You had a question there as well? Yeah, mine was primarily about what sort of information is important to you or anyone who would be reading this just to guide what are updates will be. So like the audience, the important information that you care about or that you think will be valuable as a part of the subject. And what I say is, you know, do it works for you and your stakeholders but maybe like accomplishments active work and any risks concerns. And it bear also who to tag to like is, just as an example, not that monster doing it perfectly, but it is a good example. Mon doesn't tag the product manager because a product manager is is is is already in the details. But Montag's me and then Ella Ron on the business side because we were asking for updates. Like I don't know if you're a smart, like you're maybe asked your product manager. Would they want to see these or is it noise for them? Probably Thomas would want to see them. And be noticed maybe some people in your team but they probably already know. Maybe your UX counterpart, maybe your set counterpart, maybe not, maybe they're hurting and the loop on the details. And then kind of go from there. Thank you. Seth, I like your your next item, lunch and verbalist. Yeah, so how do you question in terms of transition with our Johnson? I don't know if there's an issue that describes how we should be sending messages. I know actually stepping in in the meantime. If she's taking over the CTO channel, if we should do our questions starting now to them way to October 1. Just wanted to get an idea how to rock communication. I don't know a bit of an issue. That lists all the details but like on the things that I was coordinating directly with Eric on. I was told this was like this direction that one's going that direction like for the engineering internship program that went from Eric to Christopher and that's that's Phil's not mine. I know for a customer escalation that I was on that went from which not because of customer names. He's going to go public that went from Eric to David, DeSanto and on some abuse stuff that Jay and Phil and I and others are working on. It's actually not determined but as they was the sponsor but I think that's going to be the group of egroup members I update periodically. My guess is probably going to go to Steve Lloyd and infrastructure since we're trying to prevent infrastructure challenges. So I'd say ask and if you don't know ask and hashtag CTO. Okay. But if you have any specific questions, I might know just from other discussions. So you have to ask in our channel first or now if you'd like and we'll see we know and if not then post hashtag CTO. Okay. But it sounds like we're not going to try to dump everything on Ashley. Probably is overwhelming but we're trying to distribute it out to other people. Oh there's some are definitely going to Ashley definitely. We've been trying the engineering staff meeting document I think it was last week on Tuesday they hit the meeting and they discussed all of this in that meeting. Okay. I'm sure you can find it on calendar. Yeah. Engineering staff. It's a great question. So we added a couple new sections including thanks in gratitude. So I'll review these so from me to folks that seem to have a great job meal and presenting and answering questions on error budgets and other related things in this week's engineering allocation meeting. And thanks Thomas for taking on the R&D US R&D tax calculation project. It's a lot of work but it's a save lots of money for the company. That's all we had for today. Thanks everyone to you guys. Thanks. Have a good one.",
  "you you your make sense. Hey, I'm Ar. Hi Nathan. Hey, how you doing? Good. Hello, everyone. So, I'm going to get started. It's the sick growth data science staff meeting. And so you've probably noticed we're experimenting rather than with recording the meeting and then me uploading the YouTube later on unfiltered instead live streaming. So we'll see how that works. And remember, you know, if I don't discuss anything confidential, not that we tend to in this meeting anyway, but if you do, it's okay. We can take the we can take the recording down and make a private later. So under new hires and anniversaries, my three anniversary was last week. I'm definitely flies. Thanks for the good us. Also a reminder separately to make sure you and your teams are considering team members for discretionary bonuses. And I have a link to an example of a recent one with a little linkie smile face is one that somebody gave to me. But it just reminds me to remind everyone else to make sure they're doing this. It's a great program. That can bypass. Yeah, on top of the discretionary bonuses, we have the new gift. Give a big program. Those are released like month to a go pretty recently. It's a way for managers to celebrate families, families, you know, family families. So there's a link to the end book. And if you're looking for something to send someone, there's the Swanky TX Allible on the Swackstore now. That would be much less. I can take the next one. It is the on X two year get lab anniversary today. So we have to stay on it on that. And also welcome, Nathan, to the group transferring from different part of your group transfering different part of get lab. Thank you. Yeah, looking forward to it. Yeah, I tell you, bro. So some celebrations, Mark has won, but Mark's here can someone volunteer to provide us. Yeah, I could take that. Julian represented get lab and vulnerability research at Langdev 22. The previous week talking about Lingo. There was a lot of interest from academic institutions and offers to collaborate on all things. Code analysis. This is a celebration because for VR being a research team academic, academic links and events are very important. We were not able to do this in the last couple years for obvious reasons. And just a few links around that. Great job, Julian. And this is definitely the kind of thing we want this section. Mark was asking about that and it's definitely nice achievement. It's stuff. In news and events, I've already said this, but from. Or it in product management. The compliance development team Nathan's team is moving from the government stage to the government stage along with their counterparts already moved from PM and user design and QA testing. Who moved the government state earlier this month. So Nathan and the compliance team will move to report the fill in mid November. And fill hadn't updated on the talent assessment, I guess, somebody want to. Yeah, I can grab the. We summarized maybe not read it all but summarize it. Thanks, Neil. Yeah, so we've we're kicking off the talent and to be your talent assessment. Things get started in the 19th this week. We had a couple manager and a training sessions. There's a ton of material. There's videos, documents, sessions. There's a list of templates to utilize. I want to bring you this call a special attention to the self assessment template. I would recommend you know having all of your team members tickling at that complete that as they wish. It's it's a new version. It has get lab values remote work competencies and leadership competencies as well. It actually closely models what we see in our promotion stock. So as if you're the team members that are thinking about that track getting promoted. This will really helps support both the talent assessment as well as that track. So take a look at that. And then those AMAs I think yeah, they occurred today or yesterday based on your time zones. They should be recorded. You can take a look at the documents on the you should all have that on your calendar. Cool. Reach out to your P.B.P. people business partner if you have questions. Which is a little bit. Yep. So on next to just also some other things from our people on to partner or stable counter parts really on a. They wanted to copy over from Christopher's staff meeting. The summary is that Linux foundation and growth and development opportunities. People can level up their skills with world class training and certification programs. Get as has access to the Linux foundation course catalog. They include two for exams. To sign up there's a link for an access request and indicate the career development goal. You want to achieve and you can assign yourself. And you're manager for approval and then assigned to it another specific person for allocation. Specific redemption could it's a some good. Playing opportunities not for people just people leaders but also. Individual computers as well. Also from the staff meeting company wide mentor mentorship program from Juliana. The team is still looking for about 20 mentors. The time commitment is one hour per month for five months. 30 minute bi weekly sessions sign up gate is tomorrow up to over six six. So please do sign up for interested and encourage folks on your teams. If they're interested in being mentors and doing so as well. Sweet. I think on the big rocks. I shared this over Slack. I copied several of you as MR reviewers. I'd like to invite everybody I shared it within our threat and such channel as well. But we had thought earlier in the year about breaking out and having groups specific team commanders. I just kind of better organized create more visibility because right now the stage. Get a lot of team meetings calendars. They're very noisy and I've seen a tendency or have it to just hide those and then people end up missing things like retrospectives that we want people to be aware of in the 10. So there's a couple changes and then the other one is utilizing our Google groups for attendees or guests. I guess it's called guests in Google. To instead of individual just so we're capturing especially as we have new team members joined people leave the Google groups kind of self managed that versus I'll shoot. I forgot to add you and people are unaware of team meetings and such. So take a look at that and see how flies and resonance. I Thomas is not here. I's hoping they could share an anecdote about last week's retrospectives. We're experimented with adding everybody directly to those meetings. And the tip of the ear on that. Neil is attendance and meetings on group calendars is about one fourth of attendance when individual people are invited. It's just too hard to keep track of the group meetings and see the monitor calendar and do them so. I think that they effectively don't work in terms of attendance. People miss them. So it's a royal pain to maintain the list of people invited each meeting as people switch teams new people come on board people leave that move teams. It's a pain to manage all that but it's much more effective. I think with groups that become easier and you know we still have to get those updated. And then groups where the group is invited. Yeah. I think that separately from a group calendar. Right totally different calendars. But I was talking about that has less effectiveness on attendance. Not a group. A group being invited. I'm not sure how that how well that works or not. But I was talking about little nuance there in terms of what I was thinking. Yeah, I wish the terminology was different as well. And I think it's based on the level of a calendar again like we have the secure stage and the protect stage calendars. Those aren't that noisy but again, I've seen a tendency for team members to hide those because I just want to super clean. What do I need to think about calendar? So that's that's where the angle is with having like a assessed team calendar for instance or threatens like team calendar. And then additionally if we have Google lists, I'll come list of people and you can add those to those meetings. It kind of self manages over time. And we can also add those instead of I don't know what Thomas did again. I expanded there's like 34 something people in those meetings the retrospects. I think they could have also and maybe they did maybe just expand it those and added everybody individually. But they could have added all these group lists. But I think they also asked to make sure there's our accurate. So maybe you felt there is risky just making sure we capture everybody to set it up. It looks like from the Google calendar documentation. The end user receives the invite the same exact way right. Is that is that accurate or I don't know. I haven't really found that. I've seen invites that have an expandable group of people versus. Okay. Got it but it still shows up on their on their personal calendar. We still the option to accept a coin. Cool. I will I'll try that out too. Does anyone get an invite if they're added to the group? Do they get a calendar invite? Nathan you're not in your head. Yeah, obviously and I'm pretty sure they like to receive an invite as well. I think they'll work well. Maybe it's the best combination then. Google groups invited to can it invited the meetings because that handles when people leave. They'll get a little bit of a movement of a calendar when they get added to a group. They'll get added to the calendar. I like it. I'm going to start using it myself too probably. Thanks for bringing this up. Good stuff. I'm going to start bringing this up. So I had them on flight actually with some cross functional collaboration across legal and university team. And so community contributors and various people in engineering actually got merged just great. The draft announcement would love feedback on this is I get lab is the public draft announcement. But we love your feedback on this, but it's good summary of the merge request as well. I get lab we've opened up our development director shadow program to open source community contributors and those and underrepresented groups. This program allows the shadow participate in a development director meetings for a week. We're by mentoring great learning opportunities and explore career development. If you're an exceptional open source contributor to get lab or you're in an underrepresented group. I'm not going to read all what that is according to the people of stage on that. Check out the program to see if you'd like to apply to the. So any feedback on this it looks it fill had some already. Then if somebody wants to verbalize that. Be great. Yeah, happy to fill likes it. We'll need to consider confidential topics like we do in this meeting. Crenny meeting the shadow attend and what level of access to external shadow would get for example. Whether to have access to Google Docs. Well, so whether the shadow has ever or is currently an app. I can't see because Wayne's little Chrisers in the way an app can't for a rolling. Yeah, all good points. I think I've addressed a lot of this already based on a lot of people's feedback and sorry. My cursor was in the way. So yeah, any other any other feedback. I'm back at Spelman. I'm curious Wayne. I saw you ask in the CEO channel about the CEO shadow. Why we haven't seen any external people yet. What feedback did you get? There was actually one external participant for the CEO shadow program different than this one. It was a community contributor and. So. And there are a lot of loops to jump through. For an external person, it's still in the CEO shadow program. If you're a community contributor or a. I think it's the word community contributor. You can apply it. Said the CEO or CEO talks has a lot of. Compatential conversations and conversations involving material non public information. MNPI that you know that can be used to German financials or infer them, which. You know, could. Create, you know, insider or you know, inappropriate trading risk. I think that's the concern with the CEO shadow program is said a lot of what they discusses is confidential MNPI. A small portion of what I discuss in comparison is confidential or MNPI. So that's why there were there were a little less concerns from legal online than on CIDS and which allowed them to approve it. But it still does exist if you're a community contributor, you can a. A prolific community contributor to get lab, you can apply to be a shadow. For of CIDS, which is pretty. Yeah, I'm thinking what we call the community contributors every month every month every month and we have an award. I wonder for. I think prolifics not the word that they. But we do vote and we choose somebody. Yeah, I don't know if it needs to be somebody who's voted as the one of the month or some other measure. No, no, I know, but I'm slandering if that would that might be a. A place for you to reach out. If you're looking for specific people that might be interested in one of the others rather just opening the door. You have a whole list of people on that block us. They're really focused. Cool. It's exciting. Wait. So other topic here is from engineering staff. Please review and comment anything it's missing in the development spend initials program spend by October 21st. This is for budgeting purposes. What do we want to spend money on not people in terms of. Rex, you know, who to hire how many people to hire in various teams, but so affordable license, training things, you know services to license, etc. We start with what did we spend last year confirm we still want to spend money on those things and if so if we want to spend more of the same but also other things. So please take a look at that to those other things you'd like to add or change. And lastly just what I had in the list is thanks to Chia go. I'm always not here. But helping to figure out why the past meeting where the past meeting recordings went. They kept on failing and it's. I didn't have access to them and it's not what they were in Chia. So they can't rather than mine. And they was able to find because they took over the meeting for a week or two, which I appreciate it. So they was able to find them and they got posted the last three weeks of meetings were posted to YouTube last week. So appreciate that. Chia go. That's all we have in the agenda anything else you all want to do discuss. Thanks everyone have a great day.",
  "So, South is Georgia and West according to the Census Bureau. Okay, good to know. And when most South Carolinians identify as from the South, or what they say they're from the East Coast. I don't know. I'd have to ask my wife that question. She's the South Carolina native. But they certainly sounds like she's from the South. That's the October 19th or an APAC October 20th. Sech growth data science staff meeting. Catch them on the other things as you too, streaming started. And looks like I feel you've got the first item. Sure. Yeah, we have two new highest for growth. I want to mention names, but we have a comment we'll probably add that data on the next week. Both around the end of the month around the 30 first of October. I think it is. The end of October, beginning of November. We have one intention. Who's coming on board and another engineer for one of the teams. So, it's great. Well done to come and the rest of the item for getting a set for. So we don't have any celebrations or news and events today. We're going to for the live stream. We're going to skip. Big rock hot issue A and come back to it since it's a confidential item. Jumping down to be. Please review Chris for notes on the Q4 okay ours for development and also their summary of the e group off site read out. Lots of good information there in goals coming our way and why. Many are things for all of our teams and some are specific to growth secure govern etc. I don't want to discuss the details on live stream so that's confidential but if you're just announcing in general is okay. So when we stop we stop the YouTube will revisit that one. If there are any questions or comments about specific things any general comments. Wasn't there really surprising it was your words only? That may be correct listen you and maybe it hits the client is the different big. I was going to talk about every out step yet I will in a convex. Next time is good point. And just when on the topic of AKS, you're watching us to be looking at these now and coming up with AKS for us and our tapes. I would give it another week before doing that. As they settle and more people give feedback on them and product management, it continues to collaborate with us on them, et cetera. Good question. I couldn't hurt to start now. Just assume, you know, if you do it now, there'll be more iteration on it between, you know, in the next week. Of course, there will be continued off in our first iteration of the past that week too. First week. So on, um, get a couple announcements here. Um, I'm hosting a public exploration of imposter syndrome in the hallway. Please encourage your team to participate if they're interested in. I'm doing this actually with Play to HQ. Uh, they do these things called circles where you can just put it out there and people join if they want to mentoring platform I use. So if you think anybody on your team might be interested, please let them know. So that's an idea on improving communication. I've got an issue like here. Um, the summary is we publish communications in many different places in Slack. various places in Slack and engineering we can review in meeting notes in, you know, recordings like this and it's really hard to get the right information to the right people without learning them with things that they don't care about. So the read rates for the engagement rates, the are pretty low. So I found this thing, um, this vendor that somebody recommended to me that they're using that is kind of a combination of LinkedIn and Facebook. Um, for informal communications. Um, so you can target and now it's been rolled skill, work structure initiative, such as cross functional teams. It's, uh, gives you reading engagement rates. Uh, so we can look at effectiveness. It also has a great board chart feature. So if you're interested, you know, if you either like it or don't like it, one of your both the idea of this, please comment in the issue. And if you like it and would like to participate in a demo or see more about it, um, I've got one engineering manager, not an our team who's interested in with me and it further so far. Uh, definitely, um, at least do so. It looks like you're the upcoming show. Um, it sounds great. I haven't looked into it, um, but the functionality and the engagement rights would certainly be interesting. Um, to look at, um, however it is an additional total website that we have to learn and use. Um, yeah, that's that was my first concern as well. Uh, when people actually want to use the other tool, even if it's better. Is, is how much better it is worth it to have a different thing to log into and use. I asked the vendor that to, uh, and they think it is of course that who knows. Uh, and so see them repeating from, uh, the staff meeting and Michelle anybody want to verbalize see. Um, yeah, I can do that. So Michelle, see a dog all in development. This quarter was to increase my attention by 15% and we need help this week before the quarter closes. Please add in qualified engineers as a maintainer this week if possible to allow existing maintainers one week to review before the quarter closes. It's good. So there are two read only items that since we're, uh, we're doing just fine on time. Why don't we actually read through those. All right, I'm first. This is an FYI, but we'll all get everything on dog yet issues and agenda document and everything scheduled today before I close it out. Um, second section retro. I'm getting it scheduled for next Wednesday and Thursday. So we'll do the same thing. We've done the past few times, uh, where it's scheduled that are normal coffee chat times. And I'll just bump those down a half hour later. Um, and I will repeat what I did last time and making sure that the meeting will be on the section count or the the stage calendars, but there will also be everyone invited to it. Because that actually got more engagement last time than did previous. So just a heads up that is coming. Great. Yeah, I think that makes the difference of inviting individual people or the group, the Google group of those people are next is more made. Uh, so the other end is please review the proposal from Justin Ferris to improve the cross functional PI reviews if you're interested in doing so. He's looking at iterating on that and cross functional meeting development, product management UX and quality. So that I'm going to turn turn off the live stream and do a local record for the company.",
  "But pay attention to that as a member of the page to a Michigan election. Alright, so we are live streaming as to YouTube. It's the secret of data science staff meeting for cover 46th or NAPAC, 25th. The, got a couple topics you talked about publicly. Actually, most of them in one will stop the live stream and good private reporting for just get Latin rivers. So some new celebrations. Or A-SILver. So no new virus, the universe, for each to mention not the biggest, celebration. I have excited I got my first non-git lab team number, shadow next week, someone actually from an underrepresented group. And I have another one lined up in about two weeks where they get lab community contributed or so. I'm excited. I didn't know if anybody would care about this or if I'd get full approval of it, et cetera. So good stuff. Also under news and events. So the press release at the QCOM that David DeSanta did on GitHub security and different solutions, helps secure organizations, end-end software supply chains. It's pretty cool to see secure and governed featured so prominently. And I think as someone mentioned, NAPAC pressure talk. Lots of know that we do the Zothinnet there that we're planning, this was mentioned publicly, the pressure is not really on, definitely, of course, but need to see that. Anybody else read the fresh release and have any thoughts on other impressions? Yeah, I read the press release. It's just great to see good to see recognition. People also look at the 15.5 release post, secure and governed groups so I mentioned in the Arizona, all the features that they're doing. So in terms of big rocks and hot issues, second one we're going to go private on the first one. Up to over 31st is the end of the quarter, as they turned flies. So please be sure to put your final updates for your okay hours and comment and then with a good bad and try and tag your manager. So they can review and provide feedback for closing them. So it doesn't need to be done on the last day or first day in the next quarter, but some context would be great. We are staying with Ally for Q4, at least in development, in engineering and likely willing to issues have staff Q4 where the advancement and enhancements to get lab will give up, do okay or tracking friendly things and with the amount of single source of truth and about blue jibiscus and in the end of the next quarter, I mentioned that we've heard that recently. In a hallway, gratitude or other items of one discussed before we go off the livestream at the top of that one net. None public talk.",
  "This is the sec growth data science staff meeting for November 2nd or for those in the second year. I started a live stream a little bit early so it would all get set up before everybody joined. So I'll give it another 30 seconds or so before we get started. Actually, well, we're waiting to get started. Rafael, you want to introduce yourself. Rafael is out my first outside of get lab director shadow. Yeah, hi everyone. Rafael has introduced. I'm a senior air flow engineer at astronomer, a previous senior engineer of data and a data scientist and just learning as much as I can this week and really having a good time meeting everyone. So thank you. So, I'm still you want to cover it ever once since it's from commute. Sure, come here once to welcome new engineers to growth. Ross is a full-stack engineer and Roy who's coming in as an intern, full-stack engineer. They both joined this week. I'm seeing it's like lots of welcomes and thumbs up. Which is neat from Mark and Camille and Tiago and Nathan. Thomas looks like you have item B. Yep, absolutely. So this is New Hires and Anniversary. So we had a two year work anniversary last week and now I want to make sure we call that out. Make sure them was was was was celebrated as well. So this is. Two years within the P.I security group or glad to have them. So I'm going to see you many more. Hope to see you here with for many more. So. Good stuff. So in celebrations. I'm going to read this one since it's for Phil. So please join me in graduating Phil for discussion. I bonus for living our values of collaboration and results for nomination from Jeff Burrows. Which if I had to say is Phil helped me walk through resolving. Get conflict locally on my machine. they had never done this before and was struggling with the steps in the documentation. Phil jumped in quickly. Help fix the issues and also walked through each step and explained each step. What it was doing to the MR for better equipment them to resolve those issues on their own in the future. And a metric criteria because help is above and beyond. Phil could have just added a commitment and fixed it. But it's like time to teach me something instead of. So what's that Phil? Thanks. I am always impressed when someone opens a matrix. A metric race to update 434 handbook pages and one guy. This wasn't surprised to them quite a few conflicts there. Why 434 and one MR? But to rename some guessing. I'm glad you write that down for me. I'm not. It was at 434 or 32. I can't remember that. they was it was moving security in the handbook. It's a moving in the tire section. But what I notice is when people do this they try to read everything in one major quest. You don't need to. You can do your redirects and you can move you. But you don't have to update other people's content people link to your pages. You can do that in the follow up if you've got the redirects right. But so. I think a lot of people just are finding a place. And then they ran into problems because it takes them a week to get all the approvals and by the branches is completely out. I think. Anyway, we worked. It was interesting. I enjoyed working with J. Heinle with J. before. So that was nice. Good stuff. Seeing lots of good as we're. I'm sorry. I haven't seen Tiago in a long time. I said let Tiago less than I saw you. It was prebeared. I'm shocked. This is the new. Like. Santa's coming to town. Let's go. Just I didn't recognize you at first. And you only started growing that beer two days ago. You know it, Thomas. So anyone want to verbalize be. I put it in here. But it's a good talk through this announcement. It's actually our acting. CTO. I'll take it. This is an announcement. It's been a long time coming. I saw. So we've been. So. If you've been here long. I remember when we announced the beginnings of the. The patent program through through legal. And there was there were catalyst for that. And here are the first two that I remember going through this particular patent program. So this is a big congratulations for team members on inventive innovative work. For two patents first vulnerability tracking using scope and offsets. So this was a collaboration with. And that was a collaboration with Julian. The departed James Johnson and and Lucas so that this is a way to help us make vulnerability tracking over time, particularly for sast and anything that's related to like code level. That's like which line of code this. Availability is related to. It's designed to help us get a whole lot more accurate over time and so this and. Was really is really need innovation really glad to see this here in the second one is for scalable code testing and benchmarking. Which looks to be a collaboration within the entirety of vulnerability research. Was really an Isaac Mark denasht in Michael and so. Big thank you to all and as a promo there is a there is a bonus program associated with us. And that is available here and there's linked to the slack announcement. So. Big thanks. And hopefully more to come. Good stuff. To news and events. If I had a handbook update that documents have development should collaborate with customers during customer escalations stem from a retrospective in a recent escalation from a large customer. So and I was the author of the MR so but I also think it's important to not just because I'm the author. I'm not going to be I put in there but I don't need to verbalize it when somebody you hasn't verbalized something we to that one. I just say go ahead. More. Yeah. team ops goal of care for Q for looking for volunteers to become trainers for details in the development staff meeting agenda if you're interested. Thanks so much. Anybody want to verbalize see I know it came from me too but you hear my voice too often. Yeah sure. Do we want to invite guest speakers to this meeting from other parts of the company perhaps starting with our stable counterparts in UX. That and product. And well the mark who couldn't make it but mark marks moment is a hems would definitely be nice. We had David to Santa on secure meetings back in the day and it was quite helpful. I think that's the useful we see this in growth. We see bring stable counterparts along and we would achieve to our product meetings as well. It's really useful for sure. I do isn't finding out what other what I can to pass it on. Anybody want to volunteer to organize like reach out to people and say we we're looking for guests and get them to maybe one guest a week. That they just took anybody want to volunteer. Well, how about we have it we don't this way someone will volunteer and I'll do it this time to find the first guest and then we we put someone else to find the next guest and we just rotate through that way. So all thing lots of thumbs up so. Great thanks Phil. I was a guest in the product management meeting. I don't know a month or so though and it was. It was good to hear the questions that not from from the product managers I work with but also from product managers I. I never spoken to. So. In the hallway. Little bit thanks Mark for pointing out the Q4 okayers epic for product management. they found it before I did and let us know about it lots of good detail in there. For various things although they are in flux also thanks Thomas for finding the best in class comparison with I linked here and also. Someone else down to slide version of that same document where I think it's the same document but it's on same topic so. Good for understanding the competitive analysis the plant the okay the draft okayers from PM. But also the competitive analysis so I have not I read the. Issue is initially an issue not epic. I haven't actually read these two this document and slide that yet but I'm planning to that it's pretty insightful. On be please close out. To three okayers with a final percentage complete. Add a good bad try like well what didn't what you want to try in the future in a comment and tag your manager review you know completing this in the next week would be great. I didn't say please do complete in the next week you know if you need more time that that's fine. We're going on with collaboration and other things. We're starting to think about. Uh, hold with a planning for the end of the year. Um, and a quick discussion with Wayne about this who reported out that our first priority is to book our leave. Um, not to worry about coverage but once we've all booked our leave. I suggest we. Um, create an issue for this group and share our leave plans so that we've more about who's here who's not here and what and apps we have. Um, who are our teams going to contact if there's an escalation or production incident and they're looking for management support from one of us. Um, I'm happy to volunteer to create the issue and this and there's someone else wants to do that. And we can start out and I leave them as we. And then Wayne that's going to be. And they still wants on that idea. Yeah, I know Phil you and I just discussed this a few minutes ago. I want to say to Genders that we're not going to coordinate who takes vacation when we're going to see who takes vacation when and then figure out what the coverage is. Because people take vacation when they when they when it makes sense for them and then we'll figure out the coverage based on all of that. Thanks for the distinction. Yeah. And all just anyway, there is a special coverage as you hear those incident managers. There is a schedule obviously but many of us will be on leave and so there's a call for volunteers for those who are around. And this is around the Christmas and the experience. Anyone wants to volunteer for coverage. And I notice a few. This group already had which is awesome. Cool, so I don't know who put an item be but that's great. Thank you. Thank you. But let's not forget. Um, but please do read it. In, uh, Thanks. And gratitude. So nice job, Jay and team at getting. The bug reproduced and resolved that where some users log had trouble logging in some of the time. Some users some of the time, which made hard to reproduce, especially when they use certain password manager browser plug in. But there's really hard to track so great job by Jay and you, I think, did a lot of work on this. Yeah, thank you. You did an excellent job on following up and digging in deeper. So all the thanks they covered to him. Very happy as how that turned out. Good. Since we have a little more time, I know we don't want to stretch meetings to the time of a lot of it. But the time, but when we when we. We talk through the read only items. Things there. Good stuff in there. There's a link to a handbook page about, uh, beautifying our UI for developers interested in contributing to UI improvements. Where we're used to seeing these come through as, as okay hours to burn down. Back well, so this is, but this is a different approach. It's nothing for volunteers and what it wants to. And we can contribute to our improvements within with someone in. Are you existing? I'll read that max. Quote max not here. No, yeah. I'll read that max. Quote the idea was to empower the peer to make changes. They are deemed to help themselves and resolve directly with the. And rather than following our existing product development flow. And that's what it was interesting. And I'll link the participation that Yonic on sati can now. It's just recently completed as a part of the beautiful find the, you know, initiative. they read it redesign the login page. So that was a really big. Task and I think that should in 153 maybe 154. I'll find the issue and link it, but they definitely learned a lot from it. It was very impactful and a great experience. You're all. Do you ever want to read your item? Rubberized your name? Show thing. I create a model to revisit the decision to. Where. A discussion in a mouse that don't have a sub type. The discussions didn't didn't lean heavily either way. There was a slight inclination there to. To remove or at the very least simplified by by not requiring people to go and click there. My own personal experience is that that thing was always getting my way. Maybe the type of a mouse that I contribute. And the general idea behind my proposal was that. Contributing should be simpler and smoother. We already have a lot of tasks that are required. A lot of boxes to be checked. A lot of a. Vassages to reply. So any anytime that we are adding friction to this. To this process, I think we should be very diligent. In the values where we, you know, benefit the cost benefit of that choice. And and. I was particularly happy that it took two to a door decision. If it turns out that we absolutely need that. It's a busy thing to revert and put back in and go from there. So good overall representation of our values. I think. Next year. I go. Um, quick poll. As a thing, you know, the more we connect the better. So I'm trying to have a good question. Things versus the connect on. So a bit of a silly one. But what's your favorite slack channel. Actually like to hear everybody on this. So mine is all caps. Notice I taught I typed it in all caps. Chia, go, what's yours? Or tell them us what's yours. I see yours there. Next. Mine is random. But. Mine is one for not shouting all caps when you said it. Sorry. Sorry. Yeah, Alright. There's. There's emoji in. Um. In Slack that's a flashing all caps. So I don't know why that's my favorite. Why is it? Why do you like it, J wird. I see you're plus one there. I don't know why I'm drawing to it, but I am. Y. Yeah. Uh, the low level humor just really hits. I don't know to say it's great. Like you know what people are cracking about you. Like the simple things. I love all caps. My recent one has been the New Zealand Museum in Grand Channel just because it was a way to extend the time that I spent with the new people that I met in Paris. The first people I met in Paris and aside from Mark, who's not here, but it still Mark is the only other person that I met from GitLab and it was awesome. I met Seth there. It was great. We had bees in the balcony. And it was just neat to keep the channel. The channel's gonna go so I'll just go back to the Sex Section social. Seth, it looks like, sex social was your, your favorite as well. Yeah, it's just nice to see what other people are up to. Alan, a sex social, your sex section, social, your favorite as well or a different one. It's true. I mean, we have so many pictures from vacations and other stuff. Like, this is great to see that. I recently joined the app doors channel. I kind of like Instagram. This just people post pictures of where they can go out doors that really appeals to me. We spend so much time sitting inside in front of the computer. It's great to see where people are going. And what they're doing with this there time. And it's also because most of our company is not. In some of them is fair as different seasons. So I'm seeing a lot of autumn textures whereas we're getting a discreet. Tomorrow looks like you have some others as well here, fan of. Yeah, similar to Phil, you mentioned like it's nice to see what other people are seeing. Another cool one in that regard is office today where other people post pictures of their office view of the day when people are like traveling around it's really cool to see all the little views that they have related to that. I'll let you look at that. Yeah. And then related to that dog is always a great way to start you morning. I mean, you're a dog person. So scrolling through dog picks definitely great. And dad jokes was one that I ran into recently that just good content in their all around. Do you need to be a dad to contribute? Or they don't go at you at the entrance. They don't card you. And that was my first question too. And I saw a lot of people posting. I think everything goes as long as it's a yay dad joke. Yay for inclusivity. Indeed. Maybe it looks like you're a fan of dad jokes as well. Any others? I'd quite like office today as well. Seeing other people around the world with their offices and I like Phil mentioned different seasons. So people inside and out. But yeah, dad jokes is a out of a early-career family. It's good to use some of those in the house all this well. Good. Good. Did we hear from everybody? I think we did. Did we miss anybody? No. Yeah, I got looks like you have another ready as well here. Yeah, I had a quick one there. I'm just enjoying watching Seth having to run back and forth. Look at all the fans about the story. Get going. Go. Yeah. That looks like I cannot. I'm over. Let me see if I can knock that over that. Looks like I'm a. It's so good. It's so hard. I had my hands full with the baby. That's what they talking about. And I didn't get to meet the baby in New Zealand. So this is good. Yeah, they didn't come up. Yeah. Hello. Hello. Hello. Hello. Lastly, I know we're on live stream. But rough, any questions for any of us? Is your shadowing this week? No, no questions today. Unfortunately. Sorry. Cool. All right. Thanks everybody. Have a great day. Before we go, I want to congratulate everybody. That's in New Zealand for being in uniform today. You can and well done on coordinating outfits. I'm sorry. All right.",
  "All right, so we are doing the sick growth data science staff meeting for November 9th. And we've got one item that's primarily confidential. We'll go to a private recording for that. And then, but the other things, I think we can talk about publicly, which is great. And it looks like Tiago, you've got the first item. I do, and I was just about with the second one. The first one is who have a new backhand maintainer very soon. The feedback right now is overwhelmingly positive. I'm not expecting any blockers, so I'm sharing it a little bit early, but Mehmet will be a maintainer from Tuesday. Probably a little bit of a due. And the second one, those about to put in is that we have a study date for a new senior backhand engineering threat insights, sort of Jen. That's the second use. I'm going to write it down. It's great. It helps with whatever occurs, more maintainers. All that is well on the on the on the new hire. We've got a new start in composition analysis now for one December rather than five December. So, well, that's another one coming in. Nice. So there are some major changes to my okay, I was going to last time we discussed them, which I think was just a week ago, but you know, it's all about iteration right. I think they're mostly going to be stabilized mostly stable now, pending more feedback from you all. And also further review, et cetera. So, please start working on yours as one week has already gone in the quarter. And two plus weeks, this quarter will be low productivity due to unwork stuff at least due to the holidays. So let's, I want to review live in a non public recording, which we'll do after the other item. So we'll jump back to that. So let's see here is the senior, I don't see it. Somebody want to read meals. Well, it's a good read only. Yeah, it's really interesting. No, I'm here. Yes, a lot of people today. It's awesome. No, it's really don't. I'm just sharing my okay ours. Okay, say anyone needs influence on structure and theirs. And could as it's great, you're doing it in an issue. I think the whole company is moving to issues versus ally. I stuff to do mine and ally, but I'm putting all the details in issues wherever possible. And just doing, you know, percentage done updates in ally. So recommend doing the same. Yeah, as a part of doing the same. I don't create stuff in ally unless you really need to or you get assigned something. Do the primary work in the issues. Right. Yeah, it's so much easier to draft an issues. It's more mutable. And then yeah, I represented those all in ally this week. So there's structure and there's measurable tasks, but I'm not having my team look at those. They look at the issues as well. Cool. Thanks, Mike. Here. Nathan, you're at something. Now they were talking about a car and we were with the right on the. Yes, since it's not really only I've just added my link in there for draft ones for compliance group as well. Yeah, still need to align them in all that, but at least if people to have a look at. Great. Seth, I've got mixed item. Yeah, so I just wanted to share a tool that I use during the recent review process. It's a nice tool just to pull up everyone's comments. So you pull up people's comments for the last year, two years, whatever. Really nice way to just refresh your memory on some of the things that they worked on that might not necessarily be MRs or issues. Just some of their contributions. One of the things that I found in there is you can start looking at communication patterns. You know, people that are very grateful or people that are really pushy or they forget to ask with the next step is in their comments. So it gave me some ideas in terms of coaching and ways I can help my team be more productive. I don't know if we've looked at this as a product area. We think get lab. We focus a lot on how do we move code through the process, but not necessarily how do we help write better communication. So I don't know if there's anything in get lab where we've thought about that. So I just want to share that. I think it's with an issue. Put it with that idea there. See, see if the product manager likes. I sorry Nathan. I think it's a good idea. I think it's a good idea. I think it's a good idea. I think it's a good idea. See if the product manager likes. I sorry Nathan. I skipped over you. No, I just can't say that's a great looking tool. By the way, for doing that analysis. The thing that I that reminded me was we were looking for a. For a balance to the MRAIT. KPI, right? So, oh, you got. Excellent. That's great. But what is the quality of those MRAITs and one of the things that I thought was. Analyzing the comments in an MRAF or for to try to get the sentiments. A cool. You had, you know, good MRAITs, but the sentiment of them was fairly negative. What's going on with that. Yeah, and the other thing I found is. You know, there's some people that will do reviews or comments on issues. And they'll put in, you know, massive amounts of paragraphs, options. and that doesn't show up in the MR rate or issues that they open, but clearly they're putting in a lot of effort in responding to other people. So this will help me pull that out. You could like continue the automation. We're in a three-legged sentiment analysis. Quick little word count. Build a score. Yeah. Yeah. Actually, we do have something similar for only a only applied ML. So we do take the comments and it's just, I have to check this tool as well because I wasn't using this tool, but really looking to, so how dividing the communication to various factors, as looking into how, if you're able to, yeah, like are you going on loops in the discussions? Is there, is the content actually valuable? How is your house the written communication really has effective? And so, yeah, I look into this, what I'll share from Lish share of share our one. So, the one we have, it's mainly in a lot of people's laptop. So we just got out of and shared this as well. You can have a look. And Thomas, you want to just talk about your question. Yeah. Sure. I'll start with the question. How do you think your engineers would feel about this tool if we're able to? The reason that I was curious, I really like it, but I'm scared by it. Because if not used carefully, it has a really big, big brother feel to it. That's why I was curious. I think that's a fair point. So for us, we used it collectively to get better. So I mean, so it was more about we all, we had a okay or on effective communication. And we wanted to have enough data to actually understand how best to communicate with each other. So, so that's how it was sort of first used as well to really understand various methods. So I think a lot of them were actually already going into a weekly based on their interactions and actually seeing what was working, what was not. It's really, really, it's on the very early days. So most of it was really having fun with it. But then it was an app interest where we wanted to do something more and make it or to be honest. But it was enough in the early days, people were at least helped in understanding how best to communicate. Because our team was all new and came from all different backgrounds, cultures, everything. So yeah, that helped just everyone on page with it. I don't have any concerns with this. I mean, we can go and look for issues and MRs that are also also sorted by team members, all the information available tools like this just make it easier for us to find things. So with our transparency, our team members should be thinking about how they're communicating at all times and the opportunity to get feedback on that and improve shouldn't be something that I think members are concerned about. I wrote something along those lines, feel, I don't know how other people's will feel. The way I feel is I'm already super conscious of what I'm writing. I know that these things can be dug up from any time anywhere, where it's a computer to you or a person. But the automation does bring the scale that comes with the feeling you've been watched. I think that as long as there's the two's been used to facilitate a process that involves a human, we should be okay. It's not a computer making decisions go right. He's your rating as determined. But I think I'll agree with them going crazy. Oh, I'm sure that's coming, isn't it? That's why they hasn't shared it yet. She's clean up. Just to add to that, I think, how are you looking at what B2 actually empower team members to use themselves to look back at their own stuff and use their own self-review? Rather than just manage to use them as well. So for the entire time, I'll jump into the next one. We're good. So this was just a question whether we have any policies or processes for financial supporting open source contributors. There's a company here that has recently talked about how they do it. We're running into a project that we depend on that essentially is one volunteer maintainer. It's out in the community. They kind of do it in their spare time. This would be kind of a nice project to be able to throw some money their way and say, hey, you know, make this more a part of your job. But I don't know that GitLab has ever financially contributed to open source projects. And I don't know if there's a reluctance or what this just suddenly come up. How much you have a comment? Let's take? Yeah, sorry. I was trying to wrap up for a motoralyze. I'm not sure I had that all about us. We do have the open source program, which gives away ultimate for free. So is there? I mean, I know that's attempted to be a substitute for, I mean, you're talking about dollars where this is like an in-kind contribution. But it's, how you've been looked at this? How have you, has this been thought of? No, I mean, the projects that we're relying on are on GitHub. I mean, certainly getting them over to get GitHub would be fine. But I don't think that that would be a main driver for them. If it's something we use and they need dollars or could use them. Let me see if, if I can, if we can find financial support and we did sponsor OS for a while in the past, which has multiple open source projects. So yeah, yeah, I know we do a lot of at the organization levels, but a lot of these projects come down to like someone who on the weekends likes to program this thing and are they asking for money? Not necessarily, but of the public or the one that I have a mind now they work specifically asking. But yeah, it wasn't just a specific ask. I think our projects where people put up like the little sponsor badge on GitHub and have some of these other financial tools. Send me the details and let me know what you're recommending. Yeah, I'll see. Sounds like a reasonable thing to do. Yeah. On only slightly related to this and probably at a smallest scale, but we had a maintaining where they both and beyond for an upstream gem that we're using in in in GitHub and we offering them a voucher for for GitLab swag. So not exactly called hard cash, but you can get some cool socks or a jumping jumper or something. Cool. That's that's nice to know we can do that. Yeah, there's a form in the handbook. Bill, you want to take a handbook, too? Yeah, let's move on. Last week we talked about having a coverage issue for managers. So our team members would know who's available at the times when many of us are likely to be on holiday. I've drafted a mission here. I've done it for I think it's the 23rd of December, the 3rd of January. I've done it by day. If we want to extend it, make it longer shorter. Let's entirely address. So the ask here is if you are working during that period, add your name into the appropriate row and closer to the time we will just socialise this with all of our teams. If they need management support for anything, it could be an access request. They just want someone to approve a production change, response to an insulin. That's what I think. I didn't go the other route of putting in who's going to be on leave. I thought we'd go this way. That way we don't, it doesn't have to remind confidential. We can open it up. Thanks Bill. I added a corresponding issue for the last two weeks of November. I'm going to be out those two weeks. In many in the US, they're going to be out I think it which week it is. But the first or the second one, due to the US holiday, Thanksgiving, many are going to be out many days. So it'd be great fill and Thomas and Maun, and then also Mark who's not on. Since I'm going to be out those two, and as I'm often reminded by you all, when I take vacation, I still check messages and I shouldn't. And I appreciate you holding me accountable to that. Yes, exactly Thomas. So I'm going to try to do a really good job of escalation paths. But I need to know when you are going to be out, those those two weeks as well. So I don't say, hey, go to Thomas for extra fill for that or Maun for that and you're actually out those days too. So if you are, that's fine. It's somebody else. Seth, you've got number four. Yeah. So we just hadn't got much communication on the laptop refresh program. There's a couple notes that haven't been responded. Nathan added a note about India, which is one of the ones that recently came up for self-precurement. Just don't know if there's a better way to get communication. I think one of the things that is important is our IT team knows it like when one person asks the question other people are also looking at the answer. So it's not just one question that's going on in the answer. So when I don't know if there's a way you can ask like this and just make sure we're getting good answers to this. Happy King people in Slack and addition to issues. Not everybody keeps track. Not everybody goes in about zero. So they may have missed the things in. I have not. I mean, these are questions that other people ask that I'm just following along for the answers. Go into here to ping the person or the group. But it doesn't need to be Seth. Just be Seth asked, you know, doesn't need to be you, Seth. I got it. I need to answer as anyways. Because well, thanks for bringing it up Seth and thanks for volunteering Thomas. I put number five on there, but I don't know if someone looks, you know, important. Anybody want to volunteer to verbalize this update from our CFPH? Or summarizing verbalize? Sure, I can do that. Brian Robinson, I wanted to basically, we live streaming where I came to live stream this. I haven't actually read what they are. Stop it. Yes, it's a really good one. I wanted to make everyone aware of two MS, the first MR is additional guidance on the holiday party budget for Q4. The second is a change to the get together grant and Q4 because of the large visiting grant and Q3, the holiday party budget and Q4, the get together grant was removed for Q4. The holiday grant can be used for in-person events as well. Please also, I'll make sure you coordinate with your manager and or equal member. Improgise, Fuminian convenience, this is my course. So, what's the TLD out here? We've just got, we've got just the one thing which is the party budget. Yeah. And to use it or lose it, please organize these holiday stuff with your team. It's great team building. And some more to do. And often it waits for the last minute and then sometimes doesn't happen effectively. Just on that, the get together grant was supposed to be taken and no Vemba. Is that but that's the one we can go and experience $30. So, some people will have already done that as soon as they sang. Yeah, it was. We had to run for me to bless Friday. I'm kind of curious. Expense outstanding. I think I sort of coming that any expenses already done will be experienced but just to stop moving forward. Is there a cat off? It's nearly lunchtime. Oh, I will have to do something when I have to look at that holiday party budget. I'll be next. Wait, go side. And maybe Thomas include Mark's folks. Your team coordinates, collaborates with them often. You know, filled your team is big enough to do something separately. Maybe, maybe not, whatever you and Thomas decide and I don't know if you want to do something separate or combine. But you have the more people, the harder it is to coordinate, but the more fun of this too. But whatever you all decide, I'm definitely good with. Yeah, you've got number six. Yeah, during a retro, we had recently, there was a lot of feedback around the planning and refinement process and improvements that we can make to it. I'm curious when you receive feedback like this, like for a process that happens kind of on the fly or in person, like the person creating issues, like sometimes we get together and we do that sometimes I do that alone. And then we review those issues later. How do you keep these guidelines in place? Or like, do you have a mechanism to be like, oh, make sure we're creating issues that have a small NBC or, oh, make sure that when we're doing refinement that the MR can be small enough, but also vertically sliced. So it goes to review more easily. Well, I was at the boring answer. It's a handbook first and make sure we're documenting our prices and team handbook punch. So that was the, that was like the number one idea. But I was just like, alright, well, how, you know, besides like having the handbook of why are you doing whatever process it is, do you all have like a different way of doing that or a, uh, some other practice? The 13 sites we tend to discuss these things in in retrospectors and create follow up issues after the retro for for a bit of discussion before going to the handbook with a, but it's time to go to the handbook. We've already had this discussion of the scene is fairly on the same page with what they want to do. This helps. Yeah, agree. That's a more casual way of approaching it. Although I've seen more success with them threading sites as well. Um, with these follow up tasks if we actually have an associated MR, like we start the conversation in a proposition like proposal versus just to open dialogue. So that's, it's been a personal goal of mine in the last quarter or so to be more handbook or in it. And so, and I found myself being more successful in our proposals. So I think if you're in that situation where you're trying to contradict something that's in the handbook, start a conversation with an MR. I know they're boring answer our apologies. It's okay. Thanks for the contributions. It's, um, sorry, Jay is your question. Okay, maybe you've got the handbook up that what makes you know, because no people aren't reading it every day. Is it more? Basically, yeah, yeah. Where's the guardrail? You know, like, how do you ensure that the team is following process? How do you ensure that you, you know, you yourself are following the the correct process besides just being like, I know I've outlined that in the handbook. Um, so what options do you have for automating the suggestion? And we can automate and retrospective. You know, we can put the data in the that sees that might help. And we'll say it depends on the on the situation you're implementing. Maybe, maybe that's one place to look for. Yeah. What about, like, circling back in an upcoming retro to make sure that you've followed the new process implementation or whatever change that you've done? Yeah. Yeah. Great. Yeah. Yeah. You can get the data approach. There's always saw things as well. If it's something you can track. So rather than thanks bring that up, Jay. Good stuff. Rather than going to a private recording and going into details on the Ocaris, since we're we're over schedule time that would say review the Ocaris to which you've been you've assigned yourself or you have been assigned where I did a bit of that today or you volunteered and many people volunteered for things who just greatly appreciated. Um, that we've iterated on them a good bit over the last week. Do you want to get started on them? Um, so review these at a digital summary here. I have to answer one, there's two things to note. In addition to, please look at the ones that are assigned to you in. Like, mom, I put two on you today without telling you an advance, apologies for that, but there's things you wouldn't be surprised by most likely. But as an example, and others as well, a smart, you volunteered for one. I put your name on another, you know, etc. Um, so the two thing that is the team ops training and coordinate that issue just self reported we want 50% of the team to take team ops training. I don't need to coordinate that myself. Anybody want to volunteer to do that. Either in this group or not here as an EM or even a engineer could truly take this. Anybody want to volunteer for that or know if somebody who might want to volunteer for that? I say we choose someone who hasn't taken it from this group. That'd be fun. It could be me. Does anyone not done team ops? No, when you, that's okay. I'm just curious. Has any major? Oh, I haven't done it yet. So I'm, I'm volunteer. I've had this gone here for three years. I had my name on it. So I'm going to say not fill for that reason. Otherwise, it might be fine with that. So it's just basically, uh, hanging people. Hey, don't forget, hanging all the managers making, you know, maybe every two weeks. Hey, if you're every month, hey, if you haven't asked your team to do this, ask them and ask them also to self report. That's all. So it's probably, you know, I'll pick that up and work every two weeks. Jay, I just added a link that has all the reports of who's done it. I'm so little here. I didn't even know that existed. That's awesome. Did you, sorry, Jay, did you say you're rolling here? I'm not trying to volunteer. Okay, cool. Thank you. And last one is that is one to mention, sec addressing, sec being secure and governed, free prioritized usability issues. Omar, I think you volunteered for this one earlier. Or did, yeah, or maybe it was a different one you volunteered for. I think it was the as one as to, okay, good. Yeah, many different things in flight today. So I may remove this one. It was part of a previous iteration of P.A. Product Management, okay, or so I was kind of worried this was when you volunteered for Omar. And it's, it's not so yay. So I may remove this one. I'm still thinking about that. So cool. Are the questions about these, you know, definitely or need more information, I'm sure many of you will on many of these, you know, let your manager know that you're PM know of to PM thing that need no and that's collaborate on an asynchronous thing that does need it. Take a look. Thanks, everybody.",
  "I don't know if you can see it couldn't now. So it's the sec growth data science staff meeting, which we just discussed a confidential topic. It's what, and now we're going to discuss the non-competential things. It looks like neat of the first item. Yeah, it's just a bit of news about new hires. The compliance group has seen you from team to engineer joining on November 28th. So that's Alan, once mainly based out of New Zealand as well. So that's awesome. And we have another hopefully some good news mixed week as well around this. Thank you, Ms. Mubin. Yeah, I got next. So we noted this in Slack last week, but it's worth repeating year. So we've got a four year work anniversary in a three year, or a anniversary for the first person that's the honor that you're in to derage. So they put up with us for a long time. So this is always worth celebrating. So glad they're here. So welcome to the center of the meeting. The center of the is an outside shadow is also a community contributor. Give that. And Thomas, you got the next item. Yep, so just news and events since we're coming up on a US holiday week next week. So we're going to go ahead and schedule it for the week after. So that'll be. And then November 30th or December 1st, depending upon location, we'll go ahead and do the same thing. We've been doing with two sessions. So just want to give body heads up to those. We're coming and going to be scheduled. It's stuff. So best in class, B. I also known as B. I. C. O. K. R. is for securing governor being finalized for Q4 by product management. With an ETF distributing them by the end of this week. Once they are, please be sure to engage on them ASAP as we want to accomplish the one slated it. 4 Q4 in Q4 wherever feasible. If you would help, we can consider changing team priorities of the teams working on the OKRs and those not to help out. Please also continue to give PM feedback and ask questions about the OKRs and the associated epic's and issues. And we're going to be sure we understand the goals and ways to deliver them as intervially and effectively as possible. It sounds a little bit scary. We're getting OKRs, you know, a couple weeks into the quarter. But the thing from PM is the best in class OKRs are likely going to be the work already in the backlog. A subset of the work already in the backlog that we're already working on or planning to. And we're going to be sure to do the next two or more releases, but we don't know if we won't know for sure until we see them and I'm on vacation for the next two weeks so you won't see me. Bringing you know, finding a light on these once there are now, which will I'm ringing up here. Any questions or comments about that. So the TLDR here is we're talking with our product managers. And we're talking about for us in Q4 that you've already prioritized that relates to based in class. Yes, correct. And then we need to choose between those and other things we should choose those. That's you know, it's very, very generic. You know guidance of course, but we don't know the details yet so it has to be generic. If you haven't already done so please add a comment on good bad and try on your Q3 you three OKRs and square. Two weeks into Q4 and I tag your manager for review and then close them sometime the next week or so would be great. In the hallway, you know, should be invite newly created Google groups the new that we created for each team has optional for this meeting so that. The individual contributors are more aware of this meeting in case anyone who attend or review the notes and reporting. I just put a note I didn't think it was necessary. I think just putting it on the group calendars enough there's a lot of events on the group calendar. I want to promote the idea that any event on the calendar you can go to you don't actually have to be on the specific invite. So I didn't think it was necessary. The other concern of course. As managers, we all know this if we invite people to a meeting they feel compelled to be there something comes from Wayne people will be like, oh, I need to go and we don't want to create that but we. I definitely understand we want to make sure people feel welcome. Good point. We have also learned that people would know group calendars primarily. Maybe do a polling slack to see if people wanted added but to your points out this let them know is just because everybody doesn't mean they have to come. It's exactly the opposite so but what would it really all think any strong opinions either way. I agree with with both of six says points there really but yeah we want to we want to advertise be that I think that's the slight polls are good. I could way to do that. I'm happy you're on that if no one else wants to. I feel that you're saying thank you so amount next week. I'm out next week but I can't hold next week's meeting because many of us in the US at least think can the two are are out I'm making sure. part of next week, do that how it is. So the week after I'm also out and looking for volunteers to organize this meeting. So Phil, you just, I see you volunteered on this, Thomas, you did too. Since you just volunteered in the last thing, Phil, Thomas, you wanna take this one? Yeah, I got it. Okay, I will do the transfer of the, as appropriate. Little keeping. Also, be aware that performance indicator meeting for second data science, it's monthly is November 30th. Also, well now it's all. So please plan accordingly, Phil, Thomas, and Maund to do your pre-work or homework and participate sync or async as appropriate. And it's wanna make sure we got an act on all these from Phil and Thomas and Maund. So I already saw that. Thank you. Seth, you have a comment about this. Yeah, just thanks for calling performance indicator. One thing to see so many acronyms being used and it's really hard to know what these meetings are about is we are overdosing on acronyms. Yeah, used to mean to me product intelligence because I was a team that reported to Phil but it doesn't mean that to me anymore. But it does mean to that team. We have also overloaded terms as Maund Taylor, like to remind, but we have two teams called data science, and we have a good kit lab, Mons and another one. And that creates confusion too. Good point. These are interesting things. If you wanna look at talks about what PM's goals are and what metrics they use, the engineering metrics that we're familiar with like, error budget and percentage of issues that are maintenance versus bug versus features. But this direction might be particularly interested in the PM metrics and like usage of the systems and customer growth. I find that stuff really interesting. You might, who, where you might not, but you're definitely welcome. Phil, you've got D. Oh, we have a get together. We normally have a get together grant for the first month of the quarter because it's Q4. Instead, we have an end up year holiday party budget. We talked with Christopher Asink and they is thinking of delegating that down to the sub department level, which means we have some flexibility and what we organise. I'm interested to hear what people in this group and your team, so please share this then. What they would like to do, do you want to do something at this level? Where we have maybe three meetings in a 24 hour period, and we invite a lot of people. Do you want to break that? Do you want to break out into smaller groups? Do you want to include your counterparts? A product and stable counterparts? Yeah. I'm interested to hear what people want to do. Yeah. Carmel and the growth team, they've started an issue to discuss it. Does discuss what they want to do? Any other questions? Yeah. Yeah. Model of people actually bring this up even in the team meeting. And probably even start a issue or a poll to discuss. Last time, obviously it was very small the team. But yeah. So hopefully, by we'll get more details. I'll just ask you a question. Aside of ordering food, does anybody have any great ideas on how to cash in on that 100 bucks? Oh, last year, it was four or five years. We did Secret Santa and it was really interesting to see how engaging everyone was. It was interesting definitely. We did it. They really wanted to know what the other person would actually want us to give. So yeah, that was that was fun. So what I'll do is I'll create one issue for this group. And I'll put some of the ideas that we've had. I'll put examples of what we've done last year. And I'll share it. I'll paint you all and let us you take your way to your teams. Create your own issues to say what you want to do. And we'll all come back together in a week or two and. And we'll go as we do. We don't need to check the latest guidance on what we can. What you can purchase as part of the holiday budget. Center a call. There was an update recently. So we want to make sure things like that center is still. All right, looking at the time I'll go next. It's hard to follow that up with metrics, but I'm going to anyway. So I was talking with Derek earlier this week and he's part of a working group for Dev Sec. And we've historically within sec so secure and govern. We've had a challenge with metrics and accurate metrics collection. And so he's looking for an engineer to help them with that to help reconcile this particular situation. Looking for a point of contact that they can work with across. The section itself. And so the so I was bringing this here and can find out if ever if. I was in EEM have an engineer that might be interested in a section wide initiative that would help that would be based within the Rails platform. Help out with this. And I won't look for hand raising lives, but please. Please consider and and get back to me. I'll be asking more later this week as well. So. I've been discussing this with Derek as around because there's a lot of updates made in compliance and I have an engineer who's working. Tile on that and I'll ask them whether they're willing to look at the section. Let me get back to you. Okay. Thank you, sir. Appreciate it. You've got next. Get I opened up a team or issue for planning our section team day still trying to figure out what the date should be. But we're thinking early in December. First question is what anyone like to go. Strict library with me on planning this out. And then second question is going to be around. In the past, then it's has been fairly low. So just brainstorming ideas on how we could improve our attendance there. Yeah, could be carried out. So. So. So, So, So, So, So, So, So, So, easier or harder to contribute. So like I've been contributing in GitLab for one and a half year and like it's been a great journey like I've been with GitLab, I've offered them Google somewhere for a full bit GitLab so like it was fine and it's been the order of the poster right for yeah. Like I'm enjoying contributing to GitLab. There are plenty of questions for this group. I know you've met a few of the people are previously in various meetings but any questions for this group of engineering managers and senior engineering managers and legal counsel. Did it happen to be here today? Not right, not but I let you know like I have been okay. A apologies that we went a good bit over on time but we had a lot to discuss and discuss one good stuff so thanks everybody. Have a great.",
  "I was the first to go. I'm not sure why, but I was the first to go and that meeting. It's a nice humor. So it's a sixth birthday and science staff meeting. Tiago, thanks for getting the live stream going. Just send me the link afterwards and I'll edited and put them in the meeting notes, et cetera. And also, I'm going to welcome Slayton today, who's my director, shadow and as a student at USC. Thanks for having fun. And small thing, hey, now in a notes on your view, it still says interim on your title and zoom. You might want to change that. Have some still interim. That's why. Oh, you're still interim. Okay. Yeah. The system works. The, um, so it looks like I kneel. You have the first items. Yes, so regarding new hires and anniversaries, um, I've been hiring for two senior front enrolls. One for third-end sites, one for security policies. We are, we filled the security policies. It's Ian James. I've went out to their LinkedIn profile. He's located outside of Melbourne, Australia, starting on January 23rd. And he's going to be reporting to Alan, you know, because obviously Alan has taken over that team as a plate. So that's amazing. I've already apologized for hiring somebody. Um, under his, you know, kind of things. But they also appreciates the work involved. We also have the threat and size position. We are on the like the final steps. I'm hoping to be able to share news later this week about that and that will have a similar start to anticipated. Cool. Tiger, you're up next. Uh, on the same vein, we've got to, to you, starters. We've got Mo, who a lot of you will know and they tends to watch these, uh, their videos so high, Mo. And, and Malcolm was a new Zealand and very excited to, to get these two people. And for the Americans, it's pronounced Melbourne, not Melbourne. I don't, can't even tell you what I said. But thank you. Well, Melbourne. Melbourne. Oh, that's a great news in a new starts when our big goals expand the team. In celebrations, I've got a couple of them here. Um, so the AI assist proof of concept demo was um, put out their in public. So great work by the incubation engineering team and the AI is the AI assisted team. Uh, so monstimcy and gradulations, mon to you and your team and to the and Fred on incubation engineering. Yeah, it's neat. Got the link there. It also will internal dog fooding. So we'll love feedback. So anyone using the Good Lab via Scored Extension can, uh, can basically use it. Cool. Yep. And you can see how to use it by, uh, by my, my, I'm watching a bit, this short video. So also, um, uh, I know I'm a little bit late to celebrate 156. It was released while I was out of the office, but it was still really neat to see all the great work by secure data science and government teams, the features included get abuse rate limiting to notified of administering traders when a user downloads more than extra repositories and why time period on demand, API security scans, scheduled dependency scans and the ability to manage scan results at both the group and subgroup levels, noting that automatic revocation of leaked personal access tokens, paths is coming soon. We're internally dog fooding. So all sorts of fun stuff out great stuff out there, and it was fun to brag about them on social media too. So thanks, everyone. I got the, the next item, but just piggybacking on on the way, one of the features that came out was the ability to, uh, verify commit signatures using SSH keys, uh, Brian in 13 sites contributed to that feature in, in code source. And they got a discussion here bonus for that. And there was really good. Uh, and, and this happened to all of go ahead and announce it, but we have two new maintainers in section. Uh, Mehmet is a back end maintainer now for good luck. And me how is a maintainer for database. Um, that's great. I'm just going to signal boost, uh, you know, we had a maintainer working group, um, Michelle's leading net, and we are still looking for more maintainers. I think at the last count, they're looking for about four backing maintainers, um, a training database maintainer, if anyone's interested, and of course we're looking for marine owners for other projects and across front end as well. Um, I see that I would track that for wind and q4. So if anyone in your teams is interested or you feel should become a maintainer, and reach out to me, let me know. I'll happily work with you to see if we can make that happen. Good stuff. So a news and events. So I also noted a number of mentioned have secure government and data science work in this week's earnings call. So uh, that was interesting. And there's an unofficial transcript of that here. So you can search for what you're working on and you may see, said, or Brian, uh, mentioning it. It's going to need either in prepared comments and or when questions answers to questions from analysts. Somebody want to verbalize our Taylor's comment? Yeah, I'm happy to do that. Um, Taylor is announcing that these that make some changes to the model up stage, uh, re-naming applied ML group to now be AI assistant and they've also added a new category to AI assistant called code suggestions, which is the maturation of the AI assistant Park Sieg into a product category. Well, that's a mouthful. You have just normal all the way. It's about if you have any questions. What's all that about, Mon? Um, a lot of name change, but I hope this is the final one. So what I understand is applied ML did not make much sense. Um, and so as a group, the name as AI assistant makes a lot more sense. Um, and so and and code suggestions actually says what what it's supposed to do. So so then it's a lot more easy, but yeah. So in big rocks and hot issues, uh, one repeat this, it was I think Sam Beckham put this in a couple of different Slack channels. Part of the reason for doing this is because people wanted more context and on Neil, for example, you mentioned that the teams working on pajamas migrations are getting a little burned out, uh, because they're doing it for a while and they can kind of loss side of the why we were doing it. So it wasn't just you and people on my teams. It was others as well. So Sam put together this great video and recorded presentation about the why on the pajamas migrations. Uh, he's available for questions, suggesting people watch the video, and it's said, or so overall, um, good stuff there. And thanks for bringing that up Neil in the past. This is a good, good, great feedback. That's not the only reason that Sam did this, but it's one of the reasons. Well, I appreciate that, Wayne. Yeah, it's really cool. And I've passed this along for the team. Uh, notably, it doesn't include things that are in the enterprise could base just as an alphabetical thing. There's a in the topic to come in more about that. But our team, specifically Alexander Currency has helped actually reconcile and get that list of the locations. So our team and Alexander are going to be looking at ways to approach that and help facilitate this occur. So you can find that within my okay, I'll list if you're in the issue. But yeah, thank you. I've put a list of, uh, I'm tracking the engineers across the TV scrubbing and price. I've added a list in here. These are the issues that are assigned to our engineers. And as we close them off, we'll complete that. If there are, if anyone needs any help with that, it'll once be the track additional teams are more than happy to do that across Q4 as well. Just reach out. So I think I have the the next item. So Christopher has an issue he'll he's kind of mentioned our engineering managers. It's a sort of a continuation of the best in class conversations with us time he's wanting engineering managers to lead the conversation and actually reach out to the PM's and ask, you know, what is it that we should be getting ready to do? If not already working on in Q4, what should we be getting ready to work on in Q1? We're just approaching this from multiple angles. A lot of the times our PM's will already be doing this. Our engineering managers and product managers are already having these conversations. Christopher's just asking our EM's to actually lead that conversation. So we can be clear across development of what we need to be ready to work on in Q1. There's a few links I've shared there and the doc. Night, you had a question for a comment. Yeah, just around the doc, we from the compliance perspective, we found that it didn't quite match up. So we're already started that discussion with a PM in compliance space to look at that best in class and work out what exactly that means for us and our group, but just for reference for everyone else. Yeah, brilliant. And I think that's why Christopher's asking us, just make sure that we're all aligned on what we're looking at doing. And nearly having a KR to look at, you know, what we can get ready for and doing Q1. So you already, you're already working on this. And Tiago, you have an issue to, okay, our round and blocking big features. It's more if the conversation changes, are there other things we need to do to unlock any new features that we're talking about? So that's the sort of thing that Christopher's looking for. Yeah, in the hallway, anyone check out the OpenAI chatbot? It's equally amazing and scary. I forget, but I took a, I didn't put it in the notes here at, I respond to, um, I want to get that a couple different topics. One that Tiago reminded me of related to some questions about credit card controls and that we use for preventing crypto mining. I thought I'm going to just take that person's question and put it in chat AI and see how if the answer it comes up with is better is accurate and if so is it better than when I wrote? It was better than what I wrote. So, and I've done a couple other things and it got, thing, it came up with really great answers that were absolutely wrong. But really well put together, at least in my opinion, on other topics. But what about it? I don't know if anybody else has checked it out, but, um, looks like Nate, you have thought about it at least. Yeah, I found a Chrome plugin that actually, when you do a Google search, I've added the chat, GT, GBT response on the right-hand side so you can appear what Google one that comes up with. It's really quite amazing. I don't know what it comes up with. I'm interested to see if I can match it up with AI assist as well and get an easy coding. Anyway, but I did see that Stack overflow have blocked it for now because of it's as you mentioned, way, but well written. How did they block it? How did they block it? I'm curious. I've linked to the, um, it's the opposed about it, but yeah. I mean, our wrong gang says in Stack overflow, just you have, which contributed. It's mostly what people read your boss. This is wrong. It's a big blame. The best way to get time to answer a question on the internet is to post the wrong answer. I think, I think it's discriminating against AI, just I think, post it wrong, somebody will come in and correct her. Anybody else checked in or have any impressions of it? It was a, it was a little laggy today. I think they're getting a lot more hype. I was definitely trying to play with it to see kind of what the boundaries are on it. It's definitely made smart some interest like using that and co pilot. I'm wondering how efficient engineering managers can be at, uh, you know, jumping back into the code a little easier. So I'm curious to play with the combination of the two. I saw some examples of answers from the handbook that were pretty good. And somebody also shared the text of somebody who will talk to, to interpret shell scripts, that one was quite impressive. You know what's going to happen now is somebody's going to do a TikTok video with all of us being fired by Chad G. P. Instead of by Elon. You're the first one. No, you're doing the presentation. The person who created that video. The time to be the first one because the the answer was better, right? Yeah, for me, we we've been looking just at the back and how the models have been built. So really just looking into that for a code suggestion and it is, um, there was just a lot of them there of, uh, finer details that are so important and so different just the way they use reinforcement and learning, uh, which is really cool. Yeah. Interesting. Bill, I moved a couple of your topics up to the from read only the discussion. And so that maybe you could discuss. Go. We've got the combined engine into view catchups for a sick data science and growth starting tomorrow. Um, tomorrow might Friday but AP. Um, there are three one hour sessions in there. Um, uh, the issue link is there. They should all be on the calendar. And these are open to anyone. So if there are people or not directly on the calendar and votes, um, please go ahead and add them. Um, you should be able to eat it those, uh, invites yourself. Uh, Neil asked a good question in that issue around it's been saying, um, uh, I'll just summarize that. In terms of what we experience my understanding is that budget has been allocated at the sub department level. And so we have a lot of discretion there. Uh, the TLDR is ask your manager before you experience something to make sure it's approved. But until we, we, one of the sessions was to expense some food or a beverage. Neil's question was, will, does this have to be something that you bring to the meeting? Um, I've answered that and please please correct me if you think I'm wrong here Wayne. Um, I think no, it doesn't, but it should be coupled with attending one of the meetings or one of the one of these catchups and maybe bring that to the conversation and tell us how you're experiencing it. So, um, some suggestions where it expands to Neil and bring it along. Not everyone wants to eat in Neil and try to be colleagues while they're having a conversation. Uh, another is actually going out for a meal and come back and talk about it. Uh, who you took out for the meal. Um, um, what what you talked about and and share that with the team. Uh, a third suggestion was, um, by the ingredients for Neil that you want to cook at home and come and tell us about the recipe that we chose or um, you wanted us to plan on cooking and the conversation. Wayne, how did I do encourage you? I'm, you, you do encourage your teams to attend if they're interested in being going to get together as group and just spend a little time together, not doing work specific things. One of the interesting aspects of an all remote company spread out over the world is things like this are important and that you get out of the what you put into the future. Uh, next item was also for me this is something that Wayne and I discussed, um, a seducing, um, there are some useful tips and, and there's a video there on, uh, filtering and Gmail. Um, really go to if you're interested in that sort of zero in box, um, email policy, you can filter everything away into a subject or you don't have to look at it. Um, I try and use this right now. I'm, I haven't got a zero in box, um, but the idea behind that is I'm filtering everything with I expect to come in. So anything that comes from get lab goes into a particular directory and what I end up seeing in my inbox is the email that I wasn't expecting to get and it sort of brings them to top of mind. Um, there is a video there. It's a bit long. You kind of have to, um, fast forward to get to the filtering. Now if anyone's interested though, I'm, I'm happy to share any of the, uh, some of the filters I use on get lab, um, uh, to filter it. Yeah. And would suggest making your teams know about this in case they're not using filters in Gmail, it can really help them if they're not. Jay, do you know about this? Jay. I think I was smiling box. I'm saying that. I'm just going to say is there a company one policy that we're zero in box? I thought it was just a suggestion. It's a suggestion. It's a suggestion. It's a suggestion. It's a suggestion. It's a suggestion. It's an auto filtering or auto filing can be very useful. I think I mentioned this in one of my, um, I did mention this in one of my, uh, skip levels, uh, instead of what's better in many ways is unsubscribing from labels and other end projects where you don't want updix. So you don't have to create filters in the first place. So you're getting a bunch of stuff you don't want to, you're never going to read. Go, go deal with it at the source rather than Gmail. Um, it's a tool. I'm going to, I'm going to slide down. I just say, well, my notifications are only received to do. As I said, I did have some filters. Uh, I just, yeah, maybe maybe it's time to look at it again. Maybe somebody had better filters than I did. I was going to say one more comment. Um, fill out one of you can share a sunscreen shot of your filters as they are today. That'd be cool. Someone did that in the past and it was like much easier than trying to read through the show. Like, oh, okay, that makes sense. And even if I don't have you can export them as well, that'd be pretty sweet. I can't do it. I couldn't do it in one screenshot. I made a much larger. Oh, no. What I'll do is, and I'm, um, I'll copy and paste if you and I'll put them in our, um, shared channel. I'll do that. Awesome. Yeah. Cool. That's all we have in the agenda. And we don't like that meetings extend to the time allocated. But I do want to ask a slidon as a shadow and listening in any questions, your observations, you have a question that you have for the group. Uh, no, no, no, no, no questions for this one. Thanks. Okay. Thanks, lady.",
  "and stay to you. We'll everybody. Don't think. So it's the set growth data science staff meeting for December 14th and just a quick welcome to Tony who's an engineer outside get lab engineering director shadow for me today. Hi everybody. All right. Neil is not here yet. Somebody want to verbalize Neil's item. I'm not in the agenda yet. I will take it. So just following up from one of the items below there is a new threat inside senior son and engineer starting on January 23rd. And I will leave their name out since this being live streamed but they are in Belgium. We need. It's great. Lots of starts. So a quick celebration is a cool demo of an upcoming secure feature for automatic revocation of leaked personal access tokens that Lucas recorded as I post rolling there. Need stuff there. Under nothing under news and events today this week under big rocks and hot issues. So clarification on next steps for managers for talent assessment. I'll put a link in there. You can read all of the details there. The next due date for calibration is December 22nd at the latest. Please don't wait until the last day. So I think most engineering managers are ready to wear this. But in case you're not. Now you are. Wouldn't for a volunteer to verbalize item B. I've got it. I'll take it. So this is from Michelle. So there's an issue that's been spun up for special coverage during the holiday period. So we're looking for volunteers to sign up for shifts underneath the development manager rotation. It's just it's a piece for incident management. That's an escalation is during the holiday period itself. So if you would please take a look and if you have availability, please. Please sign up. Exhaust. So engineering managers, you know, please if you're not ready to do so track the ask in this issue, deduct your thoughts on competition gaps. It's not an okay or a mind. I believe it is for Christopher. He's tracking this across all they didn't. Haskate it to their director boards. But it's something that he's tracking. So please do take a look. And in item D. If you haven't done a check in on percent complete. On your okay hours now, I in the last two weeks. Please do so in the next week. As we on the lack of updates can make stakeholders assume the worst, such as no progress and it is probably off track due to the lack of info, just kind of human nature. So you have no to update even if you don't have anything to update. If it's like no progress, that's say that that's fine. And people know. Thomas you got item E. So this is this is truly a question. So we passed week or two. We got the new process for for offers that adds that additional approval at the end of it. And so I'm curious for those that have done hiring how much additional time is that adding to the end of the process. So. So the thing that caught my attention is that I'm seeing approved offers in greenhouse, but that doesn't mean that it has gotten that last approval. Yet and I and getting some insight on how that works and how much longer it was how much time it's adding to it was something I'm keenly interested in so if you've hired yet. If you if you've done hiring since this is announced. How much time is it adding. We did this kick in. I thought it already had. I asked because I had the person I announced that took our offer. I'm sorry for turning like was. It was held up in the contract approval, but it was due to a PTO thing and then necessarily a new process. But something to be aware of. But yeah, I don't know. Okay. Years may have gotten into the wire on the new process. Okay. Yeah. I've got one that is awaiting. The traditional approval step myself. And I hear I should hear back this week. If your candidates if you're an offer approval. And your candidate. You know, has a you know deadline to make a decision like they have competing offers. Let your recruiter know they can they can push the process to go faster. So in hallway, I'll be out next week's. Have a coverage issue here linked in the notes. I've canceled next week's staff meeting. Does anyone want to volunteer to run the one December 28th? If not, I'll cancel that one too. That's a quick show hands who's going to attend the December 28th or 29th in 8th. Okay. So answered. Uh, cancel. Uh, we have more continue to go async. And who is the next. Uh, on DRI and listening our next outside guest and a Phil organized. I think the first one. We had some folks in the legal on which is great. Who's got next? Who wants next? I was going to say is there a formal list or anything like that? No, it's just looking for a volunteer each week. What's the cadence that we're trying to bring in outside guests? What do they want to work? I'll have to top my head. I think it'd be once month, once every two months. Anyone want to volunteer to do the one in first week of January? It has an idea who they might want to have as the as a outside guest. No, no, no. Okay. Maybe they will be in the future. If not, it's okay. So under thanks and gratitude, thanks Thomas and Neil for continuing to compile and present the error and security budget in focus all our teams in the weekly engineering allocation meeting. Much appreciated. It's all we had at the agenda. Thanks for everybody. Have a great day.",
  "That's exactly what happened. Once the recording starts, it always seems to be a wrong suggestion. Well, you do have to click the acknowledge button as well. So yeah, the and it's not predictable when things are going to live. So we got the Sync group data science stack meeting for January 11th or January 12th, the music. And you're just talking about traveling and having a good time off and also trying to get sick when traveling. So I think I have the first item. Several variations. So nice job by J and team. And the entire team, I'm reducing frogs on a camp takeovers. You're requiring email verification that they've been through logs in incorrectly, right on three times. It's pretty cool. And it was unobviously hard to get right in all of these cases. In all ways, such as if the users email wasn't irrevocably in the past. We stopped sending emails. That user because it keeps our email. It makes the mail providers when you do that more likely to accept the email. What it does is that user may never give email again. And that's not in a, when they're trying to validate the user with a required email that's at all. So we had to actually integrate the email for about the N and A. The back to to read white allow us to email addresses, even if they were on the Google rules. So anyway, good stuff. Number of announcements that I put in here, but I would love it different people who are in here than because I just copied them from Christopher's definitely. Yeah, I think I can take the first two. So we've got annual comp review starting soon. I think that kicked off on Monday, maybe Tuesday. I think there was a little bit of a delay. But that should be on workday. And there should be some messages in some of the management channels for that. And then the second one is there is team member professional coaching resources available. So we've had this for a while, but this is just a friendly reminder. There is a link to the document both modern health and then there's another professional coaching resource to take advantage of those benefits. Thanks, Jeff. Any volunteers to provide? So Christopher's given us their draft Q1 development on OKS. The link to the document there will be a course looking at our Q1 OKS, some of which will have to align with Christmas and with having to look through. You didn't want to run through them in this making? Why? No, great. There's still a draft mode and not of any of them are confidential or not. But read them and then while we're coaching other things, if anybody would like to and love to take questions on them, come on to anybody has any doubt or a picture. The, um, to under big rocks and hot issues, so all people leaders should review the, um, I'm Mark rotation issued a draft to make sure everybody who's eligible to be an instant manager does so and, uh, there were some cross-blires or not everybody that's supposed to be doing it is doing it yet, but that's okay. That happens. We'll figure it. We're figuring that out. Not just for our team, but for all teams. And if somebody is not eligible, there's various reasons for that, you know, there's a way to ask for an exemption for that and just for the justification as well. Like top reason is I'm on rotations, instant major rotations. It's sometimes, you know, it's a schedule, but it's sometimes outside of work hours. It's important outside of work hours in some countries that's not possible. That's a, and we don't have a way to do the eye on my rotation where it's only during people's work hours. So that's, if, if, if a team member was in a country where they can't participate, that, you know, that's not the success. There's other reasons to do it. That looks like you've got a question. Yeah. So we've been running the program for about a year. We have the criteria of everyone that's job eight and nine. I think about half the people have not done the eye my rotation. I don't know if there's a due date to get everyone into the rotation. Not that I know of, but maybe a soft due date would be by the end of Q1. That is that reasonable. It takes some time to go through the training and, you know, doing a couple. You know, it's, it's not in the person's in the eye mock on borders completely control the schedule. You've been very micro-preposed. There's some things that they need to do. Yeah. I think like a reasonable, at least for your turn staff. And if Q1, and if you want, and if you want is, uh, what David, is that? Uh, okay. Yep. Yeah. So three three and a half ones. That's okay. It looks like you have a comment. Yeah. Um, a staff engineer's, uh, according to the job family need to be in one of the on course schedules. So they can still be in Devon Core. Most of the staff engineers should already be in that one. I think we've gone ahead and created onboarding issues for eye knock. But technically according to their job family, they could remain in Devon Core and not do eye mock. Prep, prep doesn't need though. Perhaps we need more eye mock. Um, people, so maybe that's why we're asking staff engineers to migrate to the. Yes. And we want to spread the eye mock for more people. Um, to that in the more people that do it, the less impacted is on each person. I think it's a primary motivation. Why the one. I remember it looks like you've got comment. Yeah. And just kind of a follow up to sets around timeline expectations. Um, for anyone who asked on through, I'm all come boarding completed it. How long did it took a good take to get through most of the training and or the shadowing piece of it. Just a separate timeline or internal point, similarly. I don't recall exactly how I'm at took to do the training component of it, but it wasn't. It wasn't much to it. Um, shadowing was, um, you can set up a shadow eye mock schedule. I just subscribe to notifications for incident management and jumped in on calls and did shadowing that way. I find that that was much easier than trying to suit up a schedule. And that component of it was just waiting for enough incidents so that I could shadow and actually feel comfortable joining joining as an eye mock. Um, I don't it's particularly over the rest. It's a few hours maybe you read the training. Yeah. Yeah. Definitely not bad. And I think the new folks have the luxury of getting scheduled like subscribing to this, signing up. Um, the batch fillet and I were in a foreign fattman to go. We were just got added to a schedule a number of people to even realize. Tell the notification like Patriot of the Senate email saying, welcome. Yeah. But yeah, I agree a few hours of reading. Um, and then just kind of follow the various channels. I created a group in Slack with like devescalation. I'm like general incident management. Keep an eye on those channels. Effectivity. What fills that up. Oh, and subscribe to schedule that be cool to you. You can create a shadow schedule. Good stuff. Yeah. Thanks for the tips. Great question. It's coming over the hallway. Um, I would the first use set actually moved yours from reading only the whole as I think it's a cool idea. So, the good guy said. Um, so the first thing in the hallway is a Christopher and I recommend that all EN was in above at least, maybe everyone going to release the end and watch this 10 minute video from Sid on comparison in tool landscape. That's cool. Very pertinent to best in class BIC things and other stuff. So, good stuff there. Also, we have an ed guest in this meeting since legal trouble. And anyone want to go into it identify a guest for our next meeting. Thomas your your volunteer. Okay. Cool. Yeah. And then we'll ask again next time. Who wants to do the one after that? So, any request of teams or groups or what are you thinking, Thomas? No idea yet. Security. I'm thinking in terms of Fed ramp and all of the other initiatives that we have and the increased and increase and increase emphasis on vulnerability management and security, pretty sturdy concerns that and since we built tools for this, we may have interests. I think it's a great one. You know, and not after then, another, and a guessing might be to invite a director or senior and jury manager or manager from one of the other development teams that we not by design but just by what we use, we're not part of the core part of the product. We've got a lot of things that hang off the product, integrate the product in great ways and are part of the product of course, but the other teams interact with each other a lot. Then we interact with them. And that's not a negative on anybody. We more, I think perhaps having guests, because some of my peers or, you know, your peers, occasionally will be great. We can learn what they're up to and they can hear we're up to and things like that. So other candidates would be people like that. That's who I'd probably choose if they found a room that do want to guess, but I'm going to look for other volunteers first. Just 90. Seth, and so you've got see. Yeah, so just started a MR on our PTO page, just talked about domestic and family violence as a reason to take time off. It's something that is actually a lot more common than we might realize. So, I want to get that in the handbook as a particular reason, also, to for anyone, looking at taking time off, they understand that this is something that does happen and to seek support and to make sure they're in a safe place. When I was writing this, I actually found Australia and New Zealand. There may be some other countries actually have some legislative employment law around this, actually giving people time off. So figured we could take some of that and put that in our PTO policy. And then somewhat related is just kind of a comment here. One of the things I've noticed whenever you're looking at really steep performance drops with employees, it tends to be some of these issues like relationship or divorce or health problems or a bereavement or abuse. So it's just always kind of a lens that you look at performance through and figure out like, hey, it's not that they don't want to be working or being performing, they've got something else going on. Very good. Seth, Seth, it's a glad you added it. I have some comments or some suggestions in the MR just below. So it was making me things. It reminds me it's pretty cool that everyone can be sure to mute. It's not just something we say. You put a proposal into change in people some people up slash PTO policies. It's going to get an executive accepted. I don't know. You know, the DRI's, the ERI's been reviewing it, but it's really neat that everybody feels and can make suggestions to improve things. It makes things outside, things inside and to be stronger, but also things outside and to be kind of neat. I had the last item. At least we have as you know, in the thanks and gratitude, we don't remember the things in here, but thanks Thomas, we're not committing a work day performance review hack. I think it's a great thing to call it a hack of how to go export things at a work day in a way that is usable. So appreciate it. I used it. I think many things. That's everything we have in the agenda. Let's work with us. There's a happy friends and family day or Friday. And for those taking a few Monday next week for US holiday in July, I have four days weekend for NLK Junior Day.",
  "I should announce this is the set growths and data-sounding staff waiting for January tape. I don't know. Good. Good, George. Hey, Wayne. Hey, Thomas. George, how are you, sir? I am good. How about you? Doing all right. And I am admiring the reimagined series of elements on the wall behind you. I've shared this comment so many times. Sorry. No, no, I'd try and it's fine. Completely fine. It's not that actually, I'm at the anyway. It's a design practice I used to try. Well, I go. So we'll get started in about a minute. We're all really. I know. That means everybody else is late. I say in just since this is being watched. Is it possible, philosophically, is it possible to be late? No meetings are actually. Is it possible to be late? It sounds like a quote for somewhere. I don't know. I think yes, if anyone who joins late asks people to back up in the agenda, then you're actually late and you shouldn't ask for that. So, but all me interrupt. Hey, I'm. Hey, hey, how are you doing? We've started out 30 seconds. Do you have a good long weekend? Yeah, how about everybody else? George Thomas, how was your weekend? Relaxing. I'm largely good. I just need kids to not get sick anymore. So it's the set growth data center staff meeting for January 18th. We were. Talking about other things before we get started, but when we get started. So first, welcome, George. Who is my shadow this week and a get lab core community. I have core community contributor. Hey, everyone. So, what do we jump over to? Martin, let's take you to the next item. Yeah, I just wanted to give a huge shout out to Lucas and James for receiving the special script bonus on their efforts and recent customer escalation of not only Lucas and James member of the T. I'm, thank you for not only. Proposing the discretionary bonus, but also all your efforts on this as well as Neil is not present. So great collaboration across the state section and other teams as well. So, I'm going to be a very nice person. I'm going to be a very nice person. Well, I think I have the next one too, under news and events. The just in case you missed it. Week to personal access tokens will now be automatically revoked for all get lab or team members. So you step in the continued trajectory of making this enable for all customers. So we had to pay premium damage low. At only a year, you should pay 5 34 even Yeah, another question here. HARR cupcakes told me that a lot of things would happen on January 23, Thanks, Emma. So we've got a development newsletter that we're working on. Don't talk about in this meeting before. After reviewing getting Christopher's staff meeting, that's a good feedback on it from a number of folks. So the details are here as we get started on this in this epic. The first iteration is planned for February. If you have topics that you think should be included, or you want to weigh in on the. Center included, please collaborate on the issue for the February newsletter. And also please make your teams aware so they can collaborate on the content if they're in soon. Some have already collaborated on it. I think I just post this in less than 24 hours ago. Please also review draft Q10 okay hours for secure gather, govern group and data science. I put mine in earlier today. And you know, please start working on ones for your teams. Keeping these in mind, but also coming up with your own based on the overall themes and then get that value of course. Do we want to review these live? We've done that in the past. Do we want to do that? Maybe in zoom emoji up down on do you want to or up if you want to reveal it. One yes, two yes, okay that's enough. So I'm not going to share my screen because this issue is a book. I'm. I'm in the chat. George, I think I think I said this to you as well. See you have it. Okay. Good. So some of the themes and I've started in mind based off of Christopher's et cetera, but again, these are these are all draft so objective one increasing revenue by growing a customer's own mindset. And deliver results so continue. I'm going to summarize here continued work on federal. For SLAs for vulnerabilities meeting SLAs for vulnerabilities. Who for data science or AI assisted code suggestions MBC and a model registry MBC and these the the rest of these are this one are based off the product range of ones. Under secure continuous vulnerability scanning capabilities and depending dependency scanning. And also a set for secure a vision proof of concept for real times, task scanning with the new web UI. These are things are more in flux that they're on flux. But only product is settled on it or just yet, but it's great. We've got to really look at them. Questions or comments about those. As I realize my issue has two objective three so I will fix that as I talk. Do you want do you want feedback for commentary in this issue or do you want it on agenda or do you want in slack where do you want it. How about now. Okay. Side channel. Okay. Okay. I'll start now and I'll happily go. I'll write them where I'll write them in the issue. Done as well. Continuous vulnerability scanning is competing against. I'm not sure if you're getting licensed finder and so in composition analysis. So I'm dubious that both can be achieved particularly during this particular quarter, particularly with 60 know approaching at the very end. So and I think license finder replacement has the priority. And the real time, SAS to one target being web IDE or the web IDE replacements. Product is saying that that won't be ready web IDE replacement won't be ready for SAS until Q2 at the earliest. And so there I know. I'm hearing I hear the they're not settled on that one yet like you were saying earlier and so but I'll I'll put both of them in there. Yeah, just because I've been slightly in the the source issue that PM's goals are. Okay. So they can probably probably, you know, some of this but no, that's good to know. Good feedback. Great feedback. Objective to make sure the platform to continue to be the leading depth set. Cops platform continue to. TBD iterations to close best in class gaps pending discussion of product 100% of our teams are using get labs and okay. Are tool you know we're getting our file I kind of easy one for us to achieve but important to achieve. And raising the MR rate TBD it's an overall development goal we want to look at ours as well. Objective three career growth team members team member training or 360 feedback cycle figuring that out a hiring goal TBD. To be determined town assessment follow up an action plans based on feedback and finishing. The or if we'll finish complete type of get complete the gap analysis and delivery on. TBD short term wins first of your govern growth and data science and last and eradication health. Everybody gets and continue to burn down the list of customer impacting S1 and S2 books. We made progress on we will always make progress on these including in Q4 thoughts initial pots on on these. Wayne is is objective to KR one the zittle of a lot with objective three KR four so the gap analysis and the and the big gap. Yeah, good point. Very good point. We just simplify that. Where do you think it fits better doesn't fit that well in. I mean we're ready doing the efforts on big right so between the two I would keep objective to KR one. Yeah, I'm going to look at it to do to get in the wrong place anyway good catch the before you do because I was looking at the I was looking at Christopher's okay ours yesterday. I thought I had the same questions you argued but I had separated them eventually in that I think some teams haven't finished the gap analysis and I think this is to be a follow on to the work that's happening in this quarter whether as. To two dot one is what iterations have been identified that are not the quick wins. I think they're separate though it's it's fine fine margins. I'm still going to move it that's a good point I'm going to move it to the above one. Um, because it was definitely the wrong objective just to refresh see if that looks makes more sense. Basically I moved up simplified it and and moved it up from objective three objectives to. When for application help the resolve S1 and S2 are we just looking to achieve a particular velocity on a regular basis because we'll always have. Bugs coming in right. So it's not necessarily a burn down but maybe just a cat as to where we want to be or a threshold. Maybe maybe also with a focus on over do ones. Um, I don't know if we have any current over do ones but are you tracking the equivalent okay or for cue for was that about over do ones or any. Any. Last such act of there weren't any over do ones however throughout the course, the cue for like stuff mentioned if you did pop up on their S2. But they were work through and close down as you and so. We don't want to get to zero right because as Seth you mentioned they're always coming in we want to get to zero right but we don't want to have that as a goal it would be resolving number of them which should. Perhaps be the number we have currently and some more assuming more coming. But you know we'll figure that out. That that makes sense. Yep. Great question that. I'll focus on S3 and S4 might also be a good one because the list of S3 and S4 seem to be longer usually always a longer than S1 and S2. Okay, I know we're discussing this async which is great. Please put the comments later or not if you don't have in the Google Doc which is just fine you add them to the issue. Have the great some recovery there as well for folks who didn't make the meeting or read it later or we forget all the great feedback. All right moving on to the next topic can someone volunteer to provide see. Yeah, sure. Let's see from left from left staff meeting looking for volunteers to contribute to the audit work efforts this can be any EM or higher. So we can see these can also help with the technical bits Sam and Darva have started on this but we could use some help for context these efforts just to fire our financial audit and as such are important to overall success of the company. Or details in this document link here. Yeah, I just went through this I got some time to go this I volunteered for one part. I'm looking to volunteer for others to take a look see if you're interested for you or your engineers because there's some work for. That fits nicely in people leader and some for engineers as well some automation work. Is this a requirement just as a publicly traded company or is this just a standard business practice to have a financial audit every year. I don't know. Maybe sorry. Kind of related is this the audit that like the auditor sign off on as part of that or because we also talked about audits for federal amp and audits for this and audits for Ruby gems and we use a term audit quite a bit. Looking at the. Christopher staffing ITC G controls is what it's about. What is ITC G or an actual. What I see. In cool is my friend S p and my general controls. Great question. Maybe to ask. Does it research looks like it's part of socks which would be part of being a public company. Okay. Good. Thanks for volunteering to. Lead those Thomas and thank you've got next. Right. This is just a big thank you for me is so. Particularly for everybody's participation is weekly error budget issues and we've got to link to this week's. I've been able to go there for the agenda. It was my week to represent everyone here in engineering allocation meetings that happened my yesterday and the content here was hugely helpful not only in short cutting the amount of time it took to prepare for that conversation but also providing additional context as to what was going on. And so this was was a big help so thank you. Thank you for this and it's. I appreciate it and extremely helpful. I'm sorry you got to. I was going to. Yeah. Nice. Thank you. Great. I love that much. I've been seeing it for years. I miss it when I don't see it. You know, Alexander from my chase team is is staying in my house with me and they nearly got this mug. I say people going to ask you West. Jago and what have you done to him. So they they got a different mug. If they happens to use it and join a meeting they has to join through years in the count. Collet him. Any any questions for the group George. Which is also a good question for George George has been a core team member community contributor to get lab for many years. I introduced George to Timsol men is a base of all of a second. We've known each other for years. Tims on around as well. So George there's a community community contributor. Hello again, George. We we so each other tomorrow. We saw each other tomorrow. How are you enjoying the experience? It's been a great so far, very informative. I mean, I feel close to the Guilatim and it feels good to be part of this process here. Do you feel like all the core members would benefit from the experience? Absolutely. I brought this initiative in the court in Meding a while ago. And I will bring it again. So maybe others may be interested in joining. Yeah, it's great. It's great to have you and all this coming in. Bring us closer to the community. I like it. So welcome. One thing that George and I discussed earlier is, we don't have a lot of community contributions to our stages. Secure govern growth and data science. Compared to the other stages. And one of the reasons, maybe, that people like contributing to things that they use. And most of our stuff, not all of that, most of it is in, you need an ultimate license to use. And a community contributor can get an ultimate license to do community contribution work. And I'm not going to actually gravitate towards it unless there already had a customer that has an ultimate license. So I don't know if this would, this surely would not be the only thing to increase community contributions. First of the video of some including, have slash null, this handle. He's done a lot in the past and we appreciate him. But for multiple reasons, not just this one is move a subset of secure features. And maybe govern features as well to the open core of the open source part of the product. And rather than a big binder, like you can't get any of it unless you're an ultimate customer. Where we couldn't move too much or people would have no reason to pay, but maybe move just enough where the open source community gets access to those and value them. And then indirectly, they start using them and indirectly, then they have feedback on them and they can go and prove it themselves with everybody can contribute. So I know we've talked about this as a concept in the past and just figured out what it was in front of the group for their thoughts. Wayne, have we considered switching responsibilities on on the ultimate license? So for example, as soon as we notice a new core member contribution, we reach out and say, hey, he's an ultimate license valid for whatever we comfortable with and keep track of that and just, you know, use that to build a stronger link. Well, we've had the vaccine on us, have we thought about it? I don't know if we thought about it, but it sounds like a great idea. You mean, like, amazing. You're also great, George. You received, say, hey, he's your ultimate license. What does this do? Let me go have a look. I think it's already part of the heroes program where, you know, when you contribute after some time, you also get the ultimate license for a group. But I don't remember if it's like on the first tire or the second. Cool. You've made this weird. Is it combining ideas? Thomas, you're saying, yeah, George, when you're looking for or identifying where you're going to contribute, how do you identify that opportunity? Is it by issues? Is it by things you notice as a features you want? What do you, what do you, how do you look? Well, I have multiple ways. I mean, sometimes it's just something on the application that bothers me or it's like moving one pixel to the left or or there is something that I'm always always looking for migration initiatives where you, my great to the pajamas components and make the application more robust. And yeah, sometimes I just go through the community contributions, take a look what's going on. If there's anything I can help or what things are there right person or thing. But yeah, like open-sourcing, some parts of the security features could be really nice because, you know, when you contribute to something, it would be nice to get something out of it or use it later. And I think in the past historically, we have been open-sourcing features from the premium tyres and that would be something to consider. We're almost at scheduled time. Thanks everybody. We're great.",
  "people do like a DSLR. You have like the full setup. But nobody makes like a high quality kind of like plug-in play webcam like that. Yeah. So they were complaining about that. I like to achieve that quality. You have to have this like complicated setup and nobody's scratch that it's yet. Yeah. I mean, there's a bunch of people that have like all these hacks of like doing backgrounds and then running it through OBS and like, but it's not as simple as a couple options in Zoom. I did find the new avatars in Zoom. Moji's on your iPhone. Avatars for animals. Yeah. So you can be the new. Yeah. Yeah. Yeah. So you're like, you can celebrate like that. Right. Good is the sick growth data side staff meeting for January 25th or January 26th for those in A pack. There you go, Jay. That's funny. Uh, I was in a meeting where somebody had their child on their lap during the call and everybody without saying anything, slowly transitioned to using those and started like waving like this back and forth to see if they could make for getting their child that was and it was Tim's in film and like smile and it. So anyway. Let's see where to get started today. Thomas looks like you've got first item in celebrations. Yep. As I was also in that meeting, it was lovely. So just to say, when it was lovely, it was lovely moment. And in any case, to the agenda. So this was a previously announced in Slack, but we've got we had three pretty big work in a version. He's that happened in the past week. So I've progressed to Fadion and Olivier for five years each has founding members of the secure stage. So we've been here that long. And then Isaac for three years. So congrats to to the three of them and a pledge owner here. Over Seth. Yeah. Happy Australia Day. Australia Day. Did you start on right now? Yeah. It's an Aston you might or both your two new zelanders that two Kiwis though adopted or original. Do you happen to know what Australia Day is? Yes. Yeah. We do know what Australia Day is. But the guidance don't partake. Is that good rivalry in us? But yeah. So what is Australia Day? I'm just curious. Yeah. Well Australia. Australia Day is just their sort of national data, celebrate and have a public holiday. Fair enough. Yeah. Thanks. New Zealand or what would the equivalent be in New Zealand perhaps? Yeah. We've got white hanging day which comes up in Fib. And I guess it would be somewhat too. What's the American holiday with that? The close closest would be like independent stay but it's not quite the same because it's a treaty between the Europeans and the Maori. Thanks for bringing up Seth. It was nice to take the tangent on things like this to learn something new. Using events number one, I've got OKRs or now down in GitLab versus LA. Yeah. Big win for usability, collaboration and dog food. So in big rocks and hot issues, looking for volunteer to different volunteer need to verbalize A&B. I put them in there but be great to hear from others too. I'll take it from Christopher's staff meeting. Let's see. FYI on the Ruby 3 upgrade. We're currently asking teams to perform explore tests and explore a testing on an environment that is using Ruby 3 by Jan 31. While we do have a green pipeline running on Ruby 3, this is to catch things that may not be adequately covered in our test suite. Most of the checks, the box or acknowledge they are looking into it. This is mainly for visibility but if you notice anyone on the list who may not have seen it, we'd greatly appreciate some help following up. This issue contains the timeline that is being regularly updated. I'd say anybody found any snags associated with the Ruby 3 testing? Any EMs here? We we have an engineer go through pretty extensively. The gas functionality and everything seemed pretty good. Good here. Somebody else want to volunteer to verbalize B? I can take that one. So also from Christopher, there is an exercise we need to perform that will be labor intensive. We don't distribute it. Essentially what it is is we need to review a subset of MRs for fiscal year 23 to make sure the type labeling was correct. More specifically around feature where we could use the help. If you are an EM or a bug and want to get the word out, we are choosing testers reviewers for this exercise based on who merged to about 188 folks on this list. Your individual contributor and want to help out or willing to take on some of the reviews. I would also be greatly appreciated. Think right now Christopher is doing the testing or doing some testing. Would like to distribute the work and make sure that nobody spends more than one to two hours on this. Except you have a comment. Yeah, I don't know if anyone has any more insight as to why we're doing this or how these labels are being used for finance. I'm just more curious. It asks us to do this but it doesn't say why. My guess is it's for accounting on diverse colleagues but I wasn't sure. Looking for the animated response and zoom that goes like this but there doesn't seem to be one. Actually, don't know. Maybe add to the issue. Let's say to be else. I don't remember if it was specific to those accounting definitions or indeed versus cogs but I think what you're asking is correct. I remember that from Tuesday. Staff meeting that it was like what's the are we doing accurate accounting for percentage of work with feature versus maintenance versus bugs and so forth. So I'm going to go hunting for the canonical answer. I think I've seen it. I just also want the reasons that that's really good to know is if that's a finance concern. It drives us to make sure that it's even more accurate when those labels go in. Versailles, we're just kind of curious as to what percentage people are spending. First, we need to get it within a certain accuracy. I think knowing that it's important, I will help us drive that. Yes, and look for the overall trends. Don't spend too much time on is it a bug or is it a feature? If you spend more than a minute or two thing about that, you spent too long because it averages out over time. But good point we want to make it as accurate as possible. Maybe time box that thinking about it. So it doesn't go off into the outlier on time. I feel like I'm most of the work that. Most of the work that falls in the engineering allocation, I usually always label as maintenance. Do you all find that as well? Or do you do a lot of future development outside of product? Sorry, see that again? Most of the work that I signed, the issues that I create are all I often throw type maintenance on it. It's very, very like a brand new future. I'm wondering if you find that similarly or if you're assigning a lot of new future work often. We support static analysis. I think the engineering ratio, about 60% future work. I'm about at this 60, 30, 10 was it 30% maintenance and percent bugs. I think that kind of sticks be true, but I think we've been more on maintenance recently as Thomas's shaking is that? But Jay in terms of as a general rule for us, like if engineering is an issue in the work, it tends to be bugs or maintenance. If product is initiated in the work, it tends to be features. There's obviously some exceptions to that where engineers got a great idea and product signs off on it, then that would be a feature. But as a general rule, whoever initiates the work tends to fit it into a category. That's going to a great conversation. Great. Glad you ask, okay. Thomas looks like you found the why? Yep, did find the why or at least the why as articulated from Christopher staff, meaning you've got a link straight to the thread and that agenda folks want to. It's open to open with them, get live itself. I copied and paste straight from that section. And it is a sample review. It's not reviewing every single MR. It's been put up by development over a period of time because that would be quite a lot of. Synex and next in big rocks and how to choose that section we have. My updated still draft okayers are available in the slink. When we're done, we'll go back and revisit this and do a good of private recordings. Since I think some of them are confidential. I want to mention you know, the one shared with PM and the ones that will up to my boss, Christopher, they're not finalized yet. They may continue to change based on that and further iteration, collaboration on a mentioned. That we're in good shape. I'm putting them together, but you know they're not final yet. So at the end, we'll go to a private recording and make sure those and talk through them live. How much looks like you've got? The next item. All right. Bed ramp is the saga continues so we'll call this fed ramp episode six. The the the retirement of the of the assessments, but anyway, I'll figure out a better title for it later. The next the next the next milestone where going after is called the SAR or a security assessment report. And one of the things that all groups are being asked to validate is for issues with the with the with the relevant label, which is and you'll you'll see it because it's because it is the only one that's are related to it and it's scope to fed ramp itself. Are the should they be part of the scope and if so can we get a rough estimate for it? I found there are five issues in sec. They're none and growth. They're none and data science. Four of which I question whether or not they should be in scope. So I've pinged in EMs and PMs related to those issues. My my request is a buddy. If you get it, please get that done this week. If you can, it's just like is it should it be in scope? The remaining issue is one that's that's a it looks like it's on its way to being or to being completed. So so and this is not for delivery. This is estimation. I'm like it. It should it be in scope and rough t-shirt size. Swag, I think good. I love how important this is and then at the bottom of the list we have like an issue talking about bouncy castle. Yeah and that's the one that's going to be done. Right. Okay. So that's actually a library that we're using to test for it. Are they using TLS connections to the to the to the target itself? So yes, we're using bouncy castle to bounce around different type of connection protocols. So. Yeah. You want to verbalize your comment as well? Yeah. Sure. So I was just going to ask it's their time frame on the all the expectation on the time frames and the answer to while you're. Yeah. I'm talking to them. I'm just trying to get it through this week. Yeah. Because I noticed that compliance has to and it helped it. Yeah. There's two. And I don't know if they're actually yours. It looks like Connor put them on for compliance for lack of a better group to put it on. So I once again I question scope or if there's a cops setting in trouble from like security or infrastructure to this. Yes. Yes. This is an interesting topic for us with order of in across the whole product. We actually responsible for the order of ines were just the system behind it. So anyway, I'll go through that. Thank you, sir. I think it's your being on the public agenda. I'm going to switch to a private court.",
  "I'm getting the YouTube stream going. It takes a little bit. Go, raise. All right. So it is the Sec Growth Data Science staff meeting for February 1st. For February 2nd for those in APAC. I'll try to not forget to say that. We have a guest today, Runaul, who's my outside GetLab Shadow. Welcome Runaul to the group to the meeting. Hello, Runaul. Thank you. So, looks like Phil, you've got the first item. Yes, just a new high as an anniversary section. Hey, NAM, who is an R&T abuse team. It's just celebrating one year at Goodland. While we're talking work in a version, we'll talk, I'll add three to the list from Secure. First couple of from dynamic analysis with Craig and in Arter. So Craig with us for three years, Arter with one. And the NVish was from static analysis. It's been with us for one year and time flies. I didn't realize that it had been a year since a war three since they had joined us. So what everybody's here. Oh, glad we have this as a section. It's nice to have. Ninch these. Celebrate them. So a news and events. In the next week, please close your Q4, okay, ours. The final update and add a good bad try comment. So what went well. What didn't what we might try on something similar in the future and tag in your manager for review. And also, please work on your Q1, okay, ours. We'll remove the okay, okay, ours tracking to the GitLab product itself. There's a feature. And there that we're done putting. So please work on your Q1, okay, ours and review with your manager sometime in the next week as well. And mine can be found at the link into them. The link to them there. Thanks, Wayne. Good to have a. A proposed due date for Q1, okay, us. We've been discussing them. And teams. You're saying sometime in the next week. You'd be happy. I'm making it up. It feels about right. It doesn't need to be in the next week. It feels about right. It would be great. A little bit later to be fine, too. You know, the later we get started on, we solidify them. The later we're working on them. Although they're often to reflect and not always often to reflect the most important things we're working on anyway. So I guess my statement, I just said, is not true. At least some of the time. So good stuff there. I put in some items in the hallway in hallway A. Does someone want to volunteer to verbalize them other than me? Yeah, I can take that since I have one of them as well. So please read this week's VP development staff meeting notes. Link, you know, the top is included heads up for a proposal on them all error budgets are being tracked. Joliana from HR has some cash compensation comes and that was, you know, shared recently that should become available to everybody. So check out that the private manager's channel for more information. From Nick, the not only available is being deprecated and relabeled to deprecated. So that should be pretty obvious. I don't know if there's a migration effort taking place, so we're just going to have to look up for that deprecation label. And then Michelle would like to give a huge thanks to everyone helping boost maintainer ship. We've had a 27 maintainer's across front and back room back in and database the slots quarter. That's amazing. Awesome. And then I shared this on Slack. So I'm looking at to see if we have a coupon power user, which means that we have a shared credit card essentially. When we do expenses for things like software subscriptions, we have to go through a process where we get a digital credit card. And we use that. And so there's this thing. My team has been experiencing things like grammar league, for instance. We're just fine. The process is in horrible, but now we're getting expenses declined because we can't submit an annual amount, even though the subscriptions at the annual level. That's been monthly. So they're asking us to divide the the subscription by 12 and submit that amount every month. I don't want to do this. I already don't enjoy expenses to begin my country. You don't either. Does anyone know anything about this? Otherwise, I can just continue to escalate. I don't know if you can see if. Press or someone Christopher or someone might. Have a coupon how are you, sir, Card. Sound like an intent of thing. It is the power user like is that a specific type of coupon account. I think so. Yeah. I had a coupon. I was like. They're interesting. And the issue with the called virtual cards. And then you just get it's a credit card number that has a fixed limit on it. That sounds exactly what I'm looking for. So it sounds like I could do that for my team and maybe that sensible. The doctor to handbook described that being had a department level. So I'm wondering if what does that mean department level I suppose that's sec. Is that too broad, but you know there's a couple gaps in me. My team and sec. Department level would be a Christopher's you know the development department. Even higher. Yeah. Yeah. Maybe something for the VP development. Okay. I can do that. Thanks. Did you just know myself? Sure. I mean, yeah, I have team members actively looking at cool things like this. And I know there's separate conversations around this getting better corporate licensing established. So the Grammarly for instance needed a secondary approval. The manager couldn't just out right approve that software and we had to get additional. That has now been added to a list of pre-approved software that you can get. And I think on top of that we were looking at more of a corporate license. So anyone can just like log in the Grammarly. If they want they don't look like that's procedure at all. So I can probably extend the conversation in the that that comes up. Cool. Over to thanks and gratitude Wayne. Great job. Tom and Phil and Maan. And the development portions of the recent product performance into gear P. I perform a indicator reviews for secure and govern in the similar one for data science. It's a great example of a single collaboration to avoid the need for meetings. Overall good stuff. So those meetings got canceled. That lots of people in the cancel because we did everything. Async. Not just us but product and other. Inshad portions of the quad like UX and UX design and quality team set. So good stuff there. And also thanks Alan for the automation help on the product. What did you save? We would probably took you an hour or what I'm taking me eight. It's a really appreciate the help. It's everything we had in the agenda. Um, we're not. Do you have any. Um, questions or observations for the group as you've been. Uh, shadowing this week. And if you don't that's totally fine but figure out. Ask. Yeah. Thank you. I mean, uh, just one question I had to as about O.K.Rs. And looks like you're using one tool. Uh, for having O.K.R. but you think you using. Good lab itself to manage. Uh, your team's O.K.Rs right. So I think that's a new tool which, uh, or a service which you are. Kind of creating in GitLab. So where you can manage your O.K.Rs itself. We've got. Yes. We went from a commercial third party commercial tool called ally to our own tool. And we're dog. It's a, I don't know if it's a minimal. I don't know if it's an MVP. Um, but it's, it's, you know, minimal viable product. But it's been the same vein as an MVP. Um, we're using it ourselves first which we call dog fooding. Um, I don't know if it's available to paying users yet or in the open core product. Yeah, but I think some combination that is coming to our. We're getting experience with it ourselves. Uh, and using it ourselves was collaborating on things like that in GitLab. Works much better for us and we feel for our customers than using a separate tool. Where because we're in that tool and the collaboration is pretty good already threaded Composations and things like that. Uh, and tagging people where they are rather than having to go into some different UI and some different system. But GitLab as a, but GitLab product didn't have some of the same features as OKR. It was like a, a percent complete as an example. And now that's in there and there's other things too, but um, um, track them like we had issues and sub issues but not in the way that people would want to track objectives and key results in the O in the K or in the O chaos. So, um, overall it seems to be, um, I have a couple little nitpicky things for the team, which I passed on to the product manager who said, Yep, 30 and the thank you. We appreciate it's 30 in the roadmap. That's good. So overall it seems to be working pretty well for me. Does anybody else tried the new, uh, new stuff yet or used it much and if so, what are your impressions? I look I've tried it. We were sort of demying it where we're using work items and issues. Um, and Q4, uh, yeah, it seems nice as you mentioned, the some additional features that we'll be looking forward to. But it's really good just to be dog food and not using another application to track. Is that public? Is it anywhere I can access their tool or is still currently in private? Yeah. Okay. Now it's a, um, so we're currently logging them in and get lab internal project at the moment. In terms of the direction, so this is the started offers in incubation engineering project, which is like a single engineer group. There's plenty of, uh, there's a handbook pages for the incubation engineering team. So one engineer started off sort of developing something and now it's been brought into our plan stage. We're should I put it in the dock. Yeah, it's supposed to link, uh, as well to the documentation of the feature, which is officially alpha. Oh, right. Thanks. Thanks. Well, thanks. Thanks. That bill. That it's good as well. So, um, nice. Thank you. Yeah. Objective and key results person with disability in 156. Sorry, introduced in 156 to person with disability by default in their alpha. Um, to be interested in take a look there. It has some of the details. And that's in the public documentation. Any other questions or comments for the group group? Uh, group. Do you have any questions from her, know, as, as shadow being inflicted on shadowing me and my meetings and listening to be talk a lot, which I don't want to inflict on anyone more than necessary. Yeah, I have to don't set any questions. We've got. Great. Thanks, everybody. Have a great day. Cheers.",
  "It is 74 degrees here right now. Sorry, I was running late today. It's not like it was an interesting discussion about Brazil, Fahrenheit, Celsius, and travel, etc. It's the sick growth data science staff meeting for March 1st. Glad we're getting back to these. We haven't done one in a little in a while, a little while. I have a number of the items I'm going to have to volunteer to verbalize some of them. So in celebrations, lots of features have evolved by our teams in 159. I'm looking for to brag about them on social media. All sorts of good stuff. Did I saw on a list? Somebody want to verbalize a livis comment? Yeah, I'll take it. I think it's a great question. A little bit more. It's a good question. It's a good question. It's a good question. That's a good point. I also noted, Mike Edington indicated API discovery. MVC is live. It's in the documentation. I don't remember seeing a release post yet. It sounds like it might be similar. It's a very interesting thing. 1510. 1510. It was a release right on the boundary. I think we got right after the cut off for the release post. We decided to hold it so we could make sure that it was in before celebrating it. You'll see it coming. Think in 159. I know some are behind a feature. I think I counted four or five major things across various teams. It's pretty cool. There's some news and events. Recommend and reading the slide notes from sales click off SKO. That the slides there and also some notes that Christopher had from their staff meeting. And. Mon also was there and was a presenter, which is pretty cool. Any any is a public stream live stream, mom, but any. How was that scale over your impressions. Will you take a look. Well, I think it was. Firstly, it was. It was nice to actually see from the sales lands of things that they celebrated during the product, you know, so announcement on licenses and that sort of thing is. Something that was really important to them. For me, specifically, we got to also meet. One of the customer who's dog fooding. Coats suggestions and it was great to understand from from their lands. The use cases were it and how we can actually improvise. On the model as well as the fact of. Specifically for model of the support that sales also need on basically. How to educate and sell these features really. So. Yeah, and it was great to see everyone from different different different parts as well. So there was product that was sales. Customer success. Product marketing and. Yeah. Next to that one. And I know see you paste the link to your the deck that you and Taylor used for your presentation on model ups. Yes, yes, so so in the product keynote. David spoke about everything till. Data science. And then model ups and then that went into our presentation. So yeah. Good. Last in the news and events. I also recommend to recommend to the Q on kickoff from a Sid, which I haven't done yet myself. Um, there's an AMA in there and some slides and also recording, etc. I think I pre recorded video and also recording the AMA session. So good good stuff there. I know I had them but volunteers to volunteer maybe for each one to verbalize the whole way items. Sure. Please update percentage complete on your okay ours every two weeks. I think there's a type of here I'm being online. I'm being strict online. Maybe way. I'm just a being thank you. So yes, I'm asking everybody else. I'm asking everybody do this. I haven't done it myself yet. So I'm behind to. I don't know if I'm behind to someone probably up. I'm on this but I have not one of those that is so thanks. Thanks. Thanks, Jay. Thanks for pointing on my type of to that. What I wrote made no sense. Um. I'll read the second. So from York. You are our our CTO. If you're curious about how the sales team positions get lab. Take a look at the latest customer deck. I don't know if that came out of SKO or it just in general, but you will get announced that. In thanks and gratitude. Please keep in mind. Discretionary bonuses and also the hashtag. Thanks channel both for those on your in your teams and those on other teams. You know, thank people. Thank your own teams. They should be thank each other. Thank other teams. But in discretionary bonuses. I just put into. And I just approved one discretionary bonus. Um. So just it reminds me of that and I try to use the thanks channel to let it's it's great way to show appreciation. And this one read only item. We don't need to read. So we have the agenda. Anything else that we want to discuss? Thanks everybody. Have a great day.",
  "All right, so it's the Sec group data science staff meeting for March 8th. 1, 2. Bill, you're outside today. That's neat. Yeah, it is. Actually, it doesn't start raining on me. If it does, I'll run inside half or three to the meeting. There you go. So happy fourth year get lab of nursery, Thomas. Y'all have been afflicted by with me for a long time. That's not a plan. Look at there's some congrats also from Mawne and Camille and Neil in four years as huge. It's a Thomas, you've got news item number one. Yeah, all right. So this is from the Vultment staff meeting agenda and the link is straight to the to that particular discussion thread. There's been some unplanned required stops with upgrades and DB migrations and there's a bunch of links that are available here, including information about what the database team is doing and there is an ask for other teams, which is below. So and and so there's the ask information here. I don't see Tiago, but their antidote is is is is additional as is worthwhile additional insight in and context. So worth being aware of and knowing particularly as we get into more more database centric approaches. So in the hallway, I reviewed the values of hierarchy again recently. It's we're taking a look at the M scene. Have them looked at it in a while. Click on that link there. They know, they know, they don't share screen, they don't have to. I'll share screen on this. Is sharing a handbook page results. Good results enables to keep doing the right things that's top of the pyramid iteration and transparency, the most noticeable from outside the organization, they're the second level, which also helped to enable the third level of collaboration, diversity, including inclusion and belonging and efficiency. Those are the foundation of our values, but this is kind of how the hierarchy of the values come to play because we can't always make a perfect decision on what we're doing. So this helps us make a decision when we have to balance one against another. I think and this is not a. Now, shell kind of thing. This is guidance in general. So any impressions of this or thoughts on this? No, no. Okay. I'll ask a question. Sorry, it wasn't clear why you chose to share that today. Was there any, um, particular reason? Why? Um, good question for. Um, because it's it often or values, you can't make a perfect decision. It is so in general, not about a specific situation, but recently, but often you can't make a perfect decision. Trying to get the best results with the best iteration of the best transparency and collaboration and diversity and belonging and efficiency. You have to choose that some are going to be valued. Some are going to, you're going to do more on something on others. And when needing to do that, it's worth keeping this in, you know, kind of a hierarchy in mind, but not about any specific initiative or project or anything. I think it really useful. So I figured I'd say, and I hadn't, I don't think I'd read this since I onboarded this portion of the values portion of the handbook. And I read it again recently, like I have this helps. For things in perspective. Is that an interesting question? It does. Yeah. Thank you. It's a needle. You can go ahead. Okay. I could chime in. Yeah. On the anti-vacam, we notice like. Our projects are not open for public typically in a lot of our communication is confidential. So I feel like we often make this sacrifice of transparency over results, because the work requires it to some degree. Debated potentially, when having an epic potentially actually making two epic, one which would be confidential and one which would not be just so we could exercise that transparency. So yeah, just to reiterate, this is a sacrifice that our team usually has. Similar and vulnerability. Other teams too, including vulnerability research, sometimes vulnerability research is working on as patentable. So we don't want to make it public until we're ready to as an example. And there's transparency inside the company, and transparency outside the company as well. We try to be transparent inside the company as much as possible. And also, transparent outside the company as much as possible. But I know Jay, your team is very transparent inside the company, but not necessarily the public, based on what you work on. And it helps with your collaboration with the abuse team, that's responsible for the day to day, today of protecting GitLab. And it's useful for collaborating with the growth team. It looks off in the growth changes can create risk for abuse and abuse changes can create risk for growth, like they're two sides at the same coin and some case. So all that internal transparency is great. Even if we can't be externally transparent. So it's good to bring up Thomas. I'll finish writing this in just a moment. But this whole note about what is private versus public when we default to public for a lot of things. I've really found it quite interesting that we've codified Conway's law in that we are shipping transparent by default. And we actually have to make it's actually harder to have private conversations within the product because you have to have more steps in order to make it a confidential issue or a metric question for things along those lines. So I actually think it's really neat to see how it's been codified and quite frankly appreciate it. And so just I'm not trying to contradict, but I've always been amused and interested in kind of amazed how some of these things sneak into the product that are reflection out who we are. So anyway, you just thought it was neat. Wanted to share. Yeah, it looks like you're out. Yeah, thanks. So 360 feedback, I know it's not a timely discussion, but something we maybe start thinking about especially. So the cycle normally starts in July and I've linked off to the timeline on the handbook. The way you have an OKR that talks about career development, feedback, 360 feedback, that I linked off to. So we're touching on that topic now. What I'd like the team to maybe start thinking about is comprehensive. Just all of us, I'm calling it Wayne's World. I think Thomas, you've invented that previously. I love it. But you know, all of this action's Wayne supports. But it's typically up to, and last year, I think I've been an opt-in process, kind of tied to our individual birth plans. There's like a performance management and career development context as well, which could be kind of a theory for certain team members. What I like to think about again is if we all do that, two to big call-out reasons would be when I joined three years ago, we had a great thing. I got a lot of great feedback as a new employee from my peers as able to submit feedback to my peers. A lot of us haven't had that type of feedback in the last two years. And then we have a lot of new team members, hires and reliance team members. I think it really benefit from being able to provide and get feedback from their teams. So again, we're months out. I know there's other ways to conduct 360 feedback. I think Wayne, you have a comment about that. I like the system of logistics because it's very structured. Yeah, the Phil brought my attention earlier today, my time at yesterday's time. I think that our timelines for 360 feedback cycles are not in Q1. But we have a Q1O car that mentions doing 360s. Those two things are not consistent. So another way to do a 360, if somebody wants to, again, totally optional, is informally doing anonymous Google form, which anybody can set up anonymous as a configuration option, in a Google form versus required authentication. Invite peers and manage your instinct holders and just have three questions, open-ended questions. Which should I continue doing? Which should I start doing? Which should I stop doing? And you can get really great feedback that way with those kinds of very simple questions. That's a great option. So we'll go back and update the KRFQ1 to make it clear that that's an option for this. So Neo's suggesting that later in the year, potentially we make this compulsory. Is there a, no, sorry. Yeah, that's correct. I think when I love your suggestion, I think Google forms a great way. Again, it's opt-in. It's voluntarily based. That's not a bad thing, but it's not going to include everybody. I don't know if it's in a close approach. It's more about if you want to do it, I just don't know. Again, like I've seen in the last couple of years, I think you did a survey last year. I mean, was that six, eight months ago? I feel like you did a survey. But that's yeah. Thank you. Who are your teammates? It's a better job than this. I'm more thorough job than these questions and how they organize it and the better use for interface, but this is still a. I actually prefer this for myself. I think I still use culture amp, but I don't know why I use culture amp, I prefer this, but I prefer this, but it's just you get, I get a better response rate. But the, and more insightful feedback, because it's shorter, perhaps in what we do in culture amp. I also, I think more importantly, I direct message people, hey, I love your feedback here's in an anonymous Google form. they enslack versus culture amp, which I think sends an email or maybe slack messages to, but it's from a bot or a non, and a account you don't recognize versus someone you do. So maybe I get just get a better response rate to do that. Yeah, I think that goes back to the notion of if we're all doing it. Plus there's, there's an efficiency gain too if we're doing these kind of one off people have to get used to how to submit feedback and they appropriate manner. That can be like an experience I've gone through many, many, many times I recognize the first one or two, I'm like, okay, I'm doing this, I haven't done this for a while. And then those last ones I'm like, wow, I'm going to go to this, it's like anything you do repeatedly. And I do appreciate that with the culture I'm cycle. I appreciate the response rates, you know, we've all seen. We're not getting the full people giving us feedback, we have a repressed eight or whatever people in the cutting five or six. That's still going to happen, but just another anecdote on my experience. We will ignore that, Thomas. I think it's been the last two years. Yeah, I think it's, I think it's been optional as far as I can remember. But I may not remember back or enough. Like if you make people do it 360 and then don't want to, that's not a good feel, right? And then they're going to, they're going to feel it's, you know, it's prescribed them and they don't want to do it, which is not a good feeling for any order. They might do it in a more so I'm glad we make it optional folks who give that a little out of it. And optional, not do it if they don't want to do it. In the, in the, okay, or it says do a 360 feedback or what's the other option for team member. I self directed training self directed training. So I mean, that's another option too. It's, it's about growth, right? This okay is about individual growth. Two different potential ways to do it. There are others too, but self directed training or do a 360. Just a question on timing them if because I know if I thought we've also got a car around creating growth plans for all team members. And shouldn't the feedback cycle in 360 review go into the feedback? It comes off that go into that growth plan as well. So rather than creating a growth plan now and then doing 360 review later, switching them all around. I think they do do it makes most sense to you based on all this. Definitely flexible on what, you know, to do what makes most sense based on based on, you know, where we're at and where your team is at it, separate. So reading through the handbook Nathan, for the team members that went off to 360 feedback are required to have individual growth plan beforehand. I agree to what works best, but for that guidelines, I'm like, described it as a prerequisite. Is it too good? I haven't even done afterwards. I haven't sorry, but I don't want to read through the handbook page. I'll share my screen now. So yeah, team members, we went off to the 360 feedback. There's a couple of optional steps and then a couple of required steps. This is the one I was mentioning. I may be in to misinterpreting this if I am hypothesized. And then your question is there a discussion about, yeah, updating the growth plan. You would expect that right? Because that's good information. I'm not seeing that in far as the requirements. There's definitely, you know, it's a really long cycle. Starts in July conclude halfway through September. It's like two and a half months. And then guess what happens right about here, we start talking about end of year assessment. So. Oh, but then it's the benefit of this process if for team members who want to do it. That's you're up so well for talent assessment. The end of the year, there should be no surprises because you've been working with your manager. You've been getting feedback. You know, working on those action plans. So, I'm on couldn't make today, but of course, that's fine. All meetings are optional. You'd have some discretionary bonuses. So I am going to go find those. And summarize them. It just had them and now I don't. Those usually announce in what's happening at get lab or elsewhere. Team and rubbits team, I always get those to. I always complete those to and team and rubbits. Thanks. You know, so two of them were actually nominated by me. So, there's a familiar with those, but I'm not going to read all the details, but Alexander was nominated as was. I ran outside and I was in bed, and I ran goes out. And so did the. Well, yeah. That's myired blowover and for helping with the. Data analysis, for merge requests for our, socks audit. And to very much appreciate it there. You're going to jump in and help them various ways. And then, a Mon also gift that Bruno cardoso received discretionary bonus. For countries meet solving and brainstorming ghost users or smart issue, babeling using machine learning. of to solve and drive the solution and planning to automate be training of the model, which is pretty cool. So another friendly plug to for all of us to use, you know, an appropriate discretionary bonuses to recognize people and also, you know, if it's not discretionary bonus worthy, the thanks channel as well. For us, between each other, between our teams and with others outside of our teams as well, overall good stuff. Okay, I think that's all we had for today. Thanks everybody. Cheers.",
  "All right, so we've, this is the second growth data science staff meeting for March 22nd, 2023. So Nate looks like you've got the first thing to mention. Yeah, sure. So Elia, who is now senior front team engineer in the compliance group has celebrated four year anniversary last week. So yeah, massive milestone. Nice. I'm ready. You've got the next item. Yeah, I wanted to just celebrate a feature that we recently finished rolling out completely and then enabling by default with the release of 1510. So that features automatically resolving SaaS findings with rules that are disabled. This has been a widely requested feature and something that has caused a lot of noise. We're piled up a lot of noise for our customers in terms of false positives. If basically without this feature, if we had person with disability any rules, all of these findings would have just remained in a state that they would have to manually go in and resolve. So by introducing this, we provide a way for vulnerabilities to be resolved automatically. And one of the rules that we kind of coupled with this release was a very noisy rule of the tech object injection rule. And it's been our single biggest source of false positives across all of our languages and our scanners. I think Connor listed some numbers here. It's been dismissed at least for 52,000 times from ESLID and 273,000 times from some game. So that just shows you how many times people have not cared about this rule or considered it noise. So we're happy to release this person with disability the rule and auto resolved all those instances of that vulnerability with the last release. It's great. It's really cool. I think we should announce more than just in that in your group channel, but maybe in what's happening I get lab. Yeah. And now it's a good answer. Yeah, I can definitely do that. And you'd shout out to Lucas for shepherding the change along and getting it over the finish time. Good stuff. Good stuff. So under big rocks and hot issues, one mentioned that from Nick Wynn just that which is that we're seeing some instability related to the database recently. For example, CPU spikes, partitioning large tables can be really important to improve scalability. The database team is re-farming out the table, the largest tables where actions required related issues have been added to the epic that's linked to your epic 620. And you know, respective groups have been reached out to. So if you've got a ping then you're on their list and if not you're not but it's still worth taking a look. So it's also worth, I don't think they identify owners or de-eris for all tables. So if you, it's probably taking a worth that lists of the tables to see if one is one that maybe your team wants to take the de-eris action on, de-eris roll on if you, if it's not known. So there's some more detail here but good stuff to keep an eye on. Next item also happens to be for me. So please keep an eye on the Q1 okayr for talent assessment follow up where the manager and team members have an action plan, great action plan to discuss based on have a discussion based on the feedback. So it was a review, it was talent assessment, discuss it, come up with a couple of things that are actions, it doesn't need to do this. Everything of course in the talent assessment but you know one to two to three things is usually a good measure. And we want to make sure we do this by the end of Q1 because that's the goal. And I'm behind on mine as well. So apologies to Mawn and the VR team and Thomas and Phil that I have not done this with you all. So I'm behind on this myself but. Olivier who could make it today has a question. Can somebody verbalize that? Yeah, Olivier is basically asking for a child okayr's related to it for easier management. Yeah, it's good point is you know we're dog fooding the first major police. I think it's the first major release of okay hours in GitLab. So it's a good idea we're still figuring it out but good point. I think we're planning to do a what maybe it's instead of a table. You know a markdown table in the okay hour. Wait and I feel like we had that Phil was reviewing this with me when they updated it when they was sharing their screen. But we have something that I already checked. I raised this version that we're talking about three six two feet back. We were so weeks ago. That's where this topic came up. Yeah, I think it was either a 365 review or doing this. Sorry, 360 review. Yeah, exactly that's the care I was talking about. And there's tabular form that shows your direct reports. Do you have a link to that handy you composed on the agenda? No, I think that begs the same question. Discoverability is hard. Maybe maybe I see a post do something. Well, that's that's one of the role drawn out. So yeah, it is hard to catch the track. We're also this came up on Christopher staff meeting my boss of staff meeting to have can we get a report at workday for this rather than trying to track it manually and sort of we can. But good point. We will iteratively think about. Sorry, talking over you. Yeah, it's a neat. That was the pair of care and there's a care specific that has a table that has the senior managers. Let's do that. Nice. So maybe that'll do it. I don't know. I like the checkboxes as well. We're using that on other cares. Are you all enjoying using Gillam the okay our portion of Gillam? There are some nuances. Yeah, I haven't looked at the roadmap, but I hope that there's more being developed for it. Yeah, there's a feedback issue that you can keep adding to people still adding to. That's got a good roadmap. Think what Neil said earlier about discovery, discoverability. That's true to be a little challenging at times. Remember, unmute. What I yeah, there's definitely some challenges. It's it's it's you know, maybe a couple of iterations passed a minimum viable change. But you know, it's it's still early on. What is we don't have to use multiple tools. We're all in one tool where most people are inside Gillam. So we can link issues easily and ethics, et cetera. But that doesn't mean that the other things aren't still challenges. They still are. But yeah, good good good points. On the read only items, does anybody want to discuss them? Should we should we do should we verbalize any of them or not really needed? I think the NMB might be where the other discussion. I know for the other. The other one is for security issues. We have a mandate where we backport three versions. And I think this extends. I'm going to read through it. Just for that you want to verbalize. Yeah, that's. So I'm copied it. So my from our SRE team. I think that's for team. She's. There's an initiative to extend our maintenance policy. What do we do for security issues to extend that to functional changes that we may be to resume this work. To the last three release get live versions. They're starting a pilot in 1510. There's an issue that they're asking for feedback. There's also some AMA schedule. I think that may be in the past now for the AMA. So you can look at the doc. And then yeah, my my question was what does this actually mean? I know it's been an immense amount of overhead. I had a team member that was very much love was working on security fixes. They had a strong caveat that they needed to be backported. And so the preparation of all those MRs took so much effort and tracking. And it's not a couple days of effort. You're watching these MRs for like weeks and getting various levels of approval. I think there's there's an optimization that's being well thought out by them team for virus team. But I'm wondering like what are the circumstances that we wanted back to backport our work. Why is there a pivot? We're not just forward-looking. Hey, you need this bug fix. Get the upgraded next, mega version, your minimal version. I just don't know if anyone has more context or examples that be cool. I wonder if it has anything to do with supporting federal business as it seems like those institutions don't upgrade. That is quickly. Not sure where we are with FedRAMP in those types of customers but I wonder if has anything to do with that. And I must mean that's three major versions. Like we've just supported 1314 and 15. Is that right, Neil? I think it's minor. It's minor and it's including the current version. It's two backwards. That makes sense. I'm not sure it does. That makes sense. That makes sense. Nice. I'm just getting notes on what I mentioned. That's good stuff. Thanks for spring this one up Neil. Good stuff, thanks everybody. Have a great day.",
  "Now YouTube has caught up. We need to bloodstream their caught up with here. We can get started. So it's the second growth data sign and a second growth data sign staff meeting for March 29th. And why don't we jump into things? First is big rocks and how to use is it's been great to see demos attached to issues in merge requests. It really helps to get more feedback and improve you know more collaboration, more transparency, more iteration. I would recommend we also or maybe even instead upload to get lab on a filtered increased transparency and get more feedback in those issues merge requests are not confidential. I agree. I know a few cases where team members are just trying out the process and have been trying them adding them to issues rather than uploading to unfiltered. I'll be encouraging people to do that. Sometimes there's a procedure that these are useful. They're for let's not put them on and float with my just thing. We need to work through that and break that down and actually get them uploaded. It's more of a habit in my opinion. If I see cases like this, I'll be encouraging people to upload to unfiltered. Yeah, I agree with that. I uploaded a couple of today actually to drop into a new piece of documentation. But yeah, that's that's what I got to as well as like when you see an MR go by with like the little demo you might just upload that. I think it'll have one filtered. I usually throw it as like unlisted to get away like to be like a little shy about it like at least it's like uploaded, but it's not completely public. You're thinking about rotating those into the public sphere as well. I'm just thrilled we're doing the demos in the first place and we've really ramped that up. Unfiltered is doing it on filter to is icing like that's that's the extra 5% of the value maybe 10% like the other 90% is doing them in the first place. It's for that that item. So we would like to see more dog coding of the AI code suggestions beta and I have a link here to the instructions on how to use it and we're to get feedback. So far it has been primarily volunteers, but we don't have that name volunteers. So we're getting it feedback to iterate on as we'd like what we're baiting with customers as well some data customers, but from a tip from get lab team members we're not getting as many as we'd like I'd recommend instead of instead. Doing something like having every EM identify one person from each group to dog food it and get feedback good candidates based on the features currently available are team members that use VS code. We're going to support the web IDE as well get lab web IDE as well, but we don't support that yet. I think something like in the poll 70% of get of the sec and data science team members use VS code as their primary way to code so. And write code and JavaScript go or Python. No, so I left that Ruby Ruby is not that not good yet. And not not fully featured yet in the beta it is going to be those soon. So that would be the criteria is VS code and JavaScript go or Python. What is that's. I'd like us just to signal based again so we've got development channels for our stages we'll go and ask for volunteers again or link to your issue. But our front end engineers some of our previous engineers are using go. So TDR let's ask for volunteers again and if we don't then we can perhaps get a year to nominate people. And we can go for what do you think terms for your teams. I like it for I'm going to thank you. I want to signal boost it some more I mean I think this is more timing on my part I think you started this last week while I was out and so I want to go and emphasize it more this week as well. Let's see how they please do that let's see how goes. One thing we could possibly do at least within secure and no couple teams have reaction rotation, which is and basically somebody dedicated to solving bugs, engineers, any of the work that might be interrupible throughout the milestone so this going to be something that we use as well to encourage somebody to talk through this. If we don't give volunteers. Appreciate it screen ideas. Has anyone here used it yet? I did. I'm not writing much code these days. So was not useful to me. I see. Anybody else? Were you volunteer in check? Yes, of course. I'm gonna install it right now. Oh, thank you, everyone. If Mawn were able to make it, she'd be giving a very personal plea to ask for more feedback. Okay, that's Mawn's team developing. That's an obvious. So them team would really look more feedback. Next item. So please review the very early on draft was probably an over statement. draft Q2, okay, ours. Thomas and I were talking about this earlier when we caught up as an example. they got a chance to read them before I did. Just just fine. I'm being asked to come up with a first draft of Ocaras for my teams by April 11th. For the ones that we'll share with product, I'm gonna reach out to my counterpart, Hillary, to find out ETA and hers. So we'll see where those land. Thoughts on these as they apply to your teams. So if you click on that, like this is a mentioning ones in there. Not a lot of detail yet. There's a number of product ones that will wait to get an ETA from Hillary on. Unless we already know, right? Because some of the product managers already have things that go into this. Others are things like migrate. pajamas components. We can put them back in the past. We wanna do more of that. The view three upgrade for the front end. And we'll see where the others go. So take a look at this. See if it gives you ideas both on our current Ocaras and Q1 and the Q2 potential ones. Any initial impressions yet? And I'm really not gonna get totally understand. Get lab for data science as sparked by interest. And curious what it will hold. I'm curious, here more about implementing cross stage work well between securing create. I imagine there might be something dealing with ID and CLI integration, but you'd love to hear more. I was reading up on that one a little bit more. It's interesting that it's tied to this product investment team. And so the fact that it's being tied to that and it's something that it gives a little bit more information. It kind of some samples that they're looking at. It's, I wanna know more. It's, but anyway, it was curious that that was what it was to do. It's not a lot for us to sink our teeth into yet. Figure out what these mean. But it's good to have a general idea. And we're a month before the beginning of the quarter and we're talking about draft Ocaras. That's actually good time to be talking about draft Ocaras. Not three days before. So good stuff. I'll let you know what I hear from Hillary as well. I'm sure them team earned them if we're already working on their portions. Just damn shown up and hear you. So we wanna verbalize Eran's comment in the hallway. I can take it. You wanna am I? Yeah, sure I'll take it. And fly I floor secure and government folks. Eran was starting to implement some as recently accepted. For co-isle to track secure scan configuration, please reach out if you would like to be added to the MRs. I know they had brought this in front of the the sack architecture council. Think they've reviewed it. And now I'm just working through implementation. I was just looking at this. My first thought is, that the vulnerability research team is probably really interested in this. And I was gonna give them a heads-up on it. But I see at least two vulnerability researches already. And so, yay. So nice open question. What's top of mind for everyone? For me, it's currently the AI integration which I'm co-facilitating with Taylor and when the senior PMs and Mons team is of course a key actor on this. But that's a, what is top of mind to me, what's top of mind everybody else? Randomly pick somebody to start at Nate. What's top of mind for you? Yeah, sure. Top of mind for me at the moment is the click-hap's working group and how the implementation is coming along. So that's a big part of the group compliance big makes steps for us from a maintenance and engineering perspective. The other one is the working group for the software supply chain security. That's quite an interesting one. And relevant in government. Do you have people on your team who are on the working groups each of these? Yes. Next day. Jay, what's top of mind for you? I mean, echoing what you said about AI and ShaggbT and plugins coming in. It's hard not to avoid that's my Twitter feed is just bombarded by it and is fascinating. But more personally for my team, we have launched IDA verification at the first stage, which is just email, but it gets us into this path of being able to turn on phone verification as well as credit card verification. So that's what I'm observing monitoring today. Great stuff. I'm a what's top of mind for you lately. Yeah, I actually came across that AI integration working from your message earlier today, one of my team of physionic and I were discussing how do we kind of brainstorm ideas related to a product and integrate that with OpenAI? So it was a very time-and-posed that you had. I was sure. We quickly made the idea that is currently still in the process of being formed, but for your brainstorming was to, how are we gonna incorporate an OpenAI concept into code quality? So this was one I was suggest byonic, we're gonna be keeping like a close sense on the working group itself to see how this one don't have the working group, but it enabled us to work through this idea. So that's just a learning experiment going on right now. Be. Still, it's a, let's talk about for you. Three years on PTO at the same time, they're cross-gavent, so it's just late and I, and Governor the moment. And there's a lot being delivered or a lot that's planned to be delivered across Gavent. We're keeping, we're using those. We play updates, I've learnt one of them there. So just making sure that we're on track there's a few roll-outs, sort of timelines, that we keep in track of the. Last but not least, Thomas Woods, what's top of mind for you? We're offering circuit textures for self-managed installations. This seems to be the thing that I'm, but I'm a harping that it's something I'm harping on a lot of lately. I think it's a high, I think it also informs the sizing for GitLab, dedicated instances, and we've gotten so.com and I'm a centric and are thinking at least this harms the definition of done until a varied that I'm seeing this is a potentially large blind spot. And so it's top of mind and it keeps coming up. So that's why. Yeah, I didn't understand exactly what this meant. The thermoreference architectures until I looked it up, but we're the, what we tell customers to allocate, based on the size of their installs, what what CPU and storage and memory and more, that they need and adding more requirements on them, makes it they have to allocate more. And then to plan for that when they do upgrades, like as an example, we're adding a new feature that requires additional storage. This significantly more storage than what we recommended today for example, that's not just thing of just saying it, customers need to plan for those upgrades as an example. So yeah, it's really good to bring that to me. Often the almost like 99% of the time features we add don't have those kind of requirements, but that 1% of the time it could be really impactful if we don't plan for it properly. So last time I've been here, the thanks to Thomson Thill for the insight on the monthly sec performance indicator reviews. I reviewed it earlier today, since we didn't in a sync review, yay for a sync, and fewer meetings. Any big takeaways that either of you noticed, or I'm sure Jay and Umber and Nate, you haven't looked at this for even though it doesn't even existed perhaps, but any big takeaways? Not from my perspective, no, a lot of us, we're starting to incorporate and those weekly updates anyway, particularly the error about our updates. I'd love to automate some of the things that we put into the stock unit, like the scene to send a lot of issues and things, Alper had a good AI integration that I jokingly suggested they could write to automate most of us. Yeah, Thomas, anything from here? More of a surprise than a big takeaway than that in 15, 10. Normally, it's like when we look at MRS that emerged and it given milestone, it goes maintenance around 50%, then we're gonna be around around 30% on features and then it's gonna be hovering between 10 and 20% on bugs. What was interesting in 15, 10 is that we had a high number of maintenance simmers and featured delivery MRS went below bugs, and that's the first time I've ever seen that happen. And so it was just, it was a surprise to me when I noticed that earlier today. And honestly, I think it was a one off. I think it was due because of realignment and team's trying to bring onboard new team members into their work. And so that led to more maintenance tasks being assigned to anything else, but it's something that I'm curious about when I dig it up at work. I'll come grab the screenshot of the graph and I'll put it in here in just a moment. When did I notice by the ways? I knew we were working hard in thread insights to retire the error budget exception, so that we don't need any more because we're inside error budgets and it looks like we're closing it on that, that's great. I knew that was happening soon. I knew that was in progress, I didn't know as soon as it is, so that's great. Other things in there too, that stuff kind of, I don't know, J, or Emma or Nate, if you've got chances of looking there like any questions you have about it or any interesting things that you noted as perhaps you've been skimming or we've been talking about it and you hadn't seen this before. Now at the moment, we would like to take just a little more time to skim. Yeah, I was seeking that, but also I think what Phil said it looks like a lot of us could be automated. The details can be automated. The human insight, not. It's in the human insight, it's the most valuable part. The data, when the data looks different than people expect, it gives them an opportunity to say, why is that? And to carry a sped. It might be a good thing, it might be an ad thing, it might be neither, but that's where, that's where the review of these things come in, I think. So, since it's also, we're not talking about error but it's anymore, or FCLs in a weekly meeting. Right, that meeting's been canceled, TA, another recurring meeting canceled. So, we do want to keep an eye on those things. So doing this monthly async actually is a good thing to do. Rather than weekly discussing it across all of the teams.",
  "you you be also We're going to get a third of the meeting if we don't. We'll go all the way. Okay. Sounds good. Do it another 60 seconds. Okay. Olivier's got some comments in here, but it's going to feel kind of so, oh, okay. Got a third. Great. Nice. I was just UNI reading each other. Olivier's comments said that's not a good meeting. Thank you. So it's the secret data science staff meeting for April 12th. We were just deciding on where if we should have a meeting through a sink we said if we have three we'll do it. So all a sink. Glad to be so early on. Yeah. So news and events. I've got the item there. So my draft Q2 occurs. The link is here. If you want to review. One of the most, please don't add objectives or results under these. That talking to me first. I just asked me to add all of your own that you want. But I ended up with a few more than I was expecting in Q1. And I'm not sure in our new Ocare feature, which one does delete or remove actually delete something or just unlink it from mine. So I think I accidentally deleted somebody else's in Q1. So just add your own as appropriate and let me know if you think it should some or in part roll up under mine. The some of them are about product delivery items, which are not a book. But so I don't want to talk about those, but you know some of the themes. Have a talk about those privately, but not on the live stream. So some of the themes are career growth team members, including the neuro diversity training. It's the only key result I have under that. It's kind of a lame objective with one key results. Maybe we have some other things underneath there. And also Q2 application health. Just some similar trends and some new trends like my continued migration of pajamas components for better usability. The view three upgrade, E3 upgrade and then some things are a different identify. And start resolving a number of cross functional challenges and operational excellence. We've got to turn into. But. And also burning down the usability benchmarking benchmark insights. There was also another one. And we'll see here for some reason. Readily. Yeah, it's under it's under the product ones, it's not about feature delivery. Reviewed improved documentation of securing governs. Doing a pass through the documentation for securing govern the product documentation and improving it to help it drive devs. So this is just some of the ones out there. Any. Without talking publicly here in this in this stream about the product specific delivery ones, which are confidential. Anything any feedback? Any ideas on these? The operational excellence care that you talked about, which I just kind of found right now. Was it under application health? So you can do. The cross functional challenges and operational excellence. And we talking about being more efficient or responding to unplanned work. Do you have any additional ideas on we were going with this one? I think it's a teach team. What is it? What are the team? The quad for each team? So you know, development. You set and UX think that is like one might be. Feel the one you and I talked about earlier on the. Thread insights. When can other teams contribute to code maintain by thread insights. And what does that mean when they do? Like what are the responsibilities when they do? I think that's a great example of one of maybe we refinish the work on that on changing or. Procedures on that. I haven't read the MR that you sent me a link to just yet. Of course, we just discussed it. Ten minutes ago, but you know, I think it's things along those lines. Perhaps that's in the right. I think universe and things. Great. Got it. Thank you. I think my general feedback on these OKRs and by the way, I appreciate you getting them out early because I think that was. Um, it's always hard to plan, you know, midquarter or whatever, but I feel like sometimes they lack content. And so maybe an example like you just shared would be really helpful in the description. Maybe not like a, hey, this one's for antivies, but just like a just an example. Just the kind of get people going their interaction. Great feedback. Sometimes we have examples. Sometimes we don't do it. Sometimes that little picture. I'm not too concerned about this, but sometimes if you provide an example, it sets too much. People think with that mindset and they may not think of the bigger mindset of other ideas. They may have to, but I agree. Some of these are so big. At least one example helps. I think the benefits of having at least one example is better or not. I didn't think that that was a good example till till Phil just asked the question and then I thought, oh, so then we just discussed 20 minutes ago. That might be in that area, but yeah, really good feedback. Sure. Um, in big rocks and how to issues, Thomas is not here, but Olivier is an as a comment. Leave a you mind verbalizing commases comment and then you're response. Sure. It says give me a sec to look the right doc. So I'm chirping out from another call or here is so yes, to us is mentioning that we have codified language around what is an alpha or beta feature and was become to reliability. And to my asking is that impacts our thinking on when we announce releases of new software. And did that's I mean, that's pretty. On top of my mind, currently with a primary facing the release and roll out of the license scanner, production readiness figure apparently is required for a general availability feature. I think that's a bit overwhelming. I think I mean, there is some judgment to use, but whether or not to go that process because quite a few. I started that one for the license scanner and there's a lot of information to provide. I think it's makes sense when you're spinning up a new in for a new service, whatever that is very impactful to the overall performance of the good lab instance. Definitely not for just a regular new feature in the real smallest at at at same, but this is not written this way in the doc. So that's interesting and the I don't know how I mean I haven't heard about that being shared somewhere. So I don't know what the adoption of that general documentation, but that's definitely something to be considered by the development group to say. I hadn't seen that production readiness addition there, but I think we need to look at and see when that was added and get some more context. Because I agree with you, Olivia, this this shouldn't apply to every feature that we deploy. Yeah, and I just pointed to a slack thread that was shared. It's an engineering FYI channel. But I think we're working to clarify the difference between. Alpha Bay to experiment little close beta. There's all these terms. I know with some of the recent work in the new field. We're using some of the experimental more heavily. They'll protect ourselves and also reduce the amount of documentation necessary. Let's top a mind for everyone. I think it's a nice open question to ask. I always learned something when I asked this. For me, the AI integration work in group, which is across the company. Large portion of the company, which I'm helping I'm co facilitating. AI code suggestions open beta, which is Sam Mon's team is very diligently working on. I certainly related a bit separately from the integration work. We're making progress on Q10 okay ours. I was behind on mine. A little more caught up now, but I'm still behind on mine and you know the creating and iterating on Q21s. Olivia, how about you? Top priorities continues to be a bit of scans. This is a playing through the company goal. Look, ours and the big analysis that we are previously. So this is very important work. There are a lot of work complex project, but very exciting one. It's involving many timers. So that's a lot of coordination and it's also have some overlap with a group. So collaboration, cross group, which is interesting to. Bystand scanner, I still try to roll that out. Just to about from a customer call about a call with that distribution team to figure out how we can roll this out. By default for a self managed customer without blowing up their disk size. That's a very interesting topic too. And then the OQRC isn't like you finishing Q1s and looking at Q2s. That's the season 4 QRs. We're a well as Allen's. On the call, Allen's top of mind are role-based scan result policies. UX redesign for scan execution policies and performance improvements. Yeah, for me, it's the long-winded identity verification, which we should be rolling out the second stage, the telephone stage soon. Next week, the B.S. reports dashboard and AI and how it's going to potentially integrate with the diabetes features. And then we're going to talk about that a lot, maybe. Thank you for me right now, and we have some tremendously large projects that we're wrapping up getting those into the production environment. So feedback deprecation has been going on for nearly a year now. I think at the end of the month it'll be a solid year since development started. We've also had some smaller projects that are kind of interwoven into that feature. So that's tremendous. That huge success for the team. And then we're also rolling out the interaction with vulnerabilities when you dismiss them right now. It's just an option. We're going to extend that. So you have more reasons that you've dismissed vulnerability, which is something our customers have asked for a lot. Additionally, we're jumping into roadmap discussions. We have a lot as a new PM on the team. She's jumping right in. It's right at a great moment for everything. New PM. Rob no discussion. So it's like how do we align the teams. How do we get UX engaged to reengaged so that we can kind of really have a built confidence for a six to 12 million plan, which is kind of a rarity right. Like we're normally we're thinking a lot more short term. The miss. So we're setting analysis on what's up with my means we're wrapping up a few of the language conversions to some grip. No jaskan and to Gallaudet be specific. And then in addition to that. We've had a few conversations around the architecture that we'd like to support for sacred detection and future paths. So we're in the process of evaluating a couple of different approaches and in the benchmarking phase of that. That's very, very top of my from me. And then wrap that up 16 oh deprecations are running for them. So making sure we're on track with those and coordinating that appropriately with threat insights and other teams as well. Top of my for me is supporting teams to deliver across governor to use some grace we've got some some big deliverables coming out just making sure that teams aren't locked and helping out where I can. AI. Yeah, that's it's a hot topic at the moment. So looking at how our teams and which teams need to be involved in that. We've got Q2, okay, our planning which which we're in speed through. We're looking at. You what we need to deliver in Q2 and make sure we've got alignment with our counterparts. A little bit of time, speed on say availability. There's a daily stand-up is really started for that. And one of our teams is involved there's a discussion around whether we should have had a future change lock. And sort of non-development related one would be the imaging talent program which is an extension of the internship working court. There are some discussions on the adjustments leading adjustments and Smith on where we're hitting with that and looking forward to being involved. Great. Neil, you mentioned some whole way this last week don't ever response. Do you want to you want to talk to you this? Yeah, sure. So I'll verbalize the it's noticeable that this meeting bounces around. I think that's fine, but it also can create some uncertainty and it might diminish the quality this meeting as well. I don't know what the reasons are we need to discuss that either, but I think keeping this consistent. So, I think it can be an audience here as you're having a consistent time, something we can plan around. If it goes ASNC because it's some can make it, it might actually have a discussion. In my change the dynamics of certain individuals are in this call. The discussion might be different. So, I suggest we keep the schedule consistent. What I'm hoping. extra eye, nice casual, crazy. And like last week we went ASNIC because there wasn't much to discuss, but I will try to move it less often. It's mostly the AI working group that's made me. So more of a question for this group, when wine is busy and wine can't attain this mating at the sheath of time, should we go ice-sync or should we just go to the making? Or should I, or should it be moved to a time I can? Or should it be moved? I think we've seen all three cases sort of over the last month or so. So it would be good to. And I'm finding that you're in the course. I'm finding whatever the group prefers on this. What does everybody think? It depends on the topics. I mean generally you're driving conversation and we have questions for you. So I mean that could be some interesting topic to discuss just together with you. But I think from most of them, I might be wrong, but from most of them, it makes more sense if you're in the meeting. Well, my suggestion is that we don't make last minute changes half an hour before the meeting or an hour before the meeting. If earlier in the week, we identified as a class and we want to move it. Then we've got time to take the discussion. We've got time to contribute a second to turn on what is important and needs to be that we might need to be scheduled for. How does that sound as a first iteration? We don't cancel or reach a schedule with a meeting on the day. That's great consideration. Yeah. Absolutely. I think the item to there is probably a non-public. Okay, and as I get lab but not public public. So let's cover that after we go to a private recording in last week's B. If I wanted to discuss it. Those are the only two that we did from last week that we did a sink on that there were no responses on. Which I missed myself apologies for that. All right, let me stop the public and we'll start a private.",
  "you you going So, celebrations is a nice job Thomas for your discretionary bonus for exhibiting values of collaboration and iteration. When you're worked to help us make sure project link engagement. Based on feedback you see from leadership you iterated with the team to help us get excellent business and excellent business case, which we presented leadership all through. We're working under very tight timelines. And it's almost as not here, but eight times. Camille looks like you've got one as well. Yeah, so Doug also received our discretionary bonus for supporting the route of code suggestions. Beta, I think they works pretty closely with you. And yeah, congrats to Doug. Yes, we did. And definitely well deserved by Doug. So under news and events, please see this recently, Merzim, from Sid clarifying that features are never internal only. This was at Clavray with Sid, Sid wrote it. I just had some type of editing and readability editing and wanted to go the word out. And look at this. It's a good on features are never internal only. Also for me in three B. So from Christopher and staff meeting earlier today, we don't have OKRs right now around the MR rate, but please do keep an eye on it. Looks like there might be some pockets of performance, you know, MRs per team per month challenges. So make sure. We work with the teams on this. It also, you know, might be influenced by new team members who have joined it might be due to, you know, the reshuffling we did re-organization. So, you know, keep keeping eye on this. I have not looked at our teams and where our teams are at. I have no idea. I've not looked recently. I've probably a month or two ago, and I don't remember. And nothing jumped out at me is where we have challenges with this, but please take a look. So, please up the status update the status of your Q1 OKRs, many are way behind or the status is not accurate. They show way behind them really not. Which makes people think they're really way behind. If that's what the status says. So, and apologies for the Wayne spam as I tagged many people in many of them today where I saw I noticed that for Q1 OKRs. So, apologies for that. You're hearing mostly from me and then just a little bit from Camille so far, but glad that you're able to make it today Camille. So, my draft Q2 OKRs are updated and we'll up to Christ first. You know, as appropriate, please review and continue working on your own. I also put a link here to Christ first. So, many of these are not public. So, I'd like to do is do a review of them live. But after we go off the public recording and we'll go on a private inside GitLab recording. So, we'll go through those in detail when we're done with the other topics. Phil, you have a comment. It looks like. Yeah, I think the last one you mentioned not aligning with yours without talking with you first. How would you like us to do that just to say thanks for, for your flag? Yeah, and the contact with a bunch that I didn't care about and a bunch that I didn't. And even if it's important, well, I didn't care about as much. So, don't link yours to mine without just letting me know you'd like to do that unless I've already linked something of yours to mine. Or added something for you a moment. So, just, just, you know, write me in the, you know, the publicans I get lab, I get a lot of data science leaders channels saying, at way, you know, should, should I link this one to this one, this one to mind to this one on yours that'd be great just. Good to do that just just a quick question or if you're pretty sure I wanted there, just add it and then just tag me and let me know either way it works fine ask either ask or in advance if you want to or just let me know you have just so that I, I notice it getting linked either it's fine. On some of them, I think I had like 15 things when I really wanted to track five of those 15 things and they just kind of showed up on my list because people link them in and because that looked, they looked related or were related and And there's just more than what I was looking for for me to track just just because I'm not tracking it doesn't mean you shouldn't. As an example. It, so. I'm going to say, you've got the next item. Yep. Hot off the press as in this was just made public within the past hour and a half. We've got a news back in engineer position open and dynamic analysis. And so this is a, this is now public where go in internal as well as external with the simultaneously. So I wanted to go ahead and share since we added and now we're going to. We're going to work to fill this with as quickly as we could. So here it is. I'm happy that it's here. Great. So what skills are you looking for which group is it? It's dynamic analysis. We are looking for go laying primarily. They will also be working with the, I mean, but this group is as quite poly lot. So I mean, C sharp would be welcome. Ruby on rails would be welcome python would be welcome, but predominantly is going to be expressed through go like. Great. So one of my favorite questions to ask, what's top of mind for everybody. So you want to put your notes in here and then we'll. Robilize as we go. For me, it's a I a I and a I. Uh, my top three. And then also okay ours and I've also, you know, keeping on mind continuous vulnerability scanning and federap. So the big. Uh, product management track things that are important to the company. Uh, A I in terms of, you know, of course working with mon's team very closely where I can help on code suggestions, which are really important. But now and also, uh, cloud, um, hope facilitating with Taylor the AI enablement. Working group. It's all the AI features that we're working on across the company. And also keeping an eye on what meals team is doing in terms of one of those experimental features. So that's it's on my mind. Uh, Neil looks like you've got, uh, you've had been the next one. What's on your eye? Yeah, it's not top of mind. It's way up here in the off screen. But uh, yet AI and it's extending off the experiment we launched last Friday. We have explained this vulnerability. We're iterating on that wherever you're rapidly. That's that's taken a lot of my mental capacity. Additionally, Tiago has been out for I think two weeks and he's out for I think two more weeks. Some in the pick of that, trying to oversee a really large team. So it's a little crazy. It's okay. Bill, you're out there. Uh, just wrapping up Q1, okay, hasn't defining Q2, okay? Is it's more today's job than anything else started. Elevate last week with the manager train, which is, which is interesting. Um, uh, what else is going on? Just looking at what we're going to be delivering in Q2, which is related to, okay? Jay or up next or I'm going to have Jay if you're not, you know, ready to talk. Connection seems to be, can you guys hear me, okay? Yeah. Okay, cool. Um, still on the public broadcast. Uh, some review sense security issues related to a feature that anti abuse zones. Um, It's a very late address and safety. Uh, we're launching identity verification and, yeah. For me, uh, 60 no deprecation efforts are coming to a close. So just coordinating the timelines for all of that stuff in mind. Uh, we're trying to wrap up to send up the versions, scale on no JS. So we're going to close those out and then things that didn't list quite yet, but real time staff standing. We've got a few discussions around that. Um, and then ID integration work is also. I almost. Meal looks like you're next. If you're talking to your amused. Yeah, forgot it. Uh, so yeah, I grew up with this also, uh, thinking about AI. Um, we spent quite a lot of time discussing what could be the growths role in AI. Uh, first company because we don't really fit it naturally because we don't work on features. Uh, so yeah, big thanks for, uh, to the PM's that helped us go for this. And I think we are in a good path. How we can support the efforts. Uh, and yeah, that that's been the mind topic last week. Thank you. And if I were working on some training materials for people who know little or not or nothing about AI. On the people who've learned just enough to get by quickly, which is a large number of people, but to be able to do some AI experimental features based on third party services. So that's something I was just in a discussion with Juliana and others yesterday and how can we get that training. Stuff together to point people in the right direction because you're not alone. Your team is not alone at that. Many, many teams are in that same boat. Feel. Just happy on to what Camille shared. We've been thinking a lot about AI and with regards to Q2 okay, our planning. And as Camille noted, like, we don't naturally have fit into that feature ownership area where we can just integrate AI, but are trying to think of our contribution of two lenses is one, when we get to that point when can we add AI to what we own. And then, too, is like, how can we bring awareness and raise like in the adoption of the AI features that come that we take to market through things like acquisition loops making it very easy to find them or surface them in the early product experience. And so, just to discover moments and things like that stuff that's maybe a little more naturally in our real house, but can aid in the adoption of the and hopefully. Drive revenue for the business through the specter. Yes, I'm next. So we have a lot of sources of priorities. We've got the yearly's we've got cross-functionals we've got OK ours we've got long-term road maps strategy sessions and just trying to reconcile. How each group in the stage oriented self for prioritization decisions like how does it decide what it's staffing and what's most important. How does it know where it can draw lines and where those and where those autonomy and decisions happen to be and and discerning also. What events that can happen that can change prioritization alignment for each group because we've got a lot of moving parts right here and it's. At the risk of business speak, this rather dynamic. That's a moment. Indeed, I think it's an understatement. Cool. So I'm going to turn off the YouTube live feed and then we'll talk more details about OK ours. So give me just a second. And this is public inside get lab not public to the public so feel free to stick around.",
  "have yes yes yes and a I've already started. I need a high fill. Everyone. I am. So to get started here, welcome Alex and them back to the Engineering Director shadow program that you did in the past. And you're doing it again. Any interesting learnings or observations from this week. So far. Well, probably it's very interesting to see how the company is being adopted to the ML development, not only from the technical part, where usually we work with more and with the AIC team, but also from the management side. Yeah, it looks interesting and really, probably helps us better helps us to understand. I mean, I see clearly right now that the way how we need to work from the documentation. So probably does the main part. So for me, the main, maybe, our scam of this week is that kind of path between management and the technical development, the kind of how to explain better the machine learning results that we obtain every week and how to present them better to the management. Next, let's enter. Nate looks like you got the first thing under new hires and the investors. Yeah, so Hattish from the compliance group had their first during a Vizory, could live in a Vizory last week, which was a great achievement. So congrats to Hattish. And Emma, you've got me? Yeah, just a few more to pull out in the section, Brian Williams, second year, Eulerie, the second year in James, is for sure, second round, everyone. Thanks, Emma. And we got a celebration as well, Nate. Looks like. Yeah, just something I wanted to call out was whose effort has been added to the Rails technical interviews. And this is a great achievement, because I know we're light on the interviewers and the APAC regions. So it'll be great. So what's top of mind for everyone? One of my favorite questions to ask. So what's top of mind for me is driving code suggestions usage, which is up significantly today. We're not going to have to actual numbers, no live broadcast. That's public, but great stuff there. But in make sure the system can handle a load. We had two incidents today as the load went up significantly. The system went down significantly. But Alexander is actually on that team. And Alexander was engaged on those as we're many others. And got a result, which is good. And whilst preparing for being out next week, yay. I was just starting to type in there. So many of you will have seen the quarterly results. I am A from earlier in the week. And there were some slides and recordings and an AMA doc where I said the E group and various others presented. Big thing that's come out of that is there's a change in the way we're working. There's a sense of urgency. We've got competing with GitHub and AI. It's become the big thing. And we're all learning related to what Wayne and Alexander have been saying we're all learning about machine learning and what AI is and how we develop differently. But it's got me thinking about how we manage or how we lead teams differently in the different approaches that might be needed there. So I don't have any answers yet. But it seems we're sinking a little bit more. There's definitely a lot more urgency. So I'm trying to think through how that changes how our teams operate on a day-to-day basis. I think I'm next. Matureing AI features. You know, we got through that initial push. We've got some more ideas. There's a company organizational shift to focus on beta features, less experiments. So Matureing those getting those really permanently fulfilling, I guess. We have a new one which is pretty cool to resolve a vulnerability. It's a work on progress. We're trying to gather thoughts with that user workflow. Looks like, but essentially it will offer a create MR button. So cool. Now I've been understanding the vulnerability. AI has spent some information including a code suggestion. Can I apply that as a patch, step off an MR, and continue that on? You know, the goal is to connect our development efficiencies. That's a big goal of AI is how can we, you know, improve develop a process, work with the SDLC. So yeah, we're trying to figure out what that looks like. Additionally, we hosted two AMAs, slash fireside chats this week. Mondays was a little bit smaller group because I was a pack after you've brought us a person on, but today's we had over 50 people and attendance tons of questions, great conversation. You know, we filled that 30 minutes up. Just fine. I linked to the doc so you can all see that. And I think you would have recorded somewhere and an or those go. I'm a, you are up. Yeah, that's exciting stuff. Sorry, I was just copying something over. Top of mind for me is we're working on starting to expand advance vulnerability tracking for SAS and secret detection. We've talked about this a little bit in the past. Right now we're more focused on identifying some quick wins that we can use for improvements as well as defining what the long-term path looks like. So that's top of mind. In addition to that supporting team members that are working on high priority initiatives outside of the group. So one of the things mentioned previously was AI just working with one of the team members to make sure they have everything that's needed in order to support that request. One thing that I will link down below. One of our team members, D-Rege, will be hosting a talk titled Get Now from in secure everything in which you'll talk about some of the security issues that he's come across working on the front end. How did the band against them and proactive measures others could take? To assist with protecting our platform. So that talks coming up late tonight or I guess early tomorrow depending on where you're at within the next one for us. Definitely want to check out. Thanks, Emma. I think I'm next. So top of mind for me is the software supply chain security working group and the build provenance which we have a working with NPM build at the moment and just how that feeds into the source of standard and then the adherence report which the compliance group is currently building and all of the sort of works to give it to create that. So yeah, top one for me around planning and building that. And also I think someone to my there around supporting some of our engineers working on other initiatives that I am such like that. All right, thank you. Next two things, customer work flows and how they flow through each of the stages that we have and how those represent both opportunities as well as constraints, particularly within sec since we're organized by topic rather than workflow or work streams. So any type of customer facing workflow touches both stages and it will and it will flow through a whole bunch of different groups. And so those are those represent opportunities in that there's different things that each of these stages support as well as they also provide constraints and being able to recognize where those are is strikes me as important, how important not exactly sure yet but I'm wrestling with it. And also related and importance, just since I've got dynamic analysis as well, processes of workflows within that group, we've we're recognizing overhead and trying to see if a pivot and how we work might smooth out the delivery processes that we have particularly given that it's a much smaller group than it has been in the past. Thanks Thomas. So I moved a couple items up from read only to the not read only section. So it's about there's some already some async discussion there which is great and I think maybe just important and verbal as in general. Yeah the engagement survey, if you haven't seen that in Slack or in your email or the details that there as well. Well, they's a really useful and the more teams and people you work with the more useful and interesting that the feedback is it's really something that we can look to address any feedback. So please do encourage your teams to complete this before the seventh of June. And then as mentioned before the judges will be a talk on the secure everything I'm getting out from and Neil, you had a comment about this. Yeah the pleasure of working with Raj, a couple years I guess I'm trying to remember. Couldn't keep them away from security fixes. they was always working on they is a trailblazer in terms of our that process that I think some of the presentations about a lot of overhead, a lot of things you need to do extra MRs, lots of waiting. But additionally for those I didn't know we've had past I don't think we did one last year in the last 12 months but we had the security awards program so we've had the quarterly and annual results and I looked off to the handbook page and you can see their rajas right at the top like two x the second place for contributions. So yeah they knows what you're talking about. Yeah. They do have one topic but I think it's not a public one so we'll cover that after we're done with all the others. I just have one thanks in gratitude and I was thanks Camille and Phil for keeping an eye on the code suggestions in product announcement feature flag. The MR got delayed a bit that the feature flag turns on the features but that was very much appreciated. I came in this morning go, hey it's on somebody turned it on while I was asleep that's awesome. So appreciate it.",
  "Yeah, I complain I have really wavy and hair and then my wife reminds me of all our family management friends who are less here than I do at the same age and don't complain about that. Okay, meetings, no streaming live on YouTube and I'm talking about my lack of haircuts. It's a secret data science meeting of June 7, 2023 and Thomas, you have the first item. And before I get there, I will say the advantage of my personal hair style is that I can have no bad hairdays and so I'll want to smile but moving on. Something that we've talked about before, something that has become official because that MR is now merged in the official get-laborally state is changing. There's no longer the 22nd as of 166 and so more information is available at the other under that link. I'm sure that this will cause a little bit of chaos and our planning cycles in the coming releases but change, come with. So, hearing nothing else I also see, I've got the next one. So moving on for those of us that work and go version 1910 dropped, we typically have a habit of pending to minor versions when we're using this so it's just going to basically be a rebuild. There's a bunch of security fixes that are available as a part of this so be thinking in terms of compliance and everything's only lines of federal happens so forth. So anyway, if you're working and go, this is encouragement to kick off a new pipeline and a new release on the new version and built on the new release of GoLank itself. Cool. My favorite subject, next is what's top of mind for everyone? For me it's AI and I didn't say AI three times and things are still analyzing in all our plans and execution it says AI and I know I am way behind on updating the okay areas based on the changed plans and I need to update the okay areas, at least the ones that I own or we own, but it's there. They're lagging or our actual plans are and I feel bad that it's second month of the quarter and I haven't done it but I've been slamming those things but nobody's complaining but I still need to. Could you, could you have an example of what do you, what do you like? What's changing? What should we be like? I think we said, you know, X number of experimental features in Y time and we're not we we we're more focused on that and I could work on specific features in not a large number of them. So that's like I need to update that one in particular. That's the one that in particular, I'm thinking about just you're not, I didn't scare everybody like a bunch of things you're going to change so thanks Phil. It's that one in particular I need to update and I also haven't reviewed like any of the others. I think I got to sign some things that I haven't looked at yet or I barely looked at that I need to. Okay. Thomas, how much have you? All right. I will I will link and talk at the same time. The iterative versus incremental development practices and there is a difference between the two and wondering if we're beginning to be guilty of swapping out the eye and credit from iteration to incremental. And so with us that's coming to mind because we're increasingly hearing stories of we've got when we swarm on particular areas of the code base that we've got people tripping over each other and to be wondering if that's a sign of more of an incremental approach rather than an iterative approach. I think there's a book about that adding X amount of developers late stage and our project doesn't complete the task in the earlier. Top of mind for me is a bar request which we currently have going on. I'll be limited with my details here but abuse maintenance which we have. And dogs, so GTV is looking good this week. Any new product proposal about rate limiting and rate limit exemptions? Who? Hey. Yeah. Sorry. And top of mind there for me is just the engineering changes as well recently and excuse me, the roadmap impact. So just trying to work through what that means for compliance and I don't have a short term borrow going on as well. I'm just working through all that at the moment. So that's top of mind for me. And also that the courtly earnings was very interesting. Indeed. Yeah, I'm not privy to what those are going to be of course beforehand. I'm not. Thankfully, I'm not privy to such things. I have no idea that you know, I was I was definitely refreshing my browser to see when they'd be released and how the market would would react. So but you know, I try to not look things on a daily no more monthly no quarterly basis, but it's also hard not to at least look at it in detail quarterly. Some of our teams have been working on some projects that have been going on for a long time. Jay's team has won, which is which is sort of being rolled out and managed right now. Our gross team has another I won't go into it to meet in details around that. But that's work that goes back a year and a half now, which is being rolled out this week in next week. So we're watching that. We're we're looking at last minute changes and responding to feedback. Cool. I sorry, I've been scrolling. I think I'm next. All a voice talking about AI. I think what's also type of mine is we're having two new back and into yours. Doing the team formally, 16 to but we've started the coordination. So I'm aligning on boarding buddies. I'm going to have two front end engineers kind of more project focused. So they can get going on some tasks and then back in engineers. Those are going to be coming from Tiago's team, but those will be a great resource for back and type questions to little big great hands. Cool. So I had a couple of things just looking at my to do list lower priority to do and reading myself, but this is interesting. The OS top 10 from large language models project is that their OS is open source and they're really good at open collaboration with anybody interested. I've done some stuff with OS with the OS people volunteering to work on OS things. This was pretty kind of topical in need as an OS top 10 from large language models. Not sure how much you can say there as it's it's it's it's it's it's a there but like specific recommendations are hard. So if I had time I would put a little time into it. You know, I'd go into the GitHub project and put some comments in and make some recommendations, but I don't have the time. But at least now, but maybe I will in the future and Neil, you had a thought on this. Yeah, so I think Phil has been doing awesome sharing resources. They brought this to my attention a couple of weeks. I raised to my team. We have been getting ready for beta. We have a security performance to ability evaluations specific to security this bit really well. I linked off to a thread. We'll start that conversation. A lot of interesting thoughts. What's really new about this this list is when we think about that OS code. Um, vulnerabilities are all you've seen is they're very specific to exploits things like that. Where these are more some of these are behavioral. It's like user training like expected users to do it this way or not this way. It's not like a code thing. It's more of a human thing that we need to be aware of. So yeah, very interesting topic. I was very excited a couple of weeks ago to start a brown bag. I don't know where the time went. And now I'm going to be here pretty soon. But I think that'd be really neat. Schedule a brown bag and by anyone who wants to join. We've got to just go through this list and share stories. The examples things like that is a lot of great thoughts. I'm sure that might be a good way to kick it off and expose this to everybody. I still something I can schedule by all means. I'm just not sure if I can run it. Why do you think someone from vulnerability raceage would be good to involve in there? Yeah, no, just nobody volunteer. It's definitely a volunteer kind of thing. They're really, they're also they're down to three people from four as one team members on paternity leave starting two days ago, to neat. So he's going to be out for a couple of months. And the team is actually three of the four people on the team actually volunteered for a lot of the AI work. Not specific to security, but more. Turns out they had some of the skills around various ways to do ID integration for jet brains and for NeoVim and Sublime and EMAX. And they didn't for example, proof of concept of IDE integration for AI code suggestions using something called language server protocol, LSP. There's a you write it in LSP once and then multiple things can use it to massively over simplify and say it incorrectly. But we're actually using that architecture from the vulnerability research team in a lot of the IDE, so just kind of neat. So they've been doing research. More than vulnerability research, more as of late than now getting back to the vulnerability side. And they weren't interested in this, but it's good point to. I can go by the means go ahead and get something settled for next week. Hey, I guess it's a heads up. Yeah, not a request. No, no, I think it'll be a more or less agenda less, it'll just be like we're going to review this. We're going to go through these and talk about what's stopping mine 30 minutes. And if there's more conversation, we can do more of these or we can break it up. And that's a good start. Good start. So on D, you probably we've probably all seen the the t-shirt, probably now also meme of, you know, be careful or I will replace you with a very small shell script. You know, you're wet lettering on a black t-shirt. Reminds me of this is I'm thinking about trying this private GPT thing where you basically, it's an open source project. You run a private GPT on your own computer, train no more you give it. And I don't know if I train on all my emails, all my slack messages, all my get lab issues and MRs I've worked on and Google Docs can it replace me? Like if I get the time to do this, which I probably won't, I'm going to train and all this stuff. Again, it's all private. This is in some of the data elsewhere. And then I'm going to say, I don't know, it makes like J said X what would Wayne say? Like I will take a slack message from J, I'm making a banana and said like I wonder what if if it's going to figure out what I would say. And you may actually get a response that I've copied and pasted out of this thing. I mean, it's it's not very good. It's very slow. Like it takes 45 seconds on a normal computer to do one response. And it takes hours and hours and hours to train. But we'll see. Or perhaps you all will assume that I have already had been replaced when you get a future message. We'll see. I've not loaded this yet. I'm going to think about it though. I guess I'm done. All right, we're uh, we're a couple read-only items and let's anybody want to read them. I think we're done.",
  "There's a need to know the couple of seconds. Okay. So it's the secret data science staff meeting for June 14th, 2023 or 15th, where Can you see the Clouds I'm getting. Welcome to the source content. Let me remind you to start by sharing some听 links on content about the cloud. oh, Where are we going to go to number number 138疫 crack, and such amount of attracting. Well we're going to collect net estimates earlier in the documentary for I mean, so I'm pretty much glad to, you know, test out the BLA features of the ASS existing and to see what you are actually working on. So it feels good to get inside the top level of what things are going. So in that's my learning so far. Thanks, Enjorn. So it looks like you've got the first item. Yeah, just a celebration. The free user initiative is something that grows in particular has been working on for about the last year and a half. We've been a lot of the changes of requirements, lots of changes and stops to, in terms of rolling it out. Just due to the work that's been rolling out this week. And that's been rolling out this week. And that's been growing from the team. There are some team members who aren't in growth teams anymore who worked on that. I believe even J and you're in your former growth team probably started working on this year and we've been working on that before coming into your business. So I'm massive team effort. And well done to everyone. Congrats to the team. This has been a long effort to get there with all the different nuances and many, many, many, many different iterations on the plan, not the code, necessarily, but the plan on as we got feedback from users and iterate on the business requirements is those changed, et cetera. So it's congrats. So this is live now. Is that right? Yeah, it's been and crimitially rolled out. And I think they started it like 100 users. They started putting them in read only and they flowed it down to five users. So it's the five years of luck is now in. Got it. And has that scaled existing? These aren't just users going forward. They've actually like a retroactively done it. There are still some notifications to be sent out to some existing groups. There is a little bit of ongoing work there on how to get that many emails sent out and a short space of time. So the team is still looking at optimizing for that. But certainly for new groups that's in place now. Well, groups created after a certain date. It's not just new groups. There has been multiple notifications going back from months. Living users know that this is coming and leading those groups created after a certain date of groups that have been around for a bit longer. Is still ongoing. You wanted to jump back and Jay, we can get you an issue. I'm very happy you're on that right now. So thank you very much for the offer, but going to politely decline. That's what I was thinking. Good news and events. We move code suggestions to use a Google model versus our own for the forget labs own projects today. So please do encourage your teams to try it out and provide feedback. There's an issue on how to use it and we're to provide feedback there. How much have you got? Yeah, I shamelessly copy this from the development staff agenda doc. But mid your check in reminder that as launch, there's a slack announcement and book page. Just tend to be team member led. It runs through the end of July and the follow one to that. Like an optional output of this is individual gross plans. However, if people want to participate in 360 reviews and individual gross plan is required. So there's the there's the carrot for IGPs. If folks need or want one. Thanks. I wasn't at the staff meeting which was earlier this morning. My time and I just had a couple of questions that you might be able to answer. So you're saying that 360s are still optional. And they're often okay. Yeah, do we have any any stay in that as leaders can we can we mandate this for a certain team. So if we can get useful. Or is it often at the individual level? I don't think that question was asked. But I assume it's often at individual level on way not on if you have a different understanding. I'm certainly sounds like it. If you have to create an IGP first. Yeah, that's the same. I was hoping that it would be up down to another and up to. I like 360s. I think we get valuable feedback out. Anyway, I'll follow up as Julia. My favorite question. What's top of mind for everyone and Alan put that in a sync. I'll read Alan's. Alan's top of mind items are midyear check-ins, which Thomas just mentioned. Promotion docs getting together for teams in Poland. That's neat. And also demo environment automation. Thomas looks like you've got next. Most Jay is going to chuck in front of me and as is on as an armor. Yeah, sure. I'll steal. Maybe I might all steal after that. And a quarter is coming up soon. You're kidding. Really? Am I wrong? Am I ahead by a month? I can't possibly be right. We're ahead. We're halfway through the quarter. Yeah. I said really like, oh my gosh, this quarter is gone by fast. We're halfway through. Time flies fast. Good middle middle of the quarter. So you've been warned. Let's see. So okay, ours, making sure those are on track. And my team should be resolving a bar request soon. So pretty happy to get back to roadmap work. Yeah, that's our map. Into the. Yeah, I've been involved with a customer escalation recently. That's been very top of mind. A major check-ins like Thomas mentioned that kicked out yesterday. We're also tying in some career conversations into that. So been heavily involved with that. And then also on the static analysis front vulnerability tracking has been a big focus for our team. And one that I didn't finish adding is on the secret detection front where discussing bringing the secret revocation service into the product from the scare. Excuse me, the security automation team. So that's been a discussion that we've been having. And pushing forward. Cool. For me, I hit my priorities lately or top of my AI related things in terms of the world. The Google change working with Mons and infrastructure and others on that. Promotion process planning and pre-work. I also hit inbox zero today, both in email and Slack. For the first time in a while. And that's a good feeling. Yeah, I'm jinxing myself by saying it. But because, you know, once you hit zero to the last for long, but it's nice to be actually get there. Feel. Just one of the things for where kind of is elevates, which all managers will go through. And one of the cohorts, I think we're on cohorts. There was a virtual concert group and now we're on cohort one. Other managers will get to go through that over the next six to 12 months. That's quite interesting at the moment. Looking at OKRs, making some changes mid quarter to some of our OKRs to reflect changing requirements. And just within supporting the teams on major deliverables. Cool. A quick question on elevated. If you might know, are there multiple cohorts at me at the same time? Or is it okay? I think this is, this is, there is. What's the right word? There are multiple groups in different times zones doing the doing elevate right now. And they'll be starting another round. I'm not sure when. I can find out for you if you're interested. We don't identify that. Like in the next month, I think it starts roughly. Yeah, wasn't sure if there were just staggering for once or if there were like multiple classes going on at the same time is decided to do your city. Sure, there are multiple. There are three streams that you can choose and you choose it based on time zone. They recommend that you stay in the same. Take the same class each time so that you're with the same group. Excellent. Cool. Thank you. So we're out of agenda. It's good discussion as always. On shaman, do you have any questions for Jay or Thomas or Umber or Phil or I based on what we discussed? Any questions or observations? I mean, I was able to understand a few other things to be ready on this. But I mean, I do have a question. I don't know who might be able to may know answering this. I see a lot of many things in terms of like, I see a lot of feature requests which are, you know, being directed towards you guys and then you would have to go to the document team and ask them how to build them. So I mean, how do you say that, you know, we are going to build this feature and there's not feature because you will always tell these what I'm going to answer. I want to answer. Wayne, you might be the answer of this one given. And I apologize. I started multitasking. Can can you repeat? No, I'm just curious. I mean, not the prioritization thing, but, you know, what features do you want to be building and all those things? How do we choose what features to build specifically in AI? Yeah. Yeah. Thanks, sir. I couldn't make it today. You know, all means are optional. Get lab. She'd probably have an opinion about that too. Although Phil probably does as well as some of their teams are working on AI features. And we look at where AI can be motion to act for our customers, which is hard to figure out with confidence, where it's most useful and then work in those areas. And we've done a number of experiments. Sometimes they're really, really successful and people really like them and sometimes not so much. And that's okay because we're doing, you know, minimal viable change experiments on them, so we can get user feedback. Like the code suggestions features, gotten a lot of great feedback and a lot of usage. One that one of Phil's teams specifically, Neil's team has been working on is the explain the vulnerability feature, which also is getting a lot of good feedback from the users. So that's a, it's hidden miss, but that's why we're doing minimal, we're doing experiments to cool what fail fast. If a feature does not end up being useful that we predict might be, we, we take a step back and then, remove it or at least don't move it forward. Phil, is that, is that kind of how it's working from your perspective as well or maybe you have a different perspective? Yeah, we brainstormed. And actually we asked all of our team members for ideas and then we did what we do across the project and development. We sent them up to our PM and they prioritized, they looked at, and some cases what aligns with their current roadmap. And other cases they're looking at competitors and other cases they're looking at. How easy it is to get an experimental feature deployed, but the prioritization is done at the product level like just like any other feature. So I mean, I just had one, if you don't mind, I guess what is too, because he's, you know, looking over with the static analysis thing. We actually had a meeting with finish earlier this week and so I got to know about that. Simgrip is like it's producing a lot more force positive and positive in positive. So I mean, how do you ending that like are you still transitioning with care or like like what's up? Yeah, just so I understand specifically what you're asking, you're asking about we, you've noticed that we've seen a trend in higher false positives and false negatives. Yeah, so what I mean, how is the transition going for the good level analysis and specifically in the semigrap rules that vulnerability research has been working on. Yeah, so there's a few different things we're doing right now. One is we're trying to introduce some sort of benchmarking to help with rural efficacy. That's going to talk about through a few different initiatives that we have ongoing. One specific area we're using is a tool that we're looking to build will help run like the rules that we have within static analysis or within our semigrap based analyzer against multiple projects. And then from there we'll get out put it whether what the true positive positive rates went to be so that's one initiative. Another way that we're looking to tackle this is finding more ways to get feedback from our customers on the rules that are being generated or being used. So in sense, it's a customer detects a false positive right now there's really no easy mechanism for somebody to report on that false positive. So we've talked about different ways that we can introduce that within the UI. And then also partnering with VR. Our vulnerability research team in order to address some of that feedback and also improve rural quality. So just a few ways at the moment that we're looking to tackle the false positive. Let me add on to that if you'll let me. Okay. Let me add on to that. Yeah, let me add on that a little bit. So for some graph rules their intention is to replace other open source analyzers that we already have in place and the when the when those when the rule transitions are being done. The effort is not on making the rules the best they can. The effort is to make the rules match the eggs the rules that we were replacing as completely as possible. The reason for that is that the transition path is that we'll have the new semegrip rule pack running with the other analyzer side by side. And as long as they find the same things, they show up as one vulnerability rather than two. They're being dual reported. And so when we do the transition, it's about completeness of the rules being ported. And then once we turn off the original analyzer and some greptakes over that allows for tuning for efficacy. So there's a two step process that has to happen. I mean, it's not even but as you was saying for the base analyzers. I mean, do you have proposals for all of them? I'm like, I'm not sure you'll say something for a type script. I wasn't going to make that. For type script, it started with the S lens. So there were originally 16 different SAS analyzers that are being compressed down to ideally into one, two, three or four. So, and we added in recent times, at least introduced any new analyzers outside of like I guess the main goal is. Yeah, as solid shows the main goal is consolidating to our sub-grapes analyzer. What we have, I think we're down to 10 to 12 at the moment. So ultimately the goal would be to get to one single consolidated experience across all the new facts. So. Thank you. Thank you. I'll just keep you as to. Christine does the project. Thank you for the question. Great. Yeah, great question. I'm sure. Great. Thanks everybody. Have a great day and fill. Since we didn't get to finish our previous discussion, where would jump back on that others. Yeah.",
  "I'm going to do it. Hey, Jay, how are you? Hey, Wayne, what's up? I'm doing pretty good. How are you doing? Good. Let's see how many people come today if it makes sense to still the meeting. You get a, looks like you shave and you got a haircut. It's been a while. Yes, too long. I was trying to scaring people with my lack of haircut when I joined Zoom. Let's see here. Yeah, we're forward. I have accepted, but I know we've moved this around a good couple of times. Let's give it another minute to see if we want to go. All right. We got a quorum. We got three. There we go. Hey, I'm here. So it's the sick growth data science stock meeting. Your June 27th. My shadow today may join us. May not. It's pretty late for her. Pa, wash them and the leak. But they joins. We'll let them give a quick introduction. If not, we won't. Overall, I still was item. We've graded a certain certainty. My thing is the right. On their announcement, which I think is not public. But you can see the slack link. That's pretty cool. Anyone want to volunteer to read. It's a verbalized. News and events. Item A. Maybe summarize of it. Summary summary of it. I got to go forward. Am I. Go. Yeah. So from your. We basically are looking to provide customers with a better enterprise level support experience. As a result, in order to do so. We are asking or supporting to engage development sooner when faced with challenging support tickets. Expect that request for help issues. And not contain everything required. So by using this method of including engineers earlier, we're hoping to get to those troubleshooting steps. Fast from the process so that we get these results back to the customer. Really. That kind of aligns with the let's see. I think there's a 16 to support issues that should be easy to resolve. Um, coordination with the spreadsheet with issues assigned to each group. Is that would you agree that falls right in line with the. The announcement from your. I think it's I think it's at related. Improving the sport for customers. Being more proactive than a moving sport. It makes sense. Jay, you have a way to that. Is that word group or. You might have only been tagged on it if. If you needed to be, but yeah, I will. I think that's the right. I think that's right. It's only if you so if you didn't get tagged in it. Draw for hook. At least for now. I will say on one observation our group is that is that we've seen a recent uptick. In questions being asked by a slide. The section request for help project. Um. What a bit more than in recent weeks. So it might be related to this. It might be. This might be one of the factors. Causing that, but I don't think it's the. I'm sorry. Did you say you're getting an uptick in like kind of communication from support asking for. Yeah, from from support and solutions are context both. So like requests issues help reproducing items. So like that. Um, anti abuse works in some popular areas. So there's been no shortage of contact between support and our team. Don't like a lot of interfacing. Uh, I haven't set up a weekly. Reminder or weekly like a discussion with any support team members, but I'm about to do it because. It seems like we're contacting all the time. So. Um. Yeah. Well, anecdotal there. Yeah, we have scheduled as scheduled a support same between. Engineering and support this week to see if we can identify some common trends across the index and then test issues. So we try to get the root of the problem rather than just playing. What we'll also speak. Nice. What's top of mind for everyone for me. It's AI including this merger quest on a documentation change. I've been collaborating on related some code changes on code suggestions and which models we're using. As needed covering for Thomas, well, he's out, which is not much. I've covered from Umer and Olivier and Thomas's other folks, but I do. I think involved one or two small things and also various other things and also. Long time in process blog post about shadow programs, which I think is going to go forward soon. Umer, how about you. Yeah, I'll kind of like I just mentioned the. I think that we've seen in support requests were trying to bring some ways to condition. We resolve your support those. So that's one big thing. Made your checking conversations is another. And then lastly, the death cycle is primarily worked that we're doing on the team. One of which is vulnerability tracking and then the second major item that. We're working through is secret detection, false positive testing. So just keeping tabs on those items. Yeah, AI is definitely creeping up my. My ladder of things and I'm, you know, got at the top of mind. I'll be taking over a. An effort, Sean Carroll is going to be going on vacation for a month. So I'll be stepping into manage the code suggestions testing. We're also doing model testing. I'm sure is you're where Wayne. Um, beyond that, we are closing out a bar request that anti abuse has been involved in for like three weeks. Um, and then returning back to. Businesses usual so kind of getting everybody back aligned with our roadmap and continuing that work. And then we're going to go to the next. Great. I'm going to stop the public recording and then we got one non public thing out. So you can get sick.",
  "you you you you you Okay, thank you all so much for joining us for another CS skills exchange session. I did post the notes document in the chat that we had a few folks join us after that so we post that document again in the chat. It's also part of the meeting invite. One ask that I have before we get into today's topic is that the Q3 session is live. We are looking for a few slots there to still fill. We have some AM and PM sessions. So if you happen to have a session or a topic that you'd like to cover, please take a look at that issue and let me know what date you will be available. And then we'll pass it off to Taylor to cover today's topic. We'll be talking about data sciences and there's some key components around that that Taylor is going to talk us through. So take it away. Awesome, thanks. Hi, everyone. My name is Taylor McCasslin and I'm a principal product manager for our model ops section. So today I want to talk about an intro to data science. So basically what I want to do is first start with a couple of the use cases that data scientists have with GitLab and then talk about how you can use various features of GitLab to accomplish a lot of the use cases for data scientists. So I'm going to start initially with a presentation, just kind of give you a foundation of some of the problems in use cases for data scientists and then I'm going to jump into a demo within a project. The project is something that you can work and play around with. My goal here today really is to give you a taste of how to use GitLab in a very particular way that data scientists work. So let's dive in. First, just a reminder, everything here is for informational purposes. Everything we're going to talk about is subject to change. I'll talk a little bit about some roadmap items as well so that those could shift in timelines. I want to start first by defining some of the components of model ops, the way that we're defining them internally at GitLab. So there's three areas that our model ops section is focused on today. We've got data ops, which is everything related to ELT of data, extracting loading, transforming data. We want to make it really easy for you to connect data sources to GitLab pipelines so that you can do something interesting with it. You'll see why this is important when we get to our demo here at a bit. Then we've got ML ops, which is truly all about productionizing data workloads. Now that you've got data connected to your pipeline, you want to do something interesting with it. You want to run experiments. You want to train a model. You want to test models. You want to deploy those models. All the foundations of building, training and deploying a model. This is a pretty industry standard term. You'll hear model ops and ML ops sort of interchangeably depending on the customer. A lot of it just comes down to data science is building and exploring data sets. Then finally we've got a plight of mail. This is really about taking machine learning and applying it to a production application. For us internally at GitLab, we're now starting to introduce machine learning to get that features to make them smarter or more automated. Our first feature in this area will be suggested reviewers, which we'll talk a little bit about here a bit. This is the foundation of model ops. You'll hear these terms, particularly internally is this is how we're defining our approach to machine learning. A lot of these terms are industry standards. You may hear customers talk about them as well. I want to talk a little bit about some of the common pitfalls when you've got data scientists working within a software development environment. Some of these things will sound very familiar to the early days of DevOps. You've got a lot of teams that have data scientists that kind of live off to the side. They work in silos that have special teams. A lot of times data scientists are not software engineers. They are not DevOps engineers. They're not using tools like GitLab. In many cases, they're not using standard DevOps practices. They're using one off machine learning tools that are still relatively new. If you think about it in the realm of software development, data science is still in its early days. We're still cutting our teeth on the tools, the techniques, the companies, the open source projects within the machine learning space. It's also very early. What this leads to is this hand-off function and finger pointing where there's a lot of uncertainty about how data science works. For one, it's not like a traditional software development lifecycle. It's a lot more iterative. It looks a lot like the actual process. However, it is very experimental and very different to the way that traditional software development works. Very few data science teams think about security, which is a little problematic these days, especially considering how much data we're pushing through models and what we're using models to do. That's some of the common pitfalls. If you have heard our cell strategy with GitLab, a lot of this comes down to this complexity. The current state of machine learning looks very similar to DevOps 10 years ago. Lots of point solutions, lots of individual companies and open source projects that you have to glue together. They all have different data models. They have a lot of complexity with them. There's a lot of lack of transparency between all of these tools, because they don't talk to each other. We're trying to do for machine learning what we've done for the DevOps space is bringing all of these tools together, help glue them together and make it easier to share your data across your software projects and actually do something productive with that data. A common thing that we hear when we're talking with data scientists sounds very similar to those early DevOps problems. It worked on my machine. This is what we see with data scientists, because they're not software engineers, because they're not DevOps engineers. They're downloading whatever open source package and installing whatever Python packages onto their local machines. They're downloading whatever data they can get their hands on. It's very hard to replicate data science at scale, especially when you start introducing more team members. In many cases, companies start with one or two data scientists, where this isn't a huge problem. With the moment that data scientists start producing value, creating models that do interesting things that they want to integrate into their applications, you end up in this problem of DevOps of, well, how do I replicate those results? How do I integrate that into my software development, life cycle? How do I create a repeatable cycle so that my software engineers can repeat what the data science has been able to produce on their machines? So it looks very similar to those early days of DevOps. In fact, that is really all we're trying to do with our model-op stage is bring the foundations and the best practices of DevOps to all of these individual components to really make things reputable, to make them scalable, and to introduce continuous development and integration to data scientists. We want to take data science that right now is off on an island over there. We want to bring those data scientists into GitLab so that when data science is ready to productionize models, they're already source controlled. They're already running in repeatable environments. They're already running in GitLab so that it's easier to integrate into those production applications that are built with GitLab. The way that I think about this today is that model-ops for us is really a land and expand within our existing customers. We want to bring new usage seats online with those data scientists who aren't using GitLab today, but their organizations are. So that's really how we're approaching this. When you think about the way that we're approaching data science, really what we're trying to do is create these very simple hand-offs between these different stages. When you're thinking about data ops, it's really about getting the data, cleaning it, processing it, connecting it to a GitLab pipeline. We want to connect data to code. We've got source code today, CICD, the Arbrettan Butter, things that we've done for a decade now. We want to bring that to data scientists so that they can introduce those concepts to their data science development lifecycle. It's very common to talk to data science teams and they're not even using Git. They're not using any type of CICD. So when we try to talk to them about coming on to GitLab, it's going back to the original here's introducing Git. CICD, GitLab, and getting them on board to understand how to use those technologies. And when you think about handing off models to production, this is when we take data science and we go back to DevOps, where we're introducing those repeatable software builds and ability to deploy to production. Let's take a look at some of the differences between these personas and the way that they work every single day. When you've got data ops folks, let me just build this out. We've got a lot of data engineers, data analysts, generally they'll have data in their name. These may be more similar to software developers. As it takes a lot of code to process data to collect it, to clean it, to do something with it. I've never talked with a company whose data was not messy. It was not all over the place. There's a lot of code that gets written to try to collect and clean up that mess of data. This is where your whole thing is like ELT, which is extract load and transform. There are many platforms like Snowflake, Google BigQuery that do these types of solutions to take data that's all over the place and in many different shapes and forms and standardize it and clean it and make it easy to interact with. For MLops, what you'll generally see is data scientists or machine learning engineers. While they may have the title engineer or software developer in their name, they're not like traditional software developers. They're generally not going to be building production services and solutions. Instead they're going to be very experimental. They know just enough software development skills to be dangerous. They generally try to use the simplest solution to produce value with those machine learning techniques. Then we've got, of course, our classic DevOps software engineers, DevOps engineers. They're really focused on velocity, repeatability, security, those traditional DevOps principles. When we take a look at these users and build out what they're trying to accomplish with the data ops folks, you're really trying to aggregate all of those disasperate data sets. You're trying to clean them and shape them to make them available to the business in a way that's not complex. It doesn't have a lot of cases that you'll have to handle. If you've ever interacted with business data a lot of times you end up with crazy SQL queries that have to take into account that one special case, have to merge lots of tables together. That's really what data ops tries to fix is to make all of that simpler so that it's easy and straightforward to get business answers out of data from companies. For ML ops, it's about starting to explore those data sets to find opportunities to understand business problems that could be solved with machine learning use cases. They're building and training machine learning models to answer some of those business cases and to create value for the organization. Ultimately, data science wants to help produce a value from this data and make it useful to an organization. With DevOps folks, these are the traditional things that we're familiar with already, building, testing code, writing code, repeatable CICD, deploying those things to production, doing that in a really scalable, repeatable method. One of the tools that these folks use for data ops with that ELT platforms, like I've mentioned, BigQuery, Snowflake, Singer, in some cases, Meltono, which is a company that was starting to get lab and was spun out. You've got a lot of data pipelines and data warehouses, lots of data all over the place basically. We're seeing a shift now where organizations are starting to put a lot of their data into the cloud. So tools like Snowflake, Google BigQuery, AWS, a lot of those companies are moving data into the cloud to make it easier to do something like that data. This is a lot of just SQL, honestly. We have actually a data science team here at GitLab and a data team. If you Google Data Team GitLab, you'll find all of this stuff that our own data team does to try to make sense of all of the data that we produce here at GitLab so that we can make business insights into it. We're just now starting to get to where we're applying machine learning to some of that data to really create interesting value from it. For ML ops folks, you're looking at technologies like Python notebooks, which are basically interactive environments that are generally written in Python, where you can explore data in a really fast and iterative way and in a very display focused manner so that you can see the data that you're interacting with with code. We're going to look at a Python notebook here shortly. They're using open source ML frameworks. There are tons of open source technologies that are used by data scientists. I would say most of the data science realm is open source and so there's a lot of OSS community. There's a lot of just downloading things, throwing them together in an environment and try to get them to work. You've now started to see the rise of data science platforms. In many cases, these are companies that are building around open source libraries like ML flow. If you think about, or if you've heard of a company called Databricks, they built an off-low to build a company around it, very similar to the way to get lab has. We're now seeing these data science platforms that are very point solution-based. They do particular aspects of data science really well, but they don't integrate with a lot of tools. You end up having to glue a lot of these pieces together. DevOps folks, IDEs, CICD platforms, source code management platforms. What we want to do is we want to take all of these unique individuals. We want to bring them all together within GitLab so that the handoffs of what they're developing is much more straightforward. We want to collaborate with each other. We want to help them interact with each other's code and repeatable and scalable processes. This kind of leads me now to some of the product features that we're developing. I want to make it really clear that we're still very early in this space. We're just now spinning up our teams around these different product areas. We have introduced a couple of features. Some of which I'm going to demo today, which include GPU support for GitLab Runner, which is a way to do specialized computing with our existing GitLab Runner capabilities. We've introduced better support for Python notebooks, which includes cleaning them up. And allowing them to be run within merger Quest so that you can comment and do standard merger quests with. We're now rendering those more beautifully so that if you've got embedded images and then they all show within GitLab a lot of what we've been focused on as far as what I call just quality of life improvements. These were things that didn't work on GitLab for machine learning use cases that we just had to fix so that they worked to get those data scientists to start using GitLab. So I do want to talk a little bit about what this looks like in practice and then we'll jump over to my demo. So I want to focus particularly on MLOps because this is that sort of joint piece where you take a lot of the data science and you move it into the production workloads. You've seen our infinity loop with all of our different stages. MLOps kind of has its own version of that ML loop which introduces this sort of design phase. When you think about it machine learning is a lot about getting access to data, exploring it, trying different things. You go and sort of an iteration loop just with exploration before you then actually start building and training models. Sometimes those models don't do what you think they should or don't produce any value. So you go back to the drawing board. So there's this heavy design component around machine learning that really is about what data do I have and exploring how you can use that to find value. In the middle you've got the model development piece which is really about data engineering which is what I was talking about where you're exploring that data to find new opportunities in it. You're running various data checks. This is where a lot of Python notebooks come in where you're just kind of exploring what the data looks like and how it works, where you might be able to produce value. And even just what data do I had access to? This is where you start building those models and finding those insights within the data. And then once you've got a model that is producing value, then we're going to hand it over to engineering and help them get it integrated into a software product. That's where you then kind of start all over again. You take the model and you try to reverse it into the software development ecosystem where in many cases Python is going to be the language of choice for machine learning. And so what happens if you've got a Ruby on Rails application? How do you take Python code and put it into Ruby on Rails? That's where the operations piece of data science comes in and that's a very messy process because the software engineers have to replicate what the data science is doing in the language and software stack that's being used for that production application and get it all working again. So that's where we're a P to build the really becomes a problem. And then setting up how that application triggers the model, how it does inference, how it integrates into the development lifecycle. So let's take now this sort of unique in-al-ops development process in a lay it on top of our infinity loop. And so the things you see highlighted today are some of the things that we're going to look at in our my demo project. Basically we're trying to take that experimental cycle and bring it into GitLab in our sort of infinity loop here. One thing you'll notice is that all the features that I have indicated on our graph today span across the entire GitLab lifecycle and software development lifecycle. There's something very unique about model ops for us is that rather than organizing in our traditional sort of stage and groups that are very uniquely focused, model ops is horizontal. We sit across all of GitLab. As we're introducing GitLab to these machine learning features, we're going to change various aspects of the platform across the platform to make it work for these data science use cases. So this is just the beginning. There's lots of other features that we're going to be adding here. But this is what we're going to focus on looking at practically today. And with that I'm going to pivot over to a little demo project. I just want to caveat. I'm not a software developer. So you will see things here that are probably not suitable for production. I just wanted to get something working to give you an illustration of how you can use GitLab features today in a machine learning use case. Hopefully this is big enough for everyone if I need to make it bigger. Just let me know. So I've got a software project here today. This is actually public. You can use this with customers. You can work this. You can play around with it. I want to walk you through just the foundations of some of these pieces. So I actually want to start initially with a Python notebook. So this project is handwritten digit recognition. This is a common sort of hello world for machine learning scientists where we want to take an image of a handwritten digit and we want a machine to know what digit we wrote. This is a data set that is called minced. They have a large data set for a lot of these sort of intro problems to machine learning. I used it because it's very easy to access their data set, which is why I wanted to play around with it. If you Google digit recognition, you'll find many tutorials to replicate this yourself. So let's walk through this. I've got a Python notebook open here. I want to quickly just show you what a Python notebook is. It's actually just a JSON file that produces interesting output. We have recently added support for renderings files and you can now use these within Merch request. We render it in the source code view here. But you can see that this is a mix of code and exploration where I can write code and see the immediate output of it. So I don't have to work in a software development environment just in an IDE. Instead, I can write code directly in a Python notebook and immediately see the results from it. So let me kind of walk you through just one of these notebooks and introduce you to the problem of handwritten digit recognition. So a couple things. This is how many data scientists start exploring a data set. So I first want to get access to my data set. That is something that I'm using here with torch vision. They have access to data sets. The minced company that I talked about before, they have lots of different data sets. It's included with a lot of Python packages like torch or other open source ML products. We'll look at another one here in a minute. So I'm just importing my packages here. I'm going to set some very basic parameters around what I'm trying to do in terms of loading those data sets that I was talking about earlier. One thing that's nice about using a standardized data set is that they're normally organized by training and testing data sets. I'll show you what that means here in a minute. I'm basically just loading up the data getting it connected to torch so that I can do something with it. And then starting to explore the data, look at the different images that are in this data set, exploring how many test samples I have and validation samples. So you see by writing this code, you then get the immediate output of it. So I can see that I've got 30,000 training samples here. I've got 10,000 validation samples and 10,000 test samples here. I'll explain this as we continue going through. And so what I will did was then said, let's take a look at these images. They're in this data set. I just want to see what they look like. So I've got a variety of these images here. As you can see, they're handwritten digits. And what's nice about this data set in particular, and one of the reasons I picked it is that these images are all the same size. They're optimized that you can do very basic machine learning on it without having to handle weird edge cases like image sizing, image formats and all of that. It's very straightforward. They're all very unique images. They're in the same format and shape and size. And so what we want to do here basically is we've got these images that we don't know what's in them. And we want to train a model to actually be able to recognize the digit that's in it and tell it back to us. So we're going to use a very simple multilayer perceptron model here. You don't need to know anything about data science here. If I don't know much about data science, I'm not a data scientist. But this is where you can start playing around with different machine learning models here. And so we're going to take the data that we've got and we're going to bundle all of these images into different sizes. We're going to then start running them through a machine learning project here. And so I'm going to take a couple of images and I want to see what the current data set is. This is a completely untrained model. It's a packaged model that comes with torch vision. I haven't done anything to this model yet. And so when you get an image of this one here, it's going to predict wrongly that it's a zero. This five is a six. This eight is predicted as a zero. This two is predicted as a six. This one is predicted as a zero. So completely wrong, not even close. Unnot surprising here. We have to teach the model how to look at these. And so basically all we did here was take a very generic machine learning model. And we just passed the data in and we got an output. We haven't done any training here. And so this is kind of the foundation of how machine learning works is you take an input in this case the image of a number. We run it through the model and we get now put. And it's prediction of what it thinks it is. In many cases, you'll hear this called inference with a machine learning model. We're inferring the values based on a data set that will passing the model. And so when we do get to production, this is where the handoff of models comes in. Once we train this model and get it working, we should be able to hand it to an image and it should be able to tell us what's in that image. That's inference. And that's what we would build into our software application. It looks very similar to an API where I would give it an image and it would give me back a value. And then we can build interesting applications that do something around that. That's called inference. But in this case, we haven't trained anything. So we actually need to do the training so that the model does something interesting. And so here we go doing some of the training of that model. This is where it becomes important that we've got the training and the validation data sets where we actually know the data of what that image is supposed to return so that we can tell the model about it and the model can learn that hey, this image of a one is represented as a one and then we can teach it one to 10. And so we run this through. I'm not going to go into a lot of the details of data science here. You can play around with this on your own time. Basically, we're just training this model and by passing it, bundles of images were effectively training the model to recognize the digits that are in those data sets. And you can see that by training it just one time, we actually get a really high 90% tile accuracy. We can continue training it in what's called ethics. We're in your basically retraining the model over and over and over again in a very short amount of time. We can get that accuracy up to 98, 99% to correctly identify these use cases. But you can see where Python books make this really interesting because I can start to see the actual data and it's impact on my models in a very visual way. What happens a lot with Python notebooks is that data science is kind of do this exploration. They train a model. There are many different types of models that can be used. And so a lot of it kind of goes back to your like middle school class where you were fitting a line on your graph and calculator. That's a lot of what machine learning is like is you're just trying to fit the data science or a data set into a defined model and get that model optimized to return the correct values that you're expecting. And so to do this, we're doing a lot of evaluating the training data that we've got which are just unlabeled images and the validation data that's associated with those image to say, hey model, you predicted it too. It was actually a one. So the model can learn from that. So that's what we call labeled data where the images we have the labels for them of what their correct values are. This is called supervised learning. If we didn't have the labels for it would be called unsupervised learning. Again, I'm not going to get into this specific machine learning. But that's how we can kind of pull all of this together. And so once we take this trained model that we've now taught how to identify images, let's go back to our original example and we get a one. We get predicted a one. We get given a five. It's predicted five. You can see we've got a very high level of accuracy now. Now this is a model that is useful for us and that we could take to an application to actually recognize digits. A common thing that you might want to do after you've done this is maybe introduce it to the alphabet. Start doing letter recognition. Get into OCR. So this is a exploration of this data set. We've done some interesting just see what's in the data. We've run it through a model. We've been trained that model and validated the output of it. And now we've got a model that is a high prediction. Like a 98% tile model. And so now I want to do something interesting with this. Now we've validated that we've got a model that works here. So this is kind of in that exploration step that I was talking about earlier. We're just playing with Python notebooks at this moment. So now let's go back to that software repository and take a look at how we're actually going to do something interesting with this. So the first thing I want to introduce you to is just let's take a quick look at the C.I. Amyl file here. This is where you're going to start losing data scientists because they don't understand the Amyl files. They don't understand containers. They don't understand C.I.C.D. And so we're introducing data scientists to the concepts of GitLab. One thing that we'll start doing in the future is creating C.I. templates that data scientists can use so they don't have to figure out C.I. Amyl files. They don't have to build all these steps together. We'll start building UIs for them to connect and create these C.I. Amyl files. But let's take a look at this. So we've got just two simple stages here, set up and train. And my set up environment, all I'm doing is building a Docker container that I've got in my registry here. You'll notice that I'm creating this on a schedule. I'll show you why that's important here in a second. Once we've got our environment stood up, then I want to use that C.I. image that we've published to the registry. And I want to run a simple Python file on it. That's all this project does. Very straightforward. And in fact, all we really care about is this train section. The funny part, when I was putting this together, the hardest part is just getting an environment to build your models and work in where you can just interact with them. I recently have a new in one chip on Apple. It took me hours to get a Python environment stood up on my machine that I could actually run a Python notebook on. Whereas with GitLab CI, I had a very simple Docker environment stood up almost instantly. So it's one of those where the concepts that we have within GitLab can really enable us to help data scientists scale very quickly. So let's take a look at that Docker container real fast. So we've got a Docker fire here. It's based on the TensorFlow image. If you're thinking, wait a second, you were using Torch and your Python notebook. Why are we using TensorFlow? Great point. That's exactly kind of this exploration of data science. I'm comfortable from an exploratory standpoint with Torch. It's what I was originally trained with. So I played around with Torch first. And then when I started building my software development lifecycle, I wanted to switch to TensorFlow. And so I wanted to convert what I did in that other library to this more common and well known library to make it easier for folks to use. I'm going to install some custom requirements here. Let's take a look at those real fast. In my requirements file, I've got a couple of Python packages, which are common for machine learning, TensorFlow, which is that image that was just talking about an open source machine learning framework. And notebooks, which is a Python notebook environment, we've got Torch again. And so you can build out this requirements file with whatever Python packages you need and pass those to my container. So very, very simple Docker environment. Like I said, I'm probably not even doing this the right way for a production environment. This is just what I could quickly put together. But what's really cool about this is, I don't care about the Docker environment at all. I just want to get to my machine learning tasks. So I don't want to have to continuously rebuild my environment all of the time. So what I did was basically create a where did it go? Merge requests. I wanted to create an environment where I could start building this application. And so let's actually show you in a different way. If I go to my scheduled pipelines, I've actually got a build container. Job that is just going to trigger that container build. So let's go back and take a look at that. So every week, I'm going to have a scheduled pipeline run that's going to go and build my Docker container and push it to the registry. So let's go take a look at the registry real fast. They go to the container registry. Here's my registry image. You can see in fact the last night it built. So this image is going to rebuild itself with the latest version of all of these libraries every single week. I don't have to do anything with it. I can download my Docker container here, load it into my local environment, and have a new fresh Docker image with all the things that I want in it. This is how we start getting data scientists to build repeatable environments that other software engineers and team members can start to interact with. And one thing you'll notice here, it's a pretty heavy image, too. A lot of these machine learning libraries include lots of default data sets. They include lots of different types of models. This is now an interesting opportunity where my DevOps software engineers can come and help me reduce the size of this and start creating this more repeatedly using slimmer images. All the things that may not as a DevOps engineer don't know how to do. So we've already created an environment where anybody can contribute to my Docker container and help make this environment repeatable, scalable, more usable. So I've got this environment that I can now start to do some of the interesting with. So let's take a look at that Python file that I mentioned that all of this runs. So what you're going to see here is very similar to what I did in that Python notebook, just re-implemented with the new model choice that I used here. In this case, I'm using a library called Carost. I like it because it's very straightforward. If you look at the amount of code that's here compared to my Python notebook, it's a lot less. And this is kind of the beauty of the experimental aspect of machine learning is that we can use Python notebooks to be very verbose and experimental to see what different parameters, what different models to do, and then pass that into my sort of production model here and keep it very similar. So you'll see very similar things here. I'm taking that minced dataset. I'm loading it. I'm taking a look at it. I'm creating and shaping that model. Loading in the training test samples, creating the batch that I want to use and then training my model here. We'll take a look at the output of one of these jobs here in just a second. And then printing out a couple of just data pieces. But one thing that I want to highlight is this last piece, which is this model save. Once we've trained that model, I then want to take that model and export it. There are lots of different file formats that get used for models. In this case, I'm using an H5 file, which is really just kind of like a JSON file that just takes the trained model. It's parameters and package it together so that you could load this model into your environment very easily. And so what you'll see normally with these types of models is that you want to export your model and then host it somewhere so that other people can buy that model and ingest it. If you think about retraining a model, let's say we wanted to train it on digits or on letters, I would then go and retrain this model on letter sets. And I would have it the new model that supported letters. I would then want to pass that model to my engineering team to load into the production application so they could start recognizing digits. The way to do that is to export the model so that they can import it into their environment. And so what becomes nice is that this model can actually cross languages. I'm using a Python library here, but there are Ruby on Rails, there are other language connectors that can read these models and put them into the native language, the application may be developed on. So this is kind of the way that we hand models off to production that can be used in a unique way. So let's take a look at one of these jobs real fast. Let's go into one of my pipelines. I'm going to just choose one random one here. Let's go into this one. So we've got that set up job that I was talking about earlier. Let's go to the top here. Starting with Docker, we're pulling in all of the different pieces. We're loading that tensorflow image, compiling it, pulling in all of the packages that we're defined in my requirements filed. Lots of stuff here. Like I said, these packages are big and heavy and have lots of stuff in them. And then pushing that to the registry. So there's our Docker image that works cool. Glad to see that. Now let's look at this training piece. So we've got our Docker image spinning up. It's pulling that Docker image from the registry. It's initializing, blah blah blah. So we've got the downloading the data set. So there's me loading in all my data. In this case, we're failing to load Kuda, Kuda's a GPU runner. I'm not using a GPU runner. At this point, so that's going to fail. This library likely handles that pretty gracefully. And so we then start to run that Python file that I was talking about earlier. So we've got some of the printed data sets that I was talking about. In this particular library, it comes with 60,000 training sets and 10,000 test samples. And then we're going to start running that through our TensorFlow model. Here is me looking at that model. What it's doing. Again, and I'm going to go through all of this. I'm in fact, I probably can't even explain what a lot of this does. But you can see there's some parameters in here. And then I train the model and get a test accuracy. And then I save that model is a M5 file. Cool. That's it. That's all this project does. But I had a done anything at this point. I haven't saved that model anywhere. It's in my container, but I haven't exported it in a way that I can actually get access to NCI. So let's now start to expose some of these files to this project. So I've got a couple of just sample of Merchor Quest here that do various things. And so I want to go into this model export. So let's take a look at what I'm doing in this file. All I'm doing is creating an artifact that includes this trained model. That's all I'm doing here. And so let's take a look at what this looks like. We've got our original job here. We've got our training job. We saw that earlier. In this case, I expanded the number of ethics that I could actually train it to get to something interesting. You can see that as we go through here, it's getting better in its accuracy as we go. But it's definitely not as accurate as the other one that we were using before. Totally fine, whatever. But we've got that model here. We are then uploading that artifact to the artifact of this job. And in fact, there it is. I can browse it. And now I've got this artifact that can be produced repeatedly. Very cool. Very interesting to see my model here is about two megabytes. This is we train it more. It'll get bigger. Now we're starting to get somewhere where I've got a model that's repeatedly generated with CI. Again, this is an reproduction repo, but you can imagine how we now have an environment where every week, I've got a new fresh Docker container to get spun up. It's running my model. And now I want to actually start to do something interesting when I do this model and make it usable for people. So let's go back to my merger quest and we'll move on to the next one. So now I want to actually package that model together in a way that could be pulled from different applications or from different repositories. So I'm going to take that artifact that I had before. So there's my artifact and my trained model. And now I actually want to publish it to the package repository. So I'm going to create this is the manual jobs that I can decide when I push this model version. I'm going to create a variable so that I can define this version. And then all I'm going to do is take that file. I'm going to push it to the package registry. Okay, so let's go actually take a look at our package registry. Where is it? Here we go. And there it is. There's my model. My first version of this generic package. I'm using a generic package here because it lets me put any file that I want in it in the future. We'll support proper machine learning models here. But for now, there's my file. In fact, I can download it. If I look at other, if I look at the file itself, this now allows me to interact with this package from other software projects. I can hit the get lab package registry. Can pull this model into other software packages. That's a big repeatability when here. Now I can repeatably publish this model and make it available to other software packages. So let's go back to this whole thing. Let me show you what one of these pipelines looks like. One. There we go. Let's see which one go. Oh wait, I want to look at the murder quest. So if that model, let's look at a pipeline here. Yeah, let's look at this one. So I've got this published job here. It runs my model and all it's doing here is pushing that image to the repository to the package registry. Very simple. This took me a while to figure out how to do. This is the piece that data science is aren't familiar with of how to start productionizing a lot of their data science workload. So this is where we want to really help make these things more understandable. CI templates to make this very usable. Now I want to show you one other last thing that as we start doing this, we can then layer on top of their machine learning or other get lab features. Come on, we're a little bit more requests. So now I actually want to take everything we've done this far and I just want to add security is getting to it. This is something that we offer out of the box with GetLab. It's very simple to enable. I'm going to add all of my security scanning capabilities. So I'm going to now enable our secure. Let's take a look at what this actually looks like in practice. So all I'm doing now is looking at all of the security vulnerabilities that I have. And now I've got a very useful list of things of how I can improve the security of my software project. In my Dockerfile, I'm not properly setting users. And so I'm running these things as root. I'm not using specific tagged versions of libraries. So I'm just trusting that the latest version is going to be secure and is going to work and not break things. These are really obvious DevOps things now that me as someone who is not a DevOps engineer can now very quickly understand and layer into my software development lifecycle. I can check this for least secrets. We can scan the container for security vulnerability. So you can see how just by doing some very basic things to start using the capabilities that we have with it in GetLab, we can get a lot more value and build much better machine learning models. So that's going to conclude my demo here. That's kind of an overview and an intro to how you can use various GetLab features within the machine learning context. The only last thing I'm going to leave you with is some of the things that we're working on at the moment. With our MLOps team, we are working on Python notebook enhancements to make those Python notebooks work better. We're soon to move to a model registry. So I showed you how to pass that package to the package registry. We're going to support a machine learning model package. In fact, we're going to try to create a standard around this to create a new way to share repeatedly these models. And then we're going to add support for external for advanced computing. So enabling GPU runners make them very easy to use. You saw where my GPU had failed in my test example because I'm not using the runner that has a GPU enabled on it. We support this today for self hosted runners. But that requires you to spin up a GetLab runner connected to GetLab. It puts all of the work on you. We'll soon offer GPU enabled runners natively on GetLab.com. And then on the applied ML piece, this is us taking machine learning and applying it to GetLab. Our first feature is coming out in 15.4, which is suggested reviewers, which is an novel machine learning algorithm that learns from your source code and your commit history and suggests reviewers for your merge requests based on the changes that are in them. So if we think about what we were doing with that model before, rather than images of digits and labels, we're passing it the contribution graph. And we're getting in the model to tell us back who should be a recommended reviewer. So like I said, that will be releasing in 15.4. We'll then move to suggested labels to have merger requests and issues on Mac, label themselves. And then we'll move to intelligent code security. We'll actually have a machine learning model suggesting how to fix security vulnerabilities and creating merger requests for those fixes. So those are just some of the things that the model ops team is working on. So we're kind of working in a dual track here where we want to make it easy or to run models and use them with GetLab features. And then we want to apply machine learning to getLab itself to make it smarter and more intelligent. So that is an overview of kind of the intro to data science, a quick view of how you can use existing GetLab features with machine learning capabilities. I encourage you go play around with this repository. If you Google intro to machine learning courses or just tutorials, you can leverage this framework to explore some data science models yourself with either TensorFlow or Torch. So hopefully this is useful and interesting to you. Hopefully this gives you a little insight into how data science is starting to use GetLab. And there is much, much more to come from our model ops team moving forward. I realize I have not left a huge much time for questions. But if you've got questions, put them in the dock and I can respond to Async to them. Yeah, we have actually a great list of questions in the dock. If we have a couple of minutes, we can probably take maybe one or two live and then absolutely the rest will probably have to be a think. So me and looks like you have the first one. Maybe we'll start with that and see how long we go. Yes, thank you. Thank you Taylor for the presentation. This was excellent and very, very insightful. The question I had was with regards to the Jupiter notebooks. You showed it being rendered in GetLab. Are we planning for it to be editable within GetLab, using the GetLab editor? Is that a plan in the future? Yeah, absolutely. That's what I'm vaguely calling Enterprise Jupiter notebooks. Basically, there should be a software environment behind that image that imports basically just a Python environment. We want to enable you to spin up a environment with GetLab runner to basically press play on that notebook and then be able to edit interactive with it, replay cells. Very similar to Jupiter hub. That will be a paid ultimate plus feature for customers. For now, we wanted to get it just so that you could render and view them, but we definitely want to create live running notebooks. Excellent. Thank you. I don't know if Leo's going on. Oh, there you. Leo looks like you have the next one. Yes, actually, I just wanted to know. The model that you were using, does it use pixel pattern to identify the numbers from the images? It's not using pixel patterns from the sense that nobody made those, but it is effectively what that model is doing. Basically, we handed it image and we tell it this is a one and it reverses to figure out what that one is. It is behind the scenes using something that would be similar to what we think of as a pixel pattern. In fact, in that last image, in that Python notebook, you can see basically what the machine sees of that image. Now, of course, it's not actually looking at it's data representations, but yeah, it's a good way to think about that. Great. Did you have any follow-up there, Leo? Sorry. No, I appreciate the insight. Thank you. Thank you. Probably just one more, and then we'll take the rest of the thing. Cherry looks like you have the third one. Yeah, thank you, Taylor. It's a great demo. So, I'm working with the customer, and I have seen it with other customers. They're different, they is being the work and the pipeline for platform versus the little version. Did you know the word, Risa, I can't even say that word to I'm just wondering, is our applied ML? Is it really a consider from practice point of view as a delivery pipeline? Yeah, so it's a fair way to think about that. From the standpoint of like a customer, when they turn on suggested reviewers, that's not going to be something that you're going to have to see all of the workings of these ML ops workflows. Behind the scenes, yes, we do have get lab pipelines that are extracting data, they're transforming in, they're training a model, they're inferring them all. All of that's done holistically within get lab CI, but customers won't see that. There will be a simple job that will trigger our internal pipeline, and they'll see those recommendations within their UI. But yeah, that's effectively the right way to think about it is that it is a production machine learning pipeline. And that's a building with applied ML. Cool, so there work on something we call that the next best conversation. Rive is to engage the customer and the present offers. So it's very interesting. We will get you to talk to this customer for sure. I think just lighting up all the dates, that will be great. I like your example. Yeah, that's a good plug for me. If you've got customers who mentioned machine learning, they don't know how to get started with get lab or maybe they're already doing something today. If you've got those customers, I want to talk to them. I want to have them help shape our roadmap as we develop here. I'm always happy to talk through both this example and this presentation to talk through some of our roadmap with these customers. So don't hesitate. Thank you for that. I will reach out to both the time. I paint you a few times, but finally we have some dates that will work for you and us. We had some sessions already. I think they were on the right time to look at their pipelines. They did a starting permitting data. But of course, that's a lot of challenges. Yeah, absolutely. And I mean, as you've seen here a lot of this talking with customers is just introducing them to get lab features and how they can use them in a data science context. So it's a lot of DevOps one on one with these customers teaching them how to use get lab. Awesome. I think we're about out of time. I will respond to the rest of these questions in this Google Doc. I really appreciate all of the questions. And hopefully you've found this useful. Like I said, feel free to use this repository, however you like. Do please fork it because I will use this for my own customer demos. And that's all I've got for you. Wonderful. Thank you all so much for your time. Thank you, Taylor. Great presentation today. Awesome. Thanks everyone. Thank you all.",
  "And actually did that hero. Cool. Awesome. Nice. Right on. Yeah. I had been doing contribution scenes back in 2019. Really? Cool. What you're being a good player, being a good player, it's pretty cool. What's your favorite areas of work in on the app? Get left the platform. Yeah. Don't figure your stage. The last contribution that I did was Alina feels that there were missing on how to deploy a Kubernetes cluster with Terraform and give that to CICD. So that's something that I've been working on. And it's like the, yeah, the, Most of it comes out. I also had another experience with Terraform. That has been the opportunity to To be using a kid luck for infrastructure. Oh, right on. So we don't have very few people here today. Oh, wait. As I say that, new joins. Can you. Getting started here. A couple different announcements. I will read them since I got the second one and the first one is from Nate who's not here. So Nate wanted to mention that. He's now back in maintainer. That's great. And also all announced on behalf of Thomas is welcome back for a few on the dynamic analysis team. And I mean, you've got the comment on Thomas's news item. Do you mind verivolizing? Yeah, absolutely. So it sounds like some of the team was curious about application security as the main and how to get some more training. Something that came up was we do have a really neat platform available to us. It's called secure code where it's actually a vendor that we've integrated training with. So if you're in a vulnerability, that's offered as one of those training opportunities for our users, which is really nice. They also have a very extensive platform. You can choose your language. So Thomas was informed that not only do we have access the group has extended access to all engineers. So there's I think 700 people. I might be able to rest of my videos of boatload of people were added and given access. It's good blood. It's good bandaged. So pretty much everyone in engineering shouldn't have able to log in with SSO into secure code warrior. I additionally shared that with my team. Thank you Thomas for raising us. It's great. It's a great platform. Which should become more familiar with it. They also host competitions. This is something we did like a year ago. And we had like 25 30 of us all took place in this competition where there was multiple rounds and you get points and there's a leaderboard. That's something we could also do. So as a scene of my team might be directly interested in having like a mini competition as a good lab specific training project that you can get into pretty quick or you can build your own. So you definitely explore this. Need. So second answer. I used to post a link directly when one of our teams does something interesting. They think a general public want to know about so often you know announcements or a YouTube video etc. So I've started in that directly and would like team members to post themselves and I'm happy to repost and promote. I can also put the text and formatting. You know this increases the reach of the notice of the cool things we do by not just being Wayne and also. I think LinkedIn maybe has labeled me as a spammer perhaps. So the more other people post unless I do and I just announced. Promote those messages that would be great and some have already done that but if you see a reduction in me and me nudging others to post that's why. There, anti abuse teachers might have got a band you. Perhaps perhaps or perhaps just people are sick of hearing from me on LinkedIn which is fine to maybe it's humans labeling the as spammer's not systems. Yeah, we'll see. Do if anybody's curious why the change that's why. Anyway, we want to volunteer. These items that I copy across. To verbalize it. Let's see. Lisa's that we're not triaging on the community manager class because the number of MR coaches. So that's an active number of on triage. These consider making yourself a bill if you are a coach. I knew you have a comment on that. Yeah, again, I appreciate the reminder. Somebody in their personal goals well, brought this up. I've had effective coaches on my team in the past. So I've had my current team member. They're going to reach out to Alexander. Terrence fee I recommended them as a liaison he's got a great experience they was really effective and I'm sure they still is. And kind of navigating through and pushing forward requests and it's not like the coaches have to take on this work. It's more about disseminating getting this information in the right hands and in the right teams as part of that role as well. So what I'll do is I'll ask my team member that's engaging in this to additionally share kind of their experience. I think that'll help further encourage because I think once you've done it and you get into it, it's not that big of a deal. It's a great way to contribute to the community. Just the triaging just the making sure that ends up in the right. You're a handshake. It's I think it's both it's an opportunity to kind of pick off work that you're familiar with and you want to help push forward. But you can also help facilitate finding the right the right team. Because some of this just goes into like a bucket. CC MR comes through it gets flagged as needs attention and then the coaches kind of evaluate and say okay what team is most. Similar with this scope this context versus just trying to take a song and make it more muddy you know they can get it to the right hands. I'm on right. I'm 100 here here. I have you experienced some of this burden before not finding a review. I'm a relevant area. I'm going to. Can you repeat this question. The question is around community. Merjory requests not being reviewed. That's sort of an amount of time for a really gain them into the right hands to be reviewed have you experienced any problem with that. No. No. Okay. How about all 100 just general support like you know we have SLOs internally and we try to support those for CCC requests as well that you know within two days responding we're kind of keeping things moving. How do you feel the support generally been. I don't find that a perenn are like. Like the score for. And get that. S I yes. It's like every time that I use it is mostly available so. Yeah, I never marks all of having any issue late. Legally with. Having like about requests with that or. Or something like that. Cool. It's good news. I think this came up from leave because there are metrics that show the the turnaround time the SLO. So and I think it's similar to our maintainers is the ratio is an exactly what we wanted to be to provide the the best assistance to that group of people. I can't hear you win. You're not muted though. Nothing. You can move forward with the agenda if you can't figure it out next minute. We're waiting type it out in one of us to be blessed. Okay, one more try. Yes, there it is. All right, well, I'm not sure what's going on with my zoom today. So thank. So I'll handle so you've done some changes to our documentation. Which is great. Periods J or near or fill. Do you have to know many places in our documentation for your areas of the product that. And you can point our hundreds and if you've been interested. If I'm 100, if you can review and see if you have any ideas on improvement. You want documentation links for. The stages that we're working on all specific features. Both in areas you think need more improvement versus less perhaps. Yeah, we had an issue with a list of documentation that we want to try to do. Neals not in the sea. Let's see if we can find that and we'll point it to you. This is something to go to the R. But I can definitely take that up and provide that. But yeah, there's a list of docs that we're going to review in the same manner. That's where you're looking for when this general. Okay, yeah, this would be more specific to our sack or threat into the environment management docs, but we can accept from that. Peter, if you're interested in our home, but you've done a great job with updating other parts of the get that documentation. So we'd love your feedback on these parts as well. If you're interested. Yeah, for sure. Thanks. So. Do we want to do a periodic maybe every quarter every two quarters virtual off site with everyone on the team optionally invited and the basalt leaves are optional. The agenda could say looks on the light, but I listed below. I've done a few of these with my boss Christopher with my and my. And my peers. And also I participated in some of your works or CTUs just on the left and topics, not I wasn't invited the whole thing that's overall they facilitated. Greek information sharing. Collaboration and relationship building. So some potential topics and update from fill, mind, Thomas and I under areas of responsibility and context. What we see interact with the job side the team perhaps each engineering manager does the same click over view what their group does and what they're doing and with their channel and bizarre mother excited about. And also volunteers from anyone on the team to present a top of the interest. Who get a guest speakers from other teams as if they're interested in also you know a fun activity or two thoughts should we do simple like this. Generally, yeah, I think I think that'd be great to have them all press. What would you put that would you put that like in the middle of the shorter is that way, people are kind of going already and not trying to wrap up for spin ups. It seems reasonable. It also not towards the 22nd, you know, not nearing the end of a release. Got it. So I have to do the second version of the quarter shifts. Neil Philardy. Sounds like a great idea. I know some of our product counterparts have virtual off sites this week because they're doing that to plan for the next quarter. Preg, we would do us two weeks prior to that so we could feed and talk about what we're done and what challenges we get. Yeah, towards the end of the quarter could be quite good. Yeah, I think it's great to do as well. I think it'd be an extension of this session more concentrated more dialogue focused as well. You know, here we're doing like a lot of agenda topics getting feedback, but you have to be neat to have like specific topics to discuss and depth. Great. Any volunteers to organize. In the ends, we can be no. Would we now be looking at doing it towards the end of Q3. Or the middle of Q3. Middle of Q3. Yeah. We've got a week before the 22nd. I'll try to measure and we can collaborate. I should tell me. Thanks. I know we only have a subset of the team here today. It'll let us all collaborate. Awesome. The other writers we have in the agenda are read only. So we'll leave them there. So I know Alejandro you've only been. This is your second meeting as a shadow. And then throughout the day any early impressions as of being a shadow and also any questions for Jay or Neil or Phil. Yeah, I'm the first impression that I have since the first meeting with them had this morning. And the way that give that documents is meeting like. For keeping them. Everything there is a. Falcon on on the meeting can be. Available for. For all at the persons that. That where able to join or. Or something. That helps to be. To be. And I think we're not communication in style. And I don't really say. I feel like it's really amazing. The way that give a does that. Great. To I've got you. I'm by the way, thanks for finding that issue on. Terraform and that our terraform we're not making recommendations on terraform files and code suggestions as we try to do that since you know terraform and I tried to demonstrate the feature for you. And I think that's what we're doing. We're doing it. And it's a. There was a follow up question by the way in that issue of the file the file type. So might be a. What the file type was called it said or I've and how that gets sent to the back end. You could respond to that when you get your answer to the great and again, thank you for finding that issue on that. Yep, no, it's not that will be my fairs. But reports and. Great. Thanks everybody.",
  "Okay, I have you one. Is this YCD wax, um, me T, 2024, January 17. Um, let's see, I'm going to start from the top. I actually have to move my plate to the bottom. Yes, that's a question for here. Um, and I think, yeah, we start with the, so we do not go out, you know, thank you. Thank you. I have a few updates. One is that I'm almost done with the competitive evaluation for fleet visibility and I did link the issue, which has the FIG jam file in it. Um, it is actually very, I hopeening to see what the other products are doing. I won't mention their names, but if you're interested, take a look or just wait for me to post the summary. It's probably gonna be faster if you just wait. And then a couple of things that are coming up. I wanted to get some eyes on to see if anyone has worked on something similar. One of them is that for the fleet dashboard, we're going to be adding a filter to filter by tags. And it's going to be a global filter for the dashboard. So it will filter all panels in that dashboard. Um, if anybody has worked on something similar, I just, I would appreciate your feedback on the placement of the filter there. And if it's clear. And then the other issue is a CSV export for time period. So we have, and the fleet dashboard, one of the panels is allows you to export the usage information as a CSV report. And then from there, people usually take that and like create internal dashboard or filter using whatever other tool that they're excelled or something like that. And now we want to allow them to choose the time period rather than it just by default being previous month. So I'm going to be working on that in this coming milestone. And that's more of just sharing for awareness. Yeah. If there's a good. Now I'm going to talk about the the filter while the filter placement. I'm trying to find the information in my brain, but I think when when we work on. Was it an environment dashboard was. Yes, environment dashboard. I think so. It's just that too many features. I think it's better if I just write down because I need to put an information in my brain. But I do have some feedback about this and also just some. Social validation that was done before. My question here is you have the the filter. That drop down component. Yeah. So you're thinking the future you're going to have more you know elements there and you why if that user can interact with. If we do it would only be one more and it would be. It was time you could filter by some type of time period. Those would be the only two that would be able to impact every single. Data point on the dashboard. But have you played around heavy instruments and with the that filter bar right the search bar where you have like the tokens and chips. Like what we have in merge requests the issues view. And if so why was the drop down. The option that you. Sorry everybody it's like dinner time. I've been to the computer for too many hours and like I'm like. I'm going to need to find out. But anyways, why what this instead of the Mr. Bar. Yeah, so we use the filter bar on the runners list view like the typical view that you come into if you go to the runners page. And we received feedback that. The experience filtering there isn't ideal because with tags. Specifically with tags every single time you go into that bar you have to add another tag. You have to say filter by tag add the tag and then say filter by tag add the tag and there's typically a lot of like there's pretty common sets of tags. So by allowing them to see that in a drop down. I don't know if I added a search bar in the this the menu. Yeah, so you could search for that tag and kind of like stay in one place and do it all at once. And then we just found we've also heard that it's not as discoverable when it's in the filter component because you don't know what filters are available to you. So I'm starting I haven't really edited what that page is going to look like in the future, but at some point we will. I'm going to improve the filtering for that list view. I don't know when it will be. Yeah, got it thanks for the. Take sort of insights. Yeah, I agree makes sense like you don't know what you can filter by right we repeat that. I feel there to search bar. Yeah. How about you also like show fields like below this. I'm mentioning all the tags that are selected because it's not a crowded space you have space. Yeah, because I mean, because this is the only thing that it's being filtered by and you just not being able to see all the tags which are selected. I would say that is problematic to click on the drop down and then get an idea about what this filter is all about. True, that's a great point. Initially wasn't planning on showing them all, but I think I could play around with just showing like the chips. Yeah, like the actual labels underneath. Yeah, I think that's a great addition as we to come and chin and then I also have just really really minor feedback. I just like I like that you're starting to play around with the drop down because I also could hear from you that the concern of using the current filtering bar patterns. Throke it love and then I know that we have a little bit of minor usability issue on that so I could totally get it. But I think my problem when I saw the design was like because I didn't think that the drop down is connect to the whole page. So maybe you could consider I don't know I'm just thinking out loud maybe adding a small borderline below the drop down so make. More visual higher to keep there would be helpful. Yeah, that's a good idea. I've also noticed a lot of other filter like when you use the filter component there's a gray background. Yes, I could try that out to. Yeah, maybe that would help like I guess, but I need to I need to see but you can definitely play around with it. Okay, yeah, thanks for the feedback. And then the one minor thing I wanted to say is I would be curious if people would clue into that being a filter more if it was on the right side over the left because I feel like filter and sorting place and get lab always seem to be on the right and I'm wondering if that would make it more clear or not. Okay, I'll try I'll try that too. I wasn't planning on putting this through solution validation. But now that we're talking about all the feedback I'm wondering if maybe I should just do a quick on. Unlaterated test. I don't know. Like I don't really know if I want to do that. I think it's I think it's going to be good. I just think if you have something coming up like any research that you have coming up, you can just like plug this question in. Yeah, because you implementing the very first version will not be problematic but later for making improvement. Blood this question into any research that is already scheduled. You don't have to like do something like dedicated to this. Yeah, that's a good idea. Okay. Well, thanks for all the feedback on the spot. I appreciate it. I will give it to Emily now. So I only have one announcement since I spent last week watching all the unmoderated interviews. What we've closed off the Kubernetes dashboard views with our improved preview that we were testing. We found out that users really liked it. They were able to troubleshoot, especially like the pod area a lot better than they were. Before using the single stats. So it's a great addition to the cluster UI. And the next steps are actually to work on moving this UI under the environment details page because we tested this on the environment list page. But we know we're running out of kind of space that page has a lot of problems with things loading and adding a big tree on that page is definitely not something we want to do. So we want to test out moving this over. And then also crafting up a more complex version of the tree just to see how it scales. But overall went really well. And I wanted to give a big thanks to Will who recommended this unmoderated test. We were able to complete the study over a holiday period, which I know is usually a slow period for user testing. And we were in to initially accepting to close this off in 168, but we did so thanks again, Will, for the recommendation for this. Yeah, thanks Emily. I did want to add here that you know, I think this overall process was really valuable to go through because. As I wrote the doc, we were originally planning to develop a whole another page. So we're going to have to like go through foundations team to try to get approval. I think Sengen is probably really familiar with with this process based on everything that happened last year. But through like Emily's design process and like collaboration with the P. and Victor. We ended up deciding like let's just keep it on the environments page instead of like having another page. But we, you know, have to like surface to users. So overall great work. If no one has any questions about this, I can pass it over to Vintka. Thanks Emily. I was looking for the link to your design. When I got lost in the position of the addition issue. Let me, let me post link to the figma actually because there's a tiny figma links lost in the issue, but I think it'll probably be easier if I just link directly. I mean without looking at the design, I would just share why I was looking for it because. You mentioned a good point like overloading a page is definitely not a good idea, but I was looking to understand if there is a possibility to provide a toggle between two views. Yeah, well, that's actually what we're looking to do in the details page. Like I've all between two views or two views you can access the problem with the list page right now as it has already so much information that I think added more information into it. Yeah, me, but it's the table overlocked page. Yeah. I just saw the tree view design also and I think it looks really it reminds me a little bit of the pipeline view like the I don't know what we call that tree. It looks all like a tree graph. Yeah, the graph I guess. I think it just looks very good lab. Yeah, I thought you did that was my first reaction that this looks like a graph from good love. Yeah, I used inspiration from that pipeline jobs graph actually because it's the only one in get love. I knew that kind of related size like I'm going to just use that as inspiration and see if I can work this tree into that design. So there's still a few small UX issues like think we have to clarify the arrows a bit more but yeah overall starting from there I think was a really good starting point. Thank you everyone. Yeah, coming to my point so Miran I you were able to make big changes to the way to boost management experience. I know that we are still calling it beauty find a far UI but we ended up like also doing some back in work. That was long pending which was around adding descriptions for variables so that it's easier for someone to you know find a variable that they're looking for because they might be. One with similar like a bunch of variables with kind of similar names that sometimes users create and it's very difficult to understand it is. Through a glimpse that which one was meant for what and why they were created. So now that's going to be easier but that's just one and we have made improvements with validations added a lot of them. We have improved almost like a bunch of health decks and changed than the interior the behavior of the input boxes made the drawer more pajamas compatible and also like contributed to the guidelines for jars in pajamas as a part of this. Overall this was I think a very successful one and we enjoyed a lot so expect a blog from both of us coming soon. The next one that I'm looking at right now is most dream research. I know that I've been talking about it in a very long time but it was really difficult to get people to speak with but I'm just very happy that finally we do have people and this is for the first time and. I would say ever that we have so many people who are willing to share that feedback about most trains. I'll be concluding this research this week and posting the analysis next week or like mid-off next week and I have a lot of insights that are valid for very different teams so just stay tuned. And that's all. I think I can take over and then just voice a. Bonnie's. Notes here she could use not attending. I think most of you have seen that you share the job token. Designed right in the CACD UX. And oh sorry this is the CACD UX. I'm talking at the Google level yes that's exactly the one so. If you haven't had a chance to provide her feedback. Please leave a comment over here the designs. I'm just dropping a couple of comments here and actually press that you know that she managed to use the design patterns and the whites it's really polished. I think there are a couple of things that we can provide feedback on especially on right. I'm just going to ask you a question on. Ording and then smaller interactions. But overall through nice to see her first prototypes her first design. Issue out there in the wild so. Good job Bonnie and also thanks for being there for more supporting her as her onboarding buddy. She made a. I don't know. The way she like explored all the different use cases for this feature and went into the real depth something that we missed the very first time when this this feature was being built on at project level. And we kind of built upon it over time but looking at how she made all those considerations. Before her very very first draft it was really commendable so yeah I appreciate our work on this. Yeah really cool. I'm going to leave a comment here in the document but also in Asia for her I think a good next step is to you know move that acceptance criteria and the comments that she's living in the designs area to the issue description because right about the behavior about the rules. Just the over our functionality it's good to have that in the Asian description. But if you haven't had a chance yet to review it please support Bonnie. And as you also post some questions and scenarios about the CI job token work. It's in I think is the validation issue yes this is the solution validation issue. I think you had a chance already right we to work with her on this one. Right yeah the noise was a really good question so she still has open now. I think Marcil and I we answered most of the questions almost all in fact I invited Marcil because I did not want this to be heavy on my perspective I just wanted someone to challenge what I had written. And yeah I think she has all the answers but still if somebody feels differently I would really welcome you to post your answers here. And Bonnie let us know because you probably watched this later on. And then there's two more point to additional points here she's also mentioning an next steps for that CI job token issue is to create a validation plan and start from the clean the validation so she can refer to Emily's word for dashboard on this one. Emily do you want me to voice a comment you want to voice a comment I never know how to what to do yes. It was just a funny needed any context or feedback around the solution validation that I just did a feel free to reach out and I can chat about it a little bit more. And I just have and if I know not really it's just a comment to tell her that she should create the recruitment issues so because it's not easy and get in touch with Erica. Is it Erica or will supporting the. I've been to Kyra to do to you. It's Erica. Yeah, it's very fine. Oh yes, since she also worked on the scar card and she referring to the one had a chance to see that. And I've cost more aspect that one of the suggestions of the recommendation that she's the she. That's a very good question. So, I'm also good to see right that. We'll mention an engagement also for customers on the community on the work that she's doing so think overall what he has been busy she hasn't had to just join this call us. But it's nice that she's also able to update us here about work so. So. And then if you don't have any questions, I'll pass it over to the studio. Thank you, Sian. So I have to item so the first one is about the think because I shouldn't that we did last week. So it was really great. We didn't chat about any design relevant elements because it was intended to just become a more of a technical discussion. Who was full of this question around how to use API should we change architecture. But I think it was great that I could hear a lot of from back in engineering perspective as well around this project. So. Okay. So it was really cool and we wanted to make sure that we're aligned on this in terms of the project planning perspective so we're working on the proposal from the back end architecture like planning so Bobbi was working on it and then we're also discussing. Shortly after the call and then we're keep discussing around this so the long story short while we're discussing at the moment is like how would the CIC catalog would like to become is it like get left catalog that includes a lot of different elements inside. One catalog or do we want to speak to become a Gillette CIC catalog that includes the ICD components with the city steps the city template type of component. So it's more of a discussion for the future vision and I'm happy that like we're discussing this so that we can make a proper design and product planning so that we can work on working towards the roadmap. And make sure we're aligned on the same goal so it went pretty well in my opinion and based on this we so it's only the empty issues so far but dove and I started to think about like we should validate if this vision really makes sense or not so today I just really quickly create an issue that the research issue service and deeper now. But I would probably need some of your feedback because we wanted to take this opportunity to test things out that is not exist in the market as well as it's not part of the product but we wanted to make sure if it is really aligning with the users needs or not. So it's that and my second item is also more of an FII that I also went through the solution validation it was during the holiday season and it was a moderated test or went fairly like well and. The good thing is so we wanted to test out the different visual by representing the component so currently if you check the CSE to catalog it shows one project in a row but we are asking ourselves like is it better to show the component inside the project. So it seems like it was well received but I I taught in one unrelated question to this research that how would you filter and find the components that are created by your colleagues to test out the filtering experience and actually we got a lot of constructive feedback on this we need more immediate action item. So yeah, but it was a good test and thanks Eric, if you're listening to this call for helping out planning the research issue. So yeah, if you're interested in let me know if you have better idea on how we are going to improve the filtering experience I think it would be a whole topic that I may want to share in the CSE to review meeting at some point. Any questions if not. I don't have a question you mentioned that you would want some help like some feedback but where do we go to provide that. I don't have a really well described issue but I have created one empty issue so I'm going to link it here. Where is the past sorry too many meetings. Thanks, Bill. Okay. So this is the issue that I'm going to work on in the next one or three months and I want to really revamp the whole filtering experience for example on question three needed tap here. And the second question like how does this filtering tap with the search full working well together and I'm pretty much going to start about using this pattern so I'm happy to explore alternative UI for this. Thanks for asking. So move over to hyana. So switch word updates we're seeing hiring. And the update I want to give to the team is that you may sort of change is to the role requirements that you can find the kickoff issue but also they'll review with you because most of you are part of well they're really team right. So the big change is that we are only interviewing or going to be hiring intermediate product designer so when assessing right in interviewing the candidates and also fill in this for a card. And we are moving them to the pipeline into the interview process considering them intermediate designers and also make some updates in which the priority of some of the skills both the soft skills and hard skills for this role so as you can see here. So 3 and 5 three to 5 anyways top 5 the theory skills is foundational research. I think for switch board there's a lot of have a lifting that needs to be done. And a product designer inexperience in research is really important they need to be very strong collaborators with you know the image and new in all the designers. I'm mainly looking for some on the has worked with the design team before or at least you know had some fair designers. We have interviewed designers that they were like the design team of one they were the only first designer in you know a startup etc. I think that especially for if you think of where switch board is right now where we are getting started with a lot of the core functionality of the user experience someone that has you know a bit the more of experience in collaborating with our designers to get the understanding the insights. All of how to use a design system that they are used to you know just figure in things out with a team think that's essential. If they are not already technical they are interested in the nation and CSS. So I'm sure if in your score cards you're interviewed you talk about that about those skills but. I would like them to at least. The you know proficient in basic HTML and CSS that mean to they're going to have a better understanding of design interactions that they will be able to communicate with the front end engineers right about the components that. When review merge requests they will just be able to jump in and. Do the work that we've done. And I already mentioned the usability interactions that we did to design system and experience working on complex products before I was really looking for someone that had the depth to experience. That's always tricky because it's a little bit such a specific work that you all do here. So complex products that can be like working banking you know and compliance in anything that really requires them to dig into this. And that's a messy hairy problems and showed that they cannot lie design iteration. To you know, propose a motion that they can use research skills that they think collaborate with their teams anyways involve engineering in their design process so. And I'm not sure that's what I'm looking for. And then I still have skills technical skills in general right. Like anything outside the scope of HTML and in CSS. As I mentioned, really nice if they have that tool that's to be experience maybe they're a user of GitLab maybe they I don't know the coming from one of our competitors. And also body and platform workflows or admin platforms experience that's also nice to have I think before one of my mandatory skills was that they had experience in onboarding design. But very tricky to find and combine all of that I think it's we make some tweaks there to make sure that we're not finding the right candidate on paper but that you know we find the right person for the job and that for the things that. They they need to improve this was an engineering proof that we create the opportunities for them to gain those skills. What changes I think to you mainly is you know only intermediate so if you think someone is at the intermediate level but they're like I don't know early intermediate or I don't know a very strong intermediate I'll like you to highlight that in your notes. And also when I say intermediate is like a GitLab intermediate right because a lot of other candidates that's. We interviewed recently especially end of our last year which was like two weeks ago. They had a title senior but they were like not GitLab senior right so it's difficult to not difficult but we have to. Kind of set that expectation with the who the candidates so I'll pause now see if you have any questions and if any obviously since so yeah that can I line. The expectations for the upcoming interviews. I just have one question I really haven't been getting any interviews for this role. So I was not sure if that was something was missing there or I was not sure. Yeah that has been the challenge and it's also why. We were making this changes to the the requirements. I've interviewed only a I don't know maybe. Six eight candidates in the the last four months that's not a lot right because initially the candidates that were being as creative that were qualify. They were and then that's going to answer to will question to expensive or there were two senior right they were good designers but switchboard is an area that. If if we put someone that is too senior they're going to finish the work like in a year and then what. We cannot risk like getting someone. Put is a one there so that's to be born right they it's a it's a problem area where the designers the design and they need to have them for growth right and need to be able to learn also in the fly for example. I think that's the experience in research but have never worked with a research team so they're going to collaborate with will right you going to learn something. So yes for this this role we haven't had so many interviews we had candidates that were screened and we're qualified and they moved to the team interview I think. VT coming me soon during interview a few of them but then rejected. After after a couple of of interviews. I think that's the first question that's great. I think you you kind of answered by first question so I did add a like another question. And then you know you've done you know you mentioned six to eight different interviews with some like more senior candidates. Are there any. Concerns that like intermediate candidates will still be able to match up well with what you're looking for for those top three to five. mandatory skills? Yeah, I think so. And that's the assessment we've made after having these interviews, right? And after also looking at the book of applications that we had. Right. I'm sure if I remember at some point I mentioned like I'm reviewing a thousand applications because a lot of people were applying. And as I mentioned, they are seniors where they're like not good senior. And if you will have the experience working complex products that we need or understanding research methodology etc. I do think we'll find an intermediate candidate at this level because farm and but correctly like Gina. Gina Emily and Tunjoon, right? You, the three of you, join as I think to be the level, right? But you all came with a very strong graduate, very strong experience in various specific things, right? And another thing we have to look at is that this team, you're all seniors, right? So looking at the business when we hire yet another senior in this team, right? There's all you're going to be so many staff and manager opportunities in the company. So we also need to think about that, right? The career path and development and room for the road for a new person in the team. So that's really it to the point I mentioned about, you know, compensation expectations for people there to senior depending on the location factors but also budgeting and career progression in the initial. The, as a two question or will. Yeah, thanks for going through that or me. And of course. And then a question. This is a little bit out of scope and if it's something that we can't talk about in the recording also in time of that. But as you mentioned that there's a lot of seniors on our team and that there's, you know, not every. The growth opportunities are only going to be there if there's like a certain amount of openings and etc. Are there any is there any work going on to maybe change up our. I don't know what the word is, but basically like the growth opportunities for product designers because right now like you're saying you're senior and then you're either staff or manager is there going to be like other opportunities in between that. Yes, I think we have a, I'm not sure if the merger class was version of things, so but I know that Valerie and and better to work here on the principal product designer, which is staff plus. Right, similar to what engineering and also product has that something that Christy was also working on because we need to make sure it's right that. Let's say after. Actually, you get promoted for example to senior product product design manager, you could become a director, but then what's in between right. It's a limbo that. That we need anyway, just this, a clarify and I know that this is part of the discussions. That leadership had at least, you know, last year, I'm not sure yet when this will become a. Let's say something that is in the handbook right of a job. Because we also need to have the business need for that. Right, so we didn't have staff designers in the past, right, we also didn't have senior design managers, so as we grow. We also have to make sure that we are not. We're not only adding people there are too senior because that also means we might lose them right at some point. That's all part of the changes for this particular role and also mainly because I think switchboard is complex, but it's not as large as for example. Verify right where yeah, you have a very well established set of functionality that yeah. That will require expertise that requires more senior designers. So yes answer a question, but I don't know what. Okay, thank you. Oh, the. Um, thanks for your questions, by the way, and I know that we are at almost a time, so I'm just going to drop my question here. I discussed this with some of you already, but. I want to see if you're all can help me. I have to disemble it to switchboard by the way, but I have to complete a UX score card as part of a Q4 okay are. That will require me to validate a job to be done right run a score card on a job to be done for a product area outside of CI to the outside of switchboard. I can pick anything, but I want to pick something that relates to verify in release. I'm sorry, verify any environments. It's an a room number that that changed. And my question to you is. Do you have any suggestions do any recommendations on. Cross stage job to be done that makes sense to be used for a score card that can help us, you know. Experiment or at least a sastic with the that that user experience. Taking that the recommendations will be beneficial to your product areas. That makes sense. It's late. What do you think. I have the same suggestion here that I shared with you and everyone on one. You help me like come up with this. I discussed for the designer with the other team of the other team which is go interviewing that's how like. I did not have a chance to find this for testing the usability of merging experience, which is a very wide experience. And I can totally give you the highlights the tags from the up to you that validate this. I would just need some time maybe tomorrow. I think appreciate it. I was thinking, I didn't discuss this with you, Sunjian, we didn't have time today, but I was thinking of a running job to be done, of the Reyescore card, also for one of the jobs to be done in Create, but related to the pipeline, like the editor experience, right, either the coal suggestions or using, I don't know, VS cold, whatever, using extensions, because I think that could be interesting to you, but they don't have jobs to be done for that category. So yeah, that's the type of thing I'm looking for. So I'm thinking secure could be cool. I think that could overlap with verify, just thinking about security widget, vulnerability, that all can expect to like, commit in pipelines. Yeah, that also makes sense because I think that's the way that you will or the Erica that it's Erica, that was working on a study or I think, and I think you see eye adoption and something about that related to related security and compliance work flow. Yeah, it's definitely Erica. It's Erica, right? Yeah, okay. Anyway, I forgot that, you have wheels. Part of the agenda, I'll just, oh, for now, unless will you want to, yeah, voice up coming here. Yeah, I saw that you added the link to the just to be done. Yeah, we'll file, so I was just kind of like, scrolling through it. And I came across this one. It looks like it hasn't been validated, but it touches its technically under distribution. So that's involved with, like, actually installing, get labed and then it touches on Kubernetes, which is covered by the environments team. So it looks like there's a couple different, like stages involved. So I don't know if that might be another area to consider, but unlike the one that Vita got had shared, I don't know if this is like, it's got the evidence behind it as much as hers does. Yeah, thanks, Will. I linked here the, the, the, the objective on the file. Yeah, I'll do some digging. You could just, you must be some additional information in, direction pages or these categories, this area, but appreciate the feedback. And then over to your world. Yeah, I don't really have a lot of updates. I did want to just add to what Emily said, earlier about our solution validation study, that I think she did a great job collaborating with Victor and myself on this work, and she was extremely efficient getting everything done, especially around the holidays. And she also did a great job like running the study in user testing and then providing us with updates on like each participant, asking me for feedback on like whether she needed to like, you know reject certain participants and kind of get new ones in, get higher quality feedback. And then she also consolidated all those individual sessions until like it overall. Summery, which was really detailed and awesome to see. But that's kind of it for me. Thank you, Will. And Emily, thanks for the shout out. Thanks everyone. Yeah. Yeah, I never worked. I mean, we want the, never, we wanted to, never, we want to. It's going to be forever on the internet. Thanks everyone for joining me. And first thing a little bit of time. Hope you enjoy dressing today. And that's me, Linda. Bye.",
  "Okay. It works. All right. Okay. Last team meeting of the year. I just want to start off by saying, it's been a crazy year. Thanks for being a crazy couple of years for everyone. But yeah, I'm really happy that we've made it. We have the people to be working with you all in 2022. Looking forward to much more next year. And I just wanted to ask this is anything that you have in mind that I'd like to share a ball's what you're looking forward to. Work related, no work related in the next year. For me, for example, I'm really looking forward to the GitLab. What are we calling that now? Are we calling it summits? We're calling it contributes summits, right? Yeah. I really hope it happens next year. Making all my string blends around it because it'll be super cool to make you all in person. But also, yeah, just to meet again, some of other colleagues in the subject. I think that's a high light from my GitLab. GitLab was just but also to higher the designer in switchboard. It's been a few months and boredom. So these are like my plans. And also something I'm looking forward to. And it can be quite annoying to have your manager ask you like, What do you want to share? But does anyone have something that they want to share in this last year? I'll have to say, was summits as well. So now I'm going to try and think of something else. But again, that's one that I really want to share. I have met many people from my immediate team. I have met a few people from our team, but it would be nice just to for the first time ever to meet like a big jump because of people I work with in person. Which will be really nice. I'm looking forward to figuring out what fleet visibility will look like next year. So like the first time I feel like I'm dealt with a brand new. Not really a brand new feature, but it's a new category for us. So it's the first time I've dealt with that. And I think it will be. Very fun to collaborate with a lot of groups because it's really cross group. Yeah, so I'm looking forward to collaborating and figuring out what that's going to look like. And some it will get on the summit train. Eric, could you want to say something? I was just like, I didn't want to. I said, no. Okay, then let's just be grateful for Eric. All right. This year and more Erica next year. To. Awesome. I think, yeah, I just before we jump into the agenda, just want to say I was not going to take some time off next week, but I will because it's just today's. And in case. Any of you, I know that Emily asked for my coverage for a few days, but if anyone else, that's going to be watching this recording. If you need a me as your backup simply assign yourself in get lab as busy and I don't forget, you know, to do that and also link to your coverage issue because. Yeah, from not able to cover it, we can point people in their requests to the coverage issue and it can leave comments there if you need it. Okay, then we do a start on top start from top yes, then over to Gina. I have a reminder that I'll be out of office for probably the longest amount of time I've ever taken here. So two weeks. And I linked my coverage issue there. Thank you for people who are covering for me. And then I have a few other items just for some ongoing work that's happening in early will in January when I come back. I'll work on a competitive analysis for or competitive evaluation for fleet visibility and. I will not state the competitors there, but I will be looking at, but you can read them. And then go into that issue if you want to see more Darren has already added actually a lot of screenshots. And then I'm also working on auditing existing research in summarizing that into a video about kind of like the broader pipeline performance or how to optimize my pipeline. Erica has done a lot of research in that area that I'm also going to be just like summarizing in one place. And so I'm not to my surprise there is a lot of existing stuff so we don't have to talk to more customers which is also nice because the problems are already validated. And then lastly, I worked on I worked with Graham and Alana who are two new product designers because they wanted to shadow someone while reviewing an MR so. And it was a really fun experience for me we were trying to get pipelines to be in the right states and I was like googling on the spot and. We were all just like working together they had GDK up on their instances as well and we were working together on it and it was very valuable so we. And I'm hoping that I'm able to add this officially to the onboarding template for product designers. You're welcome to jump in there. I have a comment on this one can I say I left a comment in the merge quest you know just in being a big company for feedback because. I will this help you and board bunny and they are do something similar. So I think they had this week the first merge quest shadow call they also were taking a call for like one hour so they might have some. Some feedback and I think it's a great addition because to your point right now that designers are assigned to randomly assigned to merge requests reviewing something out of sort of your stage. And even can be even scarier so I think in general doesn't opportunity to do that for us to also define when shoot that happen right and who should help them with. She'll have to mean the process so thanks for that mark. Yeah and also I met with be this morning and she was saying that she'll create another remark because I she had some larger ideas of how to. Kind of like time box a lot of the stuff that we have in the template so there's going to be more changes. After that but we were able to connect at least on the small change that I wanted to bring in. Throw on the same page there and she'll build on it. And that is it on for me. Emily. Well, um the first one is just an FI I that we're starting a new unmoderated study in 168 for a tribute on the Kubernetes dashboard. We're seeing like a lot of competitors have a tribute of this and it's a concept that we didn't initially explore when the Kubernetes dashboard was first validated so it's something we kind of want to. Sneak in there as we're working on it and a big thanks for well will's been kind of helping me with the screen here for this as well as the scenario so. Thank him for his help with this study. And then. The second thing I wanted to say is Andrew one of the friend and engineers on the team invited me to a Kubernetes meet up in Toronto last week which was a lot of fun. It was very very technical so a little bit above my knowledge. But it was nice to just go kind of network with some of the people working in this area making connections and we're also talking about maybe presenting our Kubernetes dashboard at one of these in the future. So I'm going to meet up with a Vittica to see if this is something I could do speaking about and partner with Andrew so he can kind of answer all the engineering questions that come up. But it was a lot of fun and I haven't done like an in person meet up for a while so. And if no one has questions move to Vittica who's not attending. So do you want me to voice it for her. So it's I'm talking so her first point is there's a discussion going on in PE on the time we show for duration created at started at and it'll be helpful for some young and Gina to for and she's like the epic for improvements. And then. Gina I noticed yours is read only but since we probably have time we can. Yeah, I can voice it. I brought this up. It's actually a really interesting issue. I would recommend going into it because it basically like the users saying that weren't misleading when we calculate duration for pipeline. I brought it up with a flea team just because we have a lot of overlapping features that have to do with duration and runners and wait time and everything like that. And I left a comment for Vittica because we have some specific customer recordings of how they use duration metrics and why they're using them to optimize their pipelines. And the just a summary of what the fleet team talked about. We kind of agree with the user that it is misleading because we're not including pending time in the duration, which is. I'm not saying any kind of weird. But anyways, I just she had asked me to jump in and just include some. User stories, I guess kind of what most like how customers are using this today. So that's what I did. Yeah, I can bust my comment to. I had my one of my new Vittica today and I told her there are some insights in one of the environments. That's where we searched issue. I think I did that maybe last year with summer. That has some explanation about what the release managers and that's up in the years when looking context of the payments right when they are in the pipeline pages. But looking at the point of production. What do they understand you know of that metadata what why are they looking for. When it comes to. The ratio of the pipeline or you know. Well, when it started running etc. So it might be interesting for her to look into that but also connect with you and you might have a lot of updated and more insight in that. But just as a reminder, I think related to what you said there's some connections right about those information that we need to make. For many different personas, a lot of people just go to those pages, especially the pipeline view. And look for something very specific. There's a wanna see one number or one tag. So keep that in mind when making this and coming nation, let's also look at the insights because we have them and if you're gonna find it. Ask me, oh my, oh my. Oh my no where it is, but it's it's there. And then research project is the environments dashboard redesign. I think. That's not the group level environments dash that's the original environments. Okay. Yeah, yeah, I think then you all have been a full school started and I finished that was before you join. And I think that's the main thing. But there I remember documented that in the off jail because we specifically asked about the order of the metadata and also. What they want to do. Why some of this info is relevant. But anyways, mission like one or two insights about that it's the environments dashboard redesign. Also just another small note about this. I know that the issue was about the pipeline duration and. What another thing that I was thinking about is that a lot of these metrics are. I think more useful at the job level rather than the pipeline level because each. Even if the same pipeline runs. A pipeline is designated for a project. If that pipeline runs multiple times the same it might have a difference at a job set run every time because jobs can be skipped in it whatever. So yeah, I think it's it's hard to think about this at the pipeline level just because there's so many changes that could happen with each run. And I can move on to our next point. We're done. And the next one is we need to get started. Butifying our UI in 168 there's many MRs that are merged already and now have validation in the form of that work. And I'll voice Bonnie's comment here if you want to take a look at the list of items. Maria and her are working on. I think that's just a question for Vita go and to watch this. I think it's the opposite. I think it's the biggest comment for Tika's comment to Bonnie. Okay, everybody. Yeah, that makes sense. Yeah, because Mida is in front of an engineer in Tinas. No, no, that's right. Mida is a sorry to many to many teams. You're right. No, so we're confused because it could be that 168 beautifying something with Hayes Lumi. Maybe with Mida. Anyways. Yes, they can take a look at the agenda. That's it for. Yeah, so Bonnie and Sinshu are not here. And then my only update for us which more is that still hiring, Steven interviewing. I don't have an interview scheduled for well this year and more. So hopefully by January we'll pick up with more interviews and fingers crossed hire someone. Good evening. And over to Eric. Yes, so I posted the link to the draft of the report for the runner and CI builders build. Belled feature prioritization survey. So we did a canos survey with 11 features to see how they got prioritized. The signal we would be looking for is that there was like one must have that we would want a champion. Actually, they were all either attractive or indifferent. So like kind of cool, but like nothing like that we have a clear signal to deliver on. Looking across the ones that were in the attractive bucket versus the indifferent bucket. We have a signal that users want AI assisted things like for example like custom dashboards are actually not compelling because it's more work for them. But if it can be like AI assisted then it's like attractive but still not a must have. And so Darren and Gina and I are going to go over that report and depth tomorrow. But I thought I was like give the high level overview that we don't have any must have so I think it means that we're on the right track with like figuring out those little optimizations suggestions which is a read we got from that qualitative study. And we wanted to do this just to make sure that there wasn't something that was like so cool across this bigger sample of users that we missed but we didn't so it kind of doesn't change the strategy in a way. I've also noticed that the not the sample was mainly small medium businesses. And I want yeah which is fine like I kind of had the assumption that they had different interests than the enterprise customers that we're talking to that are self managed. I we did have yeah it's totally right very keen. The thing is I think this idea that it would create more work still holds. Yes. So I agree but I guess my point is I feel like they have different needs because enterprise customers first of all are typically self managed bringing a ton of runners. So when I look at the features profile does attractive. I would imagine runner usage and compute minutes are higher up on that then for small business. Small or medium business. Yeah yeah I don't I I assigned a fidelity of signal score to to kind of help us make sense of it. I think what we can do is really look at the the the five features that are have strong. We have strong of it like a strong signal that they're in different to that's kind of the most. Okay, well it's like okay don't invest heavily in these okay yeah oh that's also so interesting though because. The notifications I don't know it's. Yeah because yeah because it's just it could end up really distracting them the notifications parts. Right. And then I think a lot of their flow. Yeah. I guess that makes sense but they're but then I guess does that mean that we just think about different ways of solving for when you're keep performance stinks. Or when your runners are. Erring out a lot like we have to figure out a different ways to give them that information. Or at least make the notifications like customizable as opposed to like across the board so it's like you don't want to get like all of their runners aren't important to them it might be like only the subset so like allowing them to. Opt in. Yeah but in model versus like opting out. Okay. It's like just we did just be really wary of creating. More things in there already busy. Yeah I don't I totally understand that this is really cool though Erica I'm going to look this is my first I'm actually seeing it so I'm going to look deeper into it and I think our meetings tomorrow. Yeah yeah and let me know like what what to do but I and I think it's a really good critique of the. Of of the sample that's like really on point I just think that. The the problems that we're seeing with those features should also have a problem like notifications for enterprise level but would skate that would scale that finding kind of right definitely yes. I was looking more at slide seven of the ones that are attractive because I figured the ones that are set they're set at week there I would have thought that they would be higher up for enterprise users but the findings that inside a definitely relate to all I totally agree. Yeah yeah so we can decide I think the thing is we didn't get a clear I mean I thought we might get a clear like chibby in here. I'm not really important that we don't want to like sprint towards any one of these particular visions and I think that like this AI assisted is the way that we want to be thinking about it. Like yeah behind the scenes they just wanted behind the scenes. Yeah okay I have some more questions but I don't want to take up a ton of time. I'm not a baro. Cool everyone don't you care. Yeah okay. And let me know if you want to sink today too. Okay. Yeah. And then or just slack about it and then the other study I'm working on is this conversational AI study which is like a team of researchers and I'm working with ops folks. And we are asking we did like an intro interview and then they're doing a diary study where every two to three days they let us know the tasks that they performed with conversational AI and how it went and they're giving us screenshots of those. And so we're getting a sense of how we might use conversational AI in the ops space. Although we think it's not related to performance optimizations like we got a signal that they that's inefficient use of their time. It's more like it looks more like they like it like as an idea generator or like getting the right language to use for documentation searches stuff like that. Anyway, but I'm threading the like really technical example tasks in that thread I linked and we'll we'll be doing the follow up interviews with them in the second and third week of January and so we'll be asking people to like give me follow up questions to ask about those tasks. Sort of put that on your radar and then the next thing is I was initially thinking that we would run that shared resources workshop at the summit, but I think we should do it after the summit because it seems like. I'll be like the only nerd being like, hey, let's work. Look at it. But let me know if you want to go rock climbing because there's a rock climbing issue that someone created. I work our rock climbing. That's all I've got. We can have the workshop while rock climbing. That's a good challenge. Okay, we're too happy with that. Let's go. But I think it's a good idea because yeah, you know, maybe some of us will also take some time off after some it's before right is some traveling. I think it's good. Well, once it gets confirmed, we can play around the summit for that and if not, we just, you know, pick a date in. You don't make you to things going to make you to right. Yeah, and if you work it to, yeah, I think it's safe for. Can we. Okay. Think that's that right we'll. Yes, already off. Anything else we didn't discuss we would like to share before we wrap out for the year. Good. This. Who? And thank you Emily for recording. This. Yeah, enjoy the rest of your day. I'll see you later. Final see you later. See you next year. Enjoy. Enjoy time off. Bye. Bye. Bye.",
  "Maybe you can, yeah. But I don't know where this is getting recorded to cloud on my computer anyway. Yeah, so I was saying that Sunjung, I learned a lot from the process that you were following and other learnings that came from that. And so like we thought that it's going to be good to kick it, kick it off as soon as possible, but not wait for everything to develop first and then think about right up like that. Yeah, yeah. This is the right way to go. Thanks for sharing. So my item on there is free a big and rather radical change from the CACD private catalog. So we we discussed this a lot and this came up and inspired me a lot. I did the journey map exercise and I didn't feel it's right to having this private CACD catalogs go for namespace because we have interviews several self-manage instance customer. It could be really multiple namespaces and not going to disclose to exact number, but it could be a lot of course it depends on the organization's size. And now dove announced this change and this will impact a bit of our product roadmap or the timeline. The design lies that impact is not that big, but I think this is the right way to go. So I was really supportive about this change and yeah, you can see that now we simplify the structure. So we'll have one organization catalog and one open source catalog that is kind of similar to the market. And I have personal updates so you can re-through it. I might need to reschedule some of our coffee chat or 101 if I need to. From next week until the end of September. I wanted to here with you all. So I am so excited. Yeah. Immodges are not working. Yeah. It's mostly common control shift that works for me to show the model or the modus. I use it a lot. I'm not ship story space. Okay, so my Anna is not here. So we can just go through this asynchronously. Maybe we can move to a research them. Yeah. Yes. So I am just analyzing all the sessions from how do we use AI to help users optimize their runner and CI builds. And it was pretty cool. We used like a decision tree process from this book on AI that I'm reading. And basically the idea is if you can put the decisions into like clear if then statements, then you have support that you can automate this workflow a bit. So we didn't know if it would be like I think about this, but that's related to this. And then this is really, but actually it's like pretty straightforward. Anyway, so I won't go into the results. But I'm working on the report. And another come out. I think I'll share with the UXR team soon and then it will come out next week. But there's really good information in there about just the details that people need to make decisions. And kind of the outcomes for optimizing pipelines. And the process is very trial and error. And set it and forget it. That's the main thing. So I think that's like one of our CI learnings just like the biggest thing I've learned. And get lab. They don't think of these things in their minds either. Like they specialize in it. They execute and then they go away. But I think that there's also findings related to how to support teamwork. So I think you'll like their report. And then next week or as soon as I kind of like get my mind into that report, then I'll start working on this. How users are using get lab on the cloud. And we'll do a little bit of a good question. Yeah, could you just elaborate a little bit more on the effort? Yeah. So I'm actually very proud of us as a product company that we're like ready to move into this space. So we're looking at kind of the connection points between get lab and the cloud. And like where the work flowing is. Kind of the tools that users are using. And that'll inform and part of the integrations that we're building. Like, June, which is DCP. But then also like long term what works well in terms of the integrating. And what does it? And that's actually where I think we can bring in some questions about resources and whether or not they should live at the project or the group level like how teams access like secrets for example. Like where does those get stored and get labed? And it is like higher up organization. Because I'm thinking a lot about this new. Chains that's coming related to organizations was to new nodes because that's like the new play with the catalog. Which I think is really smart. Which is to put it up there. But then I'm wondering like what else could we move up there? Like maybe the secrets because they're at their organized at this company level. But that's what we can look into. Talking about that Erica, I like can you like later go in chime into the issue that I've created for navigation. And like share your views there because yeah, we don't want Even though I have mentioned about like taking it out of CI CD. If there's any research that points that we should have it maybe as a part of the main navigation or out to higher level navigation. We should be aware of that and that would help us inform the methodologies that we would like select for research. Okay. I think we haven't gotten that detailed because that's actually like really tactical. Most of the research so far has been like what is a secret. But I'll look I'll look up the lens on there, but I think I won't have much. But I think from this study we might get something. Okay. And by the way, I will also pick this up based on your recommendation. Oh yeah. I'm listening today. You can read that chapter. I'll say it. Okay. I had. No, it's okay. I just have another like full hook question. Are you only looking at get lab. Sassy. There's always trying to read through your issue. And I mentioned that because one of the teams that I work with in enablements. Currently the application performance group is changing their name to cloud connector. Okay. And they're focusing on. Sumpman issues are essentially giving them access to like AI features and things that would be available through get lab SaaS, but at the self managed level. So that might be a good team to connect with. You know, in terms of this research. Okay. Awesome. Thank you. Right. I think we landed on the SaaS. Users. But not. Yeah, but either way, I think they at least be interested. So that's so helpful. Well, thank you. I'm embarrassed that I can't quickly find that in the issue. But it's okay. I'm on the other project. Yeah. So if you want to ping their entire group. I can list the the channel there. And then the P.I. can also list the PM as well. Okay. Cool. I just put thank you. Thank you. Well, go team. And then I have this this note. I have to find it in the, but I'm watching the sessions again and again. So it's coming. But there's just lovely quote about how we made a change. And the details people could see about the type lines completing. And this woman was like, I love it. I used it the other day to show my executives that I need more funding for my pipelines. Because I could show like kind of the times and how if we organize things, this would impact the times. And then she used that to find more of our product. So that's like the best. I can look this in your portfolio. Be prepared. Where are we showing the pipeline? Somebody. I'm sorry. Oh, no. I mean, I don't know. But when she was telling me, was that the new way that she can look at the metrics allowed her to make this argument. I need to pull up the, I'll pull up the quote. Okay. It was pre-owned. Any other updates before I go or. Yeah, the data technique. So I don't necessarily have anything. Connected to delivery or dedicated this week. I guess just a. Announce what I'm working on outside of that. Doing some research for organizations that I think Eric had talked about in the US week, the call earlier in the week. So I included the research issue and I've been. Constantly like adding and refining research questions there. And then I'm doing a workshop on breaking changes. Some adding people from different departments at the moment and can be doing that. It's going to be kind of a mix of async and synchronous workshop parts. I was looking for Jocelyn in the list because I mean, given how much she has been struggling with breaking changes and I see her there. Yeah, that was somebody who came up pretty quickly in conversations with the PM that I worked with Sam. He was like Jocelyn definitely let's include her. But if you have any other suggestions on who might be a good person to include. Let me know. Yeah. That's all the updates that I have this week. Okay, so with this we are at the end of the agenda. Anything that someone wants to bring up. If not, then the visit. It's in the meeting here. Bye everyone. Have a good one.",
  "Hey everyone, this is the CICD UX meeting on August 2nd. Starting with our standard topics with the, I had not wanted to mention the Q3, okay, as are being defined and will be available soon. So she'll share more info once the details are known. But two of the okay, as might center around the UX department completing the neuro diversity training and resolving usability benchmarking findings across FaireFI. So does anyone have any questions about those two? Awesome. So then we'll move into the product design section. I can voice Hannah's because there's important notice here about how she'll be providing coverage for switchboard at 20%. So hands on design work, milestone reviews and team rituals. And this will continue until that roll is back filled. And they expect to onboard five customers to get liad dedicated by the end of Q3 which puts a focus on the UX. So during this period, if you need support or she's blocking in any way, please be explicit about what's needed and you can ping DM on Slack where she's most responsive. And then currently working on email notifications and the onboarding flow, gathering requirements to improve design proposals for the deployments page for switchboard. So lots of updates to existing work and that'll be kind of where her focus time will be. And she shared a customer call from Switchboard on Monday. Think I'm next? Yes. Oh yeah. No, no, sorry. But if a no one has any questions, yeah, Gina, you can start. Okay. I have attached a report that I wrote up about the mental model research study. And there's a lot of good insights that came out of it. I'm working on recording like a video and sending it out today explaining a summary of that as well. But right now, one of our next steps is to run another survey, which is in progress and already have some data from it. So finalize the terms that we use to explain the concept buckets. And then we're also asking users where they use those things in their workflows. This will help us get a better idea of really what they think those words are. And then we can also use that to look at like pain points within the workflows that they bring up. And then the next item is that our next big thing for runners to look at runner cost visibility, which is focusing on users who bring their own runners. So we're not going to be touching shared runners on SAS because we use like our own pricing method for those users. And an example of this is like if you create a runner through AWS will provide you visibility and to how much that runner is costing you like the machine and to run the jobs and everything. So I attached the issue there, Darren added like a ton of details. And we're just going to work on scoping it down. But I talked with the team today and we're going to run a think big and think small on this because my goal is to have the process of getting the first iteration for this feature out faster than what we did for the NBC dashboard. So I think I'm next if no one has any questions. So I'm just making progress so so you can update it kind of artifacts for design sprint starting with issue templates and slide decks that you can reuse during the sprints. Just let me know if there's anything design sprint related you'd like to see template it out and I'll try and get to work on some of those. So some of the ones I haven't gotten to are each day of the sprint to those issue templates as well as I'm going to make videos that you can share asynchronously on how to conduct sort of these activities or the homework we were calling. So there's a few things I'm still working on but just wanted to share progress on that. The second thing being our we did internal research on populating a service list two weeks ago now we're doing external research. The good news is the results from both internal and external are really aligning, more seeing a lot of the same feedback from both which is great. So we're hoping to wrap that up in the next few weeks and move on to prototyping an NBC approach from this. Yeah that's a good thing that they're aligning. Yeah that's so we were worried about we were like I hope GitLab isn't using GitLab differently enough but there results will differ but there's a lot of the same thing which is really great. Okay there are no questions I can proceed with mine. So first of all thank you everyone for providing feedback last week on the secrets pucy prototype and you'll allow find our advice design on the same framework profile. The next step for me is to create an interactive prototype and share the link with everybody who has signed up for the OLE Adoptal Program for Secrets and then we'll just keep our fingers crossed to see what kind of feedback frozen. Beyond that I mean that that's a big thing that has that is about to be wrapped up this week so now I can start looking at see a job token and variable work again. Yeah besides that I have a chat with the application secure to team here at GitLab. I organized a meeting with our engineering manager product manager the security engineer from upseq MSF and it was really interesting so I tried to understand how is it that they are currently like how are their workflows related to secrets being fulfilled today with the current set of features that we're offering and they kind of walk through it and also contributed to the user story map that we've been working with. So we definitely have to keep in mind and one of the learnings was that we have to keep in mind the special responsibilities are special permissions that any security uh any any individual responsible for taking care of the security in any team organization would be requiring and that's going to differ a little from what we are going to be thinking for maintainers owners. So yeah this was a really good meeting for us and that's a oh if you don't have any other question I can move on to mine um I don't have so much to share for this week um so we had a meeting last week and this is this is this huge that I learned here is the outcome um so we decided to take a step back and then just like let's just forget about like everything and then we just focus on the user flow and then their context and then see what is the best solution for them and in which context are they starting to author the CIMO plot pipeline or maybe they want to come in to the CIMO and then maybe they want to just make some edit and then probably that's the point that they started looking to the CIME template component. So I'd like to just list down all the possible context and then make a journey map for each workflow as it seems like CINA has one question they want to verbalize your question CINA. Yeah it's a little it's kind of related I guess um I just had a question on the progress of the CISCD catalog and the navigation effort because I saw in that page now that Foundation team isn't allowing navigation changes until January of next year and I was wondering like how that impacts your team progress. I think of of course it's I don't know at this point because we are ready to start to take a step back and then I based on the outcome that I come up with like with the journey map or whatever design that we are going to discuss okay which is the best placement. So the short answer would be no I guess so from mind-resenting so okay probably the project navigation is the best location for this reasons then that means we can still ask for the request for the Foundation Pman and have those conversation back with better back up from the UX perspective and then also from the UX our perspective so I I you didn't think it will be blocker for us but at this point where you just like decided okay let's like a huge step back so I don't know we would be a plug or not just to answer your question. Yeah it does um one of the things I'm in our team call her runner today we were bringing up the fact that our project runners views and settings and everywhere else it's in the navigation like in build or in admin areas still called CICD but same thing and I was like yeah I I'd mentioned that your team pipeline authoring is like going through this and it's been quite a struggle um and yeah I'm just I'm a little nervous considering what I've seen from the stuff that you've gone through already so I might end up talking to you more about it depending on I might create an issue just to get like their input so far because someone actually commented about it on the navigation feedback issue they're like why is runners still in settings for project when it's not in any of the other views so maybe that helps but we'll see yeah so my solution would be maybe you already started to getting more numbers and data and also those verbal feedback from users and then just be ready to have those conversation with the foundation in one year really sure about like okay this is the good place for the runner yeah that'll be my only suggestion thank you yeah thank you my issue is also going to come in so I'm going to join the band in a while yeah and I think to genus one that most things that we are placing in CICD settings I'm not like very aware of what's happening with the other settings option but CICD settings those particular items are facing discovery, discovery, discovery, quality issues and I think we just need to see that what we can do and it's impacting kind of silly person the item that's inside the settings so it's not just about runners and variables it's about like how can we collectively improve the visibility of those items overall and yeah it would be a great chance for us to collaborate on this yeah totally agree yeah I just wanted to add that that are protected environment settings are there and we've had a lot of like user feedback that no one can find are protected environment settings so oh yeah yeah I remember that looking and that one is pretty divin the setting yeah it's deep and it also is sitting in an area that people are expecting it to be in and it's not linking you can't link it to the edit or just we recently started linking it from the edit environment but before you go to edit environment and it wouldn't be there so that was even more confusing I have an observation there that in most of the other products because we did a competitive revolution recently so I was just like passively looking at there are many options our like anything that's placed under settings in any of these sections I actually cascading in two steps and that's not the case with other products they have their settings navigation separate from like everything all together so doesn't feel like everything is very hidden you mean they have it like in a separate like almost on the separate level pile yeah yeah I see what I mean yeah and the one thing I noticed is sometimes we just put things like two kind of figure also when sites the setting and is it really setting or is it something to configure and yet that's sometimes confusing to me so maybe this is a good opportunity for all of us to I don't know just some fun work like settings you why version two yeah yeah well yeah we even talked about just removing protected environments from the settings and just moving it into the edit environments page because we're like that makes so much more sense for just actually in just actually those yeah and then maybe have an overview in settings but like I don't know it for us it doesn't quite make sense while it's sitting there yeah and just some a different kind of semantic grouping like for example verbose tokens secrets they would all of be kind of managed the same way but they'll be scattered all across even if it's on the same page let's see well think of something whoa yeah just wanted um if there any other it looks like research is both away this week so I don't think we'll have anything from them oh does anyone else have any other questions if not we can end this slightly earlier oh that one has a great rest of your day",
  "other. All right, today is the 19th July and we are meeting for CICD UX discussions. There are some stunning topics. There's just one impact which is by me. So in by playing security, we are kind of experiencing something like we are getting community contributions, which are not directly related to any issues. And those contributions, they are not very small in nature. So what ends up happening is it kind of like, and some being in conflict with something that we already have plans for and we try to make changes of that, we can accommodate it like we can take some parts from the contribution and get it implemented. But the discussion has been like going really long on those. And so Jocelyn and I we thought like maybe we should bring it up with the contributor success team. And that's the discussion that's going on there. So probably they might bring in a couple more automations to the process or some additional label that would require contributors to ensure that they at least look up for issues which are relating to the change that they are proposing. And that would also help us make sure that we are not discouraging anyone from making contributions because of course if we just continue the discussions for a month or more than that and just keep on changing, making changes to the decisions. It's not, I would say for someone who's taking a time from their personal time to do this work, it's not a great experience for them. So yeah, I'll keep everyone posted on what we decide or changes the contributors success team decides on rather. But this is this lack thread, but it's all happening. So if you want to just follow up. Yeah, Gina, do you? Yeah, because this happened to me as well and it gets a little frustrating especially for designers because we tend to have to go back in like retroactively design based on the contribution. And for runner we saw it was the same contributor every time who was doing it. So we ended up talking with the contributors success team and then met with them plus the contributor and kind of like talks about our roadmap so that we talk about our roadmap and also what they were thinking on from their roadmap so we can come together on those things. And what I suggested, I mean, obviously we have to like be open to them contributing. So I suggested that if they have like a new feature that they want to create, they create an issue first and tag me and that puts a lot of responsibility on us but I also think it gets us involved earlier. So that's what they've been doing lately and it actually has been helping a lot in regards to like making sure it fits in with the features that already exists or that we even like see coming up in the roadmap. Yeah, that's a very relevant example. So like this is case by case like for your team writing but to make sure that we come up with something that's generally applicable to everyone it's adding. Good for the contributors success team to come up with some new rules probably. Okay, if there are no more comments, let's move to product design. So for the first section which is which for Hannah has some updates that she's working on the backful requirement for all this position and if there's anything that any of the team members have been working on with the relief, they can just let her know so she can work on finding alternatives. Any other, I think NFI for Emily that the bears would be rotated by the end of July and for the rest of us as well of course. Yeah next is Gina. Yeah, I'm going to drop a little bit early from this meeting but also the things that I've been working on are mental model research for runners which I completed and thank you, Erica's not on the call but if she washes the recording, she helped me a ton with this project. So yeah, thank you, Erica. And she gave me a skeleton of a report to be able to fill out with all the analysis and still working progress but you're welcome to check it out. I also added a summary to the issue though. So what came out of it was to find groups of the runner concepts that users think about when they're using runners and then we also had them like come up with their own terms to describe certain definitions which almost all were not aligned with what we use at get lab but conceptually you saw some connections there. They just weren't the exact same terms. And then the final exercise was to have them match the terms to definitions like the terms that we used to get lab to the definitions and most of those they did they were not able to match accurately as we define them at get lab. So there was a lot of learnings but what I'm seeing now is the next thing I want to do with that Erica suggested is work with the team to come up with names for each group of concepts and then we're going to send out another survey to like a hundred participants if we can get that many and have them select which name like they feel best describes the concepts in that group and then ask them where they're using that and they're workflow and what tasks they're using it for. So then we can figure out how we want to make changes from there like if I wanted to make the design changes for that configuration example that I shared last week that would be like a way to help us validate that that is needed. But what I'm seeing now is to have the team is very it's difficult for the team to like understand that these concepts were grouped together because it doesn't align with how runner actually works. So yeah it's people are having a hard time looking past like the technical way that it works and then how people are grouping them and I was wondering if anyone has run into that problem and kind of how you're approaching it with your team. Will do you want to ask your questions? Well I did want to give you space to see if anybody has like thoughts on the question that you just raised because by questions a little bit unrelated. Okay. I think what Emily did with environments was something similar because they challenged the existing ideas of concepts that were like that were there since forever in GitLab. So I don't know maybe Emily any insights like how did the team take take it when they learned. But this was rather something that the team was involved in. So probably that's how it was different because it was not coming from outside but from discussions within the team. So the buyer would have been on the boat somewhere. We also had a lot of research to back that our current feature wasn't working. So I think that made it easy to convince the team because through like the group of environments research additional research before we were aware like this feature wasn't really hitting the mark. So collectively I think it was much easier to move forward versus yeah it coming from someone else. Yeah which is totally different in our use case because people are very happy with runners. It almost feels like we internally because we're touching it all the time. We have a much deeper understanding of it than our users, the majority of our users I would say but also the admins or platform engineers who are managing their own runners have a very similar understanding as we do. So it's hard to get it's hard for me to be like they're not understanding it when we know that they are because people love the feature. We want to think that helped us like I know we are at a very I would say nascent stage in the entire native secrets manager concept but one thing that helped us was the competitor evaluation because like the entire team got to like take a peek into how the rest of the industry is referring to different concepts and that also helped them relate with the research data that Erica had like greater reports on. So everything together they were able to relate one thing to another and that helped in like gain that conviction. I did definitely start with that. I have like a Google Doc with competitor terminology in it but I didn't really share that widely with their group so that could be a bit places to start a person. Thank you. Yeah, I just just certainly back to my question. How did you decide to get survey data or you know target 100 people? Yeah, so Erica suggested that we run the survey for I think a month and a half I think it was or we or when we get to 100 people like whichever one comes first and I she was saying 100 because like that's the average number that they've been seeing trends lately from surveys but if you have other suggestions I am very open to it. Yeah, I've used like survey sample size calculators which I can certainly link to here. So if you know like how many people are using a particular feature because I know that we have like it a lot of internal metrics on like who's a user of this stage group or that stage group. You could use that as a little bit of a guide and to like okay if our population is x. Then we should survey this many people to get somewhat of a representative sample size but yeah I'll include one below. Okay, I think another thing was that I was kind of saying that we need to move fast with this so she probably changed her answer based on that. Okay, yeah I mean that that certainly plays a factor too. Yeah, yeah but I still would like to look at that survey sample size and see what we do. Yeah, so I added a link there. If you click into it you'll notice that it has like three different sections. You don't really have to mess with confidence interval or margin of error. You would just have to enter the population size. So that's the number that you would have to either guess to me or you know get some analytics on to populate that field. Okay, thank you. This is nice. Great, the hearing will. Yeah, so is everyone that I noticed they're still typing going on. We're good. So just a small update with the outcome of the design sprint. We're trying to figure out the best way to go forward with the research here and we've started with a just internal customer interviews with the SRA and delivery team around the service concept getting a better idea of what they consider services. How we would populate that list and after we're done those internal interviews which I think they're like six happening this week. A big thank you to Victor for scheduling them. We'll be able to move forward with like external user testing just an update on that front. The other one is I'll be working on the design issue around designing for external jobs and I believe this touches kind of like the job details page, the jobless page and the pipeline details page. I'm not sure if any of those funnel into the verified teams that have designers but if they do just let me know so I can involve you and kind of feedback for these pages. I can help because I like I took part in a lot of free searches for those pages so I can help with any feedback but otherwise there's no designer for pipeline execution. I can also help because runner has a very small portion in this but there is at least a link to the runner that runs the job and then there's a lot of output from runner in the job log that might be helpful. Yeah this is mostly just designing it around this new like external job thing and what visuals will show but I think it'll touch like all of these pages. So figuring that out as I go forward. Yeah and also please slip mean because pipeline all thering it's not just owning but we are also touching some pipeline details page and then job log as well so I'm happy to help. Thanks a lot. Have you liked can I ask a question just on like what external job means? Yeah so this is one that I'm just starting into so I have to kind of look for it that this is as far as I know a say I job that's being executed outside of get lab. Like like on actions so for example on actions okay. No worries it's so what this is is I'm reading this out is like adding a new job status so it will represent the jobs that are currently being executed by a service external to get lab. So adding in like a pending kind of state as that is being run and then yeah so I'll actually link the engineering issue here that explains it a bit more than the design issue. Um That is helpful. I'm also reading the epic and it says external jobs are not picked up by a run and so it might not even can actually mean and if so you don't have to work me in. I mean I heard about this use case in some pre-searchs and also in the conference lately but I don't know I still cannot drop a runals it I need to read through this. Yeah this is new um for 163 I just started looking into it so as I learn more I'll reach out but it's a fairly new um being more looking into so I don't have all the details right now but I'm sure over the next week I'll have more. Okay. Okay. That's it with me. Then I will take up and so I'm mostly focusing on just one thing besides a couple of our community contributions this week that's the like finishing the POC prototype so I created the user map and flow on big jam it looks so good and um based on this particular story mapping I would be like designing the different pages or the different actions for this flow and that's for this. Another thing that I wanted to share was I'm starting to add like I added a slot to my calendar from starting next week for every Thursday morning. Kind of a time that suits all of us like all the members in the CI CD team at least and this is for like to have a sync time with me to discuss just anything that relates to public presence. We can just like use this sync time to discuss a block proposal to edit a block that's already there to like write abstracts select conferences designing content representations. I mean just anything that comes to your mind if we can use this time for that and to be like how you can add yourself you can just like go to the link add it to your calendar and for a whichever week you have booked it others would not look it for that particular week. Yeah and that is it. Yeah you could just do I like I noticed it goes to a calendar block. Do you get notified or do we have to like book it with you to let you know um let's try. I don't know if that happens because when I clicked on it it just goes to a calendar event but no one's invited to it so. Yeah okay I'm also not a part of what that is amazing. Okay now I am. I'll work further on this. Does it look very full? Well some links are getting it together. Yeah are we can just like sync runes like catch up over slack and just book slots manually this doesn't work. I'm not so good with automating things. And that's it. Yeah so in the new you can take it out. No I just wanted to say plus one but yeah it's my turn okay. So I'll type it later. So things be to Kat um I have two items that I like to share that um dove and mark and myself we're planning the Q3 that what could be the CSED catalog beta and then what could be included technically and also from the user standpoint and it's almost like divine and then we also announce a true to team and then now team is some board so that's exciting that means so just to tell you a little bit of more details we will gather more and made a data from now on so that we just just don't need to hold like this depend on the repeat.mb that user right and then this is your component a and b and c like for now like we have to just let them write everything and then just hold the depend on this instruction but you like to kind of provide a better assistance and suggestion system um so this is exciting for me because it'll open the doors for the further UX opportunities that I like to share this and the other point is probably you might see this in the MR's pipeline path um there is a now failed job number is surfacing in those tabs so that was the idea for this packathon and then luckily we get a lot of positive feedback at the same time we started to get a lot of bug reports. For example there is four failed job but so we just have a zero failed job and with even with the tabo emojis like oh okay we should fix it now so we're also working on this it's just popped up but it's exciting that people had a coming in and as they and then if you also have any other feedback please please let me know I'll also link the app to your issues so that you can leave some feedback around UX and also the UI pack as well. I think this was a really good improvement like a tiny one but so impactful. Yeah I was excited and then yeah we will have more issues. Thanks um every case on leave so moved with over to Bill. Thanks, Andrew. So I just wanted to quickly touch on additional point about the survey calculator that I spoke about a couple minutes ago added some information earlier on but I would just recommend you know working with your researcher if you decide to use one of those types of tools to figure out what's the appropriate like margin of error and confidence interval because if you leave it as is that those things are more like academic standard so tell you that you need probably more than you really do so if you have like tight deadlines or you know whatever it might be you might have to adjust that a little bit to get a better sense of what's a more reasonable number of people to survey. Outside of that I'm going to be a leave starting next Monday. I'll be out for four weeks so I'll get back second half of August. I've created my Q3 prioritization issue and linked to that here. A lot of the stuff that's represented there is just carry over from this quarter that's still active and then I'm going to be picking up some projects that are paused. And then since Ali has departed get labbed, coordinating with the PM Lauren and then Hianna to determine next steps for the solution validation project he was leading. So we're meeting on Thursday to discuss this more in sync haul but we're also having a synch discussions about it. If Hianna's going to be able to drive it while I'm out if we're going to just kind of keep it paused for the next couple of weeks until I'm back those sort of things. So that's what's going on with me right now. Enjoy your family time. Yeah. Yeah I need it. Yeah my wife goes back to work next week for the first time in like 12 weeks. So I'm going to be on full-dad duty 247. So give it her a little bit of a break. Okay so with that we have at the end of the agenda anything that anyone wants to bring up. If not then let's play more time back. Yeah let's meet later. Fine."
]