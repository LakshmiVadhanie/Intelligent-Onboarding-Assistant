# Intelligent Onboarding Assistant - Model Development Pipeline

## ğŸ“‹ Overview

A production-ready RAG (Retrieval-Augmented Generation) system for GitLab onboarding that uses semantic search to retrieve relevant information from company handbooks and meeting transcripts.

**Project Status:** âœ… Model Development Pipeline Complete

### Key Features
- ğŸ” **Advanced Semantic Retrieval**: Vector search with cross-encoder reranking
- ğŸ¤– **Free AI Generation**: Google Gemini 2.0 integration (no cost!)
- ğŸ“Š **Comprehensive Evaluation**: Precision@K, Recall@K, MRR, NDCG, RAGAS metrics
- ğŸ”¬ **Experiment Tracking**: MLflow integration for full reproducibility
- âš–ï¸ **Bias Detection**: Fairness analysis across data slices with reporting
- ğŸ¨ **Beautiful Web UI**: Streamlit interface for interactive Q&A
- ğŸ³ **Docker Ready**: Containerized deployment (12/12 tests passing)
- ğŸ”„ **CI/CD Automated**: GitHub Actions workflows

## ğŸ—ï¸ Architecture
```
Data Pipeline â†’ Embeddings â†’ Vector Store â†’ Advanced Retrieval â†’ Reranking â†’ LLM Generation
     â†“              â†“            â†“               â†“                  â†“            â†“
   (JSON)      (384-dim)    (ChromaDB)    (Dense Search)    (Cross-Encoder)  (Gemini)
```

### Components
- **Data Loading**: Local JSON + GCS support
- **Embedding Model**: `all-MiniLM-L6-v2` (384 dimensions)
- **Vector Store**: ChromaDB with cosine similarity
- **Retrieval**: Two-stage retrieval (dense + reranking)
- **Reranker**: `cross-encoder/ms-marco-MiniLM-L-6-v2`
- **LLM**: Google Gemini 2.0 Flash (FREE!)
- **Evaluation**: Multiple metrics + RAGAS framework
- **Tracking**: MLflow for experiment management

## ğŸ“ Project Structure
```
Intelligent-Onboarding-Assistant/
â”œâ”€â”€ data/                          # Data storage
â”‚   â”œâ”€â”€ debiased_data/             # Processed handbook (19 chunks)
â”‚   â””â”€â”€ meeting_transcripts/       # Meeting transcriptions
â”œâ”€â”€ src/                           # Source code
â”‚   â”œâ”€â”€ data/                      # Data loading
â”‚   â”‚   â”œâ”€â”€ load_from_gcs.py       # GCS loader
â”‚   â”‚   â””â”€â”€ load_local_data.py     # Local loader
â”‚   â”œâ”€â”€ embeddings/                # Embedding generation
â”‚   â”‚   â””â”€â”€ generate_embeddings.py
â”‚   â”œâ”€â”€ retrieval/                 # Retrieval systems
â”‚   â”‚   â”œâ”€â”€ vector_store.py        # ChromaDB interface
â”‚   â”‚   â”œâ”€â”€ retriever.py           # Baseline retriever
â”‚   â”‚   â””â”€â”€ advanced_retriever.py  # Reranking retriever
â”‚   â”œâ”€â”€ generation/                # RAG pipelines
â”‚   â”‚   â”œâ”€â”€ rag_pipeline.py        # Universal RAG (Gemini + OpenAI)
â”‚   â”‚   â””â”€â”€ gemini_rag_pipeline.py # Gemini-specific
â”‚   â”œâ”€â”€ evaluation/                # Evaluation modules
â”‚   â”‚   â”œâ”€â”€ metrics.py             # Retrieval metrics
â”‚   â”‚   â”œâ”€â”€ bias_detection.py      # Bias analysis
â”‚   â”‚   â”œâ”€â”€ sensitivity_analysis.py # Sensitivity analysis
â”‚   â”‚   â””â”€â”€ ragas_evaluator.py     # RAGAS framework
â”‚   â””â”€â”€ experiments/               # Experiment tracking
â”‚       â”œâ”€â”€ mlflow_tracking.py     # MLflow integration
â”‚       â””â”€â”€ model_registry.py      # Model versioning
â”œâ”€â”€ models/                        # Model artifacts
â”‚   â”œâ”€â”€ embeddings/                # Generated embeddings
â”‚   â”œâ”€â”€ vector_store/              # ChromaDB database
â”‚   â””â”€â”€ registry/                  # Model registry
â”œâ”€â”€ experiments/                   # Results & tracking
â”‚   â”œâ”€â”€ mlruns/                    # MLflow data
â”‚   â”œâ”€â”€ retrieval_evaluation.json  # Metrics
â”‚   â”œâ”€â”€ bias_report.json           # Bias analysis
â”‚   â”œâ”€â”€ sensitivity_analysis.json  # Sensitivity results
â”‚   â””â”€â”€ ragas_evaluation.json      # RAGAS metrics
â”œâ”€â”€ .github/workflows/             # CI/CD pipelines
â”‚   â”œâ”€â”€ ci-pipeline.yml            # Main CI
â”‚   â”œâ”€â”€ model-validation.yml       # Model validation
â”‚   â””â”€â”€ tests.yml                  # Test automation
â”œâ”€â”€ tests/                         # Unit tests
â”œâ”€â”€ app.py                         # Streamlit web UI
â”œâ”€â”€ demo.py                        # Interactive CLI demo
â”œâ”€â”€ test_pipeline.py          # Full pipeline test
â”œâ”€â”€ Dockerfile                     # Container config
â”œâ”€â”€ docker-compose.yml             # Orchestration
â”œâ”€â”€ requirements.txt               # Python dependencies
â”œâ”€â”€ requirements-docker.txt        # Docker-optimized deps
â”œâ”€â”€ .env.example                   # Config template
â”œâ”€â”€ README.md                      # This file
â”œâ”€â”€ QUICKSTART.md                  # Quick start guide
â””â”€â”€ DOCKER.md                      # Docker deployment
```

## ğŸš€ Quick Start

### Prerequisites
- Python 3.9+
- 2GB RAM minimum
- 1GB disk space
- (Optional) Google AI Studio API key for free LLM generation

### Installation

1. **Clone the repository**
```bash
git clone https://github.com/LakshmiVadhanie/Intelligent-Onboarding-Assistant.git
cd Intelligent-Onboarding-Assistant
```

2. **Install dependencies**
```bash
pip install -r requirements.txt
```

3. **(Optional) Set up free Gemini API**
```bash
# Get free key: https://aistudio.google.com/app/apikey
# Create .env file:
echo "GOOGLE_API_KEY=your-key-here" > .env
```

4. **Run comprehensive test**
```bash
python test_pipeline.py
```

**Expected:** `ğŸ‰ ALL TESTS PASSED! PIPELINE 100% COMPLETE!`


## ğŸ“Š Running the Pipeline

### Step 1: Generate Embeddings

Generate vector embeddings for all documents:
```bash
python -m src.embeddings.generate_embeddings
```

**Output:**
- `models/embeddings/embeddings.npy` - Vector embeddings
- `models/embeddings/texts.json` - Original texts
- `models/embeddings/metadata.json` - Document metadata
- `models/embeddings/model_info.json` - Model specifications

### Step 2: Build Vector Store

Index embeddings in ChromaDB:
```bash
python -m src.retrieval.vector_store
```

Output:
- `models/vector_store/` - ChromaDB database with indexed documents

### Step 3: Test Advanced Retrieval (with Reranking)
```bash
python -m src.retrieval.advanced_retriever
```

**Features:**
- Two-stage retrieval (dense + reranking)
- Cross-encoder scoring
- Interactive query testing

### Step 4: Run Full RAG Pipeline
```bash
python -m src.generation.rag_pipeline
```

**Features:**
- Retrieval + LLM generation (if API key set)
- Source citations
- Multiple test queries

### Step 5: Evaluate Performance
```bash
python -m src.evaluation.metrics
```

**Metrics:**
- Precision@K, Recall@K, F1@K, NDCG@K
- Mean Reciprocal Rank (MRR)
- Results saved to `experiments/`

### Step 6: Analyze Bias
```bash
python -m src.evaluation.bias_detection
```

**Analysis:**
- Source type distribution
- Performance disparities
- Bias report generation

### Step 7: Run Sensitivity Analysis
```bash
python -m src.evaluation.sensitivity_analysis
```

**Analyzes:**
- Query length impact
- K-value optimization
- Source type sensitivity
- Embedding dimension efficiency

### Step 8: RAGAS Evaluation
```bash
python -m src.evaluation.ragas_evaluator
```

**Metrics:**
- Context precision & recall
- Context coverage (78.57%)
- Context diversity (29.84%)

### Step 9: Track Experiments
```bash
python -m src.experiments.mlflow_tracking
```

**Then view MLflow UI:**
```bash
python -m mlflow ui --backend-store-uri file:./experiments/mlruns
```

Open: http://localhost:5000

### Step 10: Register Models
```bash
python -m src.experiments.model_registry
```

**Creates:**
- Versioned model artifacts
- SHA256 checksums
- Model cards with metadata

## ğŸ“ˆ Performance Metrics

### Current Results

| Metric | Value | Industry Benchmark | Status |
|--------|-------|-------------------|--------|
| **MRR** | 0.667 | 0.50-0.65 (good) | âœ… Above average |
| **Precision@1** | 66.7% | 50-70% (typical) | âœ… Solid |
| **Recall@K** | 66.7% | 60-75% (good) | âœ… Good |
| **NDCG@K** | 0.667 | 0.60-0.70 (good) | âœ… Good |
| **Context Coverage** | 78.57% | 70-80% (good) | âœ… Strong |

### Bias Analysis
- âœ… No significant performance disparities detected
- âœ… Balanced retrieval across sources  
- âœ… Fair representation of document types
- âœ… Source distribution: Even across all types

### Sensitivity Analysis Results
- âœ… Optimal K value: K=1 (F1: 66.67%)
- âœ… Best query type: Short queries (1-3 words)
- âœ… Embedding efficiency: Optimal (384 dims, low memory)

---

## ğŸ§ª Testing

### Run Complete Test Suite
```bash
python test_pipeline.py
```

*Tests 12 components:**
1. Data Loading âœ…
2. Embeddings âœ…
3. Vector Store âœ…
4. Baseline Retrieval âœ…
5. Advanced Retrieval (Reranking) âœ…
6. RAG Generation (Gemini) âœ…
7. Evaluation Metrics âœ…
8. Bias Detection âœ…
9. Sensitivity Analysis âœ…
10. RAGAS Evaluation âœ…
11. Model Registry âœ…
12. MLflow Tracking âœ…

### Run Individual Tests
```bash
# Test data loading
python -m src.data.load_local_data

# Test embeddings
python -m src.embeddings.generate_embeddings

# Test advanced retrieval
python -m src.retrieval.advanced_retriever

# Test full RAG
python -m src.generation.rag_pipeline
```

## ğŸ”§ Configuration

### LLM Provider Selection

**Default:** Google Gemini (FREE!)

**In `.env` file:**
```
GOOGLE_API_KEY=your-free-gemini-key
LLM_PROVIDER=gemini
```

**To use OpenAI instead:**
```
OPENAI_API_KEY=your-paid-openai-key
LLM_PROVIDER=openai
```

**Get  Gemini Key:** https://aistudio.google.com/app/apikey 

### Embedding Model

**Default:** `all-MiniLM-L6-v2` (384 dims, fast)

**Alternative:** `all-mpnet-base-v2` (768 dims, slower but better)

**Change in:** `src/embeddings/generate_embeddings.py`

### Retrieval Parameters

**Top-K retrieval:** 20 candidates  
**Top-K rerank:** 5 final results  

**Change in:** `src/retrieval/advanced_retriever.py`

### Vector Store
ChromaDB settings in src/retrieval/vector_store.py:
```python
VectorStore(
    collection_name="gitlab_onboarding",
    persist_directory="models/vector_store"
)
```

## ğŸ“š Dataset Information

**Source:** GitLab public documentation ecosystem

**Current Stats:**
- **Total chunks:** 19 documents
- **Sources:** Handbook + Meeting transcripts
- **Content types:** Legal, Sustainability, CI/CD, Operations, Privacy
- **Format:** JSON with metadata

**Data Location:**
- `data/debiased_data/` - Handbook content
- `data/meeting_transcripts/` - Meeting transcriptions

**To add more data:** Add JSON files to data folders, then regenerate embeddings

## ğŸ¤– RAG with Free AI Generation

### Using Google Gemini (Recommended - FREE!)

**Why Gemini:**
- âœ… Completely FREE (no credit card needed)
- âœ… High quality responses (Gemini 2.0)
- âœ… Fast performance
- âœ… Generous rate limits (15 RPM)
- âœ… No usage charges

**Setup:**

1. **Get free API key:** https://aistudio.google.com/app/apikey

2. **Create `.env` file** in project root:
```
GOOGLE_API_KEY=your-key-here
```

3. **Run RAG pipeline:**
```bash
python -m src.generation.rag_pipeline
```

4. **Or use Streamlit UI:**
```bash
python -m streamlit run app.py
```


## ğŸ“Š MLflow Experiment Tracking

### View Experiments Dashboard

**Start MLflow UI:**
```bash
python -m mlflow ui --backend-store-uri file:./experiments/mlruns
```
**Access at:** http://localhost:5000

### What You'll See
- ğŸ“Š All experiments with metrics
- ğŸ“ˆ Performance charts and comparisons
- ğŸ·ï¸ Model parameters and configurations
- ğŸ“ Evaluation artifacts and reports
- â±ï¸ Run history and timestamps

### Tracked Metrics
- Retrieval metrics (Precision, Recall, F1, NDCG, MRR)
- Sensitivity analysis results
- Bias detection outcomes
- Model parameters (embedding model, K values, dimensions)
- Experiment metadata (timestamps, tags, descriptions)

## ğŸ³ Docker Deployment

### Build Container
```bash
docker build -t onboarding-assistant:latest .
```

### Run Tests in Container
```bash
docker run --rm -e GOOGLE_API_KEY=your-key onboarding-assistant:latest
```

### Using Docker Compose
```bash
docker-compose up --build
```
**Container includes:**
- âœ… All dependencies pre-installed
- âœ… Complete pipeline ready to run
- âœ… Environment variable support
- âœ… Health checks configured
- âœ… Volume mounts for data persistence

## ğŸ” Troubleshooting

### Common Issues

**Issue:** `ModuleNotFoundError: No module named 'sentence_transformers'`  
**Solution:** 
```bash
pip install -r requirements.txt
```

**Issue:** `No embeddings found`  
**Solution:** Generate embeddings first
```bash
python -m src.embeddings.generate_embeddings
```

**Issue:** `ChromaDB collection not found`  
**Solution:** Build vector store
```bash
python -m src.retrieval.vector_store
```

**Issue:** `MLflow UI won't start`  
**Solution:** Check path
```bash
python -m mlflow ui --backend-store-uri file:./experiments/mlruns
```

**Issue:** `RAGAS evaluation fails`  
**Solution:** RAGAS works with context metrics (LLM optional)

**Issue:** `Gemini API quota exceeded`  
**Solution:** Wait 1 minute (rate limit: 15 requests/min)

**Issue:** `Docker build slow`  
**Solution:** Use cached build without `--no-cache` flag

---

## ğŸ¯ Key Commands Reference

| Task | Command |
|------|---------|
| **Full pipeline test** | `python test_pipeline.py` |
| **Generate embeddings** | `python -m src.embeddings.generate_embeddings` |
| **Build vector store** | `python -m src.retrieval.vector_store` |
| **Test retrieval** | `python -m src.retrieval.advanced_retriever` |
| **Run RAG (Gemini)** | `python -m src.generation.rag_pipeline` |
| **Evaluate metrics** | `python -m src.evaluation.metrics` |
| **Check bias** | `python -m src.evaluation.bias_detection` |
| **Sensitivity analysis** | `python -m src.evaluation.sensitivity_analysis` |
| **RAGAS evaluation** | `python -m src.evaluation.ragas_evaluator` |
| **Track experiments** | `python -m src.experiments.mlflow_tracking` |
| **View MLflow UI** | `python -m mlflow ui --backend-store-uri file:./experiments/mlruns` |
| **Web UI** | `python -m streamlit run app.py` |
| **Docker build** | `docker build -t onboarding-assistant:latest .` |
| **Docker run** | `docker run --rm onboarding-assistant:latest` |

---

## ğŸ‘¥ Team

Team 13:
- Akshaj Nevgi
- Lakshmi Vandhanie Ganesh
- Mithun Dineshkumar
- Saran Jagadeesan Uma
- Zankhana Pratik Mehta

## ğŸ“„ License

This project is part of an academic assignment for MLOps coursework.

## ğŸ”— Links

- GitHub Repository: https://github.com/LakshmiVadhanie/Intelligent-Onboarding-Assistant
- MLflow Dashboard: http://localhost:5000 (when running)

Last Updated: November 17, 2025